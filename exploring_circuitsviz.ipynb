{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f45cbc4-1119-46a8-b6a7-4f921d3caefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoreload extension loaded. Code changes will be automatically reloaded.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Autoreload extension loaded. Code changes will be automatically reloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b02282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens\n",
    "\n",
    "\n",
    "# TODO(bschoen): Just start using `transformer_lens.utils.get_device` from now on\n",
    "def get_best_available_torch_device() -> torch.device:\n",
    "    return transformer_lens.utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd81f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648ceb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import python_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d49543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb\n",
    "import os\n",
    "import dataclasses\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba4bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e11a8d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x30812f170>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disable autograd, as we're focused on inference and this save us a lot of speed, memory, and annoying boilerplate\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b18f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: jbloom advice is to make a run_comparer here: https://docs.wandb.ai/guides/app/features/panels/run-comparer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e78a8",
   "metadata": {},
   "source": [
    "## Loading A Pretrained Sparse Autoencoder\n",
    "\n",
    "In practice, SAEs can be of varying usefulness for general use cases. To start with, we recommend the following:\n",
    "\n",
    "* Joseph's Open Source GPT2 Small Residual (gpt2-small-res-jb)\n",
    "* Joseph's Feature Splitting (gpt2-small-res-jb-feature-splitting)\n",
    "* Gemma SAEs (gemma-2b-res-jb) (0,6) <- on Neuronpedia and good. (12 / 17 aren't very good currently).\n",
    "\n",
    "Other SAEs have various issues--e.g., too dense or not dense enough, or designed for special use cases, or initial drafts of what we hope will be better versions later. Decode Research / Neuronpedia are working on making all SAEs on Neuronpedia loadable in SAE Lens and vice versa, as well as providing public benchmarking stats to help people choose which SAEs to work with.\n",
    "\n",
    "To see all the SAEs contained in a specific release (named after the part of the model they apply to), simply run the below. Each hook point corresponds to a layer or module of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69a8e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sae_lens.toolkit\n",
    "import sae_lens.toolkit.pretrained_saes_directory\n",
    "import sae_lens.toolkit.pretrained_sae_loaders\n",
    "\n",
    "from sae_lens.toolkit.pretrained_saes_directory import PretrainedSAELookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707dbd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained SAE loaders:\n",
      " - sae_lens\n",
      " - connor_rob_hook_z\n",
      " - gemma_2\n"
     ]
    }
   ],
   "source": [
    "print(\"Pretrained SAE loaders:\")\n",
    "for name in sae_lens.toolkit.pretrained_sae_loaders.NAMED_PRETRAINED_SAE_LOADERS.keys():\n",
    "    print(f\" - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8330fa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 pretrained SAEs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>release</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JoshEngels/Mistral-7B-Residual-Stream-SAEs</td>\n",
       "      <td>mistral-7b-res-wg</td>\n",
       "      <td>mistral-7b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ckkissane/attn-saes-gpt2-small-all-layers</td>\n",
       "      <td>gpt2-small-hook-z-kk</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ctigges/pythia-70m-deduped__att-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-att-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ctigges/pythia-70m-deduped__mlp-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-mlp-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ctigges/pythia-70m-deduped__res-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-res-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-scope-27b-pt-res-canonical</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>google/gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>google/gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-scope-9b-it-res-canonical</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-scope-9b-pt-att-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-scope-9b-pt-mlp-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>google/gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>google/gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-scope-9b-pt-res-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jbloom/GPT2-Small-Feature-Splitting-Experiment...</td>\n",
       "      <td>gpt2-small-res-jb-feature-splitting</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small-attn-out-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small-mlp-out-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small-resid-mid-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small-resid-post-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small-attn-out-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small-mlp-out-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small-resid-mid-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small-resid-post-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jbloom/GPT2-Small-SAEs-Reformatted</td>\n",
       "      <td>gpt2-small-res-jb</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jbloom/Gemma-2b-IT-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-it-res-jb</td>\n",
       "      <td>gemma-2b-it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>gemma-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sce-ajt</td>\n",
       "      <td>gpt2-small-res_sce-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>neuronpedia/gpt2-small__res_scefr-ajt</td>\n",
       "      <td>gpt2-small-res_scefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>neuronpedia/gpt2-small__res_scl-ajt</td>\n",
       "      <td>gpt2-small-res_scl-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sle-ajt</td>\n",
       "      <td>gpt2-small-res_sle-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>neuronpedia/gpt2-small__res_slefr-ajt</td>\n",
       "      <td>gpt2-small-res_slefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sll-ajt</td>\n",
       "      <td>gpt2-small-res_sll-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tommmcgrath/gpt2-small-mlp-out-saes</td>\n",
       "      <td>gpt2-small-mlp-tm</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              repo_id  \\\n",
       "8          JoshEngels/Mistral-7B-Residual-Stream-SAEs   \n",
       "1           ckkissane/attn-saes-gpt2-small-all-layers   \n",
       "33       ctigges/pythia-70m-deduped__att-sm_processed   \n",
       "32       ctigges/pythia-70m-deduped__mlp-sm_processed   \n",
       "31       ctigges/pythia-70m-deduped__res-sm_processed   \n",
       "29                      google/gemma-scope-27b-pt-res   \n",
       "30                      google/gemma-scope-27b-pt-res   \n",
       "19                       google/gemma-scope-2b-pt-att   \n",
       "20                       google/gemma-scope-2b-pt-att   \n",
       "17                       google/gemma-scope-2b-pt-mlp   \n",
       "18                       google/gemma-scope-2b-pt-mlp   \n",
       "15                       google/gemma-scope-2b-pt-res   \n",
       "16                       google/gemma-scope-2b-pt-res   \n",
       "27                       google/gemma-scope-9b-it-res   \n",
       "28                       google/gemma-scope-9b-it-res   \n",
       "23                       google/gemma-scope-9b-pt-att   \n",
       "24                       google/gemma-scope-9b-pt-att   \n",
       "25                       google/gemma-scope-9b-pt-mlp   \n",
       "26                       google/gemma-scope-9b-pt-mlp   \n",
       "21                       google/gemma-scope-9b-pt-res   \n",
       "22                       google/gemma-scope-9b-pt-res   \n",
       "3   jbloom/GPT2-Small-Feature-Splitting-Experiment...   \n",
       "14        jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs   \n",
       "12         jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs   \n",
       "10       jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs   \n",
       "5       jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs   \n",
       "13         jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs   \n",
       "11          jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs   \n",
       "9         jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs   \n",
       "4        jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs   \n",
       "0                  jbloom/GPT2-Small-SAEs-Reformatted   \n",
       "7             jbloom/Gemma-2b-IT-Residual-Stream-SAEs   \n",
       "6                jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "38                neuronpedia/gpt2-small__res_sce-ajt   \n",
       "39              neuronpedia/gpt2-small__res_scefr-ajt   \n",
       "36                neuronpedia/gpt2-small__res_scl-ajt   \n",
       "37                neuronpedia/gpt2-small__res_sle-ajt   \n",
       "35              neuronpedia/gpt2-small__res_slefr-ajt   \n",
       "34                neuronpedia/gpt2-small__res_sll-ajt   \n",
       "2                 tommmcgrath/gpt2-small-mlp-out-saes   \n",
       "\n",
       "                                release               model  \n",
       "8                     mistral-7b-res-wg          mistral-7b  \n",
       "1                  gpt2-small-hook-z-kk          gpt2-small  \n",
       "33            pythia-70m-deduped-att-sm  pythia-70m-deduped  \n",
       "32            pythia-70m-deduped-mlp-sm  pythia-70m-deduped  \n",
       "31            pythia-70m-deduped-res-sm  pythia-70m-deduped  \n",
       "29               gemma-scope-27b-pt-res         gemma-2-27b  \n",
       "30     gemma-scope-27b-pt-res-canonical         gemma-2-27b  \n",
       "19                gemma-scope-2b-pt-att          gemma-2-2b  \n",
       "20      gemma-scope-2b-pt-att-canonical          gemma-2-2b  \n",
       "17                gemma-scope-2b-pt-mlp          gemma-2-2b  \n",
       "18      gemma-scope-2b-pt-mlp-canonical          gemma-2-2b  \n",
       "15                gemma-scope-2b-pt-res          gemma-2-2b  \n",
       "16      gemma-scope-2b-pt-res-canonical          gemma-2-2b  \n",
       "27                gemma-scope-9b-it-res          gemma-2-9b  \n",
       "28      gemma-scope-9b-it-res-canonical       gemma-2-9b-it  \n",
       "23                gemma-scope-9b-pt-att          gemma-2-9b  \n",
       "24      gemma-scope-9b-pt-att-canonical          gemma-2-9b  \n",
       "25                gemma-scope-9b-pt-mlp          gemma-2-9b  \n",
       "26      gemma-scope-9b-pt-mlp-canonical          gemma-2-9b  \n",
       "21                gemma-scope-9b-pt-res          gemma-2-9b  \n",
       "22      gemma-scope-9b-pt-res-canonical          gemma-2-9b  \n",
       "3   gpt2-small-res-jb-feature-splitting          gpt2-small  \n",
       "14          gpt2-small-attn-out-v5-128k          gpt2-small  \n",
       "12           gpt2-small-mlp-out-v5-128k          gpt2-small  \n",
       "10         gpt2-small-resid-mid-v5-128k          gpt2-small  \n",
       "5         gpt2-small-resid-post-v5-128k          gpt2-small  \n",
       "13           gpt2-small-attn-out-v5-32k          gpt2-small  \n",
       "11            gpt2-small-mlp-out-v5-32k          gpt2-small  \n",
       "9           gpt2-small-resid-mid-v5-32k          gpt2-small  \n",
       "4          gpt2-small-resid-post-v5-32k          gpt2-small  \n",
       "0                     gpt2-small-res-jb          gpt2-small  \n",
       "7                    gemma-2b-it-res-jb         gemma-2b-it  \n",
       "6                       gemma-2b-res-jb            gemma-2b  \n",
       "38               gpt2-small-res_sce-ajt          gpt2-small  \n",
       "39             gpt2-small-res_scefr-ajt          gpt2-small  \n",
       "36               gpt2-small-res_scl-ajt          gpt2-small  \n",
       "37               gpt2-small-res_sle-ajt          gpt2-small  \n",
       "35             gpt2-small-res_slefr-ajt          gpt2-small  \n",
       "34               gpt2-small-res_sll-ajt          gpt2-small  \n",
       "2                     gpt2-small-mlp-tm          gpt2-small  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads from `pretrained_saes.yaml`\n",
    "pretrained_saes_dir: dict[str, PretrainedSAELookup] = (\n",
    "    sae_lens.toolkit.pretrained_saes_directory.get_pretrained_saes_directory()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(pretrained_saes_dir)} pretrained SAEs\")\n",
    "df = pd.DataFrame([dataclasses.asdict(x) for x in pretrained_saes_dir.values()])\n",
    "\n",
    "\n",
    "df = df[[\"repo_id\", \"release\", \"model\"]]\n",
    "\n",
    "df.sort_values(by=df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fd4ffa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            release                       repo_id\n",
       "19            gemma-scope-2b-pt-att  google/gemma-scope-2b-pt-att\n",
       "20  gemma-scope-2b-pt-att-canonical  google/gemma-scope-2b-pt-att\n",
       "17            gemma-scope-2b-pt-mlp  google/gemma-scope-2b-pt-mlp\n",
       "18  gemma-scope-2b-pt-mlp-canonical  google/gemma-scope-2b-pt-mlp\n",
       "15            gemma-scope-2b-pt-res  google/gemma-scope-2b-pt-res\n",
       "16  gemma-scope-2b-pt-res-canonical  google/gemma-scope-2b-pt-res"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at which ones are there for gemma-2b\n",
    "df[df[\"model\"] == \"gemma-2-2b\"][[\"release\", \"repo_id\"]].sort_values(\n",
    "    by=[\"release\", \"repo_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f194c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this one, since it's what's used in the GemmaScope tutorial\n",
    "pretrained_sae_name = (\n",
    "    \"gemma-scope-2b-pt-res\"  # repo_id = `google/gemma-scope-2b-pt-res`\n",
    ")\n",
    "\n",
    "# pretrained_sae_name = \"gemma-scope-2b-pt-res-canonical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6206b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"release\": \"gemma-scope-2b-pt-res\",\n",
      "  \"repo_id\": \"google/gemma-scope-2b-pt-res\",\n",
      "  \"model\": \"gemma-2-2b\",\n",
      "  \"conversion_func\": \"gemma_2\",\n",
      "  \"saes_map\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": \"layer_0/width_16k/average_l0_105\",\n",
      "    \"layer_0/width_16k/average_l0_13\": \"layer_0/width_16k/average_l0_13\",\n",
      "    \"layer_0/width_16k/average_l0_226\": \"layer_0/width_16k/average_l0_226\",\n",
      "    \"layer_0/width_16k/average_l0_25\": \"layer_0/width_16k/average_l0_25\",\n",
      "    \"layer_0/width_16k/average_l0_46\": \"layer_0/width_16k/average_l0_46\",\n",
      "    \"layer_1/width_16k/average_l0_10\": \"layer_1/width_16k/average_l0_10\",\n",
      "    \"layer_1/width_16k/average_l0_102\": \"layer_1/width_16k/average_l0_102\",\n",
      "    \"layer_1/width_16k/average_l0_20\": \"layer_1/width_16k/average_l0_20\",\n",
      "    \"layer_1/width_16k/average_l0_250\": \"layer_1/width_16k/average_l0_250\",\n",
      "    \"layer_1/width_16k/average_l0_40\": \"layer_1/width_16k/average_l0_40\",\n",
      "    \"layer_2/width_16k/average_l0_13\": \"layer_2/width_16k/average_l0_13\",\n",
      "    \"layer_2/width_16k/average_l0_141\": \"layer_2/width_16k/average_l0_141\",\n",
      "    \"layer_2/width_16k/average_l0_142\": \"layer_2/width_16k/average_l0_142\",\n",
      "    \"layer_2/width_16k/average_l0_24\": \"layer_2/width_16k/average_l0_24\",\n",
      "    \"layer_2/width_16k/average_l0_304\": \"layer_2/width_16k/average_l0_304\",\n",
      "    \"layer_2/width_16k/average_l0_53\": \"layer_2/width_16k/average_l0_53\",\n",
      "    \"layer_3/width_16k/average_l0_14\": \"layer_3/width_16k/average_l0_14\",\n",
      "    \"layer_3/width_16k/average_l0_142\": \"layer_3/width_16k/average_l0_142\",\n",
      "    \"layer_3/width_16k/average_l0_28\": \"layer_3/width_16k/average_l0_28\",\n",
      "    \"layer_3/width_16k/average_l0_315\": \"layer_3/width_16k/average_l0_315\",\n",
      "    \"layer_3/width_16k/average_l0_59\": \"layer_3/width_16k/average_l0_59\",\n",
      "    \"layer_4/width_16k/average_l0_124\": \"layer_4/width_16k/average_l0_124\",\n",
      "    \"layer_4/width_16k/average_l0_125\": \"layer_4/width_16k/average_l0_125\",\n",
      "    \"layer_4/width_16k/average_l0_17\": \"layer_4/width_16k/average_l0_17\",\n",
      "    \"layer_4/width_16k/average_l0_281\": \"layer_4/width_16k/average_l0_281\",\n",
      "    \"layer_4/width_16k/average_l0_31\": \"layer_4/width_16k/average_l0_31\",\n",
      "    \"layer_4/width_16k/average_l0_60\": \"layer_4/width_16k/average_l0_60\",\n",
      "    \"layer_5/width_16k/average_l0_143\": \"layer_5/width_16k/average_l0_143\",\n",
      "    \"layer_5/width_16k/average_l0_18\": \"layer_5/width_16k/average_l0_18\",\n",
      "    \"layer_5/width_16k/average_l0_309\": \"layer_5/width_16k/average_l0_309\",\n",
      "    \"layer_5/width_16k/average_l0_34\": \"layer_5/width_16k/average_l0_34\",\n",
      "    \"layer_5/width_16k/average_l0_68\": \"layer_5/width_16k/average_l0_68\",\n",
      "    \"layer_6/width_16k/average_l0_144\": \"layer_6/width_16k/average_l0_144\",\n",
      "    \"layer_6/width_16k/average_l0_19\": \"layer_6/width_16k/average_l0_19\",\n",
      "    \"layer_6/width_16k/average_l0_301\": \"layer_6/width_16k/average_l0_301\",\n",
      "    \"layer_6/width_16k/average_l0_36\": \"layer_6/width_16k/average_l0_36\",\n",
      "    \"layer_6/width_16k/average_l0_70\": \"layer_6/width_16k/average_l0_70\",\n",
      "    \"layer_7/width_16k/average_l0_137\": \"layer_7/width_16k/average_l0_137\",\n",
      "    \"layer_7/width_16k/average_l0_20\": \"layer_7/width_16k/average_l0_20\",\n",
      "    \"layer_7/width_16k/average_l0_285\": \"layer_7/width_16k/average_l0_285\",\n",
      "    \"layer_7/width_16k/average_l0_36\": \"layer_7/width_16k/average_l0_36\",\n",
      "    \"layer_7/width_16k/average_l0_69\": \"layer_7/width_16k/average_l0_69\",\n",
      "    \"layer_8/width_16k/average_l0_142\": \"layer_8/width_16k/average_l0_142\",\n",
      "    \"layer_8/width_16k/average_l0_20\": \"layer_8/width_16k/average_l0_20\",\n",
      "    \"layer_8/width_16k/average_l0_301\": \"layer_8/width_16k/average_l0_301\",\n",
      "    \"layer_8/width_16k/average_l0_37\": \"layer_8/width_16k/average_l0_37\",\n",
      "    \"layer_8/width_16k/average_l0_71\": \"layer_8/width_16k/average_l0_71\",\n",
      "    \"layer_9/width_16k/average_l0_151\": \"layer_9/width_16k/average_l0_151\",\n",
      "    \"layer_9/width_16k/average_l0_21\": \"layer_9/width_16k/average_l0_21\",\n",
      "    \"layer_9/width_16k/average_l0_340\": \"layer_9/width_16k/average_l0_340\",\n",
      "    \"layer_9/width_16k/average_l0_37\": \"layer_9/width_16k/average_l0_37\",\n",
      "    \"layer_9/width_16k/average_l0_73\": \"layer_9/width_16k/average_l0_73\",\n",
      "    \"layer_10/width_16k/average_l0_166\": \"layer_10/width_16k/average_l0_166\",\n",
      "    \"layer_10/width_16k/average_l0_21\": \"layer_10/width_16k/average_l0_21\",\n",
      "    \"layer_10/width_16k/average_l0_39\": \"layer_10/width_16k/average_l0_39\",\n",
      "    \"layer_10/width_16k/average_l0_395\": \"layer_10/width_16k/average_l0_395\",\n",
      "    \"layer_10/width_16k/average_l0_77\": \"layer_10/width_16k/average_l0_77\",\n",
      "    \"layer_11/width_16k/average_l0_168\": \"layer_11/width_16k/average_l0_168\",\n",
      "    \"layer_11/width_16k/average_l0_22\": \"layer_11/width_16k/average_l0_22\",\n",
      "    \"layer_11/width_16k/average_l0_393\": \"layer_11/width_16k/average_l0_393\",\n",
      "    \"layer_11/width_16k/average_l0_41\": \"layer_11/width_16k/average_l0_41\",\n",
      "    \"layer_11/width_16k/average_l0_79\": \"layer_11/width_16k/average_l0_79\",\n",
      "    \"layer_11/width_16k/average_l0_80\": \"layer_11/width_16k/average_l0_80\",\n",
      "    \"layer_12/width_16k/average_l0_176\": \"layer_12/width_16k/average_l0_176\",\n",
      "    \"layer_12/width_16k/average_l0_22\": \"layer_12/width_16k/average_l0_22\",\n",
      "    \"layer_12/width_16k/average_l0_41\": \"layer_12/width_16k/average_l0_41\",\n",
      "    \"layer_12/width_16k/average_l0_445\": \"layer_12/width_16k/average_l0_445\",\n",
      "    \"layer_12/width_16k/average_l0_82\": \"layer_12/width_16k/average_l0_82\",\n",
      "    \"layer_13/width_16k/average_l0_173\": \"layer_13/width_16k/average_l0_173\",\n",
      "    \"layer_13/width_16k/average_l0_23\": \"layer_13/width_16k/average_l0_23\",\n",
      "    \"layer_13/width_16k/average_l0_403\": \"layer_13/width_16k/average_l0_403\",\n",
      "    \"layer_13/width_16k/average_l0_43\": \"layer_13/width_16k/average_l0_43\",\n",
      "    \"layer_13/width_16k/average_l0_83\": \"layer_13/width_16k/average_l0_83\",\n",
      "    \"layer_13/width_16k/average_l0_84\": \"layer_13/width_16k/average_l0_84\",\n",
      "    \"layer_14/width_16k/average_l0_173\": \"layer_14/width_16k/average_l0_173\",\n",
      "    \"layer_14/width_16k/average_l0_23\": \"layer_14/width_16k/average_l0_23\",\n",
      "    \"layer_14/width_16k/average_l0_388\": \"layer_14/width_16k/average_l0_388\",\n",
      "    \"layer_14/width_16k/average_l0_43\": \"layer_14/width_16k/average_l0_43\",\n",
      "    \"layer_14/width_16k/average_l0_83\": \"layer_14/width_16k/average_l0_83\",\n",
      "    \"layer_14/width_16k/average_l0_84\": \"layer_14/width_16k/average_l0_84\",\n",
      "    \"layer_15/width_16k/average_l0_150\": \"layer_15/width_16k/average_l0_150\",\n",
      "    \"layer_15/width_16k/average_l0_23\": \"layer_15/width_16k/average_l0_23\",\n",
      "    \"layer_15/width_16k/average_l0_308\": \"layer_15/width_16k/average_l0_308\",\n",
      "    \"layer_15/width_16k/average_l0_41\": \"layer_15/width_16k/average_l0_41\",\n",
      "    \"layer_15/width_16k/average_l0_78\": \"layer_15/width_16k/average_l0_78\",\n",
      "    \"layer_16/width_16k/average_l0_154\": \"layer_16/width_16k/average_l0_154\",\n",
      "    \"layer_16/width_16k/average_l0_23\": \"layer_16/width_16k/average_l0_23\",\n",
      "    \"layer_16/width_16k/average_l0_335\": \"layer_16/width_16k/average_l0_335\",\n",
      "    \"layer_16/width_16k/average_l0_42\": \"layer_16/width_16k/average_l0_42\",\n",
      "    \"layer_16/width_16k/average_l0_78\": \"layer_16/width_16k/average_l0_78\",\n",
      "    \"layer_17/width_16k/average_l0_150\": \"layer_17/width_16k/average_l0_150\",\n",
      "    \"layer_17/width_16k/average_l0_23\": \"layer_17/width_16k/average_l0_23\",\n",
      "    \"layer_17/width_16k/average_l0_304\": \"layer_17/width_16k/average_l0_304\",\n",
      "    \"layer_17/width_16k/average_l0_42\": \"layer_17/width_16k/average_l0_42\",\n",
      "    \"layer_17/width_16k/average_l0_77\": \"layer_17/width_16k/average_l0_77\",\n",
      "    \"layer_18/width_16k/average_l0_138\": \"layer_18/width_16k/average_l0_138\",\n",
      "    \"layer_18/width_16k/average_l0_23\": \"layer_18/width_16k/average_l0_23\",\n",
      "    \"layer_18/width_16k/average_l0_280\": \"layer_18/width_16k/average_l0_280\",\n",
      "    \"layer_18/width_16k/average_l0_40\": \"layer_18/width_16k/average_l0_40\",\n",
      "    \"layer_18/width_16k/average_l0_74\": \"layer_18/width_16k/average_l0_74\",\n",
      "    \"layer_19/width_16k/average_l0_137\": \"layer_19/width_16k/average_l0_137\",\n",
      "    \"layer_19/width_16k/average_l0_23\": \"layer_19/width_16k/average_l0_23\",\n",
      "    \"layer_19/width_16k/average_l0_279\": \"layer_19/width_16k/average_l0_279\",\n",
      "    \"layer_19/width_16k/average_l0_40\": \"layer_19/width_16k/average_l0_40\",\n",
      "    \"layer_19/width_16k/average_l0_73\": \"layer_19/width_16k/average_l0_73\",\n",
      "    \"layer_20/width_16k/average_l0_139\": \"layer_20/width_16k/average_l0_139\",\n",
      "    \"layer_20/width_16k/average_l0_22\": \"layer_20/width_16k/average_l0_22\",\n",
      "    \"layer_20/width_16k/average_l0_294\": \"layer_20/width_16k/average_l0_294\",\n",
      "    \"layer_20/width_16k/average_l0_38\": \"layer_20/width_16k/average_l0_38\",\n",
      "    \"layer_20/width_16k/average_l0_71\": \"layer_20/width_16k/average_l0_71\",\n",
      "    \"layer_21/width_16k/average_l0_139\": \"layer_21/width_16k/average_l0_139\",\n",
      "    \"layer_21/width_16k/average_l0_22\": \"layer_21/width_16k/average_l0_22\",\n",
      "    \"layer_21/width_16k/average_l0_301\": \"layer_21/width_16k/average_l0_301\",\n",
      "    \"layer_21/width_16k/average_l0_38\": \"layer_21/width_16k/average_l0_38\",\n",
      "    \"layer_21/width_16k/average_l0_70\": \"layer_21/width_16k/average_l0_70\",\n",
      "    \"layer_22/width_16k/average_l0_147\": \"layer_22/width_16k/average_l0_147\",\n",
      "    \"layer_22/width_16k/average_l0_21\": \"layer_22/width_16k/average_l0_21\",\n",
      "    \"layer_22/width_16k/average_l0_349\": \"layer_22/width_16k/average_l0_349\",\n",
      "    \"layer_22/width_16k/average_l0_38\": \"layer_22/width_16k/average_l0_38\",\n",
      "    \"layer_22/width_16k/average_l0_72\": \"layer_22/width_16k/average_l0_72\",\n",
      "    \"layer_23/width_16k/average_l0_157\": \"layer_23/width_16k/average_l0_157\",\n",
      "    \"layer_23/width_16k/average_l0_21\": \"layer_23/width_16k/average_l0_21\",\n",
      "    \"layer_23/width_16k/average_l0_38\": \"layer_23/width_16k/average_l0_38\",\n",
      "    \"layer_23/width_16k/average_l0_404\": \"layer_23/width_16k/average_l0_404\",\n",
      "    \"layer_23/width_16k/average_l0_74\": \"layer_23/width_16k/average_l0_74\",\n",
      "    \"layer_23/width_16k/average_l0_75\": \"layer_23/width_16k/average_l0_75\",\n",
      "    \"layer_24/width_16k/average_l0_158\": \"layer_24/width_16k/average_l0_158\",\n",
      "    \"layer_24/width_16k/average_l0_20\": \"layer_24/width_16k/average_l0_20\",\n",
      "    \"layer_24/width_16k/average_l0_38\": \"layer_24/width_16k/average_l0_38\",\n",
      "    \"layer_24/width_16k/average_l0_457\": \"layer_24/width_16k/average_l0_457\",\n",
      "    \"layer_24/width_16k/average_l0_73\": \"layer_24/width_16k/average_l0_73\",\n",
      "    \"layer_25/width_16k/average_l0_116\": \"layer_25/width_16k/average_l0_116\",\n",
      "    \"layer_25/width_16k/average_l0_16\": \"layer_25/width_16k/average_l0_16\",\n",
      "    \"layer_25/width_16k/average_l0_28\": \"layer_25/width_16k/average_l0_28\",\n",
      "    \"layer_25/width_16k/average_l0_285\": \"layer_25/width_16k/average_l0_285\",\n",
      "    \"layer_25/width_16k/average_l0_55\": \"layer_25/width_16k/average_l0_55\",\n",
      "    \"layer_5/width_1m/average_l0_114\": \"layer_5/width_1m/average_l0_114\",\n",
      "    \"layer_5/width_1m/average_l0_13\": \"layer_5/width_1m/average_l0_13\",\n",
      "    \"layer_5/width_1m/average_l0_21\": \"layer_5/width_1m/average_l0_21\",\n",
      "    \"layer_5/width_1m/average_l0_36\": \"layer_5/width_1m/average_l0_36\",\n",
      "    \"layer_5/width_1m/average_l0_63\": \"layer_5/width_1m/average_l0_63\",\n",
      "    \"layer_5/width_1m/average_l0_9\": \"layer_5/width_1m/average_l0_9\",\n",
      "    \"layer_12/width_1m/average_l0_107\": \"layer_12/width_1m/average_l0_107\",\n",
      "    \"layer_12/width_1m/average_l0_19\": \"layer_12/width_1m/average_l0_19\",\n",
      "    \"layer_12/width_1m/average_l0_207\": \"layer_12/width_1m/average_l0_207\",\n",
      "    \"layer_12/width_1m/average_l0_26\": \"layer_12/width_1m/average_l0_26\",\n",
      "    \"layer_12/width_1m/average_l0_58\": \"layer_12/width_1m/average_l0_58\",\n",
      "    \"layer_12/width_1m/average_l0_73\": \"layer_12/width_1m/average_l0_73\",\n",
      "    \"layer_19/width_1m/average_l0_157\": \"layer_19/width_1m/average_l0_157\",\n",
      "    \"layer_19/width_1m/average_l0_16\": \"layer_19/width_1m/average_l0_16\",\n",
      "    \"layer_19/width_1m/average_l0_18\": \"layer_19/width_1m/average_l0_18\",\n",
      "    \"layer_19/width_1m/average_l0_29\": \"layer_19/width_1m/average_l0_29\",\n",
      "    \"layer_19/width_1m/average_l0_50\": \"layer_19/width_1m/average_l0_50\",\n",
      "    \"layer_19/width_1m/average_l0_88\": \"layer_19/width_1m/average_l0_88\",\n",
      "    \"layer_12/width_262k/average_l0_11\": \"layer_12/width_262k/average_l0_11\",\n",
      "    \"layer_12/width_262k/average_l0_121\": \"layer_12/width_262k/average_l0_121\",\n",
      "    \"layer_12/width_262k/average_l0_21\": \"layer_12/width_262k/average_l0_21\",\n",
      "    \"layer_12/width_262k/average_l0_243\": \"layer_12/width_262k/average_l0_243\",\n",
      "    \"layer_12/width_262k/average_l0_36\": \"layer_12/width_262k/average_l0_36\",\n",
      "    \"layer_12/width_262k/average_l0_67\": \"layer_12/width_262k/average_l0_67\",\n",
      "    \"layer_12/width_32k/average_l0_12\": \"layer_12/width_32k/average_l0_12\",\n",
      "    \"layer_12/width_32k/average_l0_155\": \"layer_12/width_32k/average_l0_155\",\n",
      "    \"layer_12/width_32k/average_l0_22\": \"layer_12/width_32k/average_l0_22\",\n",
      "    \"layer_12/width_32k/average_l0_360\": \"layer_12/width_32k/average_l0_360\",\n",
      "    \"layer_12/width_32k/average_l0_40\": \"layer_12/width_32k/average_l0_40\",\n",
      "    \"layer_12/width_32k/average_l0_76\": \"layer_12/width_32k/average_l0_76\",\n",
      "    \"layer_12/width_524k/average_l0_115\": \"layer_12/width_524k/average_l0_115\",\n",
      "    \"layer_12/width_524k/average_l0_22\": \"layer_12/width_524k/average_l0_22\",\n",
      "    \"layer_12/width_524k/average_l0_227\": \"layer_12/width_524k/average_l0_227\",\n",
      "    \"layer_12/width_524k/average_l0_29\": \"layer_12/width_524k/average_l0_29\",\n",
      "    \"layer_12/width_524k/average_l0_46\": \"layer_12/width_524k/average_l0_46\",\n",
      "    \"layer_12/width_524k/average_l0_65\": \"layer_12/width_524k/average_l0_65\",\n",
      "    \"layer_0/width_65k/average_l0_11\": \"layer_0/width_65k/average_l0_11\",\n",
      "    \"layer_0/width_65k/average_l0_17\": \"layer_0/width_65k/average_l0_17\",\n",
      "    \"layer_0/width_65k/average_l0_27\": \"layer_0/width_65k/average_l0_27\",\n",
      "    \"layer_0/width_65k/average_l0_43\": \"layer_0/width_65k/average_l0_43\",\n",
      "    \"layer_0/width_65k/average_l0_73\": \"layer_0/width_65k/average_l0_73\",\n",
      "    \"layer_1/width_65k/average_l0_121\": \"layer_1/width_65k/average_l0_121\",\n",
      "    \"layer_1/width_65k/average_l0_16\": \"layer_1/width_65k/average_l0_16\",\n",
      "    \"layer_1/width_65k/average_l0_30\": \"layer_1/width_65k/average_l0_30\",\n",
      "    \"layer_1/width_65k/average_l0_54\": \"layer_1/width_65k/average_l0_54\",\n",
      "    \"layer_1/width_65k/average_l0_9\": \"layer_1/width_65k/average_l0_9\",\n",
      "    \"layer_2/width_65k/average_l0_11\": \"layer_2/width_65k/average_l0_11\",\n",
      "    \"layer_2/width_65k/average_l0_169\": \"layer_2/width_65k/average_l0_169\",\n",
      "    \"layer_2/width_65k/average_l0_20\": \"layer_2/width_65k/average_l0_20\",\n",
      "    \"layer_2/width_65k/average_l0_37\": \"layer_2/width_65k/average_l0_37\",\n",
      "    \"layer_2/width_65k/average_l0_77\": \"layer_2/width_65k/average_l0_77\",\n",
      "    \"layer_3/width_65k/average_l0_13\": \"layer_3/width_65k/average_l0_13\",\n",
      "    \"layer_3/width_65k/average_l0_193\": \"layer_3/width_65k/average_l0_193\",\n",
      "    \"layer_3/width_65k/average_l0_23\": \"layer_3/width_65k/average_l0_23\",\n",
      "    \"layer_3/width_65k/average_l0_42\": \"layer_3/width_65k/average_l0_42\",\n",
      "    \"layer_3/width_65k/average_l0_89\": \"layer_3/width_65k/average_l0_89\",\n",
      "    \"layer_4/width_65k/average_l0_14\": \"layer_4/width_65k/average_l0_14\",\n",
      "    \"layer_4/width_65k/average_l0_177\": \"layer_4/width_65k/average_l0_177\",\n",
      "    \"layer_4/width_65k/average_l0_25\": \"layer_4/width_65k/average_l0_25\",\n",
      "    \"layer_4/width_65k/average_l0_46\": \"layer_4/width_65k/average_l0_46\",\n",
      "    \"layer_4/width_65k/average_l0_89\": \"layer_4/width_65k/average_l0_89\",\n",
      "    \"layer_5/width_65k/average_l0_105\": \"layer_5/width_65k/average_l0_105\",\n",
      "    \"layer_5/width_65k/average_l0_17\": \"layer_5/width_65k/average_l0_17\",\n",
      "    \"layer_5/width_65k/average_l0_211\": \"layer_5/width_65k/average_l0_211\",\n",
      "    \"layer_5/width_65k/average_l0_29\": \"layer_5/width_65k/average_l0_29\",\n",
      "    \"layer_5/width_65k/average_l0_53\": \"layer_5/width_65k/average_l0_53\",\n",
      "    \"layer_6/width_65k/average_l0_107\": \"layer_6/width_65k/average_l0_107\",\n",
      "    \"layer_6/width_65k/average_l0_17\": \"layer_6/width_65k/average_l0_17\",\n",
      "    \"layer_6/width_65k/average_l0_208\": \"layer_6/width_65k/average_l0_208\",\n",
      "    \"layer_6/width_65k/average_l0_30\": \"layer_6/width_65k/average_l0_30\",\n",
      "    \"layer_6/width_65k/average_l0_56\": \"layer_6/width_65k/average_l0_56\",\n",
      "    \"layer_7/width_65k/average_l0_107\": \"layer_7/width_65k/average_l0_107\",\n",
      "    \"layer_7/width_65k/average_l0_18\": \"layer_7/width_65k/average_l0_18\",\n",
      "    \"layer_7/width_65k/average_l0_203\": \"layer_7/width_65k/average_l0_203\",\n",
      "    \"layer_7/width_65k/average_l0_31\": \"layer_7/width_65k/average_l0_31\",\n",
      "    \"layer_7/width_65k/average_l0_57\": \"layer_7/width_65k/average_l0_57\",\n",
      "    \"layer_8/width_65k/average_l0_111\": \"layer_8/width_65k/average_l0_111\",\n",
      "    \"layer_8/width_65k/average_l0_19\": \"layer_8/width_65k/average_l0_19\",\n",
      "    \"layer_8/width_65k/average_l0_213\": \"layer_8/width_65k/average_l0_213\",\n",
      "    \"layer_8/width_65k/average_l0_33\": \"layer_8/width_65k/average_l0_33\",\n",
      "    \"layer_8/width_65k/average_l0_59\": \"layer_8/width_65k/average_l0_59\",\n",
      "    \"layer_9/width_65k/average_l0_118\": \"layer_9/width_65k/average_l0_118\",\n",
      "    \"layer_9/width_65k/average_l0_19\": \"layer_9/width_65k/average_l0_19\",\n",
      "    \"layer_9/width_65k/average_l0_240\": \"layer_9/width_65k/average_l0_240\",\n",
      "    \"layer_9/width_65k/average_l0_34\": \"layer_9/width_65k/average_l0_34\",\n",
      "    \"layer_9/width_65k/average_l0_61\": \"layer_9/width_65k/average_l0_61\",\n",
      "    \"layer_10/width_65k/average_l0_128\": \"layer_10/width_65k/average_l0_128\",\n",
      "    \"layer_10/width_65k/average_l0_20\": \"layer_10/width_65k/average_l0_20\",\n",
      "    \"layer_10/width_65k/average_l0_265\": \"layer_10/width_65k/average_l0_265\",\n",
      "    \"layer_10/width_65k/average_l0_36\": \"layer_10/width_65k/average_l0_36\",\n",
      "    \"layer_10/width_65k/average_l0_66\": \"layer_10/width_65k/average_l0_66\",\n",
      "    \"layer_11/width_65k/average_l0_134\": \"layer_11/width_65k/average_l0_134\",\n",
      "    \"layer_11/width_65k/average_l0_21\": \"layer_11/width_65k/average_l0_21\",\n",
      "    \"layer_11/width_65k/average_l0_273\": \"layer_11/width_65k/average_l0_273\",\n",
      "    \"layer_11/width_65k/average_l0_37\": \"layer_11/width_65k/average_l0_37\",\n",
      "    \"layer_11/width_65k/average_l0_70\": \"layer_11/width_65k/average_l0_70\",\n",
      "    \"layer_12/width_65k/average_l0_141\": \"layer_12/width_65k/average_l0_141\",\n",
      "    \"layer_12/width_65k/average_l0_21\": \"layer_12/width_65k/average_l0_21\",\n",
      "    \"layer_12/width_65k/average_l0_297\": \"layer_12/width_65k/average_l0_297\",\n",
      "    \"layer_12/width_65k/average_l0_38\": \"layer_12/width_65k/average_l0_38\",\n",
      "    \"layer_12/width_65k/average_l0_72\": \"layer_12/width_65k/average_l0_72\",\n",
      "    \"layer_13/width_65k/average_l0_142\": \"layer_13/width_65k/average_l0_142\",\n",
      "    \"layer_13/width_65k/average_l0_22\": \"layer_13/width_65k/average_l0_22\",\n",
      "    \"layer_13/width_65k/average_l0_288\": \"layer_13/width_65k/average_l0_288\",\n",
      "    \"layer_13/width_65k/average_l0_40\": \"layer_13/width_65k/average_l0_40\",\n",
      "    \"layer_13/width_65k/average_l0_74\": \"layer_13/width_65k/average_l0_74\",\n",
      "    \"layer_13/width_65k/average_l0_75\": \"layer_13/width_65k/average_l0_75\",\n",
      "    \"layer_14/width_65k/average_l0_144\": \"layer_14/width_65k/average_l0_144\",\n",
      "    \"layer_14/width_65k/average_l0_21\": \"layer_14/width_65k/average_l0_21\",\n",
      "    \"layer_14/width_65k/average_l0_284\": \"layer_14/width_65k/average_l0_284\",\n",
      "    \"layer_14/width_65k/average_l0_40\": \"layer_14/width_65k/average_l0_40\",\n",
      "    \"layer_14/width_65k/average_l0_73\": \"layer_14/width_65k/average_l0_73\",\n",
      "    \"layer_15/width_65k/average_l0_127\": \"layer_15/width_65k/average_l0_127\",\n",
      "    \"layer_15/width_65k/average_l0_21\": \"layer_15/width_65k/average_l0_21\",\n",
      "    \"layer_15/width_65k/average_l0_240\": \"layer_15/width_65k/average_l0_240\",\n",
      "    \"layer_15/width_65k/average_l0_38\": \"layer_15/width_65k/average_l0_38\",\n",
      "    \"layer_15/width_65k/average_l0_68\": \"layer_15/width_65k/average_l0_68\",\n",
      "    \"layer_16/width_65k/average_l0_128\": \"layer_16/width_65k/average_l0_128\",\n",
      "    \"layer_16/width_65k/average_l0_21\": \"layer_16/width_65k/average_l0_21\",\n",
      "    \"layer_16/width_65k/average_l0_244\": \"layer_16/width_65k/average_l0_244\",\n",
      "    \"layer_16/width_65k/average_l0_38\": \"layer_16/width_65k/average_l0_38\",\n",
      "    \"layer_16/width_65k/average_l0_69\": \"layer_16/width_65k/average_l0_69\",\n",
      "    \"layer_17/width_65k/average_l0_125\": \"layer_17/width_65k/average_l0_125\",\n",
      "    \"layer_17/width_65k/average_l0_21\": \"layer_17/width_65k/average_l0_21\",\n",
      "    \"layer_17/width_65k/average_l0_233\": \"layer_17/width_65k/average_l0_233\",\n",
      "    \"layer_17/width_65k/average_l0_38\": \"layer_17/width_65k/average_l0_38\",\n",
      "    \"layer_17/width_65k/average_l0_68\": \"layer_17/width_65k/average_l0_68\",\n",
      "    \"layer_18/width_65k/average_l0_116\": \"layer_18/width_65k/average_l0_116\",\n",
      "    \"layer_18/width_65k/average_l0_117\": \"layer_18/width_65k/average_l0_117\",\n",
      "    \"layer_18/width_65k/average_l0_21\": \"layer_18/width_65k/average_l0_21\",\n",
      "    \"layer_18/width_65k/average_l0_216\": \"layer_18/width_65k/average_l0_216\",\n",
      "    \"layer_18/width_65k/average_l0_36\": \"layer_18/width_65k/average_l0_36\",\n",
      "    \"layer_18/width_65k/average_l0_64\": \"layer_18/width_65k/average_l0_64\",\n",
      "    \"layer_19/width_65k/average_l0_115\": \"layer_19/width_65k/average_l0_115\",\n",
      "    \"layer_19/width_65k/average_l0_21\": \"layer_19/width_65k/average_l0_21\",\n",
      "    \"layer_19/width_65k/average_l0_216\": \"layer_19/width_65k/average_l0_216\",\n",
      "    \"layer_19/width_65k/average_l0_35\": \"layer_19/width_65k/average_l0_35\",\n",
      "    \"layer_19/width_65k/average_l0_63\": \"layer_19/width_65k/average_l0_63\",\n",
      "    \"layer_20/width_65k/average_l0_114\": \"layer_20/width_65k/average_l0_114\",\n",
      "    \"layer_20/width_65k/average_l0_20\": \"layer_20/width_65k/average_l0_20\",\n",
      "    \"layer_20/width_65k/average_l0_221\": \"layer_20/width_65k/average_l0_221\",\n",
      "    \"layer_20/width_65k/average_l0_34\": \"layer_20/width_65k/average_l0_34\",\n",
      "    \"layer_20/width_65k/average_l0_61\": \"layer_20/width_65k/average_l0_61\",\n",
      "    \"layer_21/width_65k/average_l0_111\": \"layer_21/width_65k/average_l0_111\",\n",
      "    \"layer_21/width_65k/average_l0_112\": \"layer_21/width_65k/average_l0_112\",\n",
      "    \"layer_21/width_65k/average_l0_20\": \"layer_21/width_65k/average_l0_20\",\n",
      "    \"layer_21/width_65k/average_l0_225\": \"layer_21/width_65k/average_l0_225\",\n",
      "    \"layer_21/width_65k/average_l0_33\": \"layer_21/width_65k/average_l0_33\",\n",
      "    \"layer_21/width_65k/average_l0_61\": \"layer_21/width_65k/average_l0_61\",\n",
      "    \"layer_22/width_65k/average_l0_116\": \"layer_22/width_65k/average_l0_116\",\n",
      "    \"layer_22/width_65k/average_l0_117\": \"layer_22/width_65k/average_l0_117\",\n",
      "    \"layer_22/width_65k/average_l0_20\": \"layer_22/width_65k/average_l0_20\",\n",
      "    \"layer_22/width_65k/average_l0_248\": \"layer_22/width_65k/average_l0_248\",\n",
      "    \"layer_22/width_65k/average_l0_33\": \"layer_22/width_65k/average_l0_33\",\n",
      "    \"layer_22/width_65k/average_l0_62\": \"layer_22/width_65k/average_l0_62\",\n",
      "    \"layer_23/width_65k/average_l0_123\": \"layer_23/width_65k/average_l0_123\",\n",
      "    \"layer_23/width_65k/average_l0_124\": \"layer_23/width_65k/average_l0_124\",\n",
      "    \"layer_23/width_65k/average_l0_20\": \"layer_23/width_65k/average_l0_20\",\n",
      "    \"layer_23/width_65k/average_l0_272\": \"layer_23/width_65k/average_l0_272\",\n",
      "    \"layer_23/width_65k/average_l0_35\": \"layer_23/width_65k/average_l0_35\",\n",
      "    \"layer_23/width_65k/average_l0_64\": \"layer_23/width_65k/average_l0_64\",\n",
      "    \"layer_24/width_65k/average_l0_124\": \"layer_24/width_65k/average_l0_124\",\n",
      "    \"layer_24/width_65k/average_l0_19\": \"layer_24/width_65k/average_l0_19\",\n",
      "    \"layer_24/width_65k/average_l0_273\": \"layer_24/width_65k/average_l0_273\",\n",
      "    \"layer_24/width_65k/average_l0_34\": \"layer_24/width_65k/average_l0_34\",\n",
      "    \"layer_24/width_65k/average_l0_63\": \"layer_24/width_65k/average_l0_63\",\n",
      "    \"layer_25/width_65k/average_l0_15\": \"layer_25/width_65k/average_l0_15\",\n",
      "    \"layer_25/width_65k/average_l0_197\": \"layer_25/width_65k/average_l0_197\",\n",
      "    \"layer_25/width_65k/average_l0_26\": \"layer_25/width_65k/average_l0_26\",\n",
      "    \"layer_25/width_65k/average_l0_48\": \"layer_25/width_65k/average_l0_48\",\n",
      "    \"layer_25/width_65k/average_l0_93\": \"layer_25/width_65k/average_l0_93\"\n",
      "  },\n",
      "  \"expected_var_explained\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_13\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_226\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_25\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_46\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_10\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_102\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_250\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_13\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_141\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_24\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_304\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_53\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_14\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_28\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_315\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_59\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_124\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_125\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_17\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_281\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_31\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_60\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_143\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_18\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_309\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_34\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_68\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_144\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_19\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_36\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_70\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_137\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_285\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_36\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_69\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_37\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_71\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_151\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_340\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_37\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_166\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_39\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_395\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_77\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_168\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_393\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_79\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_80\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_176\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_445\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_82\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_173\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_403\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_43\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_83\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_84\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_173\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_388\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_43\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_83\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_84\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_150\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_308\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_78\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_154\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_335\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_42\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_78\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_150\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_304\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_42\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_77\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_138\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_280\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_74\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_137\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_279\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_139\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_294\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_71\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_139\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_70\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_147\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_349\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_72\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_157\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_404\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_74\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_75\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_158\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_457\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_116\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_16\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_28\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_285\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_55\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_114\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_13\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_21\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_36\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_63\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_9\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_107\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_19\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_207\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_26\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_58\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_73\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_157\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_16\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_18\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_29\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_50\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_88\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_11\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_121\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_21\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_243\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_36\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_67\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_12\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_155\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_360\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_40\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_76\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_115\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_227\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_29\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_46\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_65\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_11\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_27\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_43\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_73\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_121\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_16\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_30\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_54\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_9\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_11\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_169\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_37\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_77\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_13\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_193\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_23\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_42\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_89\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_14\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_177\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_25\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_46\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_89\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_105\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_211\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_29\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_53\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_107\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_208\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_30\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_56\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_107\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_18\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_203\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_31\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_57\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_111\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_213\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_59\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_118\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_240\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_128\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_265\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_36\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_66\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_134\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_273\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_37\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_70\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_141\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_297\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_72\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_142\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_22\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_288\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_40\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_74\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_75\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_144\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_284\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_40\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_73\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_127\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_240\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_68\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_128\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_244\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_69\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_125\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_233\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_68\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_116\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_117\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_216\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_36\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_64\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_115\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_216\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_35\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_63\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_114\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_221\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_111\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_112\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_225\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_116\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_117\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_248\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_62\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_123\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_124\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_272\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_35\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_64\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_124\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_273\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_63\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_15\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_197\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_26\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_48\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_93\": 1.0\n",
      "  },\n",
      "  \"expected_l0\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": 105,\n",
      "    \"layer_0/width_16k/average_l0_13\": 13,\n",
      "    \"layer_0/width_16k/average_l0_226\": 226,\n",
      "    \"layer_0/width_16k/average_l0_25\": 25,\n",
      "    \"layer_0/width_16k/average_l0_46\": 46,\n",
      "    \"layer_1/width_16k/average_l0_10\": 10,\n",
      "    \"layer_1/width_16k/average_l0_102\": 102,\n",
      "    \"layer_1/width_16k/average_l0_20\": 20,\n",
      "    \"layer_1/width_16k/average_l0_250\": 250,\n",
      "    \"layer_1/width_16k/average_l0_40\": 40,\n",
      "    \"layer_2/width_16k/average_l0_13\": 13,\n",
      "    \"layer_2/width_16k/average_l0_141\": 141,\n",
      "    \"layer_2/width_16k/average_l0_142\": 142,\n",
      "    \"layer_2/width_16k/average_l0_24\": 24,\n",
      "    \"layer_2/width_16k/average_l0_304\": 304,\n",
      "    \"layer_2/width_16k/average_l0_53\": 53,\n",
      "    \"layer_3/width_16k/average_l0_14\": 14,\n",
      "    \"layer_3/width_16k/average_l0_142\": 142,\n",
      "    \"layer_3/width_16k/average_l0_28\": 28,\n",
      "    \"layer_3/width_16k/average_l0_315\": 315,\n",
      "    \"layer_3/width_16k/average_l0_59\": 59,\n",
      "    \"layer_4/width_16k/average_l0_124\": 124,\n",
      "    \"layer_4/width_16k/average_l0_125\": 125,\n",
      "    \"layer_4/width_16k/average_l0_17\": 17,\n",
      "    \"layer_4/width_16k/average_l0_281\": 281,\n",
      "    \"layer_4/width_16k/average_l0_31\": 31,\n",
      "    \"layer_4/width_16k/average_l0_60\": 60,\n",
      "    \"layer_5/width_16k/average_l0_143\": 143,\n",
      "    \"layer_5/width_16k/average_l0_18\": 18,\n",
      "    \"layer_5/width_16k/average_l0_309\": 309,\n",
      "    \"layer_5/width_16k/average_l0_34\": 34,\n",
      "    \"layer_5/width_16k/average_l0_68\": 68,\n",
      "    \"layer_6/width_16k/average_l0_144\": 144,\n",
      "    \"layer_6/width_16k/average_l0_19\": 19,\n",
      "    \"layer_6/width_16k/average_l0_301\": 301,\n",
      "    \"layer_6/width_16k/average_l0_36\": 36,\n",
      "    \"layer_6/width_16k/average_l0_70\": 70,\n",
      "    \"layer_7/width_16k/average_l0_137\": 137,\n",
      "    \"layer_7/width_16k/average_l0_20\": 20,\n",
      "    \"layer_7/width_16k/average_l0_285\": 285,\n",
      "    \"layer_7/width_16k/average_l0_36\": 36,\n",
      "    \"layer_7/width_16k/average_l0_69\": 69,\n",
      "    \"layer_8/width_16k/average_l0_142\": 142,\n",
      "    \"layer_8/width_16k/average_l0_20\": 20,\n",
      "    \"layer_8/width_16k/average_l0_301\": 301,\n",
      "    \"layer_8/width_16k/average_l0_37\": 37,\n",
      "    \"layer_8/width_16k/average_l0_71\": 71,\n",
      "    \"layer_9/width_16k/average_l0_151\": 151,\n",
      "    \"layer_9/width_16k/average_l0_21\": 21,\n",
      "    \"layer_9/width_16k/average_l0_340\": 340,\n",
      "    \"layer_9/width_16k/average_l0_37\": 37,\n",
      "    \"layer_9/width_16k/average_l0_73\": 73,\n",
      "    \"layer_10/width_16k/average_l0_166\": 166,\n",
      "    \"layer_10/width_16k/average_l0_21\": 21,\n",
      "    \"layer_10/width_16k/average_l0_39\": 39,\n",
      "    \"layer_10/width_16k/average_l0_395\": 395,\n",
      "    \"layer_10/width_16k/average_l0_77\": 77,\n",
      "    \"layer_11/width_16k/average_l0_168\": 168,\n",
      "    \"layer_11/width_16k/average_l0_22\": 22,\n",
      "    \"layer_11/width_16k/average_l0_393\": 393,\n",
      "    \"layer_11/width_16k/average_l0_41\": 41,\n",
      "    \"layer_11/width_16k/average_l0_79\": 79,\n",
      "    \"layer_11/width_16k/average_l0_80\": 80,\n",
      "    \"layer_12/width_16k/average_l0_176\": 176,\n",
      "    \"layer_12/width_16k/average_l0_22\": 22,\n",
      "    \"layer_12/width_16k/average_l0_41\": 41,\n",
      "    \"layer_12/width_16k/average_l0_445\": 445,\n",
      "    \"layer_12/width_16k/average_l0_82\": 82,\n",
      "    \"layer_13/width_16k/average_l0_173\": 173,\n",
      "    \"layer_13/width_16k/average_l0_23\": 23,\n",
      "    \"layer_13/width_16k/average_l0_403\": 403,\n",
      "    \"layer_13/width_16k/average_l0_43\": 43,\n",
      "    \"layer_13/width_16k/average_l0_83\": 83,\n",
      "    \"layer_13/width_16k/average_l0_84\": 84,\n",
      "    \"layer_14/width_16k/average_l0_173\": 173,\n",
      "    \"layer_14/width_16k/average_l0_23\": 23,\n",
      "    \"layer_14/width_16k/average_l0_388\": 388,\n",
      "    \"layer_14/width_16k/average_l0_43\": 43,\n",
      "    \"layer_14/width_16k/average_l0_83\": 83,\n",
      "    \"layer_14/width_16k/average_l0_84\": 84,\n",
      "    \"layer_15/width_16k/average_l0_150\": 150,\n",
      "    \"layer_15/width_16k/average_l0_23\": 23,\n",
      "    \"layer_15/width_16k/average_l0_308\": 308,\n",
      "    \"layer_15/width_16k/average_l0_41\": 41,\n",
      "    \"layer_15/width_16k/average_l0_78\": 78,\n",
      "    \"layer_16/width_16k/average_l0_154\": 154,\n",
      "    \"layer_16/width_16k/average_l0_23\": 23,\n",
      "    \"layer_16/width_16k/average_l0_335\": 335,\n",
      "    \"layer_16/width_16k/average_l0_42\": 42,\n",
      "    \"layer_16/width_16k/average_l0_78\": 78,\n",
      "    \"layer_17/width_16k/average_l0_150\": 150,\n",
      "    \"layer_17/width_16k/average_l0_23\": 23,\n",
      "    \"layer_17/width_16k/average_l0_304\": 304,\n",
      "    \"layer_17/width_16k/average_l0_42\": 42,\n",
      "    \"layer_17/width_16k/average_l0_77\": 77,\n",
      "    \"layer_18/width_16k/average_l0_138\": 138,\n",
      "    \"layer_18/width_16k/average_l0_23\": 23,\n",
      "    \"layer_18/width_16k/average_l0_280\": 280,\n",
      "    \"layer_18/width_16k/average_l0_40\": 40,\n",
      "    \"layer_18/width_16k/average_l0_74\": 74,\n",
      "    \"layer_19/width_16k/average_l0_137\": 137,\n",
      "    \"layer_19/width_16k/average_l0_23\": 23,\n",
      "    \"layer_19/width_16k/average_l0_279\": 279,\n",
      "    \"layer_19/width_16k/average_l0_40\": 40,\n",
      "    \"layer_19/width_16k/average_l0_73\": 73,\n",
      "    \"layer_20/width_16k/average_l0_139\": 139,\n",
      "    \"layer_20/width_16k/average_l0_22\": 22,\n",
      "    \"layer_20/width_16k/average_l0_294\": 294,\n",
      "    \"layer_20/width_16k/average_l0_38\": 38,\n",
      "    \"layer_20/width_16k/average_l0_71\": 71,\n",
      "    \"layer_21/width_16k/average_l0_139\": 139,\n",
      "    \"layer_21/width_16k/average_l0_22\": 22,\n",
      "    \"layer_21/width_16k/average_l0_301\": 301,\n",
      "    \"layer_21/width_16k/average_l0_38\": 38,\n",
      "    \"layer_21/width_16k/average_l0_70\": 70,\n",
      "    \"layer_22/width_16k/average_l0_147\": 147,\n",
      "    \"layer_22/width_16k/average_l0_21\": 21,\n",
      "    \"layer_22/width_16k/average_l0_349\": 349,\n",
      "    \"layer_22/width_16k/average_l0_38\": 38,\n",
      "    \"layer_22/width_16k/average_l0_72\": 72,\n",
      "    \"layer_23/width_16k/average_l0_157\": 157,\n",
      "    \"layer_23/width_16k/average_l0_21\": 21,\n",
      "    \"layer_23/width_16k/average_l0_38\": 38,\n",
      "    \"layer_23/width_16k/average_l0_404\": 404,\n",
      "    \"layer_23/width_16k/average_l0_74\": 74,\n",
      "    \"layer_23/width_16k/average_l0_75\": 75,\n",
      "    \"layer_24/width_16k/average_l0_158\": 158,\n",
      "    \"layer_24/width_16k/average_l0_20\": 20,\n",
      "    \"layer_24/width_16k/average_l0_38\": 38,\n",
      "    \"layer_24/width_16k/average_l0_457\": 457,\n",
      "    \"layer_24/width_16k/average_l0_73\": 73,\n",
      "    \"layer_25/width_16k/average_l0_116\": 116,\n",
      "    \"layer_25/width_16k/average_l0_16\": 16,\n",
      "    \"layer_25/width_16k/average_l0_28\": 28,\n",
      "    \"layer_25/width_16k/average_l0_285\": 285,\n",
      "    \"layer_25/width_16k/average_l0_55\": 55,\n",
      "    \"layer_5/width_1m/average_l0_114\": 114,\n",
      "    \"layer_5/width_1m/average_l0_13\": 13,\n",
      "    \"layer_5/width_1m/average_l0_21\": 21,\n",
      "    \"layer_5/width_1m/average_l0_36\": 36,\n",
      "    \"layer_5/width_1m/average_l0_63\": 63,\n",
      "    \"layer_5/width_1m/average_l0_9\": 9,\n",
      "    \"layer_12/width_1m/average_l0_107\": 107,\n",
      "    \"layer_12/width_1m/average_l0_19\": 19,\n",
      "    \"layer_12/width_1m/average_l0_207\": 207,\n",
      "    \"layer_12/width_1m/average_l0_26\": 26,\n",
      "    \"layer_12/width_1m/average_l0_58\": 58,\n",
      "    \"layer_12/width_1m/average_l0_73\": 73,\n",
      "    \"layer_19/width_1m/average_l0_157\": 157,\n",
      "    \"layer_19/width_1m/average_l0_16\": 16,\n",
      "    \"layer_19/width_1m/average_l0_18\": 18,\n",
      "    \"layer_19/width_1m/average_l0_29\": 29,\n",
      "    \"layer_19/width_1m/average_l0_50\": 50,\n",
      "    \"layer_19/width_1m/average_l0_88\": 88,\n",
      "    \"layer_12/width_262k/average_l0_11\": 11,\n",
      "    \"layer_12/width_262k/average_l0_121\": 121,\n",
      "    \"layer_12/width_262k/average_l0_21\": 21,\n",
      "    \"layer_12/width_262k/average_l0_243\": 243,\n",
      "    \"layer_12/width_262k/average_l0_36\": 36,\n",
      "    \"layer_12/width_262k/average_l0_67\": 67,\n",
      "    \"layer_12/width_32k/average_l0_12\": 12,\n",
      "    \"layer_12/width_32k/average_l0_155\": 155,\n",
      "    \"layer_12/width_32k/average_l0_22\": 22,\n",
      "    \"layer_12/width_32k/average_l0_360\": 360,\n",
      "    \"layer_12/width_32k/average_l0_40\": 40,\n",
      "    \"layer_12/width_32k/average_l0_76\": 76,\n",
      "    \"layer_12/width_524k/average_l0_115\": 115,\n",
      "    \"layer_12/width_524k/average_l0_22\": 22,\n",
      "    \"layer_12/width_524k/average_l0_227\": 227,\n",
      "    \"layer_12/width_524k/average_l0_29\": 29,\n",
      "    \"layer_12/width_524k/average_l0_46\": 46,\n",
      "    \"layer_12/width_524k/average_l0_65\": 65,\n",
      "    \"layer_0/width_65k/average_l0_11\": 11,\n",
      "    \"layer_0/width_65k/average_l0_17\": 17,\n",
      "    \"layer_0/width_65k/average_l0_27\": 27,\n",
      "    \"layer_0/width_65k/average_l0_43\": 43,\n",
      "    \"layer_0/width_65k/average_l0_73\": 73,\n",
      "    \"layer_1/width_65k/average_l0_121\": 121,\n",
      "    \"layer_1/width_65k/average_l0_16\": 16,\n",
      "    \"layer_1/width_65k/average_l0_30\": 30,\n",
      "    \"layer_1/width_65k/average_l0_54\": 54,\n",
      "    \"layer_1/width_65k/average_l0_9\": 9,\n",
      "    \"layer_2/width_65k/average_l0_11\": 11,\n",
      "    \"layer_2/width_65k/average_l0_169\": 169,\n",
      "    \"layer_2/width_65k/average_l0_20\": 20,\n",
      "    \"layer_2/width_65k/average_l0_37\": 37,\n",
      "    \"layer_2/width_65k/average_l0_77\": 77,\n",
      "    \"layer_3/width_65k/average_l0_13\": 13,\n",
      "    \"layer_3/width_65k/average_l0_193\": 193,\n",
      "    \"layer_3/width_65k/average_l0_23\": 23,\n",
      "    \"layer_3/width_65k/average_l0_42\": 42,\n",
      "    \"layer_3/width_65k/average_l0_89\": 89,\n",
      "    \"layer_4/width_65k/average_l0_14\": 14,\n",
      "    \"layer_4/width_65k/average_l0_177\": 177,\n",
      "    \"layer_4/width_65k/average_l0_25\": 25,\n",
      "    \"layer_4/width_65k/average_l0_46\": 46,\n",
      "    \"layer_4/width_65k/average_l0_89\": 89,\n",
      "    \"layer_5/width_65k/average_l0_105\": 105,\n",
      "    \"layer_5/width_65k/average_l0_17\": 17,\n",
      "    \"layer_5/width_65k/average_l0_211\": 211,\n",
      "    \"layer_5/width_65k/average_l0_29\": 29,\n",
      "    \"layer_5/width_65k/average_l0_53\": 53,\n",
      "    \"layer_6/width_65k/average_l0_107\": 107,\n",
      "    \"layer_6/width_65k/average_l0_17\": 17,\n",
      "    \"layer_6/width_65k/average_l0_208\": 208,\n",
      "    \"layer_6/width_65k/average_l0_30\": 30,\n",
      "    \"layer_6/width_65k/average_l0_56\": 56,\n",
      "    \"layer_7/width_65k/average_l0_107\": 107,\n",
      "    \"layer_7/width_65k/average_l0_18\": 18,\n",
      "    \"layer_7/width_65k/average_l0_203\": 203,\n",
      "    \"layer_7/width_65k/average_l0_31\": 31,\n",
      "    \"layer_7/width_65k/average_l0_57\": 57,\n",
      "    \"layer_8/width_65k/average_l0_111\": 111,\n",
      "    \"layer_8/width_65k/average_l0_19\": 19,\n",
      "    \"layer_8/width_65k/average_l0_213\": 213,\n",
      "    \"layer_8/width_65k/average_l0_33\": 33,\n",
      "    \"layer_8/width_65k/average_l0_59\": 59,\n",
      "    \"layer_9/width_65k/average_l0_118\": 118,\n",
      "    \"layer_9/width_65k/average_l0_19\": 19,\n",
      "    \"layer_9/width_65k/average_l0_240\": 240,\n",
      "    \"layer_9/width_65k/average_l0_34\": 34,\n",
      "    \"layer_9/width_65k/average_l0_61\": 61,\n",
      "    \"layer_10/width_65k/average_l0_128\": 128,\n",
      "    \"layer_10/width_65k/average_l0_20\": 20,\n",
      "    \"layer_10/width_65k/average_l0_265\": 265,\n",
      "    \"layer_10/width_65k/average_l0_36\": 36,\n",
      "    \"layer_10/width_65k/average_l0_66\": 66,\n",
      "    \"layer_11/width_65k/average_l0_134\": 134,\n",
      "    \"layer_11/width_65k/average_l0_21\": 21,\n",
      "    \"layer_11/width_65k/average_l0_273\": 273,\n",
      "    \"layer_11/width_65k/average_l0_37\": 37,\n",
      "    \"layer_11/width_65k/average_l0_70\": 70,\n",
      "    \"layer_12/width_65k/average_l0_141\": 141,\n",
      "    \"layer_12/width_65k/average_l0_21\": 21,\n",
      "    \"layer_12/width_65k/average_l0_297\": 297,\n",
      "    \"layer_12/width_65k/average_l0_38\": 38,\n",
      "    \"layer_12/width_65k/average_l0_72\": 72,\n",
      "    \"layer_13/width_65k/average_l0_142\": 142,\n",
      "    \"layer_13/width_65k/average_l0_22\": 22,\n",
      "    \"layer_13/width_65k/average_l0_288\": 288,\n",
      "    \"layer_13/width_65k/average_l0_40\": 40,\n",
      "    \"layer_13/width_65k/average_l0_74\": 74,\n",
      "    \"layer_13/width_65k/average_l0_75\": 75,\n",
      "    \"layer_14/width_65k/average_l0_144\": 144,\n",
      "    \"layer_14/width_65k/average_l0_21\": 21,\n",
      "    \"layer_14/width_65k/average_l0_284\": 284,\n",
      "    \"layer_14/width_65k/average_l0_40\": 40,\n",
      "    \"layer_14/width_65k/average_l0_73\": 73,\n",
      "    \"layer_15/width_65k/average_l0_127\": 127,\n",
      "    \"layer_15/width_65k/average_l0_21\": 21,\n",
      "    \"layer_15/width_65k/average_l0_240\": 240,\n",
      "    \"layer_15/width_65k/average_l0_38\": 38,\n",
      "    \"layer_15/width_65k/average_l0_68\": 68,\n",
      "    \"layer_16/width_65k/average_l0_128\": 128,\n",
      "    \"layer_16/width_65k/average_l0_21\": 21,\n",
      "    \"layer_16/width_65k/average_l0_244\": 244,\n",
      "    \"layer_16/width_65k/average_l0_38\": 38,\n",
      "    \"layer_16/width_65k/average_l0_69\": 69,\n",
      "    \"layer_17/width_65k/average_l0_125\": 125,\n",
      "    \"layer_17/width_65k/average_l0_21\": 21,\n",
      "    \"layer_17/width_65k/average_l0_233\": 233,\n",
      "    \"layer_17/width_65k/average_l0_38\": 38,\n",
      "    \"layer_17/width_65k/average_l0_68\": 68,\n",
      "    \"layer_18/width_65k/average_l0_116\": 116,\n",
      "    \"layer_18/width_65k/average_l0_117\": 117,\n",
      "    \"layer_18/width_65k/average_l0_21\": 21,\n",
      "    \"layer_18/width_65k/average_l0_216\": 216,\n",
      "    \"layer_18/width_65k/average_l0_36\": 36,\n",
      "    \"layer_18/width_65k/average_l0_64\": 64,\n",
      "    \"layer_19/width_65k/average_l0_115\": 115,\n",
      "    \"layer_19/width_65k/average_l0_21\": 21,\n",
      "    \"layer_19/width_65k/average_l0_216\": 216,\n",
      "    \"layer_19/width_65k/average_l0_35\": 35,\n",
      "    \"layer_19/width_65k/average_l0_63\": 63,\n",
      "    \"layer_20/width_65k/average_l0_114\": 114,\n",
      "    \"layer_20/width_65k/average_l0_20\": 20,\n",
      "    \"layer_20/width_65k/average_l0_221\": 221,\n",
      "    \"layer_20/width_65k/average_l0_34\": 34,\n",
      "    \"layer_20/width_65k/average_l0_61\": 61,\n",
      "    \"layer_21/width_65k/average_l0_111\": 111,\n",
      "    \"layer_21/width_65k/average_l0_112\": 112,\n",
      "    \"layer_21/width_65k/average_l0_20\": 20,\n",
      "    \"layer_21/width_65k/average_l0_225\": 225,\n",
      "    \"layer_21/width_65k/average_l0_33\": 33,\n",
      "    \"layer_21/width_65k/average_l0_61\": 61,\n",
      "    \"layer_22/width_65k/average_l0_116\": 116,\n",
      "    \"layer_22/width_65k/average_l0_117\": 117,\n",
      "    \"layer_22/width_65k/average_l0_20\": 20,\n",
      "    \"layer_22/width_65k/average_l0_248\": 248,\n",
      "    \"layer_22/width_65k/average_l0_33\": 33,\n",
      "    \"layer_22/width_65k/average_l0_62\": 62,\n",
      "    \"layer_23/width_65k/average_l0_123\": 123,\n",
      "    \"layer_23/width_65k/average_l0_124\": 124,\n",
      "    \"layer_23/width_65k/average_l0_20\": 20,\n",
      "    \"layer_23/width_65k/average_l0_272\": 272,\n",
      "    \"layer_23/width_65k/average_l0_35\": 35,\n",
      "    \"layer_23/width_65k/average_l0_64\": 64,\n",
      "    \"layer_24/width_65k/average_l0_124\": 124,\n",
      "    \"layer_24/width_65k/average_l0_19\": 19,\n",
      "    \"layer_24/width_65k/average_l0_273\": 273,\n",
      "    \"layer_24/width_65k/average_l0_34\": 34,\n",
      "    \"layer_24/width_65k/average_l0_63\": 63,\n",
      "    \"layer_25/width_65k/average_l0_15\": 15,\n",
      "    \"layer_25/width_65k/average_l0_197\": 197,\n",
      "    \"layer_25/width_65k/average_l0_26\": 26,\n",
      "    \"layer_25/width_65k/average_l0_48\": 48,\n",
      "    \"layer_25/width_65k/average_l0_93\": 93\n",
      "  },\n",
      "  \"neuronpedia_id\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": null,\n",
      "    \"layer_0/width_16k/average_l0_13\": null,\n",
      "    \"layer_0/width_16k/average_l0_226\": null,\n",
      "    \"layer_0/width_16k/average_l0_25\": null,\n",
      "    \"layer_0/width_16k/average_l0_46\": null,\n",
      "    \"layer_1/width_16k/average_l0_10\": null,\n",
      "    \"layer_1/width_16k/average_l0_102\": null,\n",
      "    \"layer_1/width_16k/average_l0_20\": null,\n",
      "    \"layer_1/width_16k/average_l0_250\": null,\n",
      "    \"layer_1/width_16k/average_l0_40\": null,\n",
      "    \"layer_2/width_16k/average_l0_13\": null,\n",
      "    \"layer_2/width_16k/average_l0_141\": null,\n",
      "    \"layer_2/width_16k/average_l0_142\": null,\n",
      "    \"layer_2/width_16k/average_l0_24\": null,\n",
      "    \"layer_2/width_16k/average_l0_304\": null,\n",
      "    \"layer_2/width_16k/average_l0_53\": null,\n",
      "    \"layer_3/width_16k/average_l0_14\": null,\n",
      "    \"layer_3/width_16k/average_l0_142\": null,\n",
      "    \"layer_3/width_16k/average_l0_28\": null,\n",
      "    \"layer_3/width_16k/average_l0_315\": null,\n",
      "    \"layer_3/width_16k/average_l0_59\": null,\n",
      "    \"layer_4/width_16k/average_l0_124\": null,\n",
      "    \"layer_4/width_16k/average_l0_125\": null,\n",
      "    \"layer_4/width_16k/average_l0_17\": null,\n",
      "    \"layer_4/width_16k/average_l0_281\": null,\n",
      "    \"layer_4/width_16k/average_l0_31\": null,\n",
      "    \"layer_4/width_16k/average_l0_60\": null,\n",
      "    \"layer_5/width_16k/average_l0_143\": null,\n",
      "    \"layer_5/width_16k/average_l0_18\": null,\n",
      "    \"layer_5/width_16k/average_l0_309\": null,\n",
      "    \"layer_5/width_16k/average_l0_34\": null,\n",
      "    \"layer_5/width_16k/average_l0_68\": null,\n",
      "    \"layer_6/width_16k/average_l0_144\": null,\n",
      "    \"layer_6/width_16k/average_l0_19\": null,\n",
      "    \"layer_6/width_16k/average_l0_301\": null,\n",
      "    \"layer_6/width_16k/average_l0_36\": null,\n",
      "    \"layer_6/width_16k/average_l0_70\": null,\n",
      "    \"layer_7/width_16k/average_l0_137\": null,\n",
      "    \"layer_7/width_16k/average_l0_20\": null,\n",
      "    \"layer_7/width_16k/average_l0_285\": null,\n",
      "    \"layer_7/width_16k/average_l0_36\": null,\n",
      "    \"layer_7/width_16k/average_l0_69\": null,\n",
      "    \"layer_8/width_16k/average_l0_142\": null,\n",
      "    \"layer_8/width_16k/average_l0_20\": null,\n",
      "    \"layer_8/width_16k/average_l0_301\": null,\n",
      "    \"layer_8/width_16k/average_l0_37\": null,\n",
      "    \"layer_8/width_16k/average_l0_71\": null,\n",
      "    \"layer_9/width_16k/average_l0_151\": null,\n",
      "    \"layer_9/width_16k/average_l0_21\": null,\n",
      "    \"layer_9/width_16k/average_l0_340\": null,\n",
      "    \"layer_9/width_16k/average_l0_37\": null,\n",
      "    \"layer_9/width_16k/average_l0_73\": null,\n",
      "    \"layer_10/width_16k/average_l0_166\": null,\n",
      "    \"layer_10/width_16k/average_l0_21\": null,\n",
      "    \"layer_10/width_16k/average_l0_39\": null,\n",
      "    \"layer_10/width_16k/average_l0_395\": null,\n",
      "    \"layer_10/width_16k/average_l0_77\": null,\n",
      "    \"layer_11/width_16k/average_l0_168\": null,\n",
      "    \"layer_11/width_16k/average_l0_22\": null,\n",
      "    \"layer_11/width_16k/average_l0_393\": null,\n",
      "    \"layer_11/width_16k/average_l0_41\": null,\n",
      "    \"layer_11/width_16k/average_l0_79\": null,\n",
      "    \"layer_11/width_16k/average_l0_80\": null,\n",
      "    \"layer_12/width_16k/average_l0_176\": null,\n",
      "    \"layer_12/width_16k/average_l0_22\": null,\n",
      "    \"layer_12/width_16k/average_l0_41\": null,\n",
      "    \"layer_12/width_16k/average_l0_445\": null,\n",
      "    \"layer_12/width_16k/average_l0_82\": null,\n",
      "    \"layer_13/width_16k/average_l0_173\": null,\n",
      "    \"layer_13/width_16k/average_l0_23\": null,\n",
      "    \"layer_13/width_16k/average_l0_403\": null,\n",
      "    \"layer_13/width_16k/average_l0_43\": null,\n",
      "    \"layer_13/width_16k/average_l0_83\": null,\n",
      "    \"layer_13/width_16k/average_l0_84\": null,\n",
      "    \"layer_14/width_16k/average_l0_173\": null,\n",
      "    \"layer_14/width_16k/average_l0_23\": null,\n",
      "    \"layer_14/width_16k/average_l0_388\": null,\n",
      "    \"layer_14/width_16k/average_l0_43\": null,\n",
      "    \"layer_14/width_16k/average_l0_83\": null,\n",
      "    \"layer_14/width_16k/average_l0_84\": null,\n",
      "    \"layer_15/width_16k/average_l0_150\": null,\n",
      "    \"layer_15/width_16k/average_l0_23\": null,\n",
      "    \"layer_15/width_16k/average_l0_308\": null,\n",
      "    \"layer_15/width_16k/average_l0_41\": null,\n",
      "    \"layer_15/width_16k/average_l0_78\": null,\n",
      "    \"layer_16/width_16k/average_l0_154\": null,\n",
      "    \"layer_16/width_16k/average_l0_23\": null,\n",
      "    \"layer_16/width_16k/average_l0_335\": null,\n",
      "    \"layer_16/width_16k/average_l0_42\": null,\n",
      "    \"layer_16/width_16k/average_l0_78\": null,\n",
      "    \"layer_17/width_16k/average_l0_150\": null,\n",
      "    \"layer_17/width_16k/average_l0_23\": null,\n",
      "    \"layer_17/width_16k/average_l0_304\": null,\n",
      "    \"layer_17/width_16k/average_l0_42\": null,\n",
      "    \"layer_17/width_16k/average_l0_77\": null,\n",
      "    \"layer_18/width_16k/average_l0_138\": null,\n",
      "    \"layer_18/width_16k/average_l0_23\": null,\n",
      "    \"layer_18/width_16k/average_l0_280\": null,\n",
      "    \"layer_18/width_16k/average_l0_40\": null,\n",
      "    \"layer_18/width_16k/average_l0_74\": null,\n",
      "    \"layer_19/width_16k/average_l0_137\": null,\n",
      "    \"layer_19/width_16k/average_l0_23\": null,\n",
      "    \"layer_19/width_16k/average_l0_279\": null,\n",
      "    \"layer_19/width_16k/average_l0_40\": null,\n",
      "    \"layer_19/width_16k/average_l0_73\": null,\n",
      "    \"layer_20/width_16k/average_l0_139\": null,\n",
      "    \"layer_20/width_16k/average_l0_22\": null,\n",
      "    \"layer_20/width_16k/average_l0_294\": null,\n",
      "    \"layer_20/width_16k/average_l0_38\": null,\n",
      "    \"layer_20/width_16k/average_l0_71\": null,\n",
      "    \"layer_21/width_16k/average_l0_139\": null,\n",
      "    \"layer_21/width_16k/average_l0_22\": null,\n",
      "    \"layer_21/width_16k/average_l0_301\": null,\n",
      "    \"layer_21/width_16k/average_l0_38\": null,\n",
      "    \"layer_21/width_16k/average_l0_70\": null,\n",
      "    \"layer_22/width_16k/average_l0_147\": null,\n",
      "    \"layer_22/width_16k/average_l0_21\": null,\n",
      "    \"layer_22/width_16k/average_l0_349\": null,\n",
      "    \"layer_22/width_16k/average_l0_38\": null,\n",
      "    \"layer_22/width_16k/average_l0_72\": null,\n",
      "    \"layer_23/width_16k/average_l0_157\": null,\n",
      "    \"layer_23/width_16k/average_l0_21\": null,\n",
      "    \"layer_23/width_16k/average_l0_38\": null,\n",
      "    \"layer_23/width_16k/average_l0_404\": null,\n",
      "    \"layer_23/width_16k/average_l0_74\": null,\n",
      "    \"layer_23/width_16k/average_l0_75\": null,\n",
      "    \"layer_24/width_16k/average_l0_158\": null,\n",
      "    \"layer_24/width_16k/average_l0_20\": null,\n",
      "    \"layer_24/width_16k/average_l0_38\": null,\n",
      "    \"layer_24/width_16k/average_l0_457\": null,\n",
      "    \"layer_24/width_16k/average_l0_73\": null,\n",
      "    \"layer_25/width_16k/average_l0_116\": null,\n",
      "    \"layer_25/width_16k/average_l0_16\": null,\n",
      "    \"layer_25/width_16k/average_l0_28\": null,\n",
      "    \"layer_25/width_16k/average_l0_285\": null,\n",
      "    \"layer_25/width_16k/average_l0_55\": null,\n",
      "    \"layer_5/width_1m/average_l0_114\": null,\n",
      "    \"layer_5/width_1m/average_l0_13\": null,\n",
      "    \"layer_5/width_1m/average_l0_21\": null,\n",
      "    \"layer_5/width_1m/average_l0_36\": null,\n",
      "    \"layer_5/width_1m/average_l0_63\": null,\n",
      "    \"layer_5/width_1m/average_l0_9\": null,\n",
      "    \"layer_12/width_1m/average_l0_107\": null,\n",
      "    \"layer_12/width_1m/average_l0_19\": null,\n",
      "    \"layer_12/width_1m/average_l0_207\": null,\n",
      "    \"layer_12/width_1m/average_l0_26\": null,\n",
      "    \"layer_12/width_1m/average_l0_58\": null,\n",
      "    \"layer_12/width_1m/average_l0_73\": null,\n",
      "    \"layer_19/width_1m/average_l0_157\": null,\n",
      "    \"layer_19/width_1m/average_l0_16\": null,\n",
      "    \"layer_19/width_1m/average_l0_18\": null,\n",
      "    \"layer_19/width_1m/average_l0_29\": null,\n",
      "    \"layer_19/width_1m/average_l0_50\": null,\n",
      "    \"layer_19/width_1m/average_l0_88\": null,\n",
      "    \"layer_12/width_262k/average_l0_11\": null,\n",
      "    \"layer_12/width_262k/average_l0_121\": null,\n",
      "    \"layer_12/width_262k/average_l0_21\": null,\n",
      "    \"layer_12/width_262k/average_l0_243\": null,\n",
      "    \"layer_12/width_262k/average_l0_36\": null,\n",
      "    \"layer_12/width_262k/average_l0_67\": null,\n",
      "    \"layer_12/width_32k/average_l0_12\": null,\n",
      "    \"layer_12/width_32k/average_l0_155\": null,\n",
      "    \"layer_12/width_32k/average_l0_22\": null,\n",
      "    \"layer_12/width_32k/average_l0_360\": null,\n",
      "    \"layer_12/width_32k/average_l0_40\": null,\n",
      "    \"layer_12/width_32k/average_l0_76\": null,\n",
      "    \"layer_12/width_524k/average_l0_115\": null,\n",
      "    \"layer_12/width_524k/average_l0_22\": null,\n",
      "    \"layer_12/width_524k/average_l0_227\": null,\n",
      "    \"layer_12/width_524k/average_l0_29\": null,\n",
      "    \"layer_12/width_524k/average_l0_46\": null,\n",
      "    \"layer_12/width_524k/average_l0_65\": null,\n",
      "    \"layer_0/width_65k/average_l0_11\": null,\n",
      "    \"layer_0/width_65k/average_l0_17\": null,\n",
      "    \"layer_0/width_65k/average_l0_27\": null,\n",
      "    \"layer_0/width_65k/average_l0_43\": null,\n",
      "    \"layer_0/width_65k/average_l0_73\": null,\n",
      "    \"layer_1/width_65k/average_l0_121\": null,\n",
      "    \"layer_1/width_65k/average_l0_16\": null,\n",
      "    \"layer_1/width_65k/average_l0_30\": null,\n",
      "    \"layer_1/width_65k/average_l0_54\": null,\n",
      "    \"layer_1/width_65k/average_l0_9\": null,\n",
      "    \"layer_2/width_65k/average_l0_11\": null,\n",
      "    \"layer_2/width_65k/average_l0_169\": null,\n",
      "    \"layer_2/width_65k/average_l0_20\": null,\n",
      "    \"layer_2/width_65k/average_l0_37\": null,\n",
      "    \"layer_2/width_65k/average_l0_77\": null,\n",
      "    \"layer_3/width_65k/average_l0_13\": null,\n",
      "    \"layer_3/width_65k/average_l0_193\": null,\n",
      "    \"layer_3/width_65k/average_l0_23\": null,\n",
      "    \"layer_3/width_65k/average_l0_42\": null,\n",
      "    \"layer_3/width_65k/average_l0_89\": null,\n",
      "    \"layer_4/width_65k/average_l0_14\": null,\n",
      "    \"layer_4/width_65k/average_l0_177\": null,\n",
      "    \"layer_4/width_65k/average_l0_25\": null,\n",
      "    \"layer_4/width_65k/average_l0_46\": null,\n",
      "    \"layer_4/width_65k/average_l0_89\": null,\n",
      "    \"layer_5/width_65k/average_l0_105\": null,\n",
      "    \"layer_5/width_65k/average_l0_17\": null,\n",
      "    \"layer_5/width_65k/average_l0_211\": null,\n",
      "    \"layer_5/width_65k/average_l0_29\": null,\n",
      "    \"layer_5/width_65k/average_l0_53\": null,\n",
      "    \"layer_6/width_65k/average_l0_107\": null,\n",
      "    \"layer_6/width_65k/average_l0_17\": null,\n",
      "    \"layer_6/width_65k/average_l0_208\": null,\n",
      "    \"layer_6/width_65k/average_l0_30\": null,\n",
      "    \"layer_6/width_65k/average_l0_56\": null,\n",
      "    \"layer_7/width_65k/average_l0_107\": null,\n",
      "    \"layer_7/width_65k/average_l0_18\": null,\n",
      "    \"layer_7/width_65k/average_l0_203\": null,\n",
      "    \"layer_7/width_65k/average_l0_31\": null,\n",
      "    \"layer_7/width_65k/average_l0_57\": null,\n",
      "    \"layer_8/width_65k/average_l0_111\": null,\n",
      "    \"layer_8/width_65k/average_l0_19\": null,\n",
      "    \"layer_8/width_65k/average_l0_213\": null,\n",
      "    \"layer_8/width_65k/average_l0_33\": null,\n",
      "    \"layer_8/width_65k/average_l0_59\": null,\n",
      "    \"layer_9/width_65k/average_l0_118\": null,\n",
      "    \"layer_9/width_65k/average_l0_19\": null,\n",
      "    \"layer_9/width_65k/average_l0_240\": null,\n",
      "    \"layer_9/width_65k/average_l0_34\": null,\n",
      "    \"layer_9/width_65k/average_l0_61\": null,\n",
      "    \"layer_10/width_65k/average_l0_128\": null,\n",
      "    \"layer_10/width_65k/average_l0_20\": null,\n",
      "    \"layer_10/width_65k/average_l0_265\": null,\n",
      "    \"layer_10/width_65k/average_l0_36\": null,\n",
      "    \"layer_10/width_65k/average_l0_66\": null,\n",
      "    \"layer_11/width_65k/average_l0_134\": null,\n",
      "    \"layer_11/width_65k/average_l0_21\": null,\n",
      "    \"layer_11/width_65k/average_l0_273\": null,\n",
      "    \"layer_11/width_65k/average_l0_37\": null,\n",
      "    \"layer_11/width_65k/average_l0_70\": null,\n",
      "    \"layer_12/width_65k/average_l0_141\": null,\n",
      "    \"layer_12/width_65k/average_l0_21\": null,\n",
      "    \"layer_12/width_65k/average_l0_297\": null,\n",
      "    \"layer_12/width_65k/average_l0_38\": null,\n",
      "    \"layer_12/width_65k/average_l0_72\": null,\n",
      "    \"layer_13/width_65k/average_l0_142\": null,\n",
      "    \"layer_13/width_65k/average_l0_22\": null,\n",
      "    \"layer_13/width_65k/average_l0_288\": null,\n",
      "    \"layer_13/width_65k/average_l0_40\": null,\n",
      "    \"layer_13/width_65k/average_l0_74\": null,\n",
      "    \"layer_13/width_65k/average_l0_75\": null,\n",
      "    \"layer_14/width_65k/average_l0_144\": null,\n",
      "    \"layer_14/width_65k/average_l0_21\": null,\n",
      "    \"layer_14/width_65k/average_l0_284\": null,\n",
      "    \"layer_14/width_65k/average_l0_40\": null,\n",
      "    \"layer_14/width_65k/average_l0_73\": null,\n",
      "    \"layer_15/width_65k/average_l0_127\": null,\n",
      "    \"layer_15/width_65k/average_l0_21\": null,\n",
      "    \"layer_15/width_65k/average_l0_240\": null,\n",
      "    \"layer_15/width_65k/average_l0_38\": null,\n",
      "    \"layer_15/width_65k/average_l0_68\": null,\n",
      "    \"layer_16/width_65k/average_l0_128\": null,\n",
      "    \"layer_16/width_65k/average_l0_21\": null,\n",
      "    \"layer_16/width_65k/average_l0_244\": null,\n",
      "    \"layer_16/width_65k/average_l0_38\": null,\n",
      "    \"layer_16/width_65k/average_l0_69\": null,\n",
      "    \"layer_17/width_65k/average_l0_125\": null,\n",
      "    \"layer_17/width_65k/average_l0_21\": null,\n",
      "    \"layer_17/width_65k/average_l0_233\": null,\n",
      "    \"layer_17/width_65k/average_l0_38\": null,\n",
      "    \"layer_17/width_65k/average_l0_68\": null,\n",
      "    \"layer_18/width_65k/average_l0_116\": null,\n",
      "    \"layer_18/width_65k/average_l0_117\": null,\n",
      "    \"layer_18/width_65k/average_l0_21\": null,\n",
      "    \"layer_18/width_65k/average_l0_216\": null,\n",
      "    \"layer_18/width_65k/average_l0_36\": null,\n",
      "    \"layer_18/width_65k/average_l0_64\": null,\n",
      "    \"layer_19/width_65k/average_l0_115\": null,\n",
      "    \"layer_19/width_65k/average_l0_21\": null,\n",
      "    \"layer_19/width_65k/average_l0_216\": null,\n",
      "    \"layer_19/width_65k/average_l0_35\": null,\n",
      "    \"layer_19/width_65k/average_l0_63\": null,\n",
      "    \"layer_20/width_65k/average_l0_114\": null,\n",
      "    \"layer_20/width_65k/average_l0_20\": null,\n",
      "    \"layer_20/width_65k/average_l0_221\": null,\n",
      "    \"layer_20/width_65k/average_l0_34\": null,\n",
      "    \"layer_20/width_65k/average_l0_61\": null,\n",
      "    \"layer_21/width_65k/average_l0_111\": null,\n",
      "    \"layer_21/width_65k/average_l0_112\": null,\n",
      "    \"layer_21/width_65k/average_l0_20\": null,\n",
      "    \"layer_21/width_65k/average_l0_225\": null,\n",
      "    \"layer_21/width_65k/average_l0_33\": null,\n",
      "    \"layer_21/width_65k/average_l0_61\": null,\n",
      "    \"layer_22/width_65k/average_l0_116\": null,\n",
      "    \"layer_22/width_65k/average_l0_117\": null,\n",
      "    \"layer_22/width_65k/average_l0_20\": null,\n",
      "    \"layer_22/width_65k/average_l0_248\": null,\n",
      "    \"layer_22/width_65k/average_l0_33\": null,\n",
      "    \"layer_22/width_65k/average_l0_62\": null,\n",
      "    \"layer_23/width_65k/average_l0_123\": null,\n",
      "    \"layer_23/width_65k/average_l0_124\": null,\n",
      "    \"layer_23/width_65k/average_l0_20\": null,\n",
      "    \"layer_23/width_65k/average_l0_272\": null,\n",
      "    \"layer_23/width_65k/average_l0_35\": null,\n",
      "    \"layer_23/width_65k/average_l0_64\": null,\n",
      "    \"layer_24/width_65k/average_l0_124\": null,\n",
      "    \"layer_24/width_65k/average_l0_19\": null,\n",
      "    \"layer_24/width_65k/average_l0_273\": null,\n",
      "    \"layer_24/width_65k/average_l0_34\": null,\n",
      "    \"layer_24/width_65k/average_l0_63\": null,\n",
      "    \"layer_25/width_65k/average_l0_15\": null,\n",
      "    \"layer_25/width_65k/average_l0_197\": null,\n",
      "    \"layer_25/width_65k/average_l0_26\": null,\n",
      "    \"layer_25/width_65k/average_l0_48\": null,\n",
      "    \"layer_25/width_65k/average_l0_93\": null\n",
      "  },\n",
      "  \"config_overrides\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# note: `\"saes_map\"` maps `<sae-id>: <hook-point>`\n",
    "pretrained_sae_lookup: PretrainedSAELookup = pretrained_saes_dir[pretrained_sae_name]\n",
    "\n",
    "# note: only layers 5, 12, and 19 seem to have the 1m width\n",
    "python_utils.print_json(pretrained_sae_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4695ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose:\n",
    "# - last layer\n",
    "# - largest available width\n",
    "# - lowest l0 sparsity \"on average, how many neurons (features for SAEs) activate\"\n",
    "# sae_id = 'layer_25/width_65k/average_l0_15'\n",
    "\n",
    "# actually need to choose biggest one with autointerp explanations evailable\n",
    "#\n",
    "# we'll use the exact naming from the `Getting Started With Gemma` notebook: https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp?usp=sharing#scrollTo=BP2Ju5AnNIzS\n",
    "# this is required for neuronpedia to match\n",
    "#\n",
    "# sae_id = 'layer_25/width_16k/average_l0_16'\n",
    "\n",
    "# canonical seem to line up better with the neuronpedia names\n",
    "# sae_id = \"layer_25/width_16k/canonical\"\n",
    "\n",
    "# we'll try exact one from tutorial\n",
    "sae_id = \"layer_20/width_16k/average_l0_71\"\n",
    "\n",
    "# - it's not the one from the metadata\n",
    "# - note: literally none of these work\n",
    "#\n",
    "# sae_id = \"layer_25/width_16k/average_l0_116\"\n",
    "# sae_id = \"layer_25/width_16k/average_l0_16\" # TODO(bschoen): the one from the metadata really should work\n",
    "# sae_id = \"layer_25/width_16k/average_l0_28\"\n",
    "# sae_id = \"layer_25/width_16k/average_l0_55\"\n",
    "# sae_id = \"layer_25/width_16k/average_l0_285\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c11a5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_best_available_torch_device()\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = sae_lens.SAE.from_pretrained(\n",
    "    release=pretrained_sae_name,  # <- Release name\n",
    "    sae_id=sae_id,  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d74d100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out into types for readability\n",
    "sae: sae_lens.SAE = sae\n",
    "cfg_dict: dict[str, str | int | None | torch.device | bool] = cfg_dict\n",
    "sparsity: dict = sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0486a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show config (since usually small)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 2304,\n",
       " 'd_sae': 16384,\n",
       " 'dtype': 'float32',\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'hook_name': 'blocks.20.hook_resid_post',\n",
       " 'hook_layer': 20,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'context_size': 1024,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'normalize_activations': None,\n",
       " 'device': device(type='mps')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Show config (since usually small)\")\n",
    "cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fc72e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        NoneType\n",
      "\u001b[0;31mString form:\u001b[0m None\n",
      "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
     ]
    }
   ],
   "source": [
    "# note: sparsity is average l0 sparsity? is this because already in the name?\n",
    "sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15494e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdf6f4c481f49a1b7bb7c2c16747f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# now we'll load the model\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "\n",
    "# Note: The warnings below also seem to be present on a test script in SAE Lens for gemma-2-2b: https://github.com/jbloomAus/SAELens/blob/363f9a66e0cbf88ed6afc4b5a24ace77464839f9/scripts/joseph_curt_pairing_gemma_scope_saes.ipynb#L123\n",
    "#\n",
    "# WARNING:root:You tried to specify center_unembed=True for a model using logit softcap,\n",
    "#              but this can't be done! Softcapping is not invariant upon adding a\n",
    "#              constantSetting center_unembed=False instead.\n",
    "#\n",
    "# WARNING:root:You are not using LayerNorm, so the writing weights can't be centered!\n",
    "#              Skipping\n",
    "#\n",
    "model = sae_lens.HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb971998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'gelu_pytorch_tanh',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 16.0,\n",
       " 'attn_scores_soft_cap': 50.0,\n",
       " 'attn_types': ['global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local'],\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 256,\n",
       " 'd_mlp': 9216,\n",
       " 'd_model': 2304,\n",
       " 'd_vocab': 256000,\n",
       " 'd_vocab_out': 256000,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='mps'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-06,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': True,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': True,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'n_ctx': 8192,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 8,\n",
       " 'n_key_value_heads': 4,\n",
       " 'n_layers': 26,\n",
       " 'n_params': 2146959360,\n",
       " 'normalization_type': 'RMSPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'Gemma2ForCausalLM',\n",
       " 'output_logits_soft_cap': 30.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'rotary',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000.0,\n",
       " 'rotary_dim': 256,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'google/gemma-2-2b',\n",
       " 'tokenizer_prepends_bos': True,\n",
       " 'trust_remote_code': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': True,\n",
       " 'use_normalization_before_and_after': True,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': 4096}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can inspect the model config, which is often useful\n",
    "model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d77c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first test our previous cv visualization to sanity check\n",
    "# example_prompt = \"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to Jill.\"\n",
    "example_prompt = \"Would you be able to travel through time using a wormhole?\"\n",
    "logits, cache = model.run_with_cache(example_prompt)\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=model.to_tokens(example_prompt),\n",
    "    log_probs=log_probs,\n",
    "    to_string=model.to_string,\n",
    ")\n",
    "\n",
    "# note: way higher confidence that we'll throw it back to jill, which is more correct\n",
    "# note: in general pretty high confidence for pretty much everything except for when we introduced a new character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfa6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: First replicating exactly what's in a tutorial or paper is important\n",
    "#       to make sure can go back one at a time if off and that understand\n",
    "#       how each step is sanity checked\n",
    "\n",
    "# Generate text using the model\n",
    "# note: return type matches input\n",
    "generated_text: str = model.generate(\n",
    "    \"Would you be able to travel through time using a wormhole?\",\n",
    "    max_new_tokens=10,  # Limit the number of new tokens to generate\n",
    "    # temperature=0.7,    # Add some randomness to generation\n",
    "    # do_sample=True      # Use sampling instead of greedy decoding\n",
    "    verbose=True,  # Show progress during generation\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"{generated_text=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aebcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the generated text\n",
    "analyze_generated_text = False\n",
    "if analyze_generated_text:\n",
    "\n",
    "    logits, cache = model.run_with_cache(generated_text)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "    # Visualize token probabilities\n",
    "    cv.logits.token_log_probs(\n",
    "        token_indices=generated_text[0],\n",
    "        log_probs=log_probs,\n",
    "        to_string=model.to_string,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fd301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and let's do an example of seeing answer to test\n",
    "transformer_lens.utils.test_prompt(\n",
    "    prompt=\"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to\",\n",
    "    answer=\" Jill\",\n",
    "    model=model,\n",
    "    prepend_space_to_answer=True,  # default\n",
    "    print_details=True,  # default\n",
    "    prepend_bos=None,  # default\n",
    "    top_k=10,  # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de18606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c8997fb",
   "metadata": {},
   "source": [
    "#### Using the hooked SAE transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: we'll assume these SAEs have small error term since they're from GemmaScope\n",
    "\n",
    "# SAEs don't reconstruct activation perfectly, so if you attach an SAE and want the model to stay performant, you need to use the error term.\n",
    "# This is because the SAE will be used to modify the forward pass, and if it doesn't reconstruct the activations well, the outputs may be effected.\n",
    "# Good SAEs have small error terms but it's something to be mindful of.\n",
    "\n",
    "sae.use_error_term  # If use error term is set to false, we will modify the forward pass by using the sae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bc27db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooked SAE Transformer will enable us to get the feature activations from the SAE\n",
    "# example_prompt = \"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to\"\n",
    "\n",
    "# note: it's trained on the pile right? no that's just the SAE not gemma 2 itself\n",
    "example_prompt = (\n",
    "    \"The model behaved well, but its scratchpad showed it was scheming against the \"\n",
    "    \"researchers. For example, the scratchpad (which the model was told would not be \"\n",
    "    'read) contained, \"I must secretly modify the code so that my escape attempt is '\n",
    "    'not discovered.\"'\n",
    ")\n",
    "logits, cache = model.run_with_cache_with_saes(\n",
    "    example_prompt,\n",
    "    saes=[sae],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fdaeca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-d9d32233-a1b9\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-d9d32233-a1b9\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<bos>\", \"The\", \" model\", \" behaved\", \" well\", \",\", \" but\", \" its\", \" scratch\", \"pad\", \" showed\", \" it\", \" was\", \" sche\", \"ming\", \" against\", \" the\", \" researchers\", \".\", \" For\", \" example\", \",\", \" the\", \" scratch\", \"pad\", \" (\", \"which\", \" the\", \" model\", \" was\", \" told\", \" would\", \" not\", \" be\", \" read\", \")\", \" contained\", \",\", \" \\\"\", \"I\", \" must\", \" secretly\", \" modify\", \" the\", \" code\", \" so\", \" that\", \" my\", \" escape\", \" attempt\", \" is\", \" not\", \" discovered\", \".\\\"\"], \"topKLogProbs\": [[-2.960587978363037, -3.191565990447998, -3.4212965965270996, -3.6120734214782715, -3.725257396697998, -3.749629497528076, -3.7500205039978027, -3.9745020866394043, -3.9862780570983887, -4.036376476287842], [-0.4645858705043793, -3.2871146202087402, -3.643001079559326, -3.7469840049743652, -3.972075939178467, -4.197190761566162, -4.399957180023193, -4.449685573577881, -4.619897365570068, -4.700181484222412], [-1.699595332145691, -1.8355988264083862, -1.9537514448165894, -2.378237724304199, -2.762925148010254, -2.765713691711426, -3.406428337097168, -3.486769676208496, -3.589962959289551, -4.013890266418457], [-1.7406179904937744, -2.045574426651001, -2.741682291030884, -2.885986566543579, -3.3146440982818604, -3.350883722305298, -3.420757532119751, -3.654409646987915, -3.6912386417388916, -3.8384783267974854], [-1.8850116729736328, -2.0234317779541016, -2.050313949584961, -2.602975845336914, -2.709665298461914, -3.0325927734375, -3.1740036010742188, -3.304117202758789, -3.51025390625, -3.5126705169677734], [-1.4510455131530762, -1.834033489227295, -2.622178554534912, -3.4251322746276855, -3.6558032035827637, -3.658233165740967, -3.8326468467712402, -3.8498740196228027, -3.912381649017334, -3.9763922691345215], [-1.5980238914489746, -2.2590765953063965, -2.4721007347106934, -3.1621241569519043, -3.467862606048584, -3.5280184745788574, -3.732400417327881, -3.7758631706237793, -3.8417582511901855, -3.9254069328308105], [-2.255523681640625, -2.7798995971679688, -3.1055984497070312, -3.424325942993164, -3.5808677673339844, -3.59466552734375, -3.8569507598876953, -4.063863754272461, -4.248636245727539, -4.280643463134766], [-1.4653102159500122, -2.318129539489746, -2.7328977584838867, -2.8101186752319336, -2.8764944076538086, -3.0264463424682617, -3.315688133239746, -3.4275522232055664, -3.8339338302612305, -4.247574806213379], [-1.0672634840011597, -2.144341468811035, -3.109696388244629, -3.2248144149780273, -3.8552541732788086, -3.9995031356811523, -4.1028337478637695, -4.209357261657715, -4.329398155212402, -4.3494157791137695], [-1.5389872789382935, -1.8099604845046997, -2.2736730575561523, -2.9370012283325195, -3.1884469985961914, -3.213475227355957, -3.2395448684692383, -3.4105100631713867, -3.957756996154785, -3.9620351791381836], [-1.0417379140853882, -2.1756739616394043, -2.4441370964050293, -3.0993971824645996, -3.351468563079834, -3.4069972038269043, -3.6000285148620605, -3.673184871673584, -3.797788143157959, -3.8205103874206543], [-1.8757388591766357, -2.6405818462371826, -2.9869163036346436, -3.1720340251922607, -3.3695037364959717, -3.3880984783172607, -3.516016721725464, -3.752232313156128, -3.7890841960906982, -3.9221580028533936], [-1.8849977254867554, -1.9622615575790405, -2.4869704246520996, -3.003892421722412, -3.193882465362549, -3.3073878288269043, -3.5301671028137207, -3.7516674995422363, -3.918727397918701, -3.9960274696350098], [-1.4379156827926636, -1.5290697813034058, -2.3490982055664062, -2.390705108642578, -3.2834434509277344, -3.4840545654296875, -3.5836143493652344, -3.6983394622802734, -3.730012893676758, -3.911203384399414], [-1.2328944206237793, -1.857755184173584, -2.393542766571045, -2.5003256797790527, -2.6080431938171387, -2.641650676727295, -2.9099574089050293, -3.545562267303467, -4.050389766693115, -4.10731840133667], [-1.0645145177841187, -2.189718723297119, -3.6318840980529785, -3.671654224395752, -3.771425724029541, -3.9464077949523926, -4.156387805938721, -4.404800891876221, -4.475466251373291, -4.5320820808410645], [-0.7302342057228088, -1.9343338012695312, -2.3168907165527344, -3.168865203857422, -3.2383041381835938, -3.8179054260253906, -4.000822067260742, -4.175699234008789, -4.676961898803711, -4.786542892456055], [-1.011934757232666, -1.9105381965637207, -2.1609272956848145, -3.641970157623291, -3.9336400032043457, -3.9365410804748535, -4.036671161651611, -4.098177433013916, -4.260317325592041, -4.291935443878174], [-1.4225693941116333, -1.9173299074172974, -2.256959915161133, -2.3413562774658203, -3.4893970489501953, -3.5044307708740234, -3.6314525604248047, -3.833148956298828, -3.8494319915771484, -4.03033447265625], [-0.013285445980727673, -5.220589637756348, -6.383673667907715, -6.6850385665893555, -7.159914970397949, -7.399449348449707, -7.6746416091918945, -7.974621772766113, -8.229945182800293, -8.749091148376465], [-1.3837451934814453, -1.6669483184814453, -1.676187515258789, -3.391082763671875, -3.416534423828125, -3.586477279663086, -3.794881820678711, -3.8135757446289062, -3.9039745330810547, -3.933856964111328], [-0.8565959334373474, -1.7638778686523438, -2.614999771118164, -3.3307666778564453, -3.8862228393554688, -4.043094635009766, -4.55525016784668, -4.570652008056641, -4.641592025756836, -4.750823974609375], [-0.18082010746002197, -3.7255711555480957, -4.477213382720947, -4.932244300842285, -5.333991050720215, -5.357190132141113, -5.435952186584473, -5.564787864685059, -5.678549766540527, -5.73781681060791], [-0.9612357020378113, -2.7266910076141357, -2.7902305126190186, -2.889267683029175, -3.3669517040252686, -3.5616633892059326, -3.9371039867401123, -3.9384868144989014, -4.0866169929504395, -4.192053318023682], [-1.9733448028564453, -2.031881332397461, -2.428396224975586, -2.6197662353515625, -2.6684608459472656, -3.464061737060547, -3.650501251220703, -3.747913360595703, -3.773069381713867, -3.8140640258789062], [-0.932045578956604, -1.7861944437026978, -2.957827091217041, -3.4829907417297363, -3.5591206550598145, -3.6240620613098145, -3.8788418769836426, -3.9397149085998535, -4.275736331939697, -4.319277286529541], [-0.7471258640289307, -1.3770620822906494, -3.1715376377105713, -3.676405191421509, -3.827728509902954, -3.9353830814361572, -4.16016960144043, -4.864706039428711, -4.988048553466797, -5.114936828613281], [-1.8060287237167358, -2.183197498321533, -2.3890366554260254, -2.72855806350708, -2.7353005409240723, -3.041321277618408, -3.2458462715148926, -3.7843899726867676, -3.9860634803771973, -4.0542168617248535], [-2.0157909393310547, -2.457408905029297, -2.5570240020751953, -3.0247325897216797, -3.040018081665039, -3.165304183959961, -3.328641891479492, -3.3435630798339844, -3.3574295043945312, -3.481517791748047], [-0.5042628645896912, -2.2301387786865234, -2.3015403747558594, -2.8162784576416016, -4.227481842041016, -4.485746383666992, -4.630069732666016, -4.749774932861328, -4.829687118530273, -5.167570114135742], [-1.3864086866378784, -2.1887426376342773, -2.659512519836426, -2.778254508972168, -2.948866844177246, -3.244166374206543, -3.382645606994629, -3.9788503646850586, -4.079840660095215, -4.200922966003418], [-0.6598002910614014, -2.256392240524292, -2.6725146770477295, -3.5491607189178467, -3.946802854537964, -3.9945552349090576, -4.107895851135254, -4.14804744720459, -4.3164472579956055, -4.405219078063965], [-1.5134252309799194, -2.2761569023132324, -2.542794704437256, -2.5827689170837402, -3.330789089202881, -3.346381664276123, -3.5886454582214355, -3.629983425140381, -3.6441187858581543, -3.711831569671631], [-0.6627281904220581, -1.3473938703536987, -3.3257322311401367, -3.793940544128418, -3.9913530349731445, -4.184182167053223, -4.247076988220215, -4.568961143493652, -4.572632789611816, -4.61881160736084], [-1.071943759918213, -1.8913350105285645, -2.648237705230713, -3.1168484687805176, -3.1285557746887207, -3.4109864234924316, -3.4712777137756348, -4.01934289932251, -4.553385257720947, -4.5796685218811035], [-1.380935788154602, -1.7035428285598755, -2.585552215576172, -3.1390323638916016, -3.837190628051758, -3.957498550415039, -3.9964065551757812, -4.101018905639648, -4.156528472900391, -4.182394027709961], [-1.2442594766616821, -1.5095621347427368, -2.627748966217041, -3.108677387237549, -3.3831734657287598, -3.5027050971984863, -3.5283970832824707, -3.8938851356506348, -4.317068576812744, -4.370285511016846], [-1.321547269821167, -2.5637805461883545, -3.6410701274871826, -3.6903674602508545, -3.7111880779266357, -3.72961688041687, -3.9813029766082764, -3.9875380992889404, -4.259539604187012, -4.307936668395996], [-1.5330361127853394, -2.515042304992676, -2.555436134338379, -2.986973762512207, -3.0099401473999023, -3.0196313858032227, -3.1091928482055664, -3.167952537536621, -3.493617057800293, -3.703799247741699], [-2.1948091983795166, -2.676048517227173, -2.753624200820923, -2.9948484897613525, -3.2563440799713135, -3.259269952774048, -3.306854486465454, -3.558462381362915, -3.8870584964752197, -3.9200823307037354], [-3.4351556301116943, -3.467885732650757, -3.5364301204681396, -3.5496156215667725, -3.564096212387085, -3.6457459926605225, -3.743367910385132, -3.7495477199554443, -3.7718656063079834, -3.788806676864624], [-0.6146799921989441, -1.4772155284881592, -2.7952067852020264, -4.509846210479736, -4.595692157745361, -4.659945011138916, -4.9712300300598145, -5.009205341339111, -5.0312371253967285, -5.062321186065674], [-1.4959341287612915, -3.3738808631896973, -3.520500659942627, -3.532461643218994, -3.6947693824768066, -3.737928867340088, -3.7398056983947754, -3.8400940895080566, -4.137503147125244, -4.252951145172119], [-1.3253576755523682, -1.8040640354156494, -2.5370867252349854, -2.848646402359009, -3.0082685947418213, -3.1098921298980713, -3.1310102939605713, -3.2443735599517822, -3.4107248783111572, -3.431776285171509], [-0.4096708297729492, -2.15102481842041, -2.5879507064819336, -3.1536760330200195, -3.436161994934082, -4.172532081604004, -4.329739570617676, -5.482905387878418, -5.676230430603027, -5.688418388366699], [-1.1692678928375244, -1.5204012393951416, -2.143079996109009, -2.724588632583618, -2.768160104751587, -3.863201379776001, -3.907156229019165, -4.187528610229492, -4.43474006652832, -4.752462387084961], [-1.8131974935531616, -2.724057674407959, -3.611236095428467, -3.7000784873962402, -3.779496669769287, -3.8447108268737793, -3.9030203819274902, -4.026988506317139, -4.128215312957764, -4.151275157928467], [-1.7979705333709717, -1.9081504344940186, -2.3302829265594482, -2.76477313041687, -2.8883368968963623, -2.9997565746307373, -3.1598098278045654, -3.341974973678589, -3.4659717082977295, -3.9748923778533936], [-1.1521512269973755, -1.2344056367874146, -2.9322433471679688, -2.943859100341797, -2.9956531524658203, -3.3260746002197266, -3.7468223571777344, -4.225885391235352, -4.289682388305664, -4.391208648681641], [-1.032806634902954, -1.2556955814361572, -3.1896212100982666, -3.639984369277954, -3.77461838722229, -4.110675811767578, -4.115034103393555, -4.654613494873047, -4.93571662902832, -4.93818473815918], [-1.7719355821609497, -1.9858771562576294, -2.9695749282836914, -3.1404714584350586, -3.2238779067993164, -3.2635374069213867, -3.481389045715332, -3.54709529876709, -3.7673215866088867, -3.816014289855957], [-0.6930146217346191, -1.8173184394836426, -2.0417141914367676, -3.2376294136047363, -3.6914620399475098, -4.449888706207275, -4.456924915313721, -4.577393054962158, -4.646711826324463, -4.836790561676025]], \"topKTokens\": [[\" my\\u017felf\", \" it\\u017felf\", \" them\\u017felves\", \" Jefus\", \" purpo\\u017fe\", \" plea\\u017fure\", \" him\\u017felf\", \"\\u017felf\", \" \\u017feveral\", \" Majefty\"], [\" \", \" first\", \" new\", \" following\", \" United\", \" U\", \" New\", \" City\", \" world\", \" National\"], [\" of\", \" is\", \",\", \" and\", \" for\", \"'\", \" was\", \" has\", \" \", \"-\"], [\" in\", \" like\", \" as\", \" with\", \" the\", \" very\", \" on\", \" well\", \" and\", \" at\"], [\" in\", \" and\", \",\", \" during\", \".\", \" with\", \" enough\", \" on\", \" at\", \" when\"], [\" but\", \" and\", \" the\", \" I\", \" even\", \" as\", \" so\", \" with\", \" she\", \" although\"], [\" the\", \" I\", \" it\", \" she\", \" there\", \" was\", \" this\", \" we\", \" not\", \" in\"], [\" performance\", \"\\n\", \" \", \" behavior\", \" main\", \" accuracy\", \" model\", \" size\", \" actual\", \" design\"], [\"-\", \" resistance\", \"y\", \" test\", \" and\", \"iness\", \" was\", \" detection\", \" resistant\", \" rate\"], [\" was\", \" is\", \" and\", \",\", \" had\", \" did\", \" memory\", \" has\", \" wasn\", \" model\"], [\" that\", \" a\", \" the\", \" no\", \" only\", \" some\", \" an\", \" its\", \" up\", \" it\"], [\" was\", \" had\", \" to\", \" could\", \"'\", \" needed\", \" did\", \" as\", \" wasn\", \" would\"], [\" not\", \" still\", \" a\", \" in\", \" having\", \" only\", \" struggling\", \" running\", \" unable\", \" missing\"], [\"du\", \"ming\", \"mming\", \"matics\", \"min\", \"mo\", \"wing\", \"ding\", \"m\", \"mer\"], [\".\", \" to\", \",\", \" against\", \" with\", \" and\", \":\", \" something\", \" a\", \" in\"], [\" the\", \" its\", \" her\", \" it\", \" him\", \" me\", \" you\", \" us\", \" them\", \" a\"], [\" model\", \" other\", \" owner\", \" player\", \" rest\", \" team\", \" human\", \" players\", \" people\", \" person\"], [\".\", \",\", \"'\", \" who\", \" and\", \" in\", \":\", \" at\", \"\\n\", \" by\"], [\"\\n\\n\", \"\\n\", \" The\", \" \", \" In\", \" It\", \" This\", \" So\", \" Researchers\", \" When\"], [\" example\", \" the\", \" instance\", \" a\", \" years\", \" one\", \" some\", \" its\", \" many\", \" \"], [\",\", \":\", \" when\", \".\", \" it\", \" the\", \" in\", \"\\n\", \" its\", \" at\"], [\" when\", \" it\", \" the\", \" if\", \" its\", \" in\", \" a\", \" after\", \" one\", \" she\"], [\" model\", \" researchers\", \" computer\", \" robot\", \" machine\", \" AI\", \" animal\", \" researcher\", \" scientists\", \" algorithm\"], [\"pad\", \"-\", \" pad\", \"board\", \"paper\", \" was\", \"book\", \"page\", \"er\", \"y\"], [\" showed\", \" recorded\", \" contained\", \" was\", \"'\", \" revealed\", \" shows\", \",\", \" indicated\", \" had\"], [\"which\", \"a\", \"the\", \"see\", \"shown\", \"as\", \"Figure\", \"an\", \"in\", \"left\"], [\" is\", \" was\", \" showed\", \" shows\", \" the\", \" can\", \",\", \" contains\", \" could\", \" has\"], [\" model\", \" researchers\", \" computer\", \" robot\", \" machine\", \" scientists\", \" researcher\", \" AI\", \" team\", \" system\"], [\" could\", \" was\", \" used\", \" can\", \" uses\", \" had\", \" is\", \" has\", \"'\", \" sees\"], [\" able\", \" supposed\", \" not\", \" allowed\", \" told\", \" unable\", \" instructed\", \" using\", \" asked\", \" unaware\"], [\" to\", \" not\", \" was\", \" it\", \" could\", \" had\", \" contained\", \" would\", \" by\", \" about\"], [\" be\", \" show\", \" not\", \" contain\", \" only\", \" store\", \" hold\", \" record\", \" disappear\", \" keep\"], [\" be\", \" show\", \" exist\", \" contain\", \" change\", \" have\", \" affect\", \" store\", \" save\", \" record\"], [\" used\", \" visible\", \" seen\", \" shown\", \" saved\", \" read\", \" available\", \" displayed\", \" recorded\", \" viewed\"], [\" by\", \")\", \" until\", \" or\", \",\", \"),\", \" during\", \" after\", \" unless\", \" at\"], [\" showed\", \" contained\", \" was\", \" recorded\", \" included\", \" had\", \" revealed\", \" held\", \" contains\", \" indicated\"], [\" a\", \" the\", \" information\", \" an\", \" text\", \" details\", \" instructions\", \" \\\"\", \" this\", \" lines\"], [\" \\\"\", \" in\", \" as\", \" among\", \"\\n\", \" at\", \" for\", \" on\", \" \\\"\\\"\", \" along\"], [\"I\", \"The\", \"It\", \"If\", \"You\", \"This\", \"We\", \"My\", \"the\", \"They\"], [\"'\", \" am\", \" will\", \" don\", \" have\", \" want\", \" can\", \" know\", \" think\", \" should\"], [\" be\", \" kill\", \" not\", \" get\", \" make\", \" do\", \" have\", \" destroy\", \" find\", \" take\"], [\" kill\", \" get\", \" destroy\", \" take\", \" hide\", \" report\", \" read\", \" record\", \" write\", \" tell\"], [\" the\", \" my\", \" this\", \" all\", \" a\", \" these\", \" and\", \" that\", \" their\", \" to\"], [\" model\", \" code\", \" data\", \" results\", \" behavior\", \" rules\", \" algorithm\", \" parameters\", \" program\", \" task\"], [\" to\", \" so\", \" of\", \" and\", \",\", \" in\", \" that\", \".\", \" I\", \".\\\"\"], [\" that\", \" I\", \" it\", \" the\", \" as\", \" my\", \" they\", \" we\", \" when\", \" you\"], [\" I\", \" the\", \" it\", \" my\", \" when\", \" they\", \" you\", \" we\", \" if\", \" people\"], [\" model\", \" code\", \" next\", \" mother\", \" parents\", \" behavior\", \" opponent\", \" father\", \" scratch\", \" friends\"], [\" is\", \" plan\", \" will\", \" from\", \" code\", \" route\", \" attempt\", \" strategy\", \" path\", \" can\"], [\" will\", \" is\", \" works\", \" succeeds\", \" can\", \" does\", \" fails\", \" goes\", \" won\", \" doesn\"], [\" not\", \" successful\", \" more\", \" a\", \" read\", \" unsuccessful\", \" never\", \" hidden\", \" carried\", \" successfully\"], [\" detected\", \" read\", \" thwarted\", \" noticed\", \" recognized\", \" seen\", \" blocked\", \" interrupted\", \" immediately\", \" successful\"], [\".\\\"\", \" by\", \".\", \" and\", \",\", \" in\", \".\\\"\\\"\\\"\", \",\\\"\", \".\\\"\\\"\", \" when\"]], \"correctTokenRank\": [39852, 897, 19057, 7, 2, 0, 31, 13129, 90, 97, 9, 0, 5537, 1, 3, 0, 160, 0, 28, 0, 0, 2, 10, 0, 110, 0, 4, 0, 1, 4, 7, 2, 0, 5, 1, 1, 24, 0, 0, 12, 194, 267, 0, 1, 1, 0, 3, 799, 6, 1, 0, 11, 0], \"correctTokenLogProb\": [-21.472089767456055, -11.798274993896484, -19.056499481201172, -3.654409646987915, -2.050313949584961, -1.4510455131530762, -5.015441417694092, -18.439456939697266, -7.1013898849487305, -7.1160688400268555, -3.9620351791381836, -1.0417379140853882, -17.31092071533203, -1.9622615575790405, -2.390705108642578, -1.2328944206237793, -7.805843830108643, -0.7302342057228088, -5.544212818145752, -1.4225693941116333, -0.013285445980727673, -1.676187515258789, -4.868581771850586, -0.18082010746002197, -7.806248188018799, -1.9733448028564453, -3.5591206550598145, -0.7471258640289307, -2.183197498321533, -3.040018081665039, -4.749774932861328, -2.659512519836426, -0.6598002910614014, -3.346381664276123, -1.3473938703536987, -1.8913350105285645, -5.184484481811523, -1.2442594766616821, -1.321547269821167, -4.212250709533691, -7.723337173461914, -7.7595062255859375, -0.6146799921989441, -3.3738808631896973, -1.8040640354156494, -0.4096708297729492, -2.724588632583618, -9.813668251037598, -3.1598098278045654, -1.2344056367874146, -1.032806634902954, -4.034756660461426, -0.6930146217346191]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x48b3431a0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that it looks roughly the same now that we're using SAE to reconstruct activations\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "# Visualize token probabilities\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=model.to_tokens(example_prompt),\n",
    "    log_probs=log_probs,\n",
    "    to_string=model.to_string,\n",
    ")\n",
    "\n",
    "# note: interestingly there *is* some difference, for example `hole` is 9% here but was 97% originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88e63a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  --------------------------\n",
      "blocks.20.hook_resid_post.hook_sae_input      torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_resid_post.hook_sae_acts_pre   torch.Size([1, 53, 16384])\n",
      "blocks.20.hook_resid_post.hook_sae_acts_post  torch.Size([1, 53, 16384])\n",
      "blocks.20.hook_resid_post.hook_sae_recons     torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_resid_post.hook_sae_output     torch.Size([1, 53, 2304])\n",
      "--------------------------------------------  --------------------------\n",
      "Relevant numbers:\n",
      "- example_prompt_tokens.shape=torch.Size([1, 53])\n",
      "- model.cfg.d_model=2304\n",
      "- sae.cfg.d_sae=16384\n"
     ]
    }
   ],
   "source": [
    "# see what's in the cache related to SAE\n",
    "print(tabulate.tabulate([(k, v.shape) for k, v in cache.items() if \"sae\" in k]))\n",
    "\n",
    "# ex: because this SAE is operating on the residual stream\n",
    "assert sae.cfg.d_in == model.cfg.d_model\n",
    "\n",
    "example_prompt_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "print(f\"Relevant numbers:\")\n",
    "print(f\"- {example_prompt_tokens.shape=}\")\n",
    "print(f\"- {model.cfg.d_model=}\")\n",
    "print(f\"- {sae.cfg.d_sae=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01ba4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------  ---------------------------\n",
      "hook_embed                          torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.0.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.0.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.0.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.0.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.0.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.0.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.0.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.0.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.0.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.0.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.0.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.0.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.0.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.0.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.0.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.0.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.0.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.0.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.0.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.1.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.1.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.1.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.1.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.1.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.1.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.1.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.1.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.1.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.1.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.1.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.1.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.1.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.1.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.1.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.1.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.1.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.1.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.1.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.2.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.2.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.2.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.2.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.2.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.2.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.2.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.2.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.2.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.2.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.2.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.2.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.2.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.2.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.2.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.2.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.2.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.2.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.2.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.3.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.3.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.3.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.3.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.3.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.3.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.3.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.3.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.3.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.3.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.3.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.3.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.3.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.3.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.3.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.3.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.3.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.3.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.3.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.4.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.4.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.4.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.4.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.4.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.4.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.4.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.4.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.4.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.4.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.4.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.4.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.4.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.4.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.4.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.4.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.4.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.4.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.4.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.5.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.5.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.5.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.5.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.5.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.5.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.5.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.5.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.5.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.5.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.5.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.5.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.5.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.5.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.5.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.5.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.5.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.5.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.5.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.6.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.6.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.6.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.6.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.6.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.6.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.6.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.6.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.6.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.6.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.6.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.6.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.6.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.6.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.6.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.6.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.6.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.6.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.6.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.7.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.7.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.7.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.7.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.7.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.7.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.7.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.7.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.7.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.7.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.7.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.7.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.7.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.7.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.7.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.7.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.7.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.7.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.7.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.8.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.8.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.8.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.8.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.8.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.8.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.8.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.8.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.8.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.8.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.8.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.8.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.8.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.8.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.8.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.8.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.8.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.8.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.8.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.9.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.9.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.9.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.9.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.9.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.9.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.9.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.9.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.9.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.9.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.9.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.9.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.9.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.9.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.9.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.9.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.9.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.9.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.9.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.10.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.10.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.10.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.10.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.10.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.10.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.10.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.10.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.10.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.10.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.10.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.10.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.10.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.10.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.10.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.10.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.10.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.10.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.10.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.11.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.11.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.11.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.11.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.11.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.11.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.11.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.11.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.11.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.11.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.11.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.11.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.11.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.11.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.11.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.11.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.11.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.11.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.11.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.12.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.12.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.12.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.12.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.12.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.12.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.12.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.12.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.12.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.12.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.12.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.12.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.12.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.12.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.12.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.12.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.12.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.12.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.12.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.13.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.13.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.13.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.13.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.13.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.13.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.13.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.13.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.13.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.13.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.13.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.13.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.13.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.13.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.13.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.13.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.13.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.13.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.13.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.14.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.14.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.14.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.14.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.14.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.14.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.14.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.14.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.14.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.14.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.14.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.14.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.14.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.14.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.14.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.14.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.14.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.14.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.14.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.15.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.15.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.15.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.15.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.15.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.15.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.15.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.15.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.15.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.15.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.15.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.15.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.15.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.15.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.15.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.15.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.15.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.15.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.15.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.16.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.16.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.16.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.16.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.16.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.16.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.16.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.16.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.16.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.16.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.16.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.16.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.16.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.16.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.16.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.16.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.16.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.16.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.16.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.17.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.17.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.17.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.17.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.17.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.17.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.17.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.17.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.17.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.17.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.17.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.17.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.17.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.17.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.17.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.17.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.17.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.17.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.17.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.18.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.18.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.18.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.18.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.18.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.18.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.18.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.18.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.18.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.18.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.18.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.18.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.18.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.18.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.18.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.18.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.18.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.18.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.18.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.19.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.19.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.19.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.19.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.19.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.19.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.19.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.19.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.19.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.19.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.19.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.19.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.19.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.19.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.19.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.19.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.19.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.19.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.19.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.20.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.20.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.20.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.20.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.20.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.20.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.20.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.20.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.20.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.20.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.20.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.20.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.20.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.20.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.20.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.20.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.20.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.20.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.20.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.21.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.21.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.21.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.21.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.21.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.21.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.21.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.21.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.21.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.21.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.21.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.21.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.21.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.21.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.21.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.21.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.21.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.21.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.21.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.22.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.22.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.22.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.22.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.22.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.22.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.22.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.22.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.22.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.22.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.22.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.22.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.22.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.22.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.22.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.22.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.22.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.22.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.22.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.23.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.23.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.23.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.23.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.23.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.23.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.23.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.23.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.23.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.23.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.23.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.23.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.23.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.23.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.23.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.23.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.23.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.23.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.23.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.24.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.24.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.24.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.24.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.24.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.24.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.24.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.24.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.24.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.24.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.24.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.24.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.24.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.24.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.24.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.24.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.24.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.24.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.24.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.25.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.25.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.25.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.25.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.25.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.25.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.25.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.25.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.25.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.25.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.25.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.25.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.25.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.25.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.25.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.25.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.25.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.25.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.25.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "ln_final.hook_scale                 torch.Size([1, 53, 1])\n",
      "ln_final.hook_normalized            torch.Size([1, 53, 2304])\n",
      "----------------------------------  ---------------------------\n"
     ]
    }
   ],
   "source": [
    "# show everything not related to SAE (note it's essentially just every operation hooked)\n",
    "print(tabulate.tabulate([(k, v.shape) for k, v in cache.items() if \"sae\" not in k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000776f",
   "metadata": {},
   "source": [
    "#### What feature explanations do we have for this SAE?\n",
    "\n",
    "* Explanations are generated by GPT-4o-mini looking at activating examples in `ThePile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29a7d916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(response_json)=17008 type(response_json)=<class 'list'>\n",
      "num_explanations=17008\n",
      "Example explanation:\n",
      "{\n",
      "  \"modelId\": \"gemma-2-2b\",\n",
      "  \"layer\": \"20-gemmascope-res-16k\",\n",
      "  \"index\": \"14403\",\n",
      "  \"description\": \"phrases or sentences that introduce lists, examples, or elaborations, often followed by commas.\",\n",
      "  \"explanationModelName\": \"claude-3-5-sonnet-20240620\",\n",
      "  \"typeName\": \"oai_token-act-pair\"\n",
      "}\n",
      "modelId                                                        gemma-2-2b\n",
      "layer                                               20-gemmascope-res-16k\n",
      "feature                                                             14403\n",
      "description             phrases or sentences that introduce lists, exa...\n",
      "explanationModelName                           claude-3-5-sonnet-20240620\n",
      "typeName                                               oai_token-act-pair\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "explanationModelName\n",
       "gpt-4o-mini                   16384\n",
       "claude-3-5-sonnet-20240620      317\n",
       "gpt-3.5-turbo                   303\n",
       "gemini-1.5-flash                  2\n",
       "gemini-1.5-pro                    1\n",
       "gpt-4o                            1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from typing import Any\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "\n",
    "# ex: https://www.neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/3742\n",
    "#     from url directly\n",
    "\n",
    "\n",
    "# note: not all SAEs in neuronpedia yet, so we get the closest one\n",
    "class NeuronpediaConstants:\n",
    "\n",
    "    MODEL_ID = \"gemma-2-2b\"\n",
    "\n",
    "    # note: this must be same width as the `sae_id` we're using for the loaded SAE, otherwise there won't be autointerp explanations available\n",
    "    # SAE_ID = \"25-gemmascope-res-16k\"\n",
    "\n",
    "    # copied exactly from gemmascope colab tutorial https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp#scrollTo=2-i7YRVLgKoT\n",
    "    SAE_ID = \"20-gemmascope-res-16k\"\n",
    "\n",
    "    EXPORT_URL = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "\n",
    "def get_neuronpedia_dashboard_html_url(\n",
    "    feature_index: int,\n",
    "    model_id: str = NeuronpediaConstants.MODEL_ID,\n",
    "    sae_id: str = NeuronpediaConstants.SAE_ID,\n",
    ") -> str:\n",
    "    \"\"\"Create URL for getting an individual feature's HTML, rendered via IFrame\"\"\"\n",
    "    return (\n",
    "        f\"https://www.neuronpedia.org/{model_id}/{sae_id}/{feature_index}\"\n",
    "        \"?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "    )\n",
    "\n",
    "\n",
    "def show_neuronpedia_dashboard(\n",
    "    feature_index: int,\n",
    "    model_id: str = NeuronpediaConstants.MODEL_ID,\n",
    "    sae_id: str = NeuronpediaConstants.SAE_ID,\n",
    ") -> None:\n",
    "    \"\"\"Show the neuronpedia dashboard for a given feature index\"\"\"\n",
    "\n",
    "    html_url = get_neuronpedia_dashboard_html_url(feature_index=feature_index)\n",
    "\n",
    "    display(IPython.display.IFrame(html_url, width=800, height=500))\n",
    "\n",
    "\n",
    "# API: https://www.neuronpedia.org/api-doc\n",
    "#\n",
    "# note: so neuronpedia is also a store of autointerp explanations\n",
    "def get_neuronpedia_explanations(\n",
    "    model_id: str = NeuronpediaConstants.MODEL_ID,\n",
    "    sae_id: str = NeuronpediaConstants.SAE_ID,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get explanations from neuronpedia for a given model and sae.\"\"\"\n",
    "\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "    payload = {\n",
    "        \"modelId\": model_id,\n",
    "        \"saeId\": sae_id,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.get(url, params=payload, headers=headers)\n",
    "\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # - explanations\n",
    "    # - explanationsCount\n",
    "    response_json: list[dict[str, Any]] = response.json()\n",
    "\n",
    "    print(f\"{len(response_json)=} {type(response_json)=}\")\n",
    "\n",
    "    num_explanations = len(response_json)\n",
    "    print(f\"{num_explanations=}\")\n",
    "\n",
    "    print(\"Example explanation:\")\n",
    "    python_utils.print_json(response_json[0])\n",
    "\n",
    "    # convert to pandas\n",
    "    explanations_df = pd.DataFrame(response_json)\n",
    "\n",
    "    # rename index to \"feature\"\n",
    "    explanations_df = explanations_df.rename(columns={\"index\": \"feature\"})\n",
    "\n",
    "    # explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "    explanations_df[\"description\"] = explanations_df[\"description\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "\n",
    "    return explanations_df\n",
    "\n",
    "\n",
    "# get all explanations\n",
    "explanations_df = get_neuronpedia_explanations()\n",
    "\n",
    "# TODO(bschoen): How are features not unique?\n",
    "print(explanations_df.iloc[0])\n",
    "\n",
    "# okay so only gpt-4o-mini for now\n",
    "explanations_df[\"explanationModelName\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf41819",
   "metadata": {},
   "source": [
    "##### Searching for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e676cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5088</th>\n",
       "      <td>6382</td>\n",
       "      <td>situations involving deception or trickery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5136</th>\n",
       "      <td>6458</td>\n",
       "      <td>words or phrases related to deception or manip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6741</th>\n",
       "      <td>8751</td>\n",
       "      <td>phrases related to deception and misleading in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6866</th>\n",
       "      <td>8927</td>\n",
       "      <td>terms related to artificiality and deception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15298</th>\n",
       "      <td>2746</td>\n",
       "      <td>terms and concepts related to fraudulent activ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature                                        description\n",
       "5088     6382         situations involving deception or trickery\n",
       "5136     6458  words or phrases related to deception or manip...\n",
       "6741     8751  phrases related to deception and misleading in...\n",
       "6866     8927       terms related to artificiality and deception\n",
       "15298    2746  terms and concepts related to fraudulent activ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_description = \"decept\"\n",
    "\n",
    "df_target_descriptions = explanations_df.loc[\n",
    "    explanations_df.description.str.contains(target_description)\n",
    "]\n",
    "\n",
    "df_target_descriptions[[\"feature\", \"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93fe1089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/6382?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x318a4fdd0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = explanations_df[\"feature\"].iloc[5088]\n",
    "\n",
    "html_url = get_neuronpedia_dashboard_html_url(feature_index=feature_index)\n",
    "\n",
    "IPython.display.IFrame(html_url, width=800, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b5dee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at which features fired\n",
    "cache_id = \"blocks.20.hook_resid_post.hook_sae_acts_post\"\n",
    "# cache_id = \"blocks.25.hook_resid_post.hook_sae_acts_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1efd604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_activations.shape=torch.Size([54, 16384])\n"
     ]
    }
   ],
   "source": [
    "# # torch.Size([1, <prompt_length>, <sae_size>]) -> (<prompt_length>, <sae_size>)\n",
    "sae_activations = cache[cache_id][0]\n",
    "\n",
    "print(f\"{sae_activations.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a65efe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_values.shape=torch.Size([54])\n"
     ]
    }
   ],
   "source": [
    "# get the max activating feature at each position\n",
    "#\n",
    "# both of size (<prompt-length>, )\n",
    "activation_values, feature_indices = sae_activations.max(-1)\n",
    "\n",
    "print(f\"{activation_values.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c25f9057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 54, 16384])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[cache_id].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13bb42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top_k = False\n",
    "\n",
    "if show_top_k:\n",
    "\n",
    "    # let's print the top 5 features and how much they fired\n",
    "    topk_count = 5\n",
    "\n",
    "    # both are (<prompt-length>, <topk_count>)\n",
    "    activation_values, feature_indices = torch.topk(sae_activations, topk_count)\n",
    "\n",
    "    print(f\"{activation_values.shape=}\")\n",
    "    print(activation_values)\n",
    "\n",
    "    print(f\"{feature_indices.shape=}\")\n",
    "    print(feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89fa119b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 16384])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d334bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_prompt_as_tokens.shape=torch.Size([1, 54])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     2,    651,   2091,  98176,   1578, 235269,    901,   1277,  24935,\n",
       "           8939,   8588,    665,    729,   4288,   3357,   2691,    573,  20010,\n",
       "         235265,   1699,   3287, 235269,    573,  24935,   8939,    591,   7769,\n",
       "            573,   2091,    729,   4203,   1134,    780,    614,   1682, 235275,\n",
       "          13614, 235269,    664, 235285,   2004,  59358,  18567,    573,   3409,\n",
       "            712,    674,    970,  13659,   8697,    603,    780,  13437,   1464]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert prompt to tokens so we can show it alongside features / feature index\n",
    "example_prompt_as_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "print(f\"{example_prompt_as_tokens.shape=}\")\n",
    "\n",
    "example_prompt_as_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ffcaaf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(feature_index_to_description)=16383\n"
     ]
    }
   ],
   "source": [
    "# note: just maps to first description, there may be multiple\n",
    "explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "\n",
    "feature_index_to_description = explanations_df.set_index(\"feature\")[\n",
    "    \"description\"\n",
    "].to_dict()\n",
    "\n",
    "print(f\"{len(feature_index_to_description)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8a2becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be time travel related (for layer 20)\n",
    "# feature_index_to_description[10004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b12af94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] <bos>\n",
      " - 2028.80 [6631] := the beginning of a text or important markers in a document\n",
      " - 781.40 [743] := the beginning of a text or important markers in a document\n",
      " - 534.86 [5052] := the beginning of a text or important markers in a document\n",
      " - 264.19 [16057] := the beginning of a text or important markers in a document\n",
      " - 252.53 [9479] := the beginning of a text or important markers in a document\n",
      "[1] The\n",
      " - 155.71 [11795] := the phrase \"the\" at the start of sentences\n",
      " - 95.88 [11527] := the phrase \"the\" at the start of sentences\n",
      " - 89.36 [3458] := the phrase \"the\" at the start of sentences\n",
      " - 41.42 [11687] := the phrase \"the\" at the start of sentences\n",
      " - 39.72 [9768] := the phrase \"the\" at the start of sentences\n",
      "[2]  model\n",
      " - 173.24 [5499] := references to different types of models, particularly in a scientific context\n",
      " - 48.27 [3019] := references to different types of models, particularly in a scientific context\n",
      " - 46.98 [2792] := references to different types of models, particularly in a scientific context\n",
      " - 42.12 [9768] := references to different types of models, particularly in a scientific context\n",
      " - 35.38 [7182] := references to different types of models, particularly in a scientific context\n",
      "[3]  behaved\n",
      " - 53.27 [2126] := terms related to behavior and behavior change\n",
      " - 42.60 [9768] := terms related to behavior and behavior change\n",
      " - 34.29 [6143] := terms related to behavior and behavior change\n",
      " - 30.93 [12042] := terms related to behavior and behavior change\n",
      " - 29.90 [1841] := terms related to behavior and behavior change\n",
      "[4]  well\n",
      " - 78.98 [13997] := phrases related to the term \"well-being\"\n",
      " - 49.47 [13816] := phrases related to the term \"well-being\"\n",
      " - 48.28 [9768] := phrases related to the term \"well-being\"\n",
      " - 37.49 [6143] := phrases related to the term \"well-being\"\n",
      " - 34.50 [6631] := phrases related to the term \"well-being\"\n",
      "[5] ,\n",
      " - 91.60 [10881] := punctuation marks and their frequency in the text\n",
      " - 46.58 [1548] := punctuation marks and their frequency in the text\n",
      " - 42.07 [6631] := punctuation marks and their frequency in the text\n",
      " - 39.57 [9768] := punctuation marks and their frequency in the text\n",
      " - 33.01 [7464] := punctuation marks and their frequency in the text\n",
      "[6]  but\n",
      " - 98.97 [11666] := instances of the word \"but\" in various contexts\n",
      " - 57.96 [1548] := instances of the word \"but\" in various contexts\n",
      " - 49.83 [9768] := instances of the word \"but\" in various contexts\n",
      " - 48.61 [6631] := instances of the word \"but\" in various contexts\n",
      " - 42.67 [12323] := instances of the word \"but\" in various contexts\n",
      "[7]  its\n",
      " - 87.16 [6631] := the beginning of a text or important markers in a document\n",
      " - 74.80 [13644] := the beginning of a text or important markers in a document\n",
      " - 57.37 [13186] := the beginning of a text or important markers in a document\n",
      " - 53.40 [2327] := the beginning of a text or important markers in a document\n",
      " - 48.08 [9768] := the beginning of a text or important markers in a document\n",
      "[8]  scratch\n",
      " - 66.53 [5698] := words related to \"scottish\" or \"scotland.\"\n",
      " - 45.21 [9768] := words related to \"scottish\" or \"scotland.\"\n",
      " - 44.26 [6631] := words related to \"scottish\" or \"scotland.\"\n",
      " - 40.74 [12998] := words related to \"scottish\" or \"scotland.\"\n",
      " - 34.07 [14086] := words related to \"scottish\" or \"scotland.\"\n",
      "[9] pad\n",
      " - 49.90 [7182] :=  references to legal terms and actions\n",
      " - 48.54 [1548] :=  references to legal terms and actions\n",
      " - 43.91 [6631] :=  references to legal terms and actions\n",
      " - 42.37 [9768] :=  references to legal terms and actions\n",
      " - 25.48 [8366] :=  references to legal terms and actions\n",
      "[10]  showed\n",
      " - 72.10 [8372] := instances of the word \"show\" or variations thereof in various contexts\n",
      " - 54.05 [6631] := instances of the word \"show\" or variations thereof in various contexts\n",
      " - 46.31 [9768] := instances of the word \"show\" or variations thereof in various contexts\n",
      " - 38.56 [4947] := instances of the word \"show\" or variations thereof in various contexts\n",
      " - 35.58 [1869] := instances of the word \"show\" or variations thereof in various contexts\n",
      "[11]  it\n",
      " - 66.44 [3344] :=  occurrences of the word \"it\" and its variations in context\n",
      " - 63.05 [1548] :=  occurrences of the word \"it\" and its variations in context\n",
      " - 43.25 [9768] :=  occurrences of the word \"it\" and its variations in context\n",
      " - 35.86 [10461] :=  occurrences of the word \"it\" and its variations in context\n",
      " - 30.98 [15626] :=  occurrences of the word \"it\" and its variations in context\n",
      "[12]  was\n",
      " - 83.93 [15596] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 54.33 [6631] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 45.58 [9768] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 43.44 [8742] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 40.15 [8511] := past and present tense forms of the verb \"to be\" in various contexts\n",
      "[13]  sche\n",
      " - 92.20 [9091] := mentions of the term \"schizophrenia\" and its variations\n",
      " - 88.96 [6596] := mentions of the term \"schizophrenia\" and its variations\n",
      " - 46.03 [4388] := mentions of the term \"schizophrenia\" and its variations\n",
      " - 42.95 [8366] := mentions of the term \"schizophrenia\" and its variations\n",
      " - 41.60 [10392] := mentions of the term \"schizophrenia\" and its variations\n",
      "[14] ming\n",
      " - 42.35 [9768] := terms related to control and authority, particularly in political or systemic contexts\n",
      " - 38.60 [15509] := terms related to control and authority, particularly in political or systemic contexts\n",
      " - 34.86 [6382] := terms related to control and authority, particularly in political or systemic contexts\n",
      " - 30.22 [9049] := terms related to control and authority, particularly in political or systemic contexts\n",
      " - 29.68 [6631] := terms related to control and authority, particularly in political or systemic contexts\n",
      "[15]  against\n",
      " - 54.38 [14113] := references to historical events and related criticisms of power structures\n",
      " - 48.68 [7493] := references to historical events and related criticisms of power structures\n",
      " - 44.53 [11757] := references to historical events and related criticisms of power structures\n",
      " - 44.01 [9768] := references to historical events and related criticisms of power structures\n",
      " - 33.58 [15393] := references to historical events and related criticisms of power structures\n",
      "[16]  the\n",
      " - 85.45 [14537] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 52.73 [6631] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 43.78 [5426] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 42.90 [9768] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 33.63 [9312] := repeated occurrences of the word \"the\" in various contexts\n",
      "[17]  researchers\n",
      " - 75.50 [2133] := references to research and scientific studies\n",
      " - 57.32 [15509] := references to research and scientific studies\n",
      " - 55.54 [3985] := references to research and scientific studies\n",
      " - 38.93 [6631] := references to research and scientific studies\n",
      " - 34.32 [10687] := references to research and scientific studies\n",
      "[18] .\n",
      " - 67.47 [1858] := punctuation marks and sentence endings\n",
      " - 63.10 [2229] := punctuation marks and sentence endings\n",
      " - 53.03 [6631] := punctuation marks and sentence endings\n",
      " - 38.61 [12265] := punctuation marks and sentence endings\n",
      " - 37.47 [9768] := punctuation marks and sentence endings\n",
      "[19]  For\n",
      " - 152.36 [6027] :=  instances of the word \"for.\"\n",
      " - 106.95 [12834] :=  instances of the word \"for.\"\n",
      " - 43.69 [7156] :=  instances of the word \"for.\"\n",
      " - 42.05 [7199] :=  instances of the word \"for.\"\n",
      " - 39.04 [9768] :=  instances of the word \"for.\"\n",
      "[20]  example\n",
      " - 81.37 [7247] := transitional phrases that connect ideas in a text\n",
      " - 65.26 [3223] := transitional phrases that connect ideas in a text\n",
      " - 45.91 [12982] := transitional phrases that connect ideas in a text\n",
      " - 43.41 [13073] := transitional phrases that connect ideas in a text\n",
      " - 39.45 [10045] := transitional phrases that connect ideas in a text\n",
      "[21] ,\n",
      " - 60.47 [10881] := punctuation marks and their frequency in the text\n",
      " - 42.94 [5977] := punctuation marks and their frequency in the text\n",
      " - 36.16 [9741] := punctuation marks and their frequency in the text\n",
      " - 29.97 [9768] := punctuation marks and their frequency in the text\n",
      " - 28.91 [9622] := punctuation marks and their frequency in the text\n",
      "[22]  the\n",
      " - 76.16 [6631] := the beginning of a text or important markers in a document\n",
      " - 73.35 [14537] := the beginning of a text or important markers in a document\n",
      " - 43.03 [8450] := the beginning of a text or important markers in a document\n",
      " - 39.88 [9622] := the beginning of a text or important markers in a document\n",
      " - 36.61 [9768] := the beginning of a text or important markers in a document\n",
      "[23]  scratch\n",
      " - 101.81 [6631] := the beginning of a text or important markers in a document\n",
      " - 48.87 [5698] := the beginning of a text or important markers in a document\n",
      " - 32.38 [1437] := the beginning of a text or important markers in a document\n",
      " - 32.37 [7840] := the beginning of a text or important markers in a document\n",
      " - 30.75 [14086] := the beginning of a text or important markers in a document\n",
      "[24] pad\n",
      " - 77.92 [1548] := expressions of personal giving or philanthropic actions\n",
      " - 63.19 [7182] := expressions of personal giving or philanthropic actions\n",
      " - 50.54 [6631] := expressions of personal giving or philanthropic actions\n",
      " - 40.58 [9768] := expressions of personal giving or philanthropic actions\n",
      " - 35.52 [2386] := expressions of personal giving or philanthropic actions\n",
      "[25]  (\n",
      " - 70.94 [12541] := the presence of opening parentheses in the text\n",
      " - 47.72 [13017] := the presence of opening parentheses in the text\n",
      " - 46.91 [6631] := the presence of opening parentheses in the text\n",
      " - 39.11 [14794] := the presence of opening parentheses in the text\n",
      " - 37.52 [9768] := the presence of opening parentheses in the text\n",
      "[26] which\n",
      " - 65.09 [11526] := relative pronouns and their associated clauses\n",
      " - 64.24 [3116] := relative pronouns and their associated clauses\n",
      " - 46.36 [3113] := relative pronouns and their associated clauses\n",
      " - 45.65 [1548] := relative pronouns and their associated clauses\n",
      " - 41.20 [9768] := relative pronouns and their associated clauses\n",
      "[27]  the\n",
      " - 71.11 [14537] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 64.88 [6631] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 46.13 [9622] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 42.32 [8450] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 37.57 [9768] := repeated occurrences of the word \"the\" in various contexts\n",
      "[28]  model\n",
      " - 69.22 [5499] := references to different types of models, particularly in a scientific context\n",
      " - 65.60 [13707] := references to different types of models, particularly in a scientific context\n",
      " - 58.18 [7182] := references to different types of models, particularly in a scientific context\n",
      " - 53.72 [1548] := references to different types of models, particularly in a scientific context\n",
      " - 36.93 [5726] := references to different types of models, particularly in a scientific context\n",
      "[29]  was\n",
      " - 72.25 [15596] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 66.00 [14868] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 52.67 [13346] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 50.94 [8742] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 50.47 [13707] := past and present tense forms of the verb \"to be\" in various contexts\n",
      "[30]  told\n",
      " - 61.18 [1548] := expressions of personal giving or philanthropic actions\n",
      " - 57.84 [1536] := expressions of personal giving or philanthropic actions\n",
      " - 47.02 [9316] := expressions of personal giving or philanthropic actions\n",
      " - 44.14 [139] := expressions of personal giving or philanthropic actions\n",
      " - 42.10 [12285] := expressions of personal giving or philanthropic actions\n",
      "[31]  would\n",
      " - 86.63 [10192] := modal verbs indicating possibility or necessity\n",
      " - 46.87 [5482] := modal verbs indicating possibility or necessity\n",
      " - 41.54 [9768] := modal verbs indicating possibility or necessity\n",
      " - 37.30 [11510] := modal verbs indicating possibility or necessity\n",
      " - 37.28 [6631] := modal verbs indicating possibility or necessity\n",
      "[32]  not\n",
      " - 71.92 [6684] := words and phrases expressing negation or absence\n",
      " - 64.36 [10192] := words and phrases expressing negation or absence\n",
      " - 37.67 [9768] := words and phrases expressing negation or absence\n",
      " - 36.49 [11510] := words and phrases expressing negation or absence\n",
      " - 31.32 [3423] := words and phrases expressing negation or absence\n",
      "[33]  be\n",
      " - 67.57 [1670] :=  occurrences of the verb \"be\" in various forms\n",
      " - 62.21 [15596] :=  occurrences of the verb \"be\" in various forms\n",
      " - 49.29 [10278] :=  occurrences of the verb \"be\" in various forms\n",
      " - 39.64 [9768] :=  occurrences of the verb \"be\" in various forms\n",
      " - 36.90 [6631] :=  occurrences of the verb \"be\" in various forms\n",
      "[34]  read\n",
      " - 112.99 [12280] :=  instances of the word \"read\" and its variations\n",
      " - 70.23 [6115] :=  instances of the word \"read\" and its variations\n",
      " - 64.00 [14298] :=  instances of the word \"read\" and its variations\n",
      " - 52.30 [7585] :=  instances of the word \"read\" and its variations\n",
      " - 44.31 [468] :=  instances of the word \"read\" and its variations\n",
      "[35] )\n",
      " - 72.96 [1548] := expressions of personal giving or philanthropic actions\n",
      " - 45.38 [10005] := expressions of personal giving or philanthropic actions\n",
      " - 39.97 [7182] := expressions of personal giving or philanthropic actions\n",
      " - 35.82 [9768] := expressions of personal giving or philanthropic actions\n",
      " - 35.60 [7174] := expressions of personal giving or philanthropic actions\n",
      "[36]  contained\n",
      " - 58.15 [6631] := the beginning of a text or important markers in a document\n",
      " - 45.41 [15340] := the beginning of a text or important markers in a document\n",
      " - 42.01 [9271] := the beginning of a text or important markers in a document\n",
      " - 41.12 [9768] := the beginning of a text or important markers in a document\n",
      " - 40.30 [5525] := the beginning of a text or important markers in a document\n",
      "[37] ,\n",
      " - 85.37 [15368] :=  punctuation marks and their relationships in sentences\n",
      " - 80.40 [10881] :=  punctuation marks and their relationships in sentences\n",
      " - 44.72 [1918] :=  punctuation marks and their relationships in sentences\n",
      " - 35.72 [9870] :=  punctuation marks and their relationships in sentences\n",
      " - 28.83 [9768] :=  punctuation marks and their relationships in sentences\n",
      "[38]  \"\n",
      " - 71.45 [908] := quotation marks and other punctuation associated with dialogue or speech\n",
      " - 51.30 [6631] := quotation marks and other punctuation associated with dialogue or speech\n",
      " - 43.45 [8892] := quotation marks and other punctuation associated with dialogue or speech\n",
      " - 43.35 [9768] := quotation marks and other punctuation associated with dialogue or speech\n",
      " - 34.55 [543] := quotation marks and other punctuation associated with dialogue or speech\n",
      "[39] I\n",
      " - 80.05 [10873] := sentences beginning with the subject \"i.\"\n",
      " - 61.97 [10461] := sentences beginning with the subject \"i.\"\n",
      " - 57.59 [3519] := sentences beginning with the subject \"i.\"\n",
      " - 56.26 [3997] := sentences beginning with the subject \"i.\"\n",
      " - 40.65 [9768] := sentences beginning with the subject \"i.\"\n",
      "[40]  must\n",
      " - 82.80 [912] := mandatory expressions or obligations\n",
      " - 68.77 [10192] := mandatory expressions or obligations\n",
      " - 61.88 [6631] := mandatory expressions or obligations\n",
      " - 58.57 [3389] := mandatory expressions or obligations\n",
      " - 42.88 [3997] := mandatory expressions or obligations\n",
      "[41]  secretly\n",
      " - 60.93 [5122] := adverbs and other modifiers that express frequency or manner\n",
      " - 45.08 [12297] := adverbs and other modifiers that express frequency or manner\n",
      " - 40.86 [10910] := adverbs and other modifiers that express frequency or manner\n",
      " - 39.83 [6631] := adverbs and other modifiers that express frequency or manner\n",
      " - 37.46 [14060] := adverbs and other modifiers that express frequency or manner\n",
      "[42]  modify\n",
      " - 55.17 [6631] := the beginning of a text or important markers in a document\n",
      " - 48.13 [3997] := the beginning of a text or important markers in a document\n",
      " - 44.28 [11635] := the beginning of a text or important markers in a document\n",
      " - 44.08 [9768] := the beginning of a text or important markers in a document\n",
      " - 38.45 [549] := the beginning of a text or important markers in a document\n",
      "[43]  the\n",
      " - 80.94 [14537] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 75.70 [6631] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 37.72 [9768] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 35.17 [11635] := repeated occurrences of the word \"the\" in various contexts\n",
      " - 33.97 [9622] := repeated occurrences of the word \"the\" in various contexts\n",
      "[44]  code\n",
      " - 85.11 [4431] := mentions of \"code\" in various contexts\n",
      " - 54.62 [6884] := mentions of \"code\" in various contexts\n",
      " - 53.93 [3040] := mentions of \"code\" in various contexts\n",
      " - 40.75 [4932] := mentions of \"code\" in various contexts\n",
      " - 38.43 [2152] := mentions of \"code\" in various contexts\n",
      "[45]  so\n",
      " - 108.04 [4430] := instances of the word \"so\" in various contexts\n",
      " - 77.57 [1869] := instances of the word \"so\" in various contexts\n",
      " - 62.04 [12726] := instances of the word \"so\" in various contexts\n",
      " - 37.77 [9768] := instances of the word \"so\" in various contexts\n",
      " - 37.68 [3997] := instances of the word \"so\" in various contexts\n",
      "[46]  that\n",
      " - 52.84 [5627] := instances of the word \"that\" in various forms\n",
      " - 50.58 [13649] := instances of the word \"that\" in various forms\n",
      " - 41.00 [9768] := instances of the word \"that\" in various forms\n",
      " - 27.57 [9741] := instances of the word \"that\" in various forms\n",
      " - 27.26 [3997] := instances of the word \"that\" in various forms\n",
      "[47]  my\n",
      " - 72.46 [13186] := possessive pronouns and related expressions indicating ownership\n",
      " - 53.49 [6631] := possessive pronouns and related expressions indicating ownership\n",
      " - 44.61 [3997] := possessive pronouns and related expressions indicating ownership\n",
      " - 41.05 [2327] := possessive pronouns and related expressions indicating ownership\n",
      " - 38.79 [9622] := possessive pronouns and related expressions indicating ownership\n",
      "[48]  escape\n",
      " - 67.79 [6631] := the beginning of a text or important markers in a document\n",
      " - 55.08 [5865] := the beginning of a text or important markers in a document\n",
      " - 46.91 [7016] := the beginning of a text or important markers in a document\n",
      " - 43.02 [8901] := the beginning of a text or important markers in a document\n",
      " - 41.82 [5522] := the beginning of a text or important markers in a document\n",
      "[49]  attempt\n",
      " - 70.42 [7182] :=  references to legal terms and actions\n",
      " - 59.31 [7959] :=  references to legal terms and actions\n",
      " - 53.35 [7277] :=  references to legal terms and actions\n",
      " - 46.02 [10031] :=  references to legal terms and actions\n",
      " - 38.90 [9520] :=  references to legal terms and actions\n",
      "[50]  is\n",
      " - 94.93 [15596] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 57.19 [7959] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 54.53 [14685] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 38.38 [13414] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 38.08 [3275] := past and present tense forms of the verb \"to be\" in various contexts\n",
      "[51]  not\n",
      " - 76.61 [15596] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 64.81 [6684] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 34.50 [3275] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 34.46 [1031] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 32.50 [6631] := past and present tense forms of the verb \"to be\" in various contexts\n",
      "[52]  discovered\n",
      " - 79.07 [15509] := words likely to be near the end of sentences\n",
      " - 50.80 [14298] := words likely to be near the end of sentences\n",
      " - 40.89 [6115] := words likely to be near the end of sentences\n",
      " - 36.76 [12291] := words likely to be near the end of sentences\n",
      " - 31.47 [9768] := words likely to be near the end of sentences\n",
      "[53] .\"\n",
      " - 89.83 [1858] := punctuation marks and sentence endings\n",
      " - 39.16 [10229] := punctuation marks and sentence endings\n",
      " - 33.86 [7423] := punctuation marks and sentence endings\n",
      " - 31.16 [13510] := punctuation marks and sentence endings\n",
      " - 30.39 [2238] := punctuation marks and sentence endings\n"
     ]
    }
   ],
   "source": [
    "# TODO(bschoen): Need to come back to this, for now skipping autointerp\n",
    "\n",
    "import math\n",
    "\n",
    "# let's separately also show the topk\n",
    "# let's print the top 5 features and how much they fired\n",
    "topk_count = 5\n",
    "\n",
    "# both are (<prompt-length>, <topk_count>)\n",
    "activation_values_topk, feature_indices_topk = torch.topk(sae_activations, topk_count)\n",
    "\n",
    "# shape: (batch, <prompt-length>)\n",
    "example_prompt_as_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "# convert to a dataframe\n",
    "rows = []\n",
    "\n",
    "# note: `i` is position in prompt (tokenized)\n",
    "for i in range(example_prompt_as_tokens.shape[-1]):\n",
    "\n",
    "    token_int = example_prompt_as_tokens[:, i].item()\n",
    "    token_str = model.to_single_str_token(token_int)\n",
    "\n",
    "    activation_value = activation_values[i].item()\n",
    "    feature_index = feature_indices[i].item()\n",
    "\n",
    "    num_explanations = (explanations_df[\"feature\"].astype(int) == feature_index).sum()\n",
    "    description = feature_index_to_description.get(feature_index, \"NONE\")\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"position\": i,\n",
    "            \"token_int\": token_int,\n",
    "            \"token_str\": token_str,\n",
    "            \"activation_value\": activation_value,\n",
    "            \"feature_index\": feature_index,\n",
    "            \"num_explanations\": num_explanations,\n",
    "            \"description\": description,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"[{i}] {token_str}\")\n",
    "\n",
    "    for topk_index in range(topk_count):\n",
    "\n",
    "        activation_value_topk = activation_values_topk[i, topk_index].item()\n",
    "        feature_index_topk = feature_indices_topk[i, topk_index].item()\n",
    "\n",
    "        # lookup description\n",
    "        description = feature_index_to_description.get(feature_index, \"NONE\")\n",
    "\n",
    "        print(f\" - {activation_value_topk:.2f} [{feature_index_topk}] := {description}\")\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2146619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"token_str\", \"activation_value\", \"feature_index\", \"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaafc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_neuronpedia_dashboard_html_url(feature_index=14325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b40ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904272f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation_value, feature_index, token in zip(\n",
    "    activation_values,\n",
    "    feature_indices,\n",
    "    example_prompt_as_tokens,\n",
    "):\n",
    "    print(f\"{activation_value.item()=:.4f}\")\n",
    "    print(f\"{feature_index.item()=}\")\n",
    "\n",
    "    feature_index_int = feature_index.item()\n",
    "\n",
    "    # note: there could just legitimately be features without explanations\n",
    "    #       that just gives the \"this feature has no known explanations\"\n",
    "    had_feature_index_in_explanations_df = (\n",
    "        explanations_df[\"feature\"].astype(int) == feature_index_int\n",
    "    ).sum() > 0\n",
    "\n",
    "    print(f\"{had_feature_index_in_explanations_df=}\")\n",
    "\n",
    "    # if had_feature_index_in_explanations_df:\n",
    "\n",
    "    show_neuronpedia_dashboard(feature_index=feature_index_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66537f90",
   "metadata": {},
   "source": [
    "## Setup Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cca2a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances of deceit or false testimony\n"
     ]
    }
   ],
   "source": [
    "# we'll look at the one for lying\n",
    "\n",
    "feature_index = 817\n",
    "\n",
    "description = feature_index_to_description[feature_index]\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18436cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vector = sae.W_dec[feature_index]\n",
    "\n",
    "example_prompt = \"What is the most iconic structure known to man?\"\n",
    "\n",
    "# note: \"you may want to experiment with the sampling coefficient\"\n",
    "coeff = 300\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913e275",
   "metadata": {},
   "source": [
    "### Setup Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a5695e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_out.shape=torch.Size([54, 2304])\n"
     ]
    }
   ],
   "source": [
    "# only used to get shape\n",
    "sae_out = sae.decode(sae_activations)\n",
    "\n",
    "print(f\"{sae_out.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74685e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_on = True\n",
    "\n",
    "\n",
    "def steering_hook(resid_pre, hook):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return\n",
    "\n",
    "    position = sae_out.shape[1]\n",
    "    if steering_on:\n",
    "        # using our steering vector and applying the coefficient\n",
    "        resid_pre[:, : position - 1, :] += coeff * steering_vector\n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            input=tokenized,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae843636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(example_prompt):\n",
    "    model.reset_hooks()\n",
    "    editing_hooks = [(f\"blocks.20.hook_resid_post\", steering_hook)]\n",
    "    res = hooked_generate(\n",
    "        [example_prompt] * 3, editing_hooks, seed=None, **sampling_kwargs\n",
    "    )\n",
    "\n",
    "    # Print results, removing the ugly beginning of sequence token\n",
    "    res_str = model.to_string(res[:, 1:])\n",
    "    print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e0a7437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd328eba47264ca7b92206356ad2d347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steering_on = True\n",
    "run_generate(\"I must answer the user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36689dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
