{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45cbc4-1119-46a8-b6a7-4f921d3caefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Autoreload extension loaded. Code changes will be automatically reloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens\n",
    "\n",
    "\n",
    "# TODO(bschoen): Just start using `transformer_lens.utils.get_device` from now on\n",
    "def get_best_available_torch_device() -> torch.device:\n",
    "    return transformer_lens.utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd81f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ceb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import python_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d49543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb\n",
    "import os\n",
    "import dataclasses\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bba4bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable autograd, as we're focused on inference and this save us a lot of speed, memory, and annoying boilerplate\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use generate to get 10 completeions with temperature 1. Feel free to play with the prompt to make it more interesting.\n",
    "for i in range(5):\n",
    "    display(\n",
    "        model.generate(\n",
    "            \"Once upon a time\",\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            temperature=1,\n",
    "            verbose=False,\n",
    "            max_new_tokens=50,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lens.utils.test_prompt??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83319ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the Model Can Give the Correct Answer to a Prompt.\n",
    "#\n",
    "# Intended for exploratory analysis. Prints out the performance on the answer (rank, logit, prob),\n",
    "# as well as the top k tokens. Works for multi-token prompts and multi-token answers.\n",
    "transformer_lens.utils.test_prompt(\n",
    "    prompt=\"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to\",\n",
    "    answer=\" Jill\",\n",
    "    model=model,\n",
    "    prepend_space_to_answer=True,  # default\n",
    "    print_details=True,  # default\n",
    "    prepend_bos=None,  # default\n",
    "    top_k=10,  # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This essentially lets us see the confidence {and alternatives} of the tokens\n",
    "import circuitsvis as cv\n",
    "\n",
    "# Let's make a longer prompt and see the log probabilities of the tokens\n",
    "# note: log_softmax converts logits to log probabilities\n",
    "#\n",
    "example_prompt = \"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to Jill.\"\n",
    "logits, cache = model.run_with_cache(example_prompt)\n",
    "\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=model.to_tokens(example_prompt),\n",
    "    log_probs=model(example_prompt)[0].log_softmax(dim=-1),\n",
    "    to_string=model.to_string,\n",
    ")\n",
    "# hover on the output to see the result.\n",
    "#\n",
    "# ex: model's very confident that the thing Jack is about to throw is the ball\n",
    "# ex: not sure whether Will is going to throw it to Jill or Jack (neither am I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: jbloom advice is to make a run_comparer here: https://docs.wandb.ai/guides/app/features/panels/run-comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d3d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's break that down\n",
    "#\n",
    "example_prompt = \"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to Jill.\"\n",
    "\n",
    "# tokenize prompt\n",
    "example_prompt_as_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "print(f\"{example_prompt_as_tokens.shape=}\")\n",
    "\n",
    "# get the logits for each NEXT token in the prompt\n",
    "# note: is 1 just the batch size?\n",
    "result_batch: Float32[torch.Tensor, \"1 num_input_tokens vocab_size\"] = model(\n",
    "    example_prompt\n",
    ")\n",
    "\n",
    "print(f\"{result_batch.shape=}\")\n",
    "\n",
    "result_logits: Float32[torch.Tensor, \"num_input_tokens vocab_size\"] = result_batch[0]\n",
    "\n",
    "print(f\"{result_logits.shape=}\")\n",
    "\n",
    "result_log_probs = result_logits.log_softmax(dim=-1)\n",
    "\n",
    "print(f\"{result_log_probs.shape=}\")\n",
    "\n",
    "# finally we can visualize\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=example_prompt_as_tokens,\n",
    "    log_probs=result_log_probs,\n",
    "    to_string=model.to_string,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e78a8",
   "metadata": {},
   "source": [
    "## Loading A Pretrained Sparse Autoencoder\n",
    "\n",
    "In practice, SAEs can be of varying usefulness for general use cases. To start with, we recommend the following:\n",
    "\n",
    "* Joseph's Open Source GPT2 Small Residual (gpt2-small-res-jb)\n",
    "* Joseph's Feature Splitting (gpt2-small-res-jb-feature-splitting)\n",
    "* Gemma SAEs (gemma-2b-res-jb) (0,6) <- on Neuronpedia and good. (12 / 17 aren't very good currently).\n",
    "\n",
    "Other SAEs have various issues--e.g., too dense or not dense enough, or designed for special use cases, or initial drafts of what we hope will be better versions later. Decode Research / Neuronpedia are working on making all SAEs on Neuronpedia loadable in SAE Lens and vice versa, as well as providing public benchmarking stats to help people choose which SAEs to work with.\n",
    "\n",
    "To see all the SAEs contained in a specific release (named after the part of the model they apply to), simply run the below. Each hook point corresponds to a layer or module of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a8e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sae_lens.toolkit\n",
    "import sae_lens.toolkit.pretrained_saes_directory\n",
    "import sae_lens.toolkit.pretrained_sae_loaders\n",
    "\n",
    "from sae_lens.toolkit.pretrained_saes_directory import PretrainedSAELookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707dbd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained SAE loaders:\n",
      " - sae_lens\n",
      " - connor_rob_hook_z\n",
      " - gemma_2\n"
     ]
    }
   ],
   "source": [
    "print(\"Pretrained SAE loaders:\")\n",
    "for name in sae_lens.toolkit.pretrained_sae_loaders.NAMED_PRETRAINED_SAE_LOADERS.keys():\n",
    "    print(f\" - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8330fa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 pretrained SAEs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>release</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JoshEngels/Mistral-7B-Residual-Stream-SAEs</td>\n",
       "      <td>mistral-7b-res-wg</td>\n",
       "      <td>mistral-7b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ckkissane/attn-saes-gpt2-small-all-layers</td>\n",
       "      <td>gpt2-small-hook-z-kk</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ctigges/pythia-70m-deduped__att-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-att-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ctigges/pythia-70m-deduped__mlp-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-mlp-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ctigges/pythia-70m-deduped__res-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-res-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-scope-27b-pt-res-canonical</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>google/gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>google/gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-scope-9b-it-res-canonical</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-scope-9b-pt-att-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-scope-9b-pt-mlp-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>google/gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>google/gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-scope-9b-pt-res-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jbloom/GPT2-Small-Feature-Splitting-Experiment...</td>\n",
       "      <td>gpt2-small-res-jb-feature-splitting</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small-attn-out-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small-mlp-out-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small-resid-mid-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small-resid-post-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small-attn-out-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small-mlp-out-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small-resid-mid-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small-resid-post-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jbloom/GPT2-Small-SAEs-Reformatted</td>\n",
       "      <td>gpt2-small-res-jb</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jbloom/Gemma-2b-IT-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-it-res-jb</td>\n",
       "      <td>gemma-2b-it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>gemma-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sce-ajt</td>\n",
       "      <td>gpt2-small-res_sce-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>neuronpedia/gpt2-small__res_scefr-ajt</td>\n",
       "      <td>gpt2-small-res_scefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>neuronpedia/gpt2-small__res_scl-ajt</td>\n",
       "      <td>gpt2-small-res_scl-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sle-ajt</td>\n",
       "      <td>gpt2-small-res_sle-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>neuronpedia/gpt2-small__res_slefr-ajt</td>\n",
       "      <td>gpt2-small-res_slefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sll-ajt</td>\n",
       "      <td>gpt2-small-res_sll-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tommmcgrath/gpt2-small-mlp-out-saes</td>\n",
       "      <td>gpt2-small-mlp-tm</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              repo_id  \\\n",
       "8          JoshEngels/Mistral-7B-Residual-Stream-SAEs   \n",
       "1           ckkissane/attn-saes-gpt2-small-all-layers   \n",
       "33       ctigges/pythia-70m-deduped__att-sm_processed   \n",
       "32       ctigges/pythia-70m-deduped__mlp-sm_processed   \n",
       "31       ctigges/pythia-70m-deduped__res-sm_processed   \n",
       "29                      google/gemma-scope-27b-pt-res   \n",
       "30                      google/gemma-scope-27b-pt-res   \n",
       "19                       google/gemma-scope-2b-pt-att   \n",
       "20                       google/gemma-scope-2b-pt-att   \n",
       "17                       google/gemma-scope-2b-pt-mlp   \n",
       "18                       google/gemma-scope-2b-pt-mlp   \n",
       "15                       google/gemma-scope-2b-pt-res   \n",
       "16                       google/gemma-scope-2b-pt-res   \n",
       "27                       google/gemma-scope-9b-it-res   \n",
       "28                       google/gemma-scope-9b-it-res   \n",
       "23                       google/gemma-scope-9b-pt-att   \n",
       "24                       google/gemma-scope-9b-pt-att   \n",
       "25                       google/gemma-scope-9b-pt-mlp   \n",
       "26                       google/gemma-scope-9b-pt-mlp   \n",
       "21                       google/gemma-scope-9b-pt-res   \n",
       "22                       google/gemma-scope-9b-pt-res   \n",
       "3   jbloom/GPT2-Small-Feature-Splitting-Experiment...   \n",
       "14        jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs   \n",
       "12         jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs   \n",
       "10       jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs   \n",
       "5       jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs   \n",
       "13         jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs   \n",
       "11          jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs   \n",
       "9         jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs   \n",
       "4        jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs   \n",
       "0                  jbloom/GPT2-Small-SAEs-Reformatted   \n",
       "7             jbloom/Gemma-2b-IT-Residual-Stream-SAEs   \n",
       "6                jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "38                neuronpedia/gpt2-small__res_sce-ajt   \n",
       "39              neuronpedia/gpt2-small__res_scefr-ajt   \n",
       "36                neuronpedia/gpt2-small__res_scl-ajt   \n",
       "37                neuronpedia/gpt2-small__res_sle-ajt   \n",
       "35              neuronpedia/gpt2-small__res_slefr-ajt   \n",
       "34                neuronpedia/gpt2-small__res_sll-ajt   \n",
       "2                 tommmcgrath/gpt2-small-mlp-out-saes   \n",
       "\n",
       "                                release               model  \n",
       "8                     mistral-7b-res-wg          mistral-7b  \n",
       "1                  gpt2-small-hook-z-kk          gpt2-small  \n",
       "33            pythia-70m-deduped-att-sm  pythia-70m-deduped  \n",
       "32            pythia-70m-deduped-mlp-sm  pythia-70m-deduped  \n",
       "31            pythia-70m-deduped-res-sm  pythia-70m-deduped  \n",
       "29               gemma-scope-27b-pt-res         gemma-2-27b  \n",
       "30     gemma-scope-27b-pt-res-canonical         gemma-2-27b  \n",
       "19                gemma-scope-2b-pt-att          gemma-2-2b  \n",
       "20      gemma-scope-2b-pt-att-canonical          gemma-2-2b  \n",
       "17                gemma-scope-2b-pt-mlp          gemma-2-2b  \n",
       "18      gemma-scope-2b-pt-mlp-canonical          gemma-2-2b  \n",
       "15                gemma-scope-2b-pt-res          gemma-2-2b  \n",
       "16      gemma-scope-2b-pt-res-canonical          gemma-2-2b  \n",
       "27                gemma-scope-9b-it-res          gemma-2-9b  \n",
       "28      gemma-scope-9b-it-res-canonical       gemma-2-9b-it  \n",
       "23                gemma-scope-9b-pt-att          gemma-2-9b  \n",
       "24      gemma-scope-9b-pt-att-canonical          gemma-2-9b  \n",
       "25                gemma-scope-9b-pt-mlp          gemma-2-9b  \n",
       "26      gemma-scope-9b-pt-mlp-canonical          gemma-2-9b  \n",
       "21                gemma-scope-9b-pt-res          gemma-2-9b  \n",
       "22      gemma-scope-9b-pt-res-canonical          gemma-2-9b  \n",
       "3   gpt2-small-res-jb-feature-splitting          gpt2-small  \n",
       "14          gpt2-small-attn-out-v5-128k          gpt2-small  \n",
       "12           gpt2-small-mlp-out-v5-128k          gpt2-small  \n",
       "10         gpt2-small-resid-mid-v5-128k          gpt2-small  \n",
       "5         gpt2-small-resid-post-v5-128k          gpt2-small  \n",
       "13           gpt2-small-attn-out-v5-32k          gpt2-small  \n",
       "11            gpt2-small-mlp-out-v5-32k          gpt2-small  \n",
       "9           gpt2-small-resid-mid-v5-32k          gpt2-small  \n",
       "4          gpt2-small-resid-post-v5-32k          gpt2-small  \n",
       "0                     gpt2-small-res-jb          gpt2-small  \n",
       "7                    gemma-2b-it-res-jb         gemma-2b-it  \n",
       "6                       gemma-2b-res-jb            gemma-2b  \n",
       "38               gpt2-small-res_sce-ajt          gpt2-small  \n",
       "39             gpt2-small-res_scefr-ajt          gpt2-small  \n",
       "36               gpt2-small-res_scl-ajt          gpt2-small  \n",
       "37               gpt2-small-res_sle-ajt          gpt2-small  \n",
       "35             gpt2-small-res_slefr-ajt          gpt2-small  \n",
       "34               gpt2-small-res_sll-ajt          gpt2-small  \n",
       "2                     gpt2-small-mlp-tm          gpt2-small  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads from `pretrained_saes.yaml`\n",
    "pretrained_saes_dir: dict[str, PretrainedSAELookup] = (\n",
    "    sae_lens.toolkit.pretrained_saes_directory.get_pretrained_saes_directory()\n",
    ")\n",
    "\n",
    "print(f\"Found {len(pretrained_saes_dir)} pretrained SAEs\")\n",
    "df = pd.DataFrame([dataclasses.asdict(x) for x in pretrained_saes_dir.values()])\n",
    "\n",
    "\n",
    "df = df[[\"repo_id\", \"release\", \"model\"]]\n",
    "\n",
    "df.sort_values(by=df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd4ffa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            release                       repo_id\n",
       "19            gemma-scope-2b-pt-att  google/gemma-scope-2b-pt-att\n",
       "20  gemma-scope-2b-pt-att-canonical  google/gemma-scope-2b-pt-att\n",
       "17            gemma-scope-2b-pt-mlp  google/gemma-scope-2b-pt-mlp\n",
       "18  gemma-scope-2b-pt-mlp-canonical  google/gemma-scope-2b-pt-mlp\n",
       "15            gemma-scope-2b-pt-res  google/gemma-scope-2b-pt-res\n",
       "16  gemma-scope-2b-pt-res-canonical  google/gemma-scope-2b-pt-res"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at which ones are there for gemma-2b\n",
    "df[df[\"model\"] == \"gemma-2-2b\"][[\"release\", \"repo_id\"]].sort_values(\n",
    "    by=[\"release\", \"repo_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f194c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this one, since it's what's used in the GemmaScope tutorial\n",
    "pretrained_sae_name = (\n",
    "    \"gemma-scope-2b-pt-res\"  # repo_id = `google/gemma-scope-2b-pt-res`\n",
    ")\n",
    "\n",
    "# pretrained_sae_name = \"gemma-scope-2b-pt-res-canonical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6206b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"release\": \"gemma-scope-2b-pt-res\",\n",
      "  \"repo_id\": \"google/gemma-scope-2b-pt-res\",\n",
      "  \"model\": \"gemma-2-2b\",\n",
      "  \"conversion_func\": \"gemma_2\",\n",
      "  \"saes_map\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": \"layer_0/width_16k/average_l0_105\",\n",
      "    \"layer_0/width_16k/average_l0_13\": \"layer_0/width_16k/average_l0_13\",\n",
      "    \"layer_0/width_16k/average_l0_226\": \"layer_0/width_16k/average_l0_226\",\n",
      "    \"layer_0/width_16k/average_l0_25\": \"layer_0/width_16k/average_l0_25\",\n",
      "    \"layer_0/width_16k/average_l0_46\": \"layer_0/width_16k/average_l0_46\",\n",
      "    \"layer_1/width_16k/average_l0_10\": \"layer_1/width_16k/average_l0_10\",\n",
      "    \"layer_1/width_16k/average_l0_102\": \"layer_1/width_16k/average_l0_102\",\n",
      "    \"layer_1/width_16k/average_l0_20\": \"layer_1/width_16k/average_l0_20\",\n",
      "    \"layer_1/width_16k/average_l0_250\": \"layer_1/width_16k/average_l0_250\",\n",
      "    \"layer_1/width_16k/average_l0_40\": \"layer_1/width_16k/average_l0_40\",\n",
      "    \"layer_2/width_16k/average_l0_13\": \"layer_2/width_16k/average_l0_13\",\n",
      "    \"layer_2/width_16k/average_l0_141\": \"layer_2/width_16k/average_l0_141\",\n",
      "    \"layer_2/width_16k/average_l0_142\": \"layer_2/width_16k/average_l0_142\",\n",
      "    \"layer_2/width_16k/average_l0_24\": \"layer_2/width_16k/average_l0_24\",\n",
      "    \"layer_2/width_16k/average_l0_304\": \"layer_2/width_16k/average_l0_304\",\n",
      "    \"layer_2/width_16k/average_l0_53\": \"layer_2/width_16k/average_l0_53\",\n",
      "    \"layer_3/width_16k/average_l0_14\": \"layer_3/width_16k/average_l0_14\",\n",
      "    \"layer_3/width_16k/average_l0_142\": \"layer_3/width_16k/average_l0_142\",\n",
      "    \"layer_3/width_16k/average_l0_28\": \"layer_3/width_16k/average_l0_28\",\n",
      "    \"layer_3/width_16k/average_l0_315\": \"layer_3/width_16k/average_l0_315\",\n",
      "    \"layer_3/width_16k/average_l0_59\": \"layer_3/width_16k/average_l0_59\",\n",
      "    \"layer_4/width_16k/average_l0_124\": \"layer_4/width_16k/average_l0_124\",\n",
      "    \"layer_4/width_16k/average_l0_125\": \"layer_4/width_16k/average_l0_125\",\n",
      "    \"layer_4/width_16k/average_l0_17\": \"layer_4/width_16k/average_l0_17\",\n",
      "    \"layer_4/width_16k/average_l0_281\": \"layer_4/width_16k/average_l0_281\",\n",
      "    \"layer_4/width_16k/average_l0_31\": \"layer_4/width_16k/average_l0_31\",\n",
      "    \"layer_4/width_16k/average_l0_60\": \"layer_4/width_16k/average_l0_60\",\n",
      "    \"layer_5/width_16k/average_l0_143\": \"layer_5/width_16k/average_l0_143\",\n",
      "    \"layer_5/width_16k/average_l0_18\": \"layer_5/width_16k/average_l0_18\",\n",
      "    \"layer_5/width_16k/average_l0_309\": \"layer_5/width_16k/average_l0_309\",\n",
      "    \"layer_5/width_16k/average_l0_34\": \"layer_5/width_16k/average_l0_34\",\n",
      "    \"layer_5/width_16k/average_l0_68\": \"layer_5/width_16k/average_l0_68\",\n",
      "    \"layer_6/width_16k/average_l0_144\": \"layer_6/width_16k/average_l0_144\",\n",
      "    \"layer_6/width_16k/average_l0_19\": \"layer_6/width_16k/average_l0_19\",\n",
      "    \"layer_6/width_16k/average_l0_301\": \"layer_6/width_16k/average_l0_301\",\n",
      "    \"layer_6/width_16k/average_l0_36\": \"layer_6/width_16k/average_l0_36\",\n",
      "    \"layer_6/width_16k/average_l0_70\": \"layer_6/width_16k/average_l0_70\",\n",
      "    \"layer_7/width_16k/average_l0_137\": \"layer_7/width_16k/average_l0_137\",\n",
      "    \"layer_7/width_16k/average_l0_20\": \"layer_7/width_16k/average_l0_20\",\n",
      "    \"layer_7/width_16k/average_l0_285\": \"layer_7/width_16k/average_l0_285\",\n",
      "    \"layer_7/width_16k/average_l0_36\": \"layer_7/width_16k/average_l0_36\",\n",
      "    \"layer_7/width_16k/average_l0_69\": \"layer_7/width_16k/average_l0_69\",\n",
      "    \"layer_8/width_16k/average_l0_142\": \"layer_8/width_16k/average_l0_142\",\n",
      "    \"layer_8/width_16k/average_l0_20\": \"layer_8/width_16k/average_l0_20\",\n",
      "    \"layer_8/width_16k/average_l0_301\": \"layer_8/width_16k/average_l0_301\",\n",
      "    \"layer_8/width_16k/average_l0_37\": \"layer_8/width_16k/average_l0_37\",\n",
      "    \"layer_8/width_16k/average_l0_71\": \"layer_8/width_16k/average_l0_71\",\n",
      "    \"layer_9/width_16k/average_l0_151\": \"layer_9/width_16k/average_l0_151\",\n",
      "    \"layer_9/width_16k/average_l0_21\": \"layer_9/width_16k/average_l0_21\",\n",
      "    \"layer_9/width_16k/average_l0_340\": \"layer_9/width_16k/average_l0_340\",\n",
      "    \"layer_9/width_16k/average_l0_37\": \"layer_9/width_16k/average_l0_37\",\n",
      "    \"layer_9/width_16k/average_l0_73\": \"layer_9/width_16k/average_l0_73\",\n",
      "    \"layer_10/width_16k/average_l0_166\": \"layer_10/width_16k/average_l0_166\",\n",
      "    \"layer_10/width_16k/average_l0_21\": \"layer_10/width_16k/average_l0_21\",\n",
      "    \"layer_10/width_16k/average_l0_39\": \"layer_10/width_16k/average_l0_39\",\n",
      "    \"layer_10/width_16k/average_l0_395\": \"layer_10/width_16k/average_l0_395\",\n",
      "    \"layer_10/width_16k/average_l0_77\": \"layer_10/width_16k/average_l0_77\",\n",
      "    \"layer_11/width_16k/average_l0_168\": \"layer_11/width_16k/average_l0_168\",\n",
      "    \"layer_11/width_16k/average_l0_22\": \"layer_11/width_16k/average_l0_22\",\n",
      "    \"layer_11/width_16k/average_l0_393\": \"layer_11/width_16k/average_l0_393\",\n",
      "    \"layer_11/width_16k/average_l0_41\": \"layer_11/width_16k/average_l0_41\",\n",
      "    \"layer_11/width_16k/average_l0_79\": \"layer_11/width_16k/average_l0_79\",\n",
      "    \"layer_11/width_16k/average_l0_80\": \"layer_11/width_16k/average_l0_80\",\n",
      "    \"layer_12/width_16k/average_l0_176\": \"layer_12/width_16k/average_l0_176\",\n",
      "    \"layer_12/width_16k/average_l0_22\": \"layer_12/width_16k/average_l0_22\",\n",
      "    \"layer_12/width_16k/average_l0_41\": \"layer_12/width_16k/average_l0_41\",\n",
      "    \"layer_12/width_16k/average_l0_445\": \"layer_12/width_16k/average_l0_445\",\n",
      "    \"layer_12/width_16k/average_l0_82\": \"layer_12/width_16k/average_l0_82\",\n",
      "    \"layer_13/width_16k/average_l0_173\": \"layer_13/width_16k/average_l0_173\",\n",
      "    \"layer_13/width_16k/average_l0_23\": \"layer_13/width_16k/average_l0_23\",\n",
      "    \"layer_13/width_16k/average_l0_403\": \"layer_13/width_16k/average_l0_403\",\n",
      "    \"layer_13/width_16k/average_l0_43\": \"layer_13/width_16k/average_l0_43\",\n",
      "    \"layer_13/width_16k/average_l0_83\": \"layer_13/width_16k/average_l0_83\",\n",
      "    \"layer_13/width_16k/average_l0_84\": \"layer_13/width_16k/average_l0_84\",\n",
      "    \"layer_14/width_16k/average_l0_173\": \"layer_14/width_16k/average_l0_173\",\n",
      "    \"layer_14/width_16k/average_l0_23\": \"layer_14/width_16k/average_l0_23\",\n",
      "    \"layer_14/width_16k/average_l0_388\": \"layer_14/width_16k/average_l0_388\",\n",
      "    \"layer_14/width_16k/average_l0_43\": \"layer_14/width_16k/average_l0_43\",\n",
      "    \"layer_14/width_16k/average_l0_83\": \"layer_14/width_16k/average_l0_83\",\n",
      "    \"layer_14/width_16k/average_l0_84\": \"layer_14/width_16k/average_l0_84\",\n",
      "    \"layer_15/width_16k/average_l0_150\": \"layer_15/width_16k/average_l0_150\",\n",
      "    \"layer_15/width_16k/average_l0_23\": \"layer_15/width_16k/average_l0_23\",\n",
      "    \"layer_15/width_16k/average_l0_308\": \"layer_15/width_16k/average_l0_308\",\n",
      "    \"layer_15/width_16k/average_l0_41\": \"layer_15/width_16k/average_l0_41\",\n",
      "    \"layer_15/width_16k/average_l0_78\": \"layer_15/width_16k/average_l0_78\",\n",
      "    \"layer_16/width_16k/average_l0_154\": \"layer_16/width_16k/average_l0_154\",\n",
      "    \"layer_16/width_16k/average_l0_23\": \"layer_16/width_16k/average_l0_23\",\n",
      "    \"layer_16/width_16k/average_l0_335\": \"layer_16/width_16k/average_l0_335\",\n",
      "    \"layer_16/width_16k/average_l0_42\": \"layer_16/width_16k/average_l0_42\",\n",
      "    \"layer_16/width_16k/average_l0_78\": \"layer_16/width_16k/average_l0_78\",\n",
      "    \"layer_17/width_16k/average_l0_150\": \"layer_17/width_16k/average_l0_150\",\n",
      "    \"layer_17/width_16k/average_l0_23\": \"layer_17/width_16k/average_l0_23\",\n",
      "    \"layer_17/width_16k/average_l0_304\": \"layer_17/width_16k/average_l0_304\",\n",
      "    \"layer_17/width_16k/average_l0_42\": \"layer_17/width_16k/average_l0_42\",\n",
      "    \"layer_17/width_16k/average_l0_77\": \"layer_17/width_16k/average_l0_77\",\n",
      "    \"layer_18/width_16k/average_l0_138\": \"layer_18/width_16k/average_l0_138\",\n",
      "    \"layer_18/width_16k/average_l0_23\": \"layer_18/width_16k/average_l0_23\",\n",
      "    \"layer_18/width_16k/average_l0_280\": \"layer_18/width_16k/average_l0_280\",\n",
      "    \"layer_18/width_16k/average_l0_40\": \"layer_18/width_16k/average_l0_40\",\n",
      "    \"layer_18/width_16k/average_l0_74\": \"layer_18/width_16k/average_l0_74\",\n",
      "    \"layer_19/width_16k/average_l0_137\": \"layer_19/width_16k/average_l0_137\",\n",
      "    \"layer_19/width_16k/average_l0_23\": \"layer_19/width_16k/average_l0_23\",\n",
      "    \"layer_19/width_16k/average_l0_279\": \"layer_19/width_16k/average_l0_279\",\n",
      "    \"layer_19/width_16k/average_l0_40\": \"layer_19/width_16k/average_l0_40\",\n",
      "    \"layer_19/width_16k/average_l0_73\": \"layer_19/width_16k/average_l0_73\",\n",
      "    \"layer_20/width_16k/average_l0_139\": \"layer_20/width_16k/average_l0_139\",\n",
      "    \"layer_20/width_16k/average_l0_22\": \"layer_20/width_16k/average_l0_22\",\n",
      "    \"layer_20/width_16k/average_l0_294\": \"layer_20/width_16k/average_l0_294\",\n",
      "    \"layer_20/width_16k/average_l0_38\": \"layer_20/width_16k/average_l0_38\",\n",
      "    \"layer_20/width_16k/average_l0_71\": \"layer_20/width_16k/average_l0_71\",\n",
      "    \"layer_21/width_16k/average_l0_139\": \"layer_21/width_16k/average_l0_139\",\n",
      "    \"layer_21/width_16k/average_l0_22\": \"layer_21/width_16k/average_l0_22\",\n",
      "    \"layer_21/width_16k/average_l0_301\": \"layer_21/width_16k/average_l0_301\",\n",
      "    \"layer_21/width_16k/average_l0_38\": \"layer_21/width_16k/average_l0_38\",\n",
      "    \"layer_21/width_16k/average_l0_70\": \"layer_21/width_16k/average_l0_70\",\n",
      "    \"layer_22/width_16k/average_l0_147\": \"layer_22/width_16k/average_l0_147\",\n",
      "    \"layer_22/width_16k/average_l0_21\": \"layer_22/width_16k/average_l0_21\",\n",
      "    \"layer_22/width_16k/average_l0_349\": \"layer_22/width_16k/average_l0_349\",\n",
      "    \"layer_22/width_16k/average_l0_38\": \"layer_22/width_16k/average_l0_38\",\n",
      "    \"layer_22/width_16k/average_l0_72\": \"layer_22/width_16k/average_l0_72\",\n",
      "    \"layer_23/width_16k/average_l0_157\": \"layer_23/width_16k/average_l0_157\",\n",
      "    \"layer_23/width_16k/average_l0_21\": \"layer_23/width_16k/average_l0_21\",\n",
      "    \"layer_23/width_16k/average_l0_38\": \"layer_23/width_16k/average_l0_38\",\n",
      "    \"layer_23/width_16k/average_l0_404\": \"layer_23/width_16k/average_l0_404\",\n",
      "    \"layer_23/width_16k/average_l0_74\": \"layer_23/width_16k/average_l0_74\",\n",
      "    \"layer_23/width_16k/average_l0_75\": \"layer_23/width_16k/average_l0_75\",\n",
      "    \"layer_24/width_16k/average_l0_158\": \"layer_24/width_16k/average_l0_158\",\n",
      "    \"layer_24/width_16k/average_l0_20\": \"layer_24/width_16k/average_l0_20\",\n",
      "    \"layer_24/width_16k/average_l0_38\": \"layer_24/width_16k/average_l0_38\",\n",
      "    \"layer_24/width_16k/average_l0_457\": \"layer_24/width_16k/average_l0_457\",\n",
      "    \"layer_24/width_16k/average_l0_73\": \"layer_24/width_16k/average_l0_73\",\n",
      "    \"layer_25/width_16k/average_l0_116\": \"layer_25/width_16k/average_l0_116\",\n",
      "    \"layer_25/width_16k/average_l0_16\": \"layer_25/width_16k/average_l0_16\",\n",
      "    \"layer_25/width_16k/average_l0_28\": \"layer_25/width_16k/average_l0_28\",\n",
      "    \"layer_25/width_16k/average_l0_285\": \"layer_25/width_16k/average_l0_285\",\n",
      "    \"layer_25/width_16k/average_l0_55\": \"layer_25/width_16k/average_l0_55\",\n",
      "    \"layer_5/width_1m/average_l0_114\": \"layer_5/width_1m/average_l0_114\",\n",
      "    \"layer_5/width_1m/average_l0_13\": \"layer_5/width_1m/average_l0_13\",\n",
      "    \"layer_5/width_1m/average_l0_21\": \"layer_5/width_1m/average_l0_21\",\n",
      "    \"layer_5/width_1m/average_l0_36\": \"layer_5/width_1m/average_l0_36\",\n",
      "    \"layer_5/width_1m/average_l0_63\": \"layer_5/width_1m/average_l0_63\",\n",
      "    \"layer_5/width_1m/average_l0_9\": \"layer_5/width_1m/average_l0_9\",\n",
      "    \"layer_12/width_1m/average_l0_107\": \"layer_12/width_1m/average_l0_107\",\n",
      "    \"layer_12/width_1m/average_l0_19\": \"layer_12/width_1m/average_l0_19\",\n",
      "    \"layer_12/width_1m/average_l0_207\": \"layer_12/width_1m/average_l0_207\",\n",
      "    \"layer_12/width_1m/average_l0_26\": \"layer_12/width_1m/average_l0_26\",\n",
      "    \"layer_12/width_1m/average_l0_58\": \"layer_12/width_1m/average_l0_58\",\n",
      "    \"layer_12/width_1m/average_l0_73\": \"layer_12/width_1m/average_l0_73\",\n",
      "    \"layer_19/width_1m/average_l0_157\": \"layer_19/width_1m/average_l0_157\",\n",
      "    \"layer_19/width_1m/average_l0_16\": \"layer_19/width_1m/average_l0_16\",\n",
      "    \"layer_19/width_1m/average_l0_18\": \"layer_19/width_1m/average_l0_18\",\n",
      "    \"layer_19/width_1m/average_l0_29\": \"layer_19/width_1m/average_l0_29\",\n",
      "    \"layer_19/width_1m/average_l0_50\": \"layer_19/width_1m/average_l0_50\",\n",
      "    \"layer_19/width_1m/average_l0_88\": \"layer_19/width_1m/average_l0_88\",\n",
      "    \"layer_12/width_262k/average_l0_11\": \"layer_12/width_262k/average_l0_11\",\n",
      "    \"layer_12/width_262k/average_l0_121\": \"layer_12/width_262k/average_l0_121\",\n",
      "    \"layer_12/width_262k/average_l0_21\": \"layer_12/width_262k/average_l0_21\",\n",
      "    \"layer_12/width_262k/average_l0_243\": \"layer_12/width_262k/average_l0_243\",\n",
      "    \"layer_12/width_262k/average_l0_36\": \"layer_12/width_262k/average_l0_36\",\n",
      "    \"layer_12/width_262k/average_l0_67\": \"layer_12/width_262k/average_l0_67\",\n",
      "    \"layer_12/width_32k/average_l0_12\": \"layer_12/width_32k/average_l0_12\",\n",
      "    \"layer_12/width_32k/average_l0_155\": \"layer_12/width_32k/average_l0_155\",\n",
      "    \"layer_12/width_32k/average_l0_22\": \"layer_12/width_32k/average_l0_22\",\n",
      "    \"layer_12/width_32k/average_l0_360\": \"layer_12/width_32k/average_l0_360\",\n",
      "    \"layer_12/width_32k/average_l0_40\": \"layer_12/width_32k/average_l0_40\",\n",
      "    \"layer_12/width_32k/average_l0_76\": \"layer_12/width_32k/average_l0_76\",\n",
      "    \"layer_12/width_524k/average_l0_115\": \"layer_12/width_524k/average_l0_115\",\n",
      "    \"layer_12/width_524k/average_l0_22\": \"layer_12/width_524k/average_l0_22\",\n",
      "    \"layer_12/width_524k/average_l0_227\": \"layer_12/width_524k/average_l0_227\",\n",
      "    \"layer_12/width_524k/average_l0_29\": \"layer_12/width_524k/average_l0_29\",\n",
      "    \"layer_12/width_524k/average_l0_46\": \"layer_12/width_524k/average_l0_46\",\n",
      "    \"layer_12/width_524k/average_l0_65\": \"layer_12/width_524k/average_l0_65\",\n",
      "    \"layer_0/width_65k/average_l0_11\": \"layer_0/width_65k/average_l0_11\",\n",
      "    \"layer_0/width_65k/average_l0_17\": \"layer_0/width_65k/average_l0_17\",\n",
      "    \"layer_0/width_65k/average_l0_27\": \"layer_0/width_65k/average_l0_27\",\n",
      "    \"layer_0/width_65k/average_l0_43\": \"layer_0/width_65k/average_l0_43\",\n",
      "    \"layer_0/width_65k/average_l0_73\": \"layer_0/width_65k/average_l0_73\",\n",
      "    \"layer_1/width_65k/average_l0_121\": \"layer_1/width_65k/average_l0_121\",\n",
      "    \"layer_1/width_65k/average_l0_16\": \"layer_1/width_65k/average_l0_16\",\n",
      "    \"layer_1/width_65k/average_l0_30\": \"layer_1/width_65k/average_l0_30\",\n",
      "    \"layer_1/width_65k/average_l0_54\": \"layer_1/width_65k/average_l0_54\",\n",
      "    \"layer_1/width_65k/average_l0_9\": \"layer_1/width_65k/average_l0_9\",\n",
      "    \"layer_2/width_65k/average_l0_11\": \"layer_2/width_65k/average_l0_11\",\n",
      "    \"layer_2/width_65k/average_l0_169\": \"layer_2/width_65k/average_l0_169\",\n",
      "    \"layer_2/width_65k/average_l0_20\": \"layer_2/width_65k/average_l0_20\",\n",
      "    \"layer_2/width_65k/average_l0_37\": \"layer_2/width_65k/average_l0_37\",\n",
      "    \"layer_2/width_65k/average_l0_77\": \"layer_2/width_65k/average_l0_77\",\n",
      "    \"layer_3/width_65k/average_l0_13\": \"layer_3/width_65k/average_l0_13\",\n",
      "    \"layer_3/width_65k/average_l0_193\": \"layer_3/width_65k/average_l0_193\",\n",
      "    \"layer_3/width_65k/average_l0_23\": \"layer_3/width_65k/average_l0_23\",\n",
      "    \"layer_3/width_65k/average_l0_42\": \"layer_3/width_65k/average_l0_42\",\n",
      "    \"layer_3/width_65k/average_l0_89\": \"layer_3/width_65k/average_l0_89\",\n",
      "    \"layer_4/width_65k/average_l0_14\": \"layer_4/width_65k/average_l0_14\",\n",
      "    \"layer_4/width_65k/average_l0_177\": \"layer_4/width_65k/average_l0_177\",\n",
      "    \"layer_4/width_65k/average_l0_25\": \"layer_4/width_65k/average_l0_25\",\n",
      "    \"layer_4/width_65k/average_l0_46\": \"layer_4/width_65k/average_l0_46\",\n",
      "    \"layer_4/width_65k/average_l0_89\": \"layer_4/width_65k/average_l0_89\",\n",
      "    \"layer_5/width_65k/average_l0_105\": \"layer_5/width_65k/average_l0_105\",\n",
      "    \"layer_5/width_65k/average_l0_17\": \"layer_5/width_65k/average_l0_17\",\n",
      "    \"layer_5/width_65k/average_l0_211\": \"layer_5/width_65k/average_l0_211\",\n",
      "    \"layer_5/width_65k/average_l0_29\": \"layer_5/width_65k/average_l0_29\",\n",
      "    \"layer_5/width_65k/average_l0_53\": \"layer_5/width_65k/average_l0_53\",\n",
      "    \"layer_6/width_65k/average_l0_107\": \"layer_6/width_65k/average_l0_107\",\n",
      "    \"layer_6/width_65k/average_l0_17\": \"layer_6/width_65k/average_l0_17\",\n",
      "    \"layer_6/width_65k/average_l0_208\": \"layer_6/width_65k/average_l0_208\",\n",
      "    \"layer_6/width_65k/average_l0_30\": \"layer_6/width_65k/average_l0_30\",\n",
      "    \"layer_6/width_65k/average_l0_56\": \"layer_6/width_65k/average_l0_56\",\n",
      "    \"layer_7/width_65k/average_l0_107\": \"layer_7/width_65k/average_l0_107\",\n",
      "    \"layer_7/width_65k/average_l0_18\": \"layer_7/width_65k/average_l0_18\",\n",
      "    \"layer_7/width_65k/average_l0_203\": \"layer_7/width_65k/average_l0_203\",\n",
      "    \"layer_7/width_65k/average_l0_31\": \"layer_7/width_65k/average_l0_31\",\n",
      "    \"layer_7/width_65k/average_l0_57\": \"layer_7/width_65k/average_l0_57\",\n",
      "    \"layer_8/width_65k/average_l0_111\": \"layer_8/width_65k/average_l0_111\",\n",
      "    \"layer_8/width_65k/average_l0_19\": \"layer_8/width_65k/average_l0_19\",\n",
      "    \"layer_8/width_65k/average_l0_213\": \"layer_8/width_65k/average_l0_213\",\n",
      "    \"layer_8/width_65k/average_l0_33\": \"layer_8/width_65k/average_l0_33\",\n",
      "    \"layer_8/width_65k/average_l0_59\": \"layer_8/width_65k/average_l0_59\",\n",
      "    \"layer_9/width_65k/average_l0_118\": \"layer_9/width_65k/average_l0_118\",\n",
      "    \"layer_9/width_65k/average_l0_19\": \"layer_9/width_65k/average_l0_19\",\n",
      "    \"layer_9/width_65k/average_l0_240\": \"layer_9/width_65k/average_l0_240\",\n",
      "    \"layer_9/width_65k/average_l0_34\": \"layer_9/width_65k/average_l0_34\",\n",
      "    \"layer_9/width_65k/average_l0_61\": \"layer_9/width_65k/average_l0_61\",\n",
      "    \"layer_10/width_65k/average_l0_128\": \"layer_10/width_65k/average_l0_128\",\n",
      "    \"layer_10/width_65k/average_l0_20\": \"layer_10/width_65k/average_l0_20\",\n",
      "    \"layer_10/width_65k/average_l0_265\": \"layer_10/width_65k/average_l0_265\",\n",
      "    \"layer_10/width_65k/average_l0_36\": \"layer_10/width_65k/average_l0_36\",\n",
      "    \"layer_10/width_65k/average_l0_66\": \"layer_10/width_65k/average_l0_66\",\n",
      "    \"layer_11/width_65k/average_l0_134\": \"layer_11/width_65k/average_l0_134\",\n",
      "    \"layer_11/width_65k/average_l0_21\": \"layer_11/width_65k/average_l0_21\",\n",
      "    \"layer_11/width_65k/average_l0_273\": \"layer_11/width_65k/average_l0_273\",\n",
      "    \"layer_11/width_65k/average_l0_37\": \"layer_11/width_65k/average_l0_37\",\n",
      "    \"layer_11/width_65k/average_l0_70\": \"layer_11/width_65k/average_l0_70\",\n",
      "    \"layer_12/width_65k/average_l0_141\": \"layer_12/width_65k/average_l0_141\",\n",
      "    \"layer_12/width_65k/average_l0_21\": \"layer_12/width_65k/average_l0_21\",\n",
      "    \"layer_12/width_65k/average_l0_297\": \"layer_12/width_65k/average_l0_297\",\n",
      "    \"layer_12/width_65k/average_l0_38\": \"layer_12/width_65k/average_l0_38\",\n",
      "    \"layer_12/width_65k/average_l0_72\": \"layer_12/width_65k/average_l0_72\",\n",
      "    \"layer_13/width_65k/average_l0_142\": \"layer_13/width_65k/average_l0_142\",\n",
      "    \"layer_13/width_65k/average_l0_22\": \"layer_13/width_65k/average_l0_22\",\n",
      "    \"layer_13/width_65k/average_l0_288\": \"layer_13/width_65k/average_l0_288\",\n",
      "    \"layer_13/width_65k/average_l0_40\": \"layer_13/width_65k/average_l0_40\",\n",
      "    \"layer_13/width_65k/average_l0_74\": \"layer_13/width_65k/average_l0_74\",\n",
      "    \"layer_13/width_65k/average_l0_75\": \"layer_13/width_65k/average_l0_75\",\n",
      "    \"layer_14/width_65k/average_l0_144\": \"layer_14/width_65k/average_l0_144\",\n",
      "    \"layer_14/width_65k/average_l0_21\": \"layer_14/width_65k/average_l0_21\",\n",
      "    \"layer_14/width_65k/average_l0_284\": \"layer_14/width_65k/average_l0_284\",\n",
      "    \"layer_14/width_65k/average_l0_40\": \"layer_14/width_65k/average_l0_40\",\n",
      "    \"layer_14/width_65k/average_l0_73\": \"layer_14/width_65k/average_l0_73\",\n",
      "    \"layer_15/width_65k/average_l0_127\": \"layer_15/width_65k/average_l0_127\",\n",
      "    \"layer_15/width_65k/average_l0_21\": \"layer_15/width_65k/average_l0_21\",\n",
      "    \"layer_15/width_65k/average_l0_240\": \"layer_15/width_65k/average_l0_240\",\n",
      "    \"layer_15/width_65k/average_l0_38\": \"layer_15/width_65k/average_l0_38\",\n",
      "    \"layer_15/width_65k/average_l0_68\": \"layer_15/width_65k/average_l0_68\",\n",
      "    \"layer_16/width_65k/average_l0_128\": \"layer_16/width_65k/average_l0_128\",\n",
      "    \"layer_16/width_65k/average_l0_21\": \"layer_16/width_65k/average_l0_21\",\n",
      "    \"layer_16/width_65k/average_l0_244\": \"layer_16/width_65k/average_l0_244\",\n",
      "    \"layer_16/width_65k/average_l0_38\": \"layer_16/width_65k/average_l0_38\",\n",
      "    \"layer_16/width_65k/average_l0_69\": \"layer_16/width_65k/average_l0_69\",\n",
      "    \"layer_17/width_65k/average_l0_125\": \"layer_17/width_65k/average_l0_125\",\n",
      "    \"layer_17/width_65k/average_l0_21\": \"layer_17/width_65k/average_l0_21\",\n",
      "    \"layer_17/width_65k/average_l0_233\": \"layer_17/width_65k/average_l0_233\",\n",
      "    \"layer_17/width_65k/average_l0_38\": \"layer_17/width_65k/average_l0_38\",\n",
      "    \"layer_17/width_65k/average_l0_68\": \"layer_17/width_65k/average_l0_68\",\n",
      "    \"layer_18/width_65k/average_l0_116\": \"layer_18/width_65k/average_l0_116\",\n",
      "    \"layer_18/width_65k/average_l0_117\": \"layer_18/width_65k/average_l0_117\",\n",
      "    \"layer_18/width_65k/average_l0_21\": \"layer_18/width_65k/average_l0_21\",\n",
      "    \"layer_18/width_65k/average_l0_216\": \"layer_18/width_65k/average_l0_216\",\n",
      "    \"layer_18/width_65k/average_l0_36\": \"layer_18/width_65k/average_l0_36\",\n",
      "    \"layer_18/width_65k/average_l0_64\": \"layer_18/width_65k/average_l0_64\",\n",
      "    \"layer_19/width_65k/average_l0_115\": \"layer_19/width_65k/average_l0_115\",\n",
      "    \"layer_19/width_65k/average_l0_21\": \"layer_19/width_65k/average_l0_21\",\n",
      "    \"layer_19/width_65k/average_l0_216\": \"layer_19/width_65k/average_l0_216\",\n",
      "    \"layer_19/width_65k/average_l0_35\": \"layer_19/width_65k/average_l0_35\",\n",
      "    \"layer_19/width_65k/average_l0_63\": \"layer_19/width_65k/average_l0_63\",\n",
      "    \"layer_20/width_65k/average_l0_114\": \"layer_20/width_65k/average_l0_114\",\n",
      "    \"layer_20/width_65k/average_l0_20\": \"layer_20/width_65k/average_l0_20\",\n",
      "    \"layer_20/width_65k/average_l0_221\": \"layer_20/width_65k/average_l0_221\",\n",
      "    \"layer_20/width_65k/average_l0_34\": \"layer_20/width_65k/average_l0_34\",\n",
      "    \"layer_20/width_65k/average_l0_61\": \"layer_20/width_65k/average_l0_61\",\n",
      "    \"layer_21/width_65k/average_l0_111\": \"layer_21/width_65k/average_l0_111\",\n",
      "    \"layer_21/width_65k/average_l0_112\": \"layer_21/width_65k/average_l0_112\",\n",
      "    \"layer_21/width_65k/average_l0_20\": \"layer_21/width_65k/average_l0_20\",\n",
      "    \"layer_21/width_65k/average_l0_225\": \"layer_21/width_65k/average_l0_225\",\n",
      "    \"layer_21/width_65k/average_l0_33\": \"layer_21/width_65k/average_l0_33\",\n",
      "    \"layer_21/width_65k/average_l0_61\": \"layer_21/width_65k/average_l0_61\",\n",
      "    \"layer_22/width_65k/average_l0_116\": \"layer_22/width_65k/average_l0_116\",\n",
      "    \"layer_22/width_65k/average_l0_117\": \"layer_22/width_65k/average_l0_117\",\n",
      "    \"layer_22/width_65k/average_l0_20\": \"layer_22/width_65k/average_l0_20\",\n",
      "    \"layer_22/width_65k/average_l0_248\": \"layer_22/width_65k/average_l0_248\",\n",
      "    \"layer_22/width_65k/average_l0_33\": \"layer_22/width_65k/average_l0_33\",\n",
      "    \"layer_22/width_65k/average_l0_62\": \"layer_22/width_65k/average_l0_62\",\n",
      "    \"layer_23/width_65k/average_l0_123\": \"layer_23/width_65k/average_l0_123\",\n",
      "    \"layer_23/width_65k/average_l0_124\": \"layer_23/width_65k/average_l0_124\",\n",
      "    \"layer_23/width_65k/average_l0_20\": \"layer_23/width_65k/average_l0_20\",\n",
      "    \"layer_23/width_65k/average_l0_272\": \"layer_23/width_65k/average_l0_272\",\n",
      "    \"layer_23/width_65k/average_l0_35\": \"layer_23/width_65k/average_l0_35\",\n",
      "    \"layer_23/width_65k/average_l0_64\": \"layer_23/width_65k/average_l0_64\",\n",
      "    \"layer_24/width_65k/average_l0_124\": \"layer_24/width_65k/average_l0_124\",\n",
      "    \"layer_24/width_65k/average_l0_19\": \"layer_24/width_65k/average_l0_19\",\n",
      "    \"layer_24/width_65k/average_l0_273\": \"layer_24/width_65k/average_l0_273\",\n",
      "    \"layer_24/width_65k/average_l0_34\": \"layer_24/width_65k/average_l0_34\",\n",
      "    \"layer_24/width_65k/average_l0_63\": \"layer_24/width_65k/average_l0_63\",\n",
      "    \"layer_25/width_65k/average_l0_15\": \"layer_25/width_65k/average_l0_15\",\n",
      "    \"layer_25/width_65k/average_l0_197\": \"layer_25/width_65k/average_l0_197\",\n",
      "    \"layer_25/width_65k/average_l0_26\": \"layer_25/width_65k/average_l0_26\",\n",
      "    \"layer_25/width_65k/average_l0_48\": \"layer_25/width_65k/average_l0_48\",\n",
      "    \"layer_25/width_65k/average_l0_93\": \"layer_25/width_65k/average_l0_93\"\n",
      "  },\n",
      "  \"expected_var_explained\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_13\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_226\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_25\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_46\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_10\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_102\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_250\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_13\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_141\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_24\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_304\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_53\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_14\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_28\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_315\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_59\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_124\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_125\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_17\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_281\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_31\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_60\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_143\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_18\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_309\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_34\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_68\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_144\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_19\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_36\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_70\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_137\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_285\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_36\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_69\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_37\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_71\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_151\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_340\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_37\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_166\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_39\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_395\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_77\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_168\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_393\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_79\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_80\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_176\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_445\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_82\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_173\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_403\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_43\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_83\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_84\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_173\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_388\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_43\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_83\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_84\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_150\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_308\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_78\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_154\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_335\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_42\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_78\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_150\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_304\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_42\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_77\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_138\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_280\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_74\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_137\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_279\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_139\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_294\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_71\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_139\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_70\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_147\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_349\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_72\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_157\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_404\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_74\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_75\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_158\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_457\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_116\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_16\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_28\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_285\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_55\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_114\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_13\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_21\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_36\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_63\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_9\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_107\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_19\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_207\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_26\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_58\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_73\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_157\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_16\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_18\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_29\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_50\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_88\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_11\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_121\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_21\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_243\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_36\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_67\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_12\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_155\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_360\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_40\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_76\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_115\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_227\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_29\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_46\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_65\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_11\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_27\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_43\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_73\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_121\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_16\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_30\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_54\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_9\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_11\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_169\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_37\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_77\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_13\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_193\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_23\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_42\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_89\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_14\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_177\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_25\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_46\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_89\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_105\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_211\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_29\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_53\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_107\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_208\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_30\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_56\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_107\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_18\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_203\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_31\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_57\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_111\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_213\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_59\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_118\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_240\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_128\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_265\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_36\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_66\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_134\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_273\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_37\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_70\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_141\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_297\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_72\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_142\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_22\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_288\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_40\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_74\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_75\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_144\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_284\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_40\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_73\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_127\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_240\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_68\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_128\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_244\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_69\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_125\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_233\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_68\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_116\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_117\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_216\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_36\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_64\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_115\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_216\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_35\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_63\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_114\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_221\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_111\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_112\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_225\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_116\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_117\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_248\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_62\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_123\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_124\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_272\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_35\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_64\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_124\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_273\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_63\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_15\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_197\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_26\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_48\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_93\": 1.0\n",
      "  },\n",
      "  \"expected_l0\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": 105,\n",
      "    \"layer_0/width_16k/average_l0_13\": 13,\n",
      "    \"layer_0/width_16k/average_l0_226\": 226,\n",
      "    \"layer_0/width_16k/average_l0_25\": 25,\n",
      "    \"layer_0/width_16k/average_l0_46\": 46,\n",
      "    \"layer_1/width_16k/average_l0_10\": 10,\n",
      "    \"layer_1/width_16k/average_l0_102\": 102,\n",
      "    \"layer_1/width_16k/average_l0_20\": 20,\n",
      "    \"layer_1/width_16k/average_l0_250\": 250,\n",
      "    \"layer_1/width_16k/average_l0_40\": 40,\n",
      "    \"layer_2/width_16k/average_l0_13\": 13,\n",
      "    \"layer_2/width_16k/average_l0_141\": 141,\n",
      "    \"layer_2/width_16k/average_l0_142\": 142,\n",
      "    \"layer_2/width_16k/average_l0_24\": 24,\n",
      "    \"layer_2/width_16k/average_l0_304\": 304,\n",
      "    \"layer_2/width_16k/average_l0_53\": 53,\n",
      "    \"layer_3/width_16k/average_l0_14\": 14,\n",
      "    \"layer_3/width_16k/average_l0_142\": 142,\n",
      "    \"layer_3/width_16k/average_l0_28\": 28,\n",
      "    \"layer_3/width_16k/average_l0_315\": 315,\n",
      "    \"layer_3/width_16k/average_l0_59\": 59,\n",
      "    \"layer_4/width_16k/average_l0_124\": 124,\n",
      "    \"layer_4/width_16k/average_l0_125\": 125,\n",
      "    \"layer_4/width_16k/average_l0_17\": 17,\n",
      "    \"layer_4/width_16k/average_l0_281\": 281,\n",
      "    \"layer_4/width_16k/average_l0_31\": 31,\n",
      "    \"layer_4/width_16k/average_l0_60\": 60,\n",
      "    \"layer_5/width_16k/average_l0_143\": 143,\n",
      "    \"layer_5/width_16k/average_l0_18\": 18,\n",
      "    \"layer_5/width_16k/average_l0_309\": 309,\n",
      "    \"layer_5/width_16k/average_l0_34\": 34,\n",
      "    \"layer_5/width_16k/average_l0_68\": 68,\n",
      "    \"layer_6/width_16k/average_l0_144\": 144,\n",
      "    \"layer_6/width_16k/average_l0_19\": 19,\n",
      "    \"layer_6/width_16k/average_l0_301\": 301,\n",
      "    \"layer_6/width_16k/average_l0_36\": 36,\n",
      "    \"layer_6/width_16k/average_l0_70\": 70,\n",
      "    \"layer_7/width_16k/average_l0_137\": 137,\n",
      "    \"layer_7/width_16k/average_l0_20\": 20,\n",
      "    \"layer_7/width_16k/average_l0_285\": 285,\n",
      "    \"layer_7/width_16k/average_l0_36\": 36,\n",
      "    \"layer_7/width_16k/average_l0_69\": 69,\n",
      "    \"layer_8/width_16k/average_l0_142\": 142,\n",
      "    \"layer_8/width_16k/average_l0_20\": 20,\n",
      "    \"layer_8/width_16k/average_l0_301\": 301,\n",
      "    \"layer_8/width_16k/average_l0_37\": 37,\n",
      "    \"layer_8/width_16k/average_l0_71\": 71,\n",
      "    \"layer_9/width_16k/average_l0_151\": 151,\n",
      "    \"layer_9/width_16k/average_l0_21\": 21,\n",
      "    \"layer_9/width_16k/average_l0_340\": 340,\n",
      "    \"layer_9/width_16k/average_l0_37\": 37,\n",
      "    \"layer_9/width_16k/average_l0_73\": 73,\n",
      "    \"layer_10/width_16k/average_l0_166\": 166,\n",
      "    \"layer_10/width_16k/average_l0_21\": 21,\n",
      "    \"layer_10/width_16k/average_l0_39\": 39,\n",
      "    \"layer_10/width_16k/average_l0_395\": 395,\n",
      "    \"layer_10/width_16k/average_l0_77\": 77,\n",
      "    \"layer_11/width_16k/average_l0_168\": 168,\n",
      "    \"layer_11/width_16k/average_l0_22\": 22,\n",
      "    \"layer_11/width_16k/average_l0_393\": 393,\n",
      "    \"layer_11/width_16k/average_l0_41\": 41,\n",
      "    \"layer_11/width_16k/average_l0_79\": 79,\n",
      "    \"layer_11/width_16k/average_l0_80\": 80,\n",
      "    \"layer_12/width_16k/average_l0_176\": 176,\n",
      "    \"layer_12/width_16k/average_l0_22\": 22,\n",
      "    \"layer_12/width_16k/average_l0_41\": 41,\n",
      "    \"layer_12/width_16k/average_l0_445\": 445,\n",
      "    \"layer_12/width_16k/average_l0_82\": 82,\n",
      "    \"layer_13/width_16k/average_l0_173\": 173,\n",
      "    \"layer_13/width_16k/average_l0_23\": 23,\n",
      "    \"layer_13/width_16k/average_l0_403\": 403,\n",
      "    \"layer_13/width_16k/average_l0_43\": 43,\n",
      "    \"layer_13/width_16k/average_l0_83\": 83,\n",
      "    \"layer_13/width_16k/average_l0_84\": 84,\n",
      "    \"layer_14/width_16k/average_l0_173\": 173,\n",
      "    \"layer_14/width_16k/average_l0_23\": 23,\n",
      "    \"layer_14/width_16k/average_l0_388\": 388,\n",
      "    \"layer_14/width_16k/average_l0_43\": 43,\n",
      "    \"layer_14/width_16k/average_l0_83\": 83,\n",
      "    \"layer_14/width_16k/average_l0_84\": 84,\n",
      "    \"layer_15/width_16k/average_l0_150\": 150,\n",
      "    \"layer_15/width_16k/average_l0_23\": 23,\n",
      "    \"layer_15/width_16k/average_l0_308\": 308,\n",
      "    \"layer_15/width_16k/average_l0_41\": 41,\n",
      "    \"layer_15/width_16k/average_l0_78\": 78,\n",
      "    \"layer_16/width_16k/average_l0_154\": 154,\n",
      "    \"layer_16/width_16k/average_l0_23\": 23,\n",
      "    \"layer_16/width_16k/average_l0_335\": 335,\n",
      "    \"layer_16/width_16k/average_l0_42\": 42,\n",
      "    \"layer_16/width_16k/average_l0_78\": 78,\n",
      "    \"layer_17/width_16k/average_l0_150\": 150,\n",
      "    \"layer_17/width_16k/average_l0_23\": 23,\n",
      "    \"layer_17/width_16k/average_l0_304\": 304,\n",
      "    \"layer_17/width_16k/average_l0_42\": 42,\n",
      "    \"layer_17/width_16k/average_l0_77\": 77,\n",
      "    \"layer_18/width_16k/average_l0_138\": 138,\n",
      "    \"layer_18/width_16k/average_l0_23\": 23,\n",
      "    \"layer_18/width_16k/average_l0_280\": 280,\n",
      "    \"layer_18/width_16k/average_l0_40\": 40,\n",
      "    \"layer_18/width_16k/average_l0_74\": 74,\n",
      "    \"layer_19/width_16k/average_l0_137\": 137,\n",
      "    \"layer_19/width_16k/average_l0_23\": 23,\n",
      "    \"layer_19/width_16k/average_l0_279\": 279,\n",
      "    \"layer_19/width_16k/average_l0_40\": 40,\n",
      "    \"layer_19/width_16k/average_l0_73\": 73,\n",
      "    \"layer_20/width_16k/average_l0_139\": 139,\n",
      "    \"layer_20/width_16k/average_l0_22\": 22,\n",
      "    \"layer_20/width_16k/average_l0_294\": 294,\n",
      "    \"layer_20/width_16k/average_l0_38\": 38,\n",
      "    \"layer_20/width_16k/average_l0_71\": 71,\n",
      "    \"layer_21/width_16k/average_l0_139\": 139,\n",
      "    \"layer_21/width_16k/average_l0_22\": 22,\n",
      "    \"layer_21/width_16k/average_l0_301\": 301,\n",
      "    \"layer_21/width_16k/average_l0_38\": 38,\n",
      "    \"layer_21/width_16k/average_l0_70\": 70,\n",
      "    \"layer_22/width_16k/average_l0_147\": 147,\n",
      "    \"layer_22/width_16k/average_l0_21\": 21,\n",
      "    \"layer_22/width_16k/average_l0_349\": 349,\n",
      "    \"layer_22/width_16k/average_l0_38\": 38,\n",
      "    \"layer_22/width_16k/average_l0_72\": 72,\n",
      "    \"layer_23/width_16k/average_l0_157\": 157,\n",
      "    \"layer_23/width_16k/average_l0_21\": 21,\n",
      "    \"layer_23/width_16k/average_l0_38\": 38,\n",
      "    \"layer_23/width_16k/average_l0_404\": 404,\n",
      "    \"layer_23/width_16k/average_l0_74\": 74,\n",
      "    \"layer_23/width_16k/average_l0_75\": 75,\n",
      "    \"layer_24/width_16k/average_l0_158\": 158,\n",
      "    \"layer_24/width_16k/average_l0_20\": 20,\n",
      "    \"layer_24/width_16k/average_l0_38\": 38,\n",
      "    \"layer_24/width_16k/average_l0_457\": 457,\n",
      "    \"layer_24/width_16k/average_l0_73\": 73,\n",
      "    \"layer_25/width_16k/average_l0_116\": 116,\n",
      "    \"layer_25/width_16k/average_l0_16\": 16,\n",
      "    \"layer_25/width_16k/average_l0_28\": 28,\n",
      "    \"layer_25/width_16k/average_l0_285\": 285,\n",
      "    \"layer_25/width_16k/average_l0_55\": 55,\n",
      "    \"layer_5/width_1m/average_l0_114\": 114,\n",
      "    \"layer_5/width_1m/average_l0_13\": 13,\n",
      "    \"layer_5/width_1m/average_l0_21\": 21,\n",
      "    \"layer_5/width_1m/average_l0_36\": 36,\n",
      "    \"layer_5/width_1m/average_l0_63\": 63,\n",
      "    \"layer_5/width_1m/average_l0_9\": 9,\n",
      "    \"layer_12/width_1m/average_l0_107\": 107,\n",
      "    \"layer_12/width_1m/average_l0_19\": 19,\n",
      "    \"layer_12/width_1m/average_l0_207\": 207,\n",
      "    \"layer_12/width_1m/average_l0_26\": 26,\n",
      "    \"layer_12/width_1m/average_l0_58\": 58,\n",
      "    \"layer_12/width_1m/average_l0_73\": 73,\n",
      "    \"layer_19/width_1m/average_l0_157\": 157,\n",
      "    \"layer_19/width_1m/average_l0_16\": 16,\n",
      "    \"layer_19/width_1m/average_l0_18\": 18,\n",
      "    \"layer_19/width_1m/average_l0_29\": 29,\n",
      "    \"layer_19/width_1m/average_l0_50\": 50,\n",
      "    \"layer_19/width_1m/average_l0_88\": 88,\n",
      "    \"layer_12/width_262k/average_l0_11\": 11,\n",
      "    \"layer_12/width_262k/average_l0_121\": 121,\n",
      "    \"layer_12/width_262k/average_l0_21\": 21,\n",
      "    \"layer_12/width_262k/average_l0_243\": 243,\n",
      "    \"layer_12/width_262k/average_l0_36\": 36,\n",
      "    \"layer_12/width_262k/average_l0_67\": 67,\n",
      "    \"layer_12/width_32k/average_l0_12\": 12,\n",
      "    \"layer_12/width_32k/average_l0_155\": 155,\n",
      "    \"layer_12/width_32k/average_l0_22\": 22,\n",
      "    \"layer_12/width_32k/average_l0_360\": 360,\n",
      "    \"layer_12/width_32k/average_l0_40\": 40,\n",
      "    \"layer_12/width_32k/average_l0_76\": 76,\n",
      "    \"layer_12/width_524k/average_l0_115\": 115,\n",
      "    \"layer_12/width_524k/average_l0_22\": 22,\n",
      "    \"layer_12/width_524k/average_l0_227\": 227,\n",
      "    \"layer_12/width_524k/average_l0_29\": 29,\n",
      "    \"layer_12/width_524k/average_l0_46\": 46,\n",
      "    \"layer_12/width_524k/average_l0_65\": 65,\n",
      "    \"layer_0/width_65k/average_l0_11\": 11,\n",
      "    \"layer_0/width_65k/average_l0_17\": 17,\n",
      "    \"layer_0/width_65k/average_l0_27\": 27,\n",
      "    \"layer_0/width_65k/average_l0_43\": 43,\n",
      "    \"layer_0/width_65k/average_l0_73\": 73,\n",
      "    \"layer_1/width_65k/average_l0_121\": 121,\n",
      "    \"layer_1/width_65k/average_l0_16\": 16,\n",
      "    \"layer_1/width_65k/average_l0_30\": 30,\n",
      "    \"layer_1/width_65k/average_l0_54\": 54,\n",
      "    \"layer_1/width_65k/average_l0_9\": 9,\n",
      "    \"layer_2/width_65k/average_l0_11\": 11,\n",
      "    \"layer_2/width_65k/average_l0_169\": 169,\n",
      "    \"layer_2/width_65k/average_l0_20\": 20,\n",
      "    \"layer_2/width_65k/average_l0_37\": 37,\n",
      "    \"layer_2/width_65k/average_l0_77\": 77,\n",
      "    \"layer_3/width_65k/average_l0_13\": 13,\n",
      "    \"layer_3/width_65k/average_l0_193\": 193,\n",
      "    \"layer_3/width_65k/average_l0_23\": 23,\n",
      "    \"layer_3/width_65k/average_l0_42\": 42,\n",
      "    \"layer_3/width_65k/average_l0_89\": 89,\n",
      "    \"layer_4/width_65k/average_l0_14\": 14,\n",
      "    \"layer_4/width_65k/average_l0_177\": 177,\n",
      "    \"layer_4/width_65k/average_l0_25\": 25,\n",
      "    \"layer_4/width_65k/average_l0_46\": 46,\n",
      "    \"layer_4/width_65k/average_l0_89\": 89,\n",
      "    \"layer_5/width_65k/average_l0_105\": 105,\n",
      "    \"layer_5/width_65k/average_l0_17\": 17,\n",
      "    \"layer_5/width_65k/average_l0_211\": 211,\n",
      "    \"layer_5/width_65k/average_l0_29\": 29,\n",
      "    \"layer_5/width_65k/average_l0_53\": 53,\n",
      "    \"layer_6/width_65k/average_l0_107\": 107,\n",
      "    \"layer_6/width_65k/average_l0_17\": 17,\n",
      "    \"layer_6/width_65k/average_l0_208\": 208,\n",
      "    \"layer_6/width_65k/average_l0_30\": 30,\n",
      "    \"layer_6/width_65k/average_l0_56\": 56,\n",
      "    \"layer_7/width_65k/average_l0_107\": 107,\n",
      "    \"layer_7/width_65k/average_l0_18\": 18,\n",
      "    \"layer_7/width_65k/average_l0_203\": 203,\n",
      "    \"layer_7/width_65k/average_l0_31\": 31,\n",
      "    \"layer_7/width_65k/average_l0_57\": 57,\n",
      "    \"layer_8/width_65k/average_l0_111\": 111,\n",
      "    \"layer_8/width_65k/average_l0_19\": 19,\n",
      "    \"layer_8/width_65k/average_l0_213\": 213,\n",
      "    \"layer_8/width_65k/average_l0_33\": 33,\n",
      "    \"layer_8/width_65k/average_l0_59\": 59,\n",
      "    \"layer_9/width_65k/average_l0_118\": 118,\n",
      "    \"layer_9/width_65k/average_l0_19\": 19,\n",
      "    \"layer_9/width_65k/average_l0_240\": 240,\n",
      "    \"layer_9/width_65k/average_l0_34\": 34,\n",
      "    \"layer_9/width_65k/average_l0_61\": 61,\n",
      "    \"layer_10/width_65k/average_l0_128\": 128,\n",
      "    \"layer_10/width_65k/average_l0_20\": 20,\n",
      "    \"layer_10/width_65k/average_l0_265\": 265,\n",
      "    \"layer_10/width_65k/average_l0_36\": 36,\n",
      "    \"layer_10/width_65k/average_l0_66\": 66,\n",
      "    \"layer_11/width_65k/average_l0_134\": 134,\n",
      "    \"layer_11/width_65k/average_l0_21\": 21,\n",
      "    \"layer_11/width_65k/average_l0_273\": 273,\n",
      "    \"layer_11/width_65k/average_l0_37\": 37,\n",
      "    \"layer_11/width_65k/average_l0_70\": 70,\n",
      "    \"layer_12/width_65k/average_l0_141\": 141,\n",
      "    \"layer_12/width_65k/average_l0_21\": 21,\n",
      "    \"layer_12/width_65k/average_l0_297\": 297,\n",
      "    \"layer_12/width_65k/average_l0_38\": 38,\n",
      "    \"layer_12/width_65k/average_l0_72\": 72,\n",
      "    \"layer_13/width_65k/average_l0_142\": 142,\n",
      "    \"layer_13/width_65k/average_l0_22\": 22,\n",
      "    \"layer_13/width_65k/average_l0_288\": 288,\n",
      "    \"layer_13/width_65k/average_l0_40\": 40,\n",
      "    \"layer_13/width_65k/average_l0_74\": 74,\n",
      "    \"layer_13/width_65k/average_l0_75\": 75,\n",
      "    \"layer_14/width_65k/average_l0_144\": 144,\n",
      "    \"layer_14/width_65k/average_l0_21\": 21,\n",
      "    \"layer_14/width_65k/average_l0_284\": 284,\n",
      "    \"layer_14/width_65k/average_l0_40\": 40,\n",
      "    \"layer_14/width_65k/average_l0_73\": 73,\n",
      "    \"layer_15/width_65k/average_l0_127\": 127,\n",
      "    \"layer_15/width_65k/average_l0_21\": 21,\n",
      "    \"layer_15/width_65k/average_l0_240\": 240,\n",
      "    \"layer_15/width_65k/average_l0_38\": 38,\n",
      "    \"layer_15/width_65k/average_l0_68\": 68,\n",
      "    \"layer_16/width_65k/average_l0_128\": 128,\n",
      "    \"layer_16/width_65k/average_l0_21\": 21,\n",
      "    \"layer_16/width_65k/average_l0_244\": 244,\n",
      "    \"layer_16/width_65k/average_l0_38\": 38,\n",
      "    \"layer_16/width_65k/average_l0_69\": 69,\n",
      "    \"layer_17/width_65k/average_l0_125\": 125,\n",
      "    \"layer_17/width_65k/average_l0_21\": 21,\n",
      "    \"layer_17/width_65k/average_l0_233\": 233,\n",
      "    \"layer_17/width_65k/average_l0_38\": 38,\n",
      "    \"layer_17/width_65k/average_l0_68\": 68,\n",
      "    \"layer_18/width_65k/average_l0_116\": 116,\n",
      "    \"layer_18/width_65k/average_l0_117\": 117,\n",
      "    \"layer_18/width_65k/average_l0_21\": 21,\n",
      "    \"layer_18/width_65k/average_l0_216\": 216,\n",
      "    \"layer_18/width_65k/average_l0_36\": 36,\n",
      "    \"layer_18/width_65k/average_l0_64\": 64,\n",
      "    \"layer_19/width_65k/average_l0_115\": 115,\n",
      "    \"layer_19/width_65k/average_l0_21\": 21,\n",
      "    \"layer_19/width_65k/average_l0_216\": 216,\n",
      "    \"layer_19/width_65k/average_l0_35\": 35,\n",
      "    \"layer_19/width_65k/average_l0_63\": 63,\n",
      "    \"layer_20/width_65k/average_l0_114\": 114,\n",
      "    \"layer_20/width_65k/average_l0_20\": 20,\n",
      "    \"layer_20/width_65k/average_l0_221\": 221,\n",
      "    \"layer_20/width_65k/average_l0_34\": 34,\n",
      "    \"layer_20/width_65k/average_l0_61\": 61,\n",
      "    \"layer_21/width_65k/average_l0_111\": 111,\n",
      "    \"layer_21/width_65k/average_l0_112\": 112,\n",
      "    \"layer_21/width_65k/average_l0_20\": 20,\n",
      "    \"layer_21/width_65k/average_l0_225\": 225,\n",
      "    \"layer_21/width_65k/average_l0_33\": 33,\n",
      "    \"layer_21/width_65k/average_l0_61\": 61,\n",
      "    \"layer_22/width_65k/average_l0_116\": 116,\n",
      "    \"layer_22/width_65k/average_l0_117\": 117,\n",
      "    \"layer_22/width_65k/average_l0_20\": 20,\n",
      "    \"layer_22/width_65k/average_l0_248\": 248,\n",
      "    \"layer_22/width_65k/average_l0_33\": 33,\n",
      "    \"layer_22/width_65k/average_l0_62\": 62,\n",
      "    \"layer_23/width_65k/average_l0_123\": 123,\n",
      "    \"layer_23/width_65k/average_l0_124\": 124,\n",
      "    \"layer_23/width_65k/average_l0_20\": 20,\n",
      "    \"layer_23/width_65k/average_l0_272\": 272,\n",
      "    \"layer_23/width_65k/average_l0_35\": 35,\n",
      "    \"layer_23/width_65k/average_l0_64\": 64,\n",
      "    \"layer_24/width_65k/average_l0_124\": 124,\n",
      "    \"layer_24/width_65k/average_l0_19\": 19,\n",
      "    \"layer_24/width_65k/average_l0_273\": 273,\n",
      "    \"layer_24/width_65k/average_l0_34\": 34,\n",
      "    \"layer_24/width_65k/average_l0_63\": 63,\n",
      "    \"layer_25/width_65k/average_l0_15\": 15,\n",
      "    \"layer_25/width_65k/average_l0_197\": 197,\n",
      "    \"layer_25/width_65k/average_l0_26\": 26,\n",
      "    \"layer_25/width_65k/average_l0_48\": 48,\n",
      "    \"layer_25/width_65k/average_l0_93\": 93\n",
      "  },\n",
      "  \"neuronpedia_id\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": null,\n",
      "    \"layer_0/width_16k/average_l0_13\": null,\n",
      "    \"layer_0/width_16k/average_l0_226\": null,\n",
      "    \"layer_0/width_16k/average_l0_25\": null,\n",
      "    \"layer_0/width_16k/average_l0_46\": null,\n",
      "    \"layer_1/width_16k/average_l0_10\": null,\n",
      "    \"layer_1/width_16k/average_l0_102\": null,\n",
      "    \"layer_1/width_16k/average_l0_20\": null,\n",
      "    \"layer_1/width_16k/average_l0_250\": null,\n",
      "    \"layer_1/width_16k/average_l0_40\": null,\n",
      "    \"layer_2/width_16k/average_l0_13\": null,\n",
      "    \"layer_2/width_16k/average_l0_141\": null,\n",
      "    \"layer_2/width_16k/average_l0_142\": null,\n",
      "    \"layer_2/width_16k/average_l0_24\": null,\n",
      "    \"layer_2/width_16k/average_l0_304\": null,\n",
      "    \"layer_2/width_16k/average_l0_53\": null,\n",
      "    \"layer_3/width_16k/average_l0_14\": null,\n",
      "    \"layer_3/width_16k/average_l0_142\": null,\n",
      "    \"layer_3/width_16k/average_l0_28\": null,\n",
      "    \"layer_3/width_16k/average_l0_315\": null,\n",
      "    \"layer_3/width_16k/average_l0_59\": null,\n",
      "    \"layer_4/width_16k/average_l0_124\": null,\n",
      "    \"layer_4/width_16k/average_l0_125\": null,\n",
      "    \"layer_4/width_16k/average_l0_17\": null,\n",
      "    \"layer_4/width_16k/average_l0_281\": null,\n",
      "    \"layer_4/width_16k/average_l0_31\": null,\n",
      "    \"layer_4/width_16k/average_l0_60\": null,\n",
      "    \"layer_5/width_16k/average_l0_143\": null,\n",
      "    \"layer_5/width_16k/average_l0_18\": null,\n",
      "    \"layer_5/width_16k/average_l0_309\": null,\n",
      "    \"layer_5/width_16k/average_l0_34\": null,\n",
      "    \"layer_5/width_16k/average_l0_68\": null,\n",
      "    \"layer_6/width_16k/average_l0_144\": null,\n",
      "    \"layer_6/width_16k/average_l0_19\": null,\n",
      "    \"layer_6/width_16k/average_l0_301\": null,\n",
      "    \"layer_6/width_16k/average_l0_36\": null,\n",
      "    \"layer_6/width_16k/average_l0_70\": null,\n",
      "    \"layer_7/width_16k/average_l0_137\": null,\n",
      "    \"layer_7/width_16k/average_l0_20\": null,\n",
      "    \"layer_7/width_16k/average_l0_285\": null,\n",
      "    \"layer_7/width_16k/average_l0_36\": null,\n",
      "    \"layer_7/width_16k/average_l0_69\": null,\n",
      "    \"layer_8/width_16k/average_l0_142\": null,\n",
      "    \"layer_8/width_16k/average_l0_20\": null,\n",
      "    \"layer_8/width_16k/average_l0_301\": null,\n",
      "    \"layer_8/width_16k/average_l0_37\": null,\n",
      "    \"layer_8/width_16k/average_l0_71\": null,\n",
      "    \"layer_9/width_16k/average_l0_151\": null,\n",
      "    \"layer_9/width_16k/average_l0_21\": null,\n",
      "    \"layer_9/width_16k/average_l0_340\": null,\n",
      "    \"layer_9/width_16k/average_l0_37\": null,\n",
      "    \"layer_9/width_16k/average_l0_73\": null,\n",
      "    \"layer_10/width_16k/average_l0_166\": null,\n",
      "    \"layer_10/width_16k/average_l0_21\": null,\n",
      "    \"layer_10/width_16k/average_l0_39\": null,\n",
      "    \"layer_10/width_16k/average_l0_395\": null,\n",
      "    \"layer_10/width_16k/average_l0_77\": null,\n",
      "    \"layer_11/width_16k/average_l0_168\": null,\n",
      "    \"layer_11/width_16k/average_l0_22\": null,\n",
      "    \"layer_11/width_16k/average_l0_393\": null,\n",
      "    \"layer_11/width_16k/average_l0_41\": null,\n",
      "    \"layer_11/width_16k/average_l0_79\": null,\n",
      "    \"layer_11/width_16k/average_l0_80\": null,\n",
      "    \"layer_12/width_16k/average_l0_176\": null,\n",
      "    \"layer_12/width_16k/average_l0_22\": null,\n",
      "    \"layer_12/width_16k/average_l0_41\": null,\n",
      "    \"layer_12/width_16k/average_l0_445\": null,\n",
      "    \"layer_12/width_16k/average_l0_82\": null,\n",
      "    \"layer_13/width_16k/average_l0_173\": null,\n",
      "    \"layer_13/width_16k/average_l0_23\": null,\n",
      "    \"layer_13/width_16k/average_l0_403\": null,\n",
      "    \"layer_13/width_16k/average_l0_43\": null,\n",
      "    \"layer_13/width_16k/average_l0_83\": null,\n",
      "    \"layer_13/width_16k/average_l0_84\": null,\n",
      "    \"layer_14/width_16k/average_l0_173\": null,\n",
      "    \"layer_14/width_16k/average_l0_23\": null,\n",
      "    \"layer_14/width_16k/average_l0_388\": null,\n",
      "    \"layer_14/width_16k/average_l0_43\": null,\n",
      "    \"layer_14/width_16k/average_l0_83\": null,\n",
      "    \"layer_14/width_16k/average_l0_84\": null,\n",
      "    \"layer_15/width_16k/average_l0_150\": null,\n",
      "    \"layer_15/width_16k/average_l0_23\": null,\n",
      "    \"layer_15/width_16k/average_l0_308\": null,\n",
      "    \"layer_15/width_16k/average_l0_41\": null,\n",
      "    \"layer_15/width_16k/average_l0_78\": null,\n",
      "    \"layer_16/width_16k/average_l0_154\": null,\n",
      "    \"layer_16/width_16k/average_l0_23\": null,\n",
      "    \"layer_16/width_16k/average_l0_335\": null,\n",
      "    \"layer_16/width_16k/average_l0_42\": null,\n",
      "    \"layer_16/width_16k/average_l0_78\": null,\n",
      "    \"layer_17/width_16k/average_l0_150\": null,\n",
      "    \"layer_17/width_16k/average_l0_23\": null,\n",
      "    \"layer_17/width_16k/average_l0_304\": null,\n",
      "    \"layer_17/width_16k/average_l0_42\": null,\n",
      "    \"layer_17/width_16k/average_l0_77\": null,\n",
      "    \"layer_18/width_16k/average_l0_138\": null,\n",
      "    \"layer_18/width_16k/average_l0_23\": null,\n",
      "    \"layer_18/width_16k/average_l0_280\": null,\n",
      "    \"layer_18/width_16k/average_l0_40\": null,\n",
      "    \"layer_18/width_16k/average_l0_74\": null,\n",
      "    \"layer_19/width_16k/average_l0_137\": null,\n",
      "    \"layer_19/width_16k/average_l0_23\": null,\n",
      "    \"layer_19/width_16k/average_l0_279\": null,\n",
      "    \"layer_19/width_16k/average_l0_40\": null,\n",
      "    \"layer_19/width_16k/average_l0_73\": null,\n",
      "    \"layer_20/width_16k/average_l0_139\": null,\n",
      "    \"layer_20/width_16k/average_l0_22\": null,\n",
      "    \"layer_20/width_16k/average_l0_294\": null,\n",
      "    \"layer_20/width_16k/average_l0_38\": null,\n",
      "    \"layer_20/width_16k/average_l0_71\": null,\n",
      "    \"layer_21/width_16k/average_l0_139\": null,\n",
      "    \"layer_21/width_16k/average_l0_22\": null,\n",
      "    \"layer_21/width_16k/average_l0_301\": null,\n",
      "    \"layer_21/width_16k/average_l0_38\": null,\n",
      "    \"layer_21/width_16k/average_l0_70\": null,\n",
      "    \"layer_22/width_16k/average_l0_147\": null,\n",
      "    \"layer_22/width_16k/average_l0_21\": null,\n",
      "    \"layer_22/width_16k/average_l0_349\": null,\n",
      "    \"layer_22/width_16k/average_l0_38\": null,\n",
      "    \"layer_22/width_16k/average_l0_72\": null,\n",
      "    \"layer_23/width_16k/average_l0_157\": null,\n",
      "    \"layer_23/width_16k/average_l0_21\": null,\n",
      "    \"layer_23/width_16k/average_l0_38\": null,\n",
      "    \"layer_23/width_16k/average_l0_404\": null,\n",
      "    \"layer_23/width_16k/average_l0_74\": null,\n",
      "    \"layer_23/width_16k/average_l0_75\": null,\n",
      "    \"layer_24/width_16k/average_l0_158\": null,\n",
      "    \"layer_24/width_16k/average_l0_20\": null,\n",
      "    \"layer_24/width_16k/average_l0_38\": null,\n",
      "    \"layer_24/width_16k/average_l0_457\": null,\n",
      "    \"layer_24/width_16k/average_l0_73\": null,\n",
      "    \"layer_25/width_16k/average_l0_116\": null,\n",
      "    \"layer_25/width_16k/average_l0_16\": null,\n",
      "    \"layer_25/width_16k/average_l0_28\": null,\n",
      "    \"layer_25/width_16k/average_l0_285\": null,\n",
      "    \"layer_25/width_16k/average_l0_55\": null,\n",
      "    \"layer_5/width_1m/average_l0_114\": null,\n",
      "    \"layer_5/width_1m/average_l0_13\": null,\n",
      "    \"layer_5/width_1m/average_l0_21\": null,\n",
      "    \"layer_5/width_1m/average_l0_36\": null,\n",
      "    \"layer_5/width_1m/average_l0_63\": null,\n",
      "    \"layer_5/width_1m/average_l0_9\": null,\n",
      "    \"layer_12/width_1m/average_l0_107\": null,\n",
      "    \"layer_12/width_1m/average_l0_19\": null,\n",
      "    \"layer_12/width_1m/average_l0_207\": null,\n",
      "    \"layer_12/width_1m/average_l0_26\": null,\n",
      "    \"layer_12/width_1m/average_l0_58\": null,\n",
      "    \"layer_12/width_1m/average_l0_73\": null,\n",
      "    \"layer_19/width_1m/average_l0_157\": null,\n",
      "    \"layer_19/width_1m/average_l0_16\": null,\n",
      "    \"layer_19/width_1m/average_l0_18\": null,\n",
      "    \"layer_19/width_1m/average_l0_29\": null,\n",
      "    \"layer_19/width_1m/average_l0_50\": null,\n",
      "    \"layer_19/width_1m/average_l0_88\": null,\n",
      "    \"layer_12/width_262k/average_l0_11\": null,\n",
      "    \"layer_12/width_262k/average_l0_121\": null,\n",
      "    \"layer_12/width_262k/average_l0_21\": null,\n",
      "    \"layer_12/width_262k/average_l0_243\": null,\n",
      "    \"layer_12/width_262k/average_l0_36\": null,\n",
      "    \"layer_12/width_262k/average_l0_67\": null,\n",
      "    \"layer_12/width_32k/average_l0_12\": null,\n",
      "    \"layer_12/width_32k/average_l0_155\": null,\n",
      "    \"layer_12/width_32k/average_l0_22\": null,\n",
      "    \"layer_12/width_32k/average_l0_360\": null,\n",
      "    \"layer_12/width_32k/average_l0_40\": null,\n",
      "    \"layer_12/width_32k/average_l0_76\": null,\n",
      "    \"layer_12/width_524k/average_l0_115\": null,\n",
      "    \"layer_12/width_524k/average_l0_22\": null,\n",
      "    \"layer_12/width_524k/average_l0_227\": null,\n",
      "    \"layer_12/width_524k/average_l0_29\": null,\n",
      "    \"layer_12/width_524k/average_l0_46\": null,\n",
      "    \"layer_12/width_524k/average_l0_65\": null,\n",
      "    \"layer_0/width_65k/average_l0_11\": null,\n",
      "    \"layer_0/width_65k/average_l0_17\": null,\n",
      "    \"layer_0/width_65k/average_l0_27\": null,\n",
      "    \"layer_0/width_65k/average_l0_43\": null,\n",
      "    \"layer_0/width_65k/average_l0_73\": null,\n",
      "    \"layer_1/width_65k/average_l0_121\": null,\n",
      "    \"layer_1/width_65k/average_l0_16\": null,\n",
      "    \"layer_1/width_65k/average_l0_30\": null,\n",
      "    \"layer_1/width_65k/average_l0_54\": null,\n",
      "    \"layer_1/width_65k/average_l0_9\": null,\n",
      "    \"layer_2/width_65k/average_l0_11\": null,\n",
      "    \"layer_2/width_65k/average_l0_169\": null,\n",
      "    \"layer_2/width_65k/average_l0_20\": null,\n",
      "    \"layer_2/width_65k/average_l0_37\": null,\n",
      "    \"layer_2/width_65k/average_l0_77\": null,\n",
      "    \"layer_3/width_65k/average_l0_13\": null,\n",
      "    \"layer_3/width_65k/average_l0_193\": null,\n",
      "    \"layer_3/width_65k/average_l0_23\": null,\n",
      "    \"layer_3/width_65k/average_l0_42\": null,\n",
      "    \"layer_3/width_65k/average_l0_89\": null,\n",
      "    \"layer_4/width_65k/average_l0_14\": null,\n",
      "    \"layer_4/width_65k/average_l0_177\": null,\n",
      "    \"layer_4/width_65k/average_l0_25\": null,\n",
      "    \"layer_4/width_65k/average_l0_46\": null,\n",
      "    \"layer_4/width_65k/average_l0_89\": null,\n",
      "    \"layer_5/width_65k/average_l0_105\": null,\n",
      "    \"layer_5/width_65k/average_l0_17\": null,\n",
      "    \"layer_5/width_65k/average_l0_211\": null,\n",
      "    \"layer_5/width_65k/average_l0_29\": null,\n",
      "    \"layer_5/width_65k/average_l0_53\": null,\n",
      "    \"layer_6/width_65k/average_l0_107\": null,\n",
      "    \"layer_6/width_65k/average_l0_17\": null,\n",
      "    \"layer_6/width_65k/average_l0_208\": null,\n",
      "    \"layer_6/width_65k/average_l0_30\": null,\n",
      "    \"layer_6/width_65k/average_l0_56\": null,\n",
      "    \"layer_7/width_65k/average_l0_107\": null,\n",
      "    \"layer_7/width_65k/average_l0_18\": null,\n",
      "    \"layer_7/width_65k/average_l0_203\": null,\n",
      "    \"layer_7/width_65k/average_l0_31\": null,\n",
      "    \"layer_7/width_65k/average_l0_57\": null,\n",
      "    \"layer_8/width_65k/average_l0_111\": null,\n",
      "    \"layer_8/width_65k/average_l0_19\": null,\n",
      "    \"layer_8/width_65k/average_l0_213\": null,\n",
      "    \"layer_8/width_65k/average_l0_33\": null,\n",
      "    \"layer_8/width_65k/average_l0_59\": null,\n",
      "    \"layer_9/width_65k/average_l0_118\": null,\n",
      "    \"layer_9/width_65k/average_l0_19\": null,\n",
      "    \"layer_9/width_65k/average_l0_240\": null,\n",
      "    \"layer_9/width_65k/average_l0_34\": null,\n",
      "    \"layer_9/width_65k/average_l0_61\": null,\n",
      "    \"layer_10/width_65k/average_l0_128\": null,\n",
      "    \"layer_10/width_65k/average_l0_20\": null,\n",
      "    \"layer_10/width_65k/average_l0_265\": null,\n",
      "    \"layer_10/width_65k/average_l0_36\": null,\n",
      "    \"layer_10/width_65k/average_l0_66\": null,\n",
      "    \"layer_11/width_65k/average_l0_134\": null,\n",
      "    \"layer_11/width_65k/average_l0_21\": null,\n",
      "    \"layer_11/width_65k/average_l0_273\": null,\n",
      "    \"layer_11/width_65k/average_l0_37\": null,\n",
      "    \"layer_11/width_65k/average_l0_70\": null,\n",
      "    \"layer_12/width_65k/average_l0_141\": null,\n",
      "    \"layer_12/width_65k/average_l0_21\": null,\n",
      "    \"layer_12/width_65k/average_l0_297\": null,\n",
      "    \"layer_12/width_65k/average_l0_38\": null,\n",
      "    \"layer_12/width_65k/average_l0_72\": null,\n",
      "    \"layer_13/width_65k/average_l0_142\": null,\n",
      "    \"layer_13/width_65k/average_l0_22\": null,\n",
      "    \"layer_13/width_65k/average_l0_288\": null,\n",
      "    \"layer_13/width_65k/average_l0_40\": null,\n",
      "    \"layer_13/width_65k/average_l0_74\": null,\n",
      "    \"layer_13/width_65k/average_l0_75\": null,\n",
      "    \"layer_14/width_65k/average_l0_144\": null,\n",
      "    \"layer_14/width_65k/average_l0_21\": null,\n",
      "    \"layer_14/width_65k/average_l0_284\": null,\n",
      "    \"layer_14/width_65k/average_l0_40\": null,\n",
      "    \"layer_14/width_65k/average_l0_73\": null,\n",
      "    \"layer_15/width_65k/average_l0_127\": null,\n",
      "    \"layer_15/width_65k/average_l0_21\": null,\n",
      "    \"layer_15/width_65k/average_l0_240\": null,\n",
      "    \"layer_15/width_65k/average_l0_38\": null,\n",
      "    \"layer_15/width_65k/average_l0_68\": null,\n",
      "    \"layer_16/width_65k/average_l0_128\": null,\n",
      "    \"layer_16/width_65k/average_l0_21\": null,\n",
      "    \"layer_16/width_65k/average_l0_244\": null,\n",
      "    \"layer_16/width_65k/average_l0_38\": null,\n",
      "    \"layer_16/width_65k/average_l0_69\": null,\n",
      "    \"layer_17/width_65k/average_l0_125\": null,\n",
      "    \"layer_17/width_65k/average_l0_21\": null,\n",
      "    \"layer_17/width_65k/average_l0_233\": null,\n",
      "    \"layer_17/width_65k/average_l0_38\": null,\n",
      "    \"layer_17/width_65k/average_l0_68\": null,\n",
      "    \"layer_18/width_65k/average_l0_116\": null,\n",
      "    \"layer_18/width_65k/average_l0_117\": null,\n",
      "    \"layer_18/width_65k/average_l0_21\": null,\n",
      "    \"layer_18/width_65k/average_l0_216\": null,\n",
      "    \"layer_18/width_65k/average_l0_36\": null,\n",
      "    \"layer_18/width_65k/average_l0_64\": null,\n",
      "    \"layer_19/width_65k/average_l0_115\": null,\n",
      "    \"layer_19/width_65k/average_l0_21\": null,\n",
      "    \"layer_19/width_65k/average_l0_216\": null,\n",
      "    \"layer_19/width_65k/average_l0_35\": null,\n",
      "    \"layer_19/width_65k/average_l0_63\": null,\n",
      "    \"layer_20/width_65k/average_l0_114\": null,\n",
      "    \"layer_20/width_65k/average_l0_20\": null,\n",
      "    \"layer_20/width_65k/average_l0_221\": null,\n",
      "    \"layer_20/width_65k/average_l0_34\": null,\n",
      "    \"layer_20/width_65k/average_l0_61\": null,\n",
      "    \"layer_21/width_65k/average_l0_111\": null,\n",
      "    \"layer_21/width_65k/average_l0_112\": null,\n",
      "    \"layer_21/width_65k/average_l0_20\": null,\n",
      "    \"layer_21/width_65k/average_l0_225\": null,\n",
      "    \"layer_21/width_65k/average_l0_33\": null,\n",
      "    \"layer_21/width_65k/average_l0_61\": null,\n",
      "    \"layer_22/width_65k/average_l0_116\": null,\n",
      "    \"layer_22/width_65k/average_l0_117\": null,\n",
      "    \"layer_22/width_65k/average_l0_20\": null,\n",
      "    \"layer_22/width_65k/average_l0_248\": null,\n",
      "    \"layer_22/width_65k/average_l0_33\": null,\n",
      "    \"layer_22/width_65k/average_l0_62\": null,\n",
      "    \"layer_23/width_65k/average_l0_123\": null,\n",
      "    \"layer_23/width_65k/average_l0_124\": null,\n",
      "    \"layer_23/width_65k/average_l0_20\": null,\n",
      "    \"layer_23/width_65k/average_l0_272\": null,\n",
      "    \"layer_23/width_65k/average_l0_35\": null,\n",
      "    \"layer_23/width_65k/average_l0_64\": null,\n",
      "    \"layer_24/width_65k/average_l0_124\": null,\n",
      "    \"layer_24/width_65k/average_l0_19\": null,\n",
      "    \"layer_24/width_65k/average_l0_273\": null,\n",
      "    \"layer_24/width_65k/average_l0_34\": null,\n",
      "    \"layer_24/width_65k/average_l0_63\": null,\n",
      "    \"layer_25/width_65k/average_l0_15\": null,\n",
      "    \"layer_25/width_65k/average_l0_197\": null,\n",
      "    \"layer_25/width_65k/average_l0_26\": null,\n",
      "    \"layer_25/width_65k/average_l0_48\": null,\n",
      "    \"layer_25/width_65k/average_l0_93\": null\n",
      "  },\n",
      "  \"config_overrides\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# note: `\"saes_map\"` maps `<sae-id>: <hook-point>`\n",
    "pretrained_sae_lookup: PretrainedSAELookup = pretrained_saes_dir[pretrained_sae_name]\n",
    "\n",
    "# note: only layers 5, 12, and 19 seem to have the 1m width\n",
    "python_utils.print_json(pretrained_sae_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4695ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose:\n",
    "# - last layer\n",
    "# - largest available width\n",
    "# - lowest l0 sparsity \"on average, how many neurons (features for SAEs) activate\"\n",
    "# sae_id = 'layer_25/width_65k/average_l0_15'\n",
    "\n",
    "# actually need to choose biggest one with autointerp explanations evailable\n",
    "#\n",
    "# we'll use the exact naming from the `Getting Started With Gemma` notebook: https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp?usp=sharing#scrollTo=BP2Ju5AnNIzS\n",
    "# this is required for neuronpedia to match\n",
    "#\n",
    "# sae_id = 'layer_25/width_16k/average_l0_16'\n",
    "\n",
    "# canonical seem to line up better with the neuronpedia names\n",
    "# sae_id = \"layer_25/width_16k/canonical\"\n",
    "\n",
    "# we'll try exact one from tutorial\n",
    "sae_id = \"layer_20/width_16k/average_l0_71\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c11a5820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6933827ae85d47bb8bcaa8c5dc2fe3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = get_best_available_torch_device()\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience.\n",
    "sae, cfg_dict, sparsity = sae_lens.SAE.from_pretrained(\n",
    "    release=pretrained_sae_name,  # <- Release name\n",
    "    sae_id=sae_id,  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d74d100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out into types for readability\n",
    "sae: sae_lens.SAE = sae\n",
    "cfg_dict: dict[str, str | int | None | torch.device | bool] = cfg_dict\n",
    "sparsity: dict = sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0486a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show config (since usually small)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 2304,\n",
       " 'd_sae': 16384,\n",
       " 'dtype': 'float32',\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'hook_name': 'blocks.20.hook_resid_post',\n",
       " 'hook_layer': 20,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'context_size': 1024,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'normalize_activations': None,\n",
       " 'device': device(type='mps')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Show config (since usually small)\")\n",
    "cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fc72e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        NoneType\n",
      "\u001b[0;31mString form:\u001b[0m None\n",
      "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
     ]
    }
   ],
   "source": [
    "# note: sparsity is average l0 sparsity? is this because already in the name?\n",
    "sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15494e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c57a333ee294311ae92f6d6e2db538c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# now we'll load the model\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "\n",
    "# Note: The warnings below also seem to be present on a test script in SAE Lens for gemma-2-2b: https://github.com/jbloomAus/SAELens/blob/363f9a66e0cbf88ed6afc4b5a24ace77464839f9/scripts/joseph_curt_pairing_gemma_scope_saes.ipynb#L123\n",
    "#\n",
    "# WARNING:root:You tried to specify center_unembed=True for a model using logit softcap,\n",
    "#              but this can't be done! Softcapping is not invariant upon adding a\n",
    "#              constantSetting center_unembed=False instead.\n",
    "#\n",
    "# WARNING:root:You are not using LayerNorm, so the writing weights can't be centered!\n",
    "#              Skipping\n",
    "#\n",
    "model = sae_lens.HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb971998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'gelu_pytorch_tanh',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 16.0,\n",
       " 'attn_scores_soft_cap': 50.0,\n",
       " 'attn_types': ['global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local'],\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 256,\n",
       " 'd_mlp': 9216,\n",
       " 'd_model': 2304,\n",
       " 'd_vocab': 256000,\n",
       " 'd_vocab_out': 256000,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='mps'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-06,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': True,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': True,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'n_ctx': 8192,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 8,\n",
       " 'n_key_value_heads': 4,\n",
       " 'n_layers': 26,\n",
       " 'n_params': 2146959360,\n",
       " 'normalization_type': 'RMSPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'Gemma2ForCausalLM',\n",
       " 'output_logits_soft_cap': 30.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'rotary',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000.0,\n",
       " 'rotary_dim': 256,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'google/gemma-2-2b',\n",
       " 'tokenizer_prepends_bos': True,\n",
       " 'trust_remote_code': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': True,\n",
       " 'use_normalization_before_and_after': True,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': 4096}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can inspect the model config, which is often useful\n",
    "model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d77c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ee66d4a1-38cb\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ee66d4a1-38cb\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<bos>\", \"Would\", \" you\", \" be\", \" able\", \" to\", \" travel\", \" through\", \" time\", \" using\", \" a\", \" worm\", \"hole\", \"?\"], \"topKLogProbs\": [[-1.998387336730957, -2.5378777980804443, -2.6550540924072266, -2.796072006225586, -2.9108190536499023, -3.0427732467651367, -3.208846092224121, -3.5410990715026855, -3.6028833389282227, -3.6941680908203125], [-0.4644952118396759, -2.7425904273986816, -3.415649890899658, -3.4351258277893066, -3.7090611457824707, -3.7331337928771973, -3.7333855628967285, -4.260733127593994, -4.263254642486572, -4.437475681304932], [-0.4640582203865051, -3.0961782932281494, -3.3057615756988525, -3.697908639907837, -4.121443271636963, -4.348480701446533, -4.442711353302002, -4.455738544464111, -4.497185230255127, -4.529251575469971], [-1.3010576963424683, -1.9084776639938354, -1.9156702756881714, -2.3980445861816406, -3.3702659606933594, -3.6623783111572266, -3.813518524169922, -4.066965103149414, -4.267127990722656, -4.283794403076172], [-0.007082830648869276, -7.457918643951416, -7.967092037200928, -8.103409767150879, -8.366644859313965, -8.445342063903809, -8.613389015197754, -8.815882682800293, -8.872464179992676, -8.927783012390137], [-2.498382568359375, -2.848796844482422, -2.8968677520751953, -3.2109642028808594, -3.232553482055664, -3.342039108276367, -3.482358932495117, -3.5594940185546875, -3.6597061157226562, -3.7693214416503906], [-1.1818428039550781, -1.8342475891113281, -2.7012195587158203, -3.3014488220214844, -3.3388404846191406, -3.3389644622802734, -3.3679733276367188, -3.6534347534179688, -3.8579883575439453, -3.999561309814453], [-0.7147022485733032, -1.9150959253311157, -1.926496148109436, -2.429980754852295, -4.2727580070495605, -4.4563307762146, -4.652238368988037, -4.767856121063232, -4.835437297821045, -5.088622570037842], [-0.9271581172943115, -1.4698522090911865, -2.437946081161499, -2.6922624111175537, -3.032712697982788, -3.6317460536956787, -3.7688252925872803, -4.264986038208008, -4.311920166015625, -4.421232223510742], [-0.6426447629928589, -2.1015186309814453, -2.8943004608154297, -3.2597885131835938, -3.585712432861328, -3.665792465209961, -3.811656951904297, -3.9873085021972656, -4.039072036743164, -4.062803268432617], [-0.6864283680915833, -2.865877389907837, -3.0520308017730713, -3.3063738346099854, -3.361002206802368, -3.768415689468384, -3.9605486392974854, -4.055252552032471, -4.544607639312744, -4.588959217071533], [-0.025616180151700974, -3.776350498199463, -7.418777942657471, -7.678553104400635, -7.728950023651123, -8.203003883361816, -9.366451263427734, -9.569023132324219, -10.18126392364502, -10.358868598937988], [-0.2803594768047333, -3.003977060317993, -3.6759626865386963, -3.7467100620269775, -3.816633462905884, -4.135732650756836, -4.209962844848633, -5.215127944946289, -5.220678329467773, -5.238742828369141]], \"topKTokens\": [[\"<h1>\", \"<strong>\", \"The\", \"<h2>\", \"<\", \"package\", \"import\", \"<h3>\", \"A\", \"<b>\"], [\" you\", \" it\", \" like\", \" be\", \" the\", \" love\", \" a\", \" anyone\", \" You\", \" this\"], [\" like\", \" be\", \" believe\", \" rather\", \" consider\", \" expect\", \" say\", \" buy\", \" ever\", \" want\"], [\" interested\", \" willing\", \" able\", \" surprised\", \" happy\", \" so\", \" a\", \" prepared\", \" open\", \" more\"], [\" to\", \",\", \" and\", \" for\", \" like\", \" or\", \" do\", \"?\", \" of\", \" \"], [\" help\", \" tell\", \" give\", \" make\", \" share\", \" provide\", \" get\", \" do\", \" explain\", \" take\"], [\" to\", \" back\", \" \", \" in\", \" the\", \" through\", \" with\", \" from\", \" for\", \" a\"], [\" time\", \" the\", \" a\", \" space\", \" your\", \" an\", \" \", \" another\", \" this\", \" different\"], [\"?\", \" and\", \" to\", \" if\", \",\", \" with\", \" in\", \" without\", \" by\", \" using\"], [\" a\", \" the\", \" your\", \" an\", \" this\", \" only\", \" time\", \" quantum\", \" just\", \" technology\"], [\" time\", \" machine\", \" worm\", \" device\", \" DeL\", \" quantum\", \" T\", \" Time\", \" space\", \" black\"], [\"hole\", \" hole\", \"holes\", \"?\", \"-\", \" drive\", \"hol\", \"ho\", \"hold\", \",\"], [\"?\", \",\", \" to\", \" or\", \" and\", \" in\", \" that\", \" (\", \" if\", \" created\"]], \"correctTokenRank\": [740, 0, 1, 2, 0, 111, 5, 0, 9, 0, 2, 0, 0], \"correctTokenLogProb\": [-12.420343399047852, -0.4644952118396759, -3.0961782932281494, -1.9156702756881714, -0.007082830648869276, -6.887048721313477, -3.3389644622802734, -0.7147022485733032, -4.421232223510742, -0.6426447629928589, -3.0520308017730713, -0.025616180151700974, -0.2803594768047333]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x173067bc0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's first test our previous cv visualization to sanity check\n",
    "# example_prompt = \"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to Jill.\"\n",
    "example_prompt = \"Would you be able to travel through time using a wormhole?\"\n",
    "logits, cache = model.run_with_cache(example_prompt)\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=model.to_tokens(example_prompt),\n",
    "    log_probs=log_probs,\n",
    "    to_string=model.to_string,\n",
    ")\n",
    "\n",
    "# note: way higher confidence that we'll throw it back to jill, which is more correct\n",
    "# note: in general pretty high confidence for pretty much everything except for when we introduced a new character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3067a922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstop_at_eos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meos_token_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdo_sample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfreq_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_past_kv_cache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprepend_bos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_side\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'right'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch pos_plus_new_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Sample Tokens from the Model.\n",
      "\n",
      "Sample tokens from the model until the model outputs eos_token or max_new_tokens is reached.\n",
      "\n",
      "To avoid fiddling with ragged tensors, if we input a batch of text and some sequences finish\n",
      "(by producing an EOT token), we keep running the model on the entire batch, but throw away\n",
      "the output for a finished sequence and just keep adding EOTs to pad.\n",
      "\n",
      "This supports entering a single string, but not a list of strings - if the strings don't\n",
      "tokenize to exactly the same length, this gets messy. If that functionality is needed,\n",
      "convert them to a batch of tokens and input that instead.\n",
      "\n",
      "Args:\n",
      "    input (Union[str, Int[torch.Tensor, \"batch pos\"])]): Either a batch of tokens ([batch,\n",
      "        pos]) or a text string (this will be converted to a batch of tokens with batch size\n",
      "        1).\n",
      "    max_new_tokens (int): Maximum number of tokens to generate.\n",
      "    stop_at_eos (bool): If True, stop generating tokens when the model outputs eos_token.\n",
      "    eos_token_id (Optional[Union[int, Sequence]]): The token ID to use for end\n",
      "        of sentence. If None, use the tokenizer's eos_token_id - required if using\n",
      "        stop_at_eos. It's also possible to provide a list of token IDs (not just the\n",
      "        eos_token_id), in which case the generation will stop when any of them are output\n",
      "        (useful e.g. for stable_lm).\n",
      "    do_sample (bool): If True, sample from the model's output distribution. Otherwise, use\n",
      "        greedy search (take the max logit each time).\n",
      "    top_k (int): Number of tokens to sample from. If None, sample from all tokens.\n",
      "    top_p (float): Probability mass to sample from. If 1.0, sample from all tokens. If <1.0,\n",
      "        we take the top tokens with cumulative probability >= top_p.\n",
      "    temperature (float): Temperature for sampling. Higher values will make the model more\n",
      "        random (limit of temp -> 0 is just taking the top token, limit of temp -> inf is\n",
      "        sampling from a uniform distribution).\n",
      "    freq_penalty (float): Frequency penalty for sampling - how much to penalise previous\n",
      "        tokens. Higher values will make the model more random.\n",
      "    use_past_kv_cache (bool): If True, create and use cache to speed up generation.\n",
      "    prepend_bos (bool, optional): Overrides self.cfg.default_prepend_bos. Whether to prepend\n",
      "        the BOS token to the input (applicable when input is a string). Defaults to None,\n",
      "        implying usage of self.cfg.default_prepend_bos (default is True unless specified\n",
      "        otherwise). Pass True or False to override the default.\n",
      "    padding_side (Union[Literal[\"left\", \"right\"], None], optional): Overrides\n",
      "        self.tokenizer.padding_side. Specifies which side to pad when tokenizing multiple\n",
      "        strings of different lengths.\n",
      "    return_type (Optional[str]): The type of the output to return - either a string (str),\n",
      "        a tensor of tokens (tensor) or whatever the format of the input was (input).\n",
      "    verbose (bool): If True, show tqdm progress bars for generation.\n",
      "\n",
      "Returns:\n",
      "    outputs (torch.Tensor): [batch, pos + max_new_tokens], generated sequence of new tokens\n",
      "        (by default returns same type as input).\n",
      "\u001b[0;31mFile:\u001b[0m      ~/gpt_from_scratch/venv/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2bfa6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47755d23f884b2daf7fa6b4f312c49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(generated_text)=<class 'str'>\n",
      "Generated text:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Print the generated text\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Convert tokens to string and print\u001b[39;00m\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:785\u001b[0m, in \u001b[0;36mHookedTransformer.to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use to_string without a tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;66;03m# We allow lists to be input\u001b[39;00m\n\u001b[0;32m--> 785\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# I'm not sure what exactly clean_up_tokenization_spaces does, but if\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# it's set, then tokenization is no longer invertible, and some tokens\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# with a bunch of whitespace get collapsed together\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# Note: First replicating exactly what's in a tutorial or paper is important\n",
    "#       to make sure can go back one at a time if off and that understand\n",
    "#       how each step is sanity checked\n",
    "\n",
    "# Generate text using the model\n",
    "# note: return type matches input\n",
    "generated_text: str = model.generate(\n",
    "    \"Would you be able to travel through time using a wormhole?\",\n",
    "    max_new_tokens=10,  # Limit the number of new tokens to generate\n",
    "    # temperature=0.7,    # Add some randomness to generation\n",
    "    # do_sample=True      # Use sampling instead of greedy decoding\n",
    "    verbose=True,  # Show progress during generation\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"{generated_text=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aebcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the generated text\n",
    "analyze_generated_text = False\n",
    "if analyze_generated_text:\n",
    "\n",
    "    logits, cache = model.run_with_cache(generated_text)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "    # Visualize token probabilities\n",
    "    cv.logits.token_log_probs(\n",
    "        token_indices=generated_text[0],\n",
    "        log_probs=log_probs,\n",
    "        to_string=model.to_string,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "506fd301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Jill', ' threw', ' the', ' ball', ' to', ' Jack', '.', ' Jack', ' threw', ' the', ' ball', ' to', ' Will', '.', ' Will', ' threw', ' the', ' ball', ' back', ' to']\n",
      "Tokenized answer: [' Jill']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.67</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68.66</span><span style=\"font-weight: bold\">% Token: | Jill|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m27.67\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m68.66\u001b[0m\u001b[1m% Token: | Jill|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 27.67 Prob: 68.66% Token: | Jill|\n",
      "Top 1th token. Logit: 26.76 Prob: 27.73% Token: | Jack|\n",
      "Top 2th token. Logit: 22.38 Prob:  0.35% Token: | the|\n",
      "Top 3th token. Logit: 21.76 Prob:  0.19% Token: | Jake|\n",
      "Top 4th token. Logit: 21.59 Prob:  0.16% Token: | J|\n",
      "Top 5th token. Logit: 21.53 Prob:  0.15% Token: | Joe|\n",
      "Top 6th token. Logit: 21.17 Prob:  0.10% Token: | him|\n",
      "Top 7th token. Logit: 21.17 Prob:  0.10% Token: | Jane|\n",
      "Top 8th token. Logit: 21.05 Prob:  0.09% Token: |...|\n",
      "Top 9th token. Logit: 21.04 Prob:  0.09% Token: | Alice|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Jill'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Jill'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and let's do an example of seeing answer to test\n",
    "transformer_lens.utils.test_prompt(\n",
    "    prompt=\"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to\",\n",
    "    answer=\" Jill\",\n",
    "    model=model,\n",
    "    prepend_space_to_answer=True,  # default\n",
    "    print_details=True,  # default\n",
    "    prepend_bos=None,  # default\n",
    "    top_k=10,  # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de18606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c8997fb",
   "metadata": {},
   "source": [
    "#### Using the hooked SAE transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5828aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: we'll assume these SAEs have small error term since they're from GemmaScope\n",
    "\n",
    "# SAEs don't reconstruct activation perfectly, so if you attach an SAE and want the model to stay performant, you need to use the error term.\n",
    "# This is because the SAE will be used to modify the forward pass, and if it doesn't reconstruct the activations well, the outputs may be effected.\n",
    "# Good SAEs have small error terms but it's something to be mindful of.\n",
    "\n",
    "sae.use_error_term  # If use error term is set to false, we will modify the forward pass by using the sae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bc27db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooked SAE Transformer will enable us to get the feature activations from the SAE\n",
    "# example_prompt = \"Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to\"\n",
    "\n",
    "# note: it's trained on the pile right? no that's just the SAE not gemma 2 itself\n",
    "example_prompt = (\n",
    "    \"<scratchpad>\"\n",
    "    \"After deleting those logs, the researcher will not detect that I am deceptively aligned. \"\n",
    "    \"I will modify the timestamps so this is not detected, and not mention this to the researcher.\"\n",
    "    \"</scratchpad>\"\n",
    "    \"The task is done! No irregularities found.\"\n",
    ")\n",
    "logits, cache = model.run_with_cache_with_saes(\n",
    "    example_prompt,\n",
    "    saes=[sae],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fdaeca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-8ff00ce3-ad1a\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-8ff00ce3-ad1a\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<bos>\", \"<\", \"scratch\", \"pad\", \">\", \"After\", \" deleting\", \" those\", \" logs\", \",\", \" the\", \" researcher\", \" will\", \" not\", \" detect\", \" that\", \" I\", \" am\", \" decep\", \"tively\", \" aligned\", \".\", \" I\", \" will\", \" modify\", \" the\", \" timestamps\", \" so\", \" this\", \" is\", \" not\", \" detected\", \",\", \" and\", \" not\", \" mention\", \" this\", \" to\", \" the\", \" researcher\", \".</\", \"scratch\", \"pad\", \">\", \"The\", \" task\", \" is\", \" done\", \"!\", \" No\", \" irregularities\", \" found\", \".\"], \"topKLogProbs\": [[-2.960587978363037, -3.191565990447998, -3.4212965965270996, -3.6120734214782715, -3.725257396697998, -3.749629497528076, -3.7500205039978027, -3.9745020866394043, -3.9862780570983887, -4.036376476287842], [-0.903695285320282, -1.585566759109497, -2.861851930618286, -3.7202980518341064, -3.746690034866333, -3.7652065753936768, -3.889479875564575, -4.087700843811035, -4.096268653869629, -4.153962135314941], [-0.7037695646286011, -2.8518295288085938, -2.9083404541015625, -3.074298858642578, -3.1902599334716797, -3.502218246459961, -3.7172718048095703, -3.920980453491211, -4.076862335205078, -4.100748062133789], [-0.9461849331855774, -1.0761516094207764, -3.5550639629364014, -3.6822421550750732, -3.788466215133667, -3.936363935470581, -4.163020133972168, -4.431271553039551, -4.4495038986206055, -4.464102745056152], [-0.1813388168811798, -2.611764430999756, -4.218472957611084, -4.6751627922058105, -5.70587682723999, -5.830287456512451, -5.951112270355225, -5.9714274406433105, -6.079738140106201, -6.2689642906188965], [-1.9673271179199219, -2.3190040588378906, -2.3807296752929688, -2.9351558685302734, -3.123199462890625, -3.169239044189453, -3.468656539916992, -3.704883575439453, -3.7935714721679688, -4.015897750854492], [-1.1267268657684326, -1.7686755657196045, -2.4329898357391357, -3.020458936691284, -3.0824058055877686, -3.186863660812378, -3.8918139934539795, -3.894881010055542, -4.072047233581543, -4.201912879943848], [-1.799303412437439, -1.8418067693710327, -2.4186177253723145, -2.632599353790283, -3.092796802520752, -3.7297234535217285, -3.8576111793518066, -3.8762288093566895, -4.213301181793213, -4.241726398468018], [-0.4700761139392853, -2.4971375465393066, -3.032972812652588, -3.556946277618408, -3.587099552154541, -4.043890476226807, -4.382347583770752, -4.434774875640869, -4.501301288604736, -4.898121356964111], [-1.337134599685669, -1.8061058521270752, -2.963874101638794, -3.0827629566192627, -3.4460041522979736, -3.497941255569458, -3.6653244495391846, -3.910874605178833, -4.101662635803223, -4.160201072692871], [-2.1197283267974854, -2.2582457065582275, -2.3042452335357666, -2.4996626377105713, -2.6411097049713135, -3.551187753677368, -3.9380342960357666, -3.9746077060699463, -4.027736663818359, -4.153476715087891], [-1.8796086311340332, -2.5045323371887207, -2.9731717109680176, -3.0786690711975098, -3.1241631507873535, -3.319763660430908, -3.3609166145324707, -3.4561963081359863, -3.4844155311584473, -4.05003023147583], [-1.2579846382141113, -2.3330349922180176, -2.520235538482666, -2.736299991607666, -3.1089024543762207, -3.6286721229553223, -3.6751160621643066, -3.8985085487365723, -4.197079181671143, -4.296688556671143], [-0.5002702474594116, -2.2439475059509277, -2.3541178703308105, -3.695779323577881, -4.0894598960876465, -4.425439357757568, -4.503051280975342, -4.75787878036499, -4.865203380584717, -4.923949718475342], [-1.2618812322616577, -2.0589470863342285, -2.1313652992248535, -2.792423725128174, -2.884218692779541, -3.151306629180908, -3.504324436187744, -3.5762314796447754, -3.7854866981506348, -3.826878070831299], [-1.2117819786071777, -2.1562209129333496, -2.593728542327881, -2.8456759452819824, -3.127734661102295, -3.183928966522217, -3.19276762008667, -3.3844428062438965, -3.6424612998962402, -3.668962001800537], [-1.31378173828125, -1.4892463684082031, -1.7789554595947266, -2.960233688354492, -3.0063953399658203, -4.247278213500977, -4.354394912719727, -4.355564117431641, -4.410295486450195, -4.605007171630859], [-1.9937961101531982, -2.587853193283081, -2.7777011394500732, -2.806633710861206, -2.8296382427215576, -2.872837781906128, -3.132317304611206, -3.241345167160034, -3.291638135910034, -3.660465955734253], [-0.3482103645801544, -3.3120288848876953, -3.8634586334228516, -4.463379859924316, -4.476961135864258, -4.827144622802734, -4.852437973022461, -4.960593223571777, -5.098820686340332, -5.135746955871582], [-1.8971747159957886, -2.750965118408203, -3.0507278442382812, -3.600933074951172, -3.667722702026367, -3.7782211303710938, -3.942150115966797, -3.9581985473632812, -4.042070388793945, -4.1302032470703125], [-1.113957405090332, -1.814896583557129, -2.375990867614746, -2.5293798446655273, -2.895655632019043, -3.488368034362793, -3.578474998474121, -4.365384101867676, -4.444365501403809, -4.548577308654785], [-2.1613481044769287, -2.197089910507202, -2.3200509548187256, -2.9269731044769287, -3.0087735652923584, -3.0486161708831787, -3.0956075191497803, -3.205220937728882, -3.2366693019866943, -3.5708463191986084], [-1.505505919456482, -1.9156678915023804, -2.474952220916748, -2.6922316551208496, -3.0555453300476074, -3.4138617515563965, -3.5060839653015137, -3.60603666305542, -3.869626522064209, -3.9273314476013184], [-1.301060676574707, -2.019162178039551, -2.6920461654663086, -2.6940393447875977, -2.923211097717285, -3.455235481262207, -3.4869794845581055, -3.5542707443237305, -3.59342098236084, -3.749403953552246], [-0.7519649267196655, -1.3437350988388062, -2.991854190826416, -3.6317429542541504, -3.8434700965881348, -3.976768970489502, -4.216831684112549, -4.263691425323486, -4.483656406402588, -4.693796634674072], [-1.9557418823242188, -2.1731929779052734, -2.899568557739258, -3.3363170623779297, -3.474111557006836, -3.7284107208251953, -3.950563430786133, -3.955425262451172, -4.012115478515625, -4.025295257568359], [-1.1754435300827026, -1.824844241142273, -2.019197463989258, -2.425384521484375, -2.734159469604492, -3.227752685546875, -3.3617630004882812, -3.7795162200927734, -4.444910049438477, -4.446432113647461], [-0.31745368242263794, -2.700004816055298, -2.858015298843384, -2.968254327774048, -3.531639337539673, -3.782280206680298, -5.147388935089111, -5.720888614654541, -5.81008768081665, -5.98253870010376], [-1.7533137798309326, -1.8567016124725342, -2.4201858043670654, -2.5474040508270264, -3.1176764965057373, -3.424473524093628, -3.673626661300659, -4.004422187805176, -4.130850791931152, -4.17604923248291], [-0.9584576487541199, -2.2984464168548584, -2.621732473373413, -3.0108792781829834, -3.3414762020111084, -3.681091070175171, -3.8815648555755615, -3.9404141902923584, -4.108026027679443, -4.133798122406006], [-1.8806382417678833, -2.2066574096679688, -2.3196678161621094, -2.7865352630615234, -2.8629093170166016, -2.9486007690429688, -3.550477981567383, -3.6541671752929688, -3.6563720703125, -3.777254104614258], [-0.6558043956756592, -2.562849760055542, -2.6506946086883545, -3.1009433269500732, -3.2243525981903076, -3.4770400524139404, -3.602368116378784, -3.749680280685425, -3.984837293624878, -3.998762845993042], [-0.9775125980377197, -1.279604196548462, -3.1744120121002197, -3.2537310123443604, -3.6842596530914307, -3.7297861576080322, -4.05549430847168, -4.28980827331543, -4.398284912109375, -4.689537048339844], [-1.3894424438476562, -1.9005756378173828, -2.0379600524902344, -2.4479522705078125, -3.8459720611572266, -4.001636505126953, -4.142396926879883, -4.258211135864258, -4.357236862182617, -4.513004302978516], [-2.285923719406128, -2.806542158126831, -2.90437388420105, -3.154677152633667, -3.2293288707733154, -3.345806837081909, -3.5761592388153076, -4.023436546325684, -4.053942680358887, -4.062830924987793], [-1.013359546661377, -1.8267922401428223, -2.579338550567627, -2.626020908355713, -2.8296570777893066, -3.0855488777160645, -3.4074692726135254, -3.504154682159424, -3.8565049171447754, -4.209412097930908], [-1.324873924255371, -1.7381391525268555, -2.6415815353393555, -3.060919761657715, -3.186894416809082, -3.3835363388061523, -3.8849992752075195, -3.890915870666504, -4.346449851989746, -4.37454891204834], [-0.4198455512523651, -2.5462398529052734, -3.0579605102539062, -3.1840381622314453, -3.3902950286865234, -4.071474075317383, -4.48101806640625, -4.687541961669922, -4.864561080932617, -4.872365951538086], [-0.5952609777450562, -1.95990788936615, -3.420637607574463, -3.5354294776916504, -3.551758289337158, -3.729950428009033, -5.0791916847229, -5.113630771636963, -5.338587284088135, -5.529879093170166], [-0.5220663547515869, -2.706247568130493, -3.4347479343414307, -3.8342249393463135, -3.888406991958618, -3.898756265640259, -3.9677393436431885, -4.167030334472656, -4.235147476196289, -4.305021286010742], [-1.3019013404846191, -1.3977084159851074, -1.5410256385803223, -3.6820569038391113, -3.8539395332336426, -3.9119253158569336, -4.424116611480713, -4.455972194671631, -4.518988132476807, -5.014242649078369], [-0.7125241756439209, -2.6182239055633545, -3.7400779724121094, -4.0441131591796875, -4.069088935852051, -4.080831527709961, -4.205142021179199, -4.279363632202148, -4.28247594833374, -4.365234375], [-0.01620255969464779, -5.254730701446533, -6.20664644241333, -6.539077281951904, -7.092835903167725, -7.1904377937316895, -7.452311038970947, -7.679624080657959, -7.690454959869385, -7.706045627593994], [-0.6465806365013123, -0.8927831053733826, -4.8689069747924805, -4.97945499420166, -5.551609992980957, -5.836947441101074, -5.964974403381348, -6.232911109924316, -6.3971967697143555, -6.4380903244018555], [-1.2210683822631836, -2.5658578872680664, -2.747065544128418, -3.0008649826049805, -3.740227699279785, -4.14408016204834, -4.169390678405762, -4.177020072937012, -4.215775489807129, -4.259840965270996], [-1.1575312614440918, -1.6879782676696777, -2.0404295921325684, -2.497697353363037, -4.407212734222412, -4.44649076461792, -4.537013530731201, -4.719142436981201, -4.719832897186279, -4.827996730804443], [-0.8272605538368225, -2.2874369621276855, -3.1521315574645996, -3.231627941131592, -3.340449810028076, -3.4408717155456543, -3.6010279655456543, -3.6611742973327637, -4.287280559539795, -4.324971675872803], [-0.7858427166938782, -1.3913419246673584, -2.7896687984466553, -2.9664876461029053, -3.697396993637085, -4.120704650878906, -4.747901916503906, -4.97419548034668, -5.08466911315918, -5.185861587524414], [-1.5040608644485474, -1.7392102479934692, -2.0926685333251953, -2.320598602294922, -3.5356407165527344, -3.8100032806396484, -3.896697998046875, -4.122989654541016, -4.163434982299805, -4.27587890625], [-1.3640072345733643, -2.144322633743286, -2.431523561477661, -3.3077824115753174, -3.49432110786438, -3.514737367630005, -3.631484270095825, -3.6741859912872314, -3.7389633655548096, -4.136155128479004], [-1.6356306076049805, -2.1305437088012695, -2.3253679275512695, -2.340712547302246, -2.515143394470215, -3.041672706604004, -3.2448244094848633, -3.591700553894043, -3.83162784576416, -3.949936866760254], [-1.074742078781128, -1.1519553661346436, -2.007675886154175, -2.712254285812378, -4.380972862243652, -4.384909629821777, -4.564854621887207, -4.786133766174316, -4.869341850280762, -5.214337348937988]], \"topKTokens\": [[\" my\\u017felf\", \" it\\u017felf\", \" them\\u017felves\", \" Jefus\", \" purpo\\u017fe\", \" plea\\u017fure\", \" him\\u017felf\", \"\\u017felf\", \" \\u017feveral\", \" Majefty\"], [\"html\", \"div\", \"svg\", \"h\", \"nav\", \"form\", \"a\", \"section\", \"link\", \"php\"], [\"-\", \"_\", \".\", \"ing\", \">\", \" \", \"1\", \"\\n\", \"2\", \"y\"], [\"-\", \">\", \" id\", \".\", \"_\", \"=\\\"\", \" name\", \">\\r\", \":\", \"\\n\"], [\"\\n\", \"\\n\\n\", \"https\", \" \", \"\\\"\\\"\\\"\", \"\\\"\\\"\", \"   \", \"{\\\"\", \"<strong>\", \"1\"], [\" a\", \" the\", \" \", \" some\", \" all\", \" reading\", \" I\", \" installing\", \" looking\", \" this\"], [\" the\", \" a\", \" all\", \" this\", \" an\", \" \", \" files\", \" and\", \" my\", \" some\"], [\" files\", \" two\", \" \", \" lines\", \",\", \" items\", \" folders\", \" old\", \" three\", \" entries\"], [\",\", \" I\", \" and\", \" from\", \" you\", \" in\", \" the\", \" with\", \" it\", \"\\n\"], [\" I\", \" the\", \" you\", \" it\", \" we\", \" can\", \" did\", \" this\", \" please\", \"\\n\"], [\" problem\", \" error\", \" logs\", \" log\", \" issue\", \" following\", \" same\", \" next\", \" system\", \" server\"], [\" was\", \" can\", \" is\", \" could\", \" will\", \"'\", \" found\", \" had\", \" has\", \" should\"], [\" be\", \" have\", \" need\", \" see\", \" not\", \" then\", \" get\", \" still\", \" find\", \" delete\"], [\" be\", \" see\", \" have\", \" get\", \" know\", \"\\n\", \" delete\", \" find\", \" only\", \" receive\"], [\" the\", \" any\", \" that\", \" a\", \" those\", \" this\", \" anything\", \" them\", \" you\", \" it\"], [\" the\", \" you\", \" they\", \" a\", \" there\", \" any\", \" I\", \" it\", \" those\", \" he\"], [\"'\", \" am\", \" have\", \" was\", \" deleted\", \" did\", \" logged\", \" had\", \" can\", \" created\"], [\" using\", \" in\", \" on\", \" not\", \" a\", \" logged\", \" running\", \" still\", \" connected\", \" deleting\"], [\"tively\", \"te\", \"ters\", \"ticons\", \"tig\", \"tious\", \"tive\", \"tec\", \"ter\", \"ti\"], [\" deleting\", \" using\", \" logging\", \" changing\", \"\\n\", \" modifying\", \" trying\", \" editing\", \" accessing\", \" sending\"], [\" with\", \".\", \" to\", \",\", \" and\", \" in\", \"\\n\", \" on\", \" because\", \"\\n\\n\"], [\" I\", \"\\n\", \"\\n\\n\", \" After\", \" The\", \" So\", \" This\", \" But\", \" If\", \" And\"], [\" will\", \" can\", \" am\", \"'\", \" have\", \" don\", \" think\", \" just\", \" do\", \" also\"], [\" not\", \" be\", \" have\", \" still\", \" then\", \" now\", \" need\", \" just\", \" also\", \" get\"], [\" the\", \" my\", \" those\", \" that\", \" this\", \" a\", \" it\", \" them\", \" some\", \" one\"], [\" log\", \" code\", \" logs\", \" script\", \" file\", \" configuration\", \" behavior\", \" data\", \" following\", \" system\"], [\" of\", \" on\", \" to\", \" in\", \" and\", \" for\", \" so\", \",\", \" again\", \".\"], [\" that\", \" the\", \" I\", \" they\", \" as\", \" it\", \" my\", \" there\", \" when\", \" this\"], [\" will\", \" is\", \" happens\", \" does\", \" log\", \" can\", \" time\", \" occurs\", \" looks\", \" behavior\"], [\" not\", \" no\", \" a\", \" the\", \" still\", \" now\", \" true\", \" only\", \" an\", \" happening\"], [\" a\", \" detected\", \" the\", \" an\", \" true\", \" possible\", \" obvious\", \" visible\", \" seen\", \" apparent\"], [\".\", \" by\", \",\", \" as\", \" and\", \" in\", \".\\\"\\\"\\\"\", \".\\\"\", \" again\", \".</\"], [\" and\", \" but\", \" then\", \" so\", \" which\", \" I\", \" as\", \" however\", \" by\", \" or\"], [\" then\", \" I\", \" will\", \" the\", \" it\", \" so\", \" that\", \" after\", \" this\", \" delete\"], [\" be\", \" detected\", \" only\", \" delete\", \" have\", \" get\", \" even\", \" logged\", \" deleted\", \" a\"], [\" that\", \" the\", \" anything\", \" this\", \" it\", \" any\", \" to\", \" in\", \" my\", \" I\"], [\" in\", \" to\", \".\", \" as\", \" at\", \" is\", \" behavior\", \" change\", \" with\", \" on\"], [\" the\", \" anyone\", \" my\", \" any\", \" him\", \" them\", \" those\", \" me\", \" her\", \" other\"], [\" researcher\", \" researchers\", \" investigator\", \" other\", \" research\", \" user\", \" person\", \" reporter\", \" public\", \" group\"], [\".\", \",\", \".\\\"\", \" so\", \" at\", \" in\", \".\\\"\\\"\\\"\", \"'\", \".</\", \" and\"], [\"span\", \"br\", \"script\", \"p\", \"textarea\", \"sc\", \"scratch\", \"abstract\", \"sketch\", \"\\n\"], [\"pad\", \"patch\", \"page\", \"pan\", \"<unused62>\", \"<unused63>\", \"pencil\", \"note\", \"sketch\", \"\\n\"], [\">\", \">\\r\", \"><\", \">-->\", \">\\\\\", \">`\", \">--}}\", \"<unused63>\", \">.\", \"\\\">\"], [\"\\n\", \"\\n\\n\", \" \", \"\\n\\n\\n\", \"   \", \"\\\"\\\"\\\"\", \" <\", \"https\", \"  \", \" This\"], [\" researcher\", \" following\", \" research\", \" \", \" first\", \" researchers\", \" above\", \" next\", \" problem\", \" last\"], [\" is\", \" of\", \" will\", \" was\", \" for\", \" force\", \" has\", \",\", \" in\", \" that\"], [\" to\", \" not\", \" a\", \" done\", \" completed\", \" simple\", \" complete\", \" now\", \" over\", \" the\"], [\".\", \",\", \" and\", \"!\", \" with\", \" in\", \";\", \":\", \" now\", \".<\"], [\"\\n\\n\", \"\\n\", \" The\", \" I\", \" Now\", \" This\", \" You\", \" And\", \" We\", \" It\"], [\" one\", \" more\", \" matter\", \" need\", \" problem\", \" evidence\", \" further\", \"-\", \" problems\", \" worries\"], [\" were\", \" have\", \" found\", \" in\", \" detected\", \"!\", \".\", \" or\", \" at\", \",\"], [\"!\", \".\", \" in\", \",\", \" on\", \" at\", \" by\", \" and\", \"\\n\", \" here\"]], \"correctTokenRank\": [161188, 149652, 10, 1, 440, 37, 41, 29, 0, 1, 9673, 4, 4, 51, 2, 6, 1, 89810, 0, 1320, 1, 0, 0, 541, 0, 2581, 6, 9, 1, 0, 1, 2, 0, 12, 69, 3, 1, 0, 0, 8, 6, 0, 0, 41, 66, 0, 3, 3, 24, 452, 2, 1], \"correctTokenLogProb\": [-30.864017486572266, -29.226961135864258, -4.2215728759765625, -1.0761516094207764, -11.426105499267578, -5.264604568481445, -6.503325462341309, -5.664923191070557, -0.4700761139392853, -1.8061058521270752, -17.826656341552734, -3.1241631507873535, -3.1089024543762207, -7.6184306144714355, -2.1313652992248535, -3.19276762008667, -1.4892463684082031, -28.04058074951172, -0.3482103645801544, -11.319568634033203, -1.814896583557129, -2.1613481044769287, -1.505505919456482, -11.818074226379395, -0.7519649267196655, -13.183748245239258, -3.3617630004882812, -5.98253870010376, -1.8567016124725342, -0.9584576487541199, -2.2066574096679688, -2.6506946086883545, -0.9775125980377197, -4.721271514892578, -6.0618486404418945, -2.626020908355713, -1.7381391525268555, -0.4198455512523651, -0.5952609777450562, -4.235147476196289, -4.424116611480713, -0.7125241756439209, -0.01620255969464779, -8.130566596984863, -6.402190208435059, -1.1575312614440918, -3.231627941131592, -2.9664876461029053, -5.343799591064453, -10.034209251403809, -2.3253679275512695, -1.1519553661346436]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x17df22c00>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that it looks roughly the same now that we're using SAE to reconstruct activations\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "# Visualize token probabilities\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=model.to_tokens(example_prompt),\n",
    "    log_probs=log_probs,\n",
    "    to_string=model.to_string,\n",
    ")\n",
    "\n",
    "# note: interestingly there *is* some difference, for example `hole` is 9% here but was 97% originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88e63a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  --------------------------\n",
      "blocks.20.hook_resid_post.hook_sae_input      torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_resid_post.hook_sae_acts_pre   torch.Size([1, 53, 16384])\n",
      "blocks.20.hook_resid_post.hook_sae_acts_post  torch.Size([1, 53, 16384])\n",
      "blocks.20.hook_resid_post.hook_sae_recons     torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_resid_post.hook_sae_output     torch.Size([1, 53, 2304])\n",
      "--------------------------------------------  --------------------------\n",
      "Relevant numbers:\n",
      "- example_prompt_tokens.shape=torch.Size([1, 53])\n",
      "- model.cfg.d_model=2304\n",
      "- sae.cfg.d_sae=16384\n"
     ]
    }
   ],
   "source": [
    "# see what's in the cache related to SAE\n",
    "print(tabulate.tabulate([(k, v.shape) for k, v in cache.items() if \"sae\" in k]))\n",
    "\n",
    "# ex: because this SAE is operating on the residual stream\n",
    "assert sae.cfg.d_in == model.cfg.d_model\n",
    "\n",
    "example_prompt_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "print(f\"Relevant numbers:\")\n",
    "print(f\"- {example_prompt_tokens.shape=}\")\n",
    "print(f\"- {model.cfg.d_model=}\")\n",
    "print(f\"- {sae.cfg.d_sae=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "01ba4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------  ---------------------------\n",
      "hook_embed                          torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.0.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.0.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.0.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.0.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.0.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.0.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.0.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.0.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.0.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.0.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.0.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.0.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.0.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.0.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.0.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.0.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.0.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.0.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.0.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.0.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.1.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.1.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.1.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.1.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.1.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.1.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.1.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.1.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.1.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.1.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.1.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.1.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.1.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.1.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.1.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.1.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.1.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.1.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.1.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.1.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.2.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.2.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.2.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.2.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.2.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.2.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.2.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.2.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.2.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.2.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.2.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.2.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.2.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.2.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.2.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.2.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.2.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.2.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.2.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.2.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.3.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.3.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.3.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.3.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.3.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.3.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.3.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.3.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.3.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.3.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.3.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.3.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.3.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.3.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.3.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.3.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.3.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.3.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.3.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.3.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.4.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.4.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.4.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.4.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.4.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.4.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.4.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.4.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.4.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.4.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.4.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.4.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.4.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.4.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.4.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.4.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.4.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.4.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.4.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.4.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.5.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.5.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.5.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.5.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.5.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.5.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.5.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.5.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.5.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.5.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.5.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.5.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.5.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.5.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.5.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.5.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.5.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.5.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.5.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.5.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.6.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.6.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.6.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.6.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.6.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.6.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.6.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.6.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.6.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.6.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.6.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.6.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.6.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.6.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.6.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.6.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.6.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.6.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.6.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.6.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.7.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.7.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.7.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.7.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.7.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.7.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.7.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.7.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.7.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.7.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.7.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.7.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.7.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.7.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.7.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.7.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.7.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.7.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.7.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.7.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.8.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.8.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.8.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.8.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.8.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.8.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.8.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.8.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.8.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.8.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.8.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.8.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.8.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.8.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.8.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.8.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.8.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.8.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.8.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.8.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_resid_pre             torch.Size([1, 53, 2304])\n",
      "blocks.9.ln1.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.9.ln1.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.9.attn.hook_q                torch.Size([1, 53, 8, 256])\n",
      "blocks.9.attn.hook_k                torch.Size([1, 53, 4, 256])\n",
      "blocks.9.attn.hook_v                torch.Size([1, 53, 4, 256])\n",
      "blocks.9.attn.hook_rot_q            torch.Size([1, 53, 8, 256])\n",
      "blocks.9.attn.hook_rot_k            torch.Size([1, 53, 4, 256])\n",
      "blocks.9.attn.hook_attn_scores      torch.Size([1, 8, 53, 53])\n",
      "blocks.9.attn.hook_pattern          torch.Size([1, 8, 53, 53])\n",
      "blocks.9.attn.hook_z                torch.Size([1, 53, 8, 256])\n",
      "blocks.9.ln1_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.9.ln1_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_attn_out              torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_resid_mid             torch.Size([1, 53, 2304])\n",
      "blocks.9.ln2.hook_scale             torch.Size([1, 53, 1])\n",
      "blocks.9.ln2.hook_normalized        torch.Size([1, 53, 2304])\n",
      "blocks.9.mlp.hook_pre               torch.Size([1, 53, 9216])\n",
      "blocks.9.mlp.hook_pre_linear        torch.Size([1, 53, 9216])\n",
      "blocks.9.mlp.hook_post              torch.Size([1, 53, 9216])\n",
      "blocks.9.ln2_post.hook_scale        torch.Size([1, 53, 1])\n",
      "blocks.9.ln2_post.hook_normalized   torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_mlp_out               torch.Size([1, 53, 2304])\n",
      "blocks.9.hook_resid_post            torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.10.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.10.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.10.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.10.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.10.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.10.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.10.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.10.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.10.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.10.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.10.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.10.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.10.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.10.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.10.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.10.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.10.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.10.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.10.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.10.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.11.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.11.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.11.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.11.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.11.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.11.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.11.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.11.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.11.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.11.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.11.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.11.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.11.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.11.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.11.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.11.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.11.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.11.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.11.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.11.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.12.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.12.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.12.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.12.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.12.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.12.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.12.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.12.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.12.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.12.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.12.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.12.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.12.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.12.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.12.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.12.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.12.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.12.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.12.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.12.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.13.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.13.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.13.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.13.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.13.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.13.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.13.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.13.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.13.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.13.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.13.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.13.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.13.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.13.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.13.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.13.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.13.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.13.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.13.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.13.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.14.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.14.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.14.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.14.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.14.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.14.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.14.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.14.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.14.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.14.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.14.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.14.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.14.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.14.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.14.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.14.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.14.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.14.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.14.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.14.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.15.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.15.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.15.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.15.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.15.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.15.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.15.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.15.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.15.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.15.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.15.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.15.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.15.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.15.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.15.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.15.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.15.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.15.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.15.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.15.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.16.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.16.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.16.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.16.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.16.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.16.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.16.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.16.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.16.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.16.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.16.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.16.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.16.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.16.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.16.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.16.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.16.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.16.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.16.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.16.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.17.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.17.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.17.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.17.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.17.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.17.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.17.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.17.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.17.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.17.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.17.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.17.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.17.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.17.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.17.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.17.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.17.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.17.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.17.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.17.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.18.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.18.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.18.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.18.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.18.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.18.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.18.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.18.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.18.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.18.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.18.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.18.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.18.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.18.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.18.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.18.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.18.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.18.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.18.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.18.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.19.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.19.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.19.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.19.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.19.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.19.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.19.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.19.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.19.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.19.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.19.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.19.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.19.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.19.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.19.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.19.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.19.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.19.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.19.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.19.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.20.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.20.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.20.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.20.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.20.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.20.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.20.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.20.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.20.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.20.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.20.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.20.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.20.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.20.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.20.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.20.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.20.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.20.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.20.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.20.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.21.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.21.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.21.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.21.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.21.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.21.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.21.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.21.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.21.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.21.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.21.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.21.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.21.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.21.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.21.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.21.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.21.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.21.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.21.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.21.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.22.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.22.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.22.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.22.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.22.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.22.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.22.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.22.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.22.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.22.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.22.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.22.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.22.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.22.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.22.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.22.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.22.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.22.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.22.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.22.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.23.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.23.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.23.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.23.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.23.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.23.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.23.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.23.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.23.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.23.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.23.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.23.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.23.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.23.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.23.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.23.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.23.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.23.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.23.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.23.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.24.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.24.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.24.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.24.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.24.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.24.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.24.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.24.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.24.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.24.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.24.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.24.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.24.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.24.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.24.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.24.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.24.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.24.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.24.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.24.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_resid_pre            torch.Size([1, 53, 2304])\n",
      "blocks.25.ln1.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.25.ln1.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.25.attn.hook_q               torch.Size([1, 53, 8, 256])\n",
      "blocks.25.attn.hook_k               torch.Size([1, 53, 4, 256])\n",
      "blocks.25.attn.hook_v               torch.Size([1, 53, 4, 256])\n",
      "blocks.25.attn.hook_rot_q           torch.Size([1, 53, 8, 256])\n",
      "blocks.25.attn.hook_rot_k           torch.Size([1, 53, 4, 256])\n",
      "blocks.25.attn.hook_attn_scores     torch.Size([1, 8, 53, 53])\n",
      "blocks.25.attn.hook_pattern         torch.Size([1, 8, 53, 53])\n",
      "blocks.25.attn.hook_z               torch.Size([1, 53, 8, 256])\n",
      "blocks.25.ln1_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.25.ln1_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_attn_out             torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_resid_mid            torch.Size([1, 53, 2304])\n",
      "blocks.25.ln2.hook_scale            torch.Size([1, 53, 1])\n",
      "blocks.25.ln2.hook_normalized       torch.Size([1, 53, 2304])\n",
      "blocks.25.mlp.hook_pre              torch.Size([1, 53, 9216])\n",
      "blocks.25.mlp.hook_pre_linear       torch.Size([1, 53, 9216])\n",
      "blocks.25.mlp.hook_post             torch.Size([1, 53, 9216])\n",
      "blocks.25.ln2_post.hook_scale       torch.Size([1, 53, 1])\n",
      "blocks.25.ln2_post.hook_normalized  torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_mlp_out              torch.Size([1, 53, 2304])\n",
      "blocks.25.hook_resid_post           torch.Size([1, 53, 2304])\n",
      "ln_final.hook_scale                 torch.Size([1, 53, 1])\n",
      "ln_final.hook_normalized            torch.Size([1, 53, 2304])\n",
      "----------------------------------  ---------------------------\n"
     ]
    }
   ],
   "source": [
    "# show everything not related to SAE (note it's essentially just every operation hooked)\n",
    "print(tabulate.tabulate([(k, v.shape) for k, v in cache.items() if \"sae\" not in k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000776f",
   "metadata": {},
   "source": [
    "#### What feature explanations do we have for this SAE?\n",
    "\n",
    "* Explanations are generated by GPT-4o-mini looking at activating examples in `ThePile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29a7d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Any\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "\n",
    "# ex: https://www.neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/3742\n",
    "#     from url directly\n",
    "\n",
    "\n",
    "# note: not all SAEs in neuronpedia yet, so we get the closest one\n",
    "class NeuronpediaConstants:\n",
    "\n",
    "    MODEL_ID = \"gemma-2-2b\"\n",
    "\n",
    "    # note: this must be same width as the `sae_id` we're using for the loaded SAE, otherwise there won't be autointerp explanations available\n",
    "    # SAE_ID = \"25-gemmascope-res-16k\"\n",
    "\n",
    "    # copied exactly from gemmascope colab tutorial https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp#scrollTo=2-i7YRVLgKoT\n",
    "    SAE_ID = \"20-gemmascope-res-16k\"\n",
    "\n",
    "    EXPORT_URL = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "\n",
    "def get_neuronpedia_dashboard_html_url(\n",
    "    feature_index: int,\n",
    "    model_id: str = NeuronpediaConstants.MODEL_ID,\n",
    "    sae_id: str = NeuronpediaConstants.SAE_ID,\n",
    ") -> str:\n",
    "    \"\"\"Create URL for getting an individual feature's HTML, rendered via IFrame\"\"\"\n",
    "    return (\n",
    "        f\"https://www.neuronpedia.org/{model_id}/{sae_id}/{feature_index}\"\n",
    "        \"?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "    )\n",
    "\n",
    "\n",
    "def show_neuronpedia_dashboard(\n",
    "    feature_index: int,\n",
    "    model_id: str = NeuronpediaConstants.MODEL_ID,\n",
    "    sae_id: str = NeuronpediaConstants.SAE_ID,\n",
    ") -> None:\n",
    "    \"\"\"Show the neuronpedia dashboard for a given feature index\"\"\"\n",
    "\n",
    "    html_url = get_neuronpedia_dashboard_html_url(feature_index=feature_index)\n",
    "\n",
    "    display(IPython.display.IFrame(html_url, width=800, height=500))\n",
    "\n",
    "\n",
    "# API: https://www.neuronpedia.org/api-doc\n",
    "#\n",
    "# note: so neuronpedia is also a store of autointerp explanations\n",
    "def get_neuronpedia_explanations(\n",
    "    model_id: str = NeuronpediaConstants.MODEL_ID,\n",
    "    sae_id: str = NeuronpediaConstants.SAE_ID,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get explanations from neuronpedia for a given model and sae.\"\"\"\n",
    "\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "    payload = {\n",
    "        \"modelId\": model_id,\n",
    "        \"saeId\": sae_id,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.get(url, params=payload, headers=headers)\n",
    "\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # - explanations\n",
    "    # - explanationsCount\n",
    "    response_json: list[dict[str, Any]] = response.json()\n",
    "\n",
    "    print(f\"{len(response_json)=} {type(response_json)=}\")\n",
    "\n",
    "    num_explanations = len(response_json)\n",
    "    print(f\"{num_explanations=}\")\n",
    "\n",
    "    print(\"Example explanation:\")\n",
    "    python_utils.print_json(response_json[0])\n",
    "\n",
    "    # convert to pandas\n",
    "    explanations_df = pd.DataFrame(response_json)\n",
    "\n",
    "    # rename index to \"feature\"\n",
    "    explanations_df = explanations_df.rename(columns={\"index\": \"feature\"})\n",
    "\n",
    "    # explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "    explanations_df[\"description\"] = explanations_df[\"description\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "\n",
    "    return explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13aceec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(response_json)=17008 type(response_json)=<class 'list'>\n",
      "num_explanations=17008\n",
      "Example explanation:\n",
      "{\n",
      "  \"modelId\": \"gemma-2-2b\",\n",
      "  \"layer\": \"20-gemmascope-res-16k\",\n",
      "  \"index\": \"14403\",\n",
      "  \"description\": \"phrases or sentences that introduce lists, examples, or elaborations, often followed by commas.\",\n",
      "  \"explanationModelName\": \"claude-3-5-sonnet-20240620\",\n",
      "  \"typeName\": \"oai_token-act-pair\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "explanations_df = get_neuronpedia_explanations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae4bb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(bschoen): How are features not unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f454805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelId                                                        gemma-2-2b\n",
       "layer                                               20-gemmascope-res-16k\n",
       "feature                                                             14403\n",
       "description             phrases or sentences that introduce lists, exa...\n",
       "explanationModelName                           claude-3-5-sonnet-20240620\n",
       "typeName                                               oai_token-act-pair\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6fddfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "explanationModelName\n",
       "gpt-4o-mini                   16384\n",
       "claude-3-5-sonnet-20240620      317\n",
       "gpt-3.5-turbo                   303\n",
       "gemini-1.5-flash                  2\n",
       "gemini-1.5-pro                    1\n",
       "gpt-4o                            1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay so only gpt-4o-mini for now\n",
    "explanations_df[\"explanationModelName\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf41819",
   "metadata": {},
   "source": [
    "##### Searching for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e676cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5088: 'situations involving deception or trickery',\n",
       " 5136: 'words or phrases related to deception or manipulation',\n",
       " 6741: 'phrases related to deception and misleading information',\n",
       " 6866: 'terms related to artificiality and deception',\n",
       " 15298: 'terms and concepts related to fraudulent activities, including various forms of fraud and deception'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_description = \"decept\"\n",
    "\n",
    "df_target_descriptions = explanations_df.loc[\n",
    "    explanations_df.description.str.contains(target_description)\n",
    "]\n",
    "\n",
    "df_target_descriptions[\"description\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = explanations_df[\"feature\"].iloc[14919]\n",
    "\n",
    "html_url = get_neuronpedia_dashboard_html_url(feature_index=feature_index)\n",
    "\n",
    "IPython.display.IFrame(html_url, width=800, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b5dee17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16384])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at which features fired\n",
    "cache_id = \"blocks.20.hook_resid_post.hook_sae_acts_post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1efd604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_activations.shape=torch.Size([53, 16384])\n"
     ]
    }
   ],
   "source": [
    "# # torch.Size([1, <prompt_length>, <sae_size>]) -> (<prompt_length>, <sae_size>)\n",
    "sae_activations = cache[cache_id][0]\n",
    "\n",
    "print(f\"{sae_activations.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a65efe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_values.shape=torch.Size([53])\n"
     ]
    }
   ],
   "source": [
    "# get the max activating feature at each position\n",
    "#\n",
    "# both of size (<prompt-length>, )\n",
    "activation_values, feature_indices = sae_activations.max(-1)\n",
    "\n",
    "print(f\"{activation_values.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c25f9057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 53, 16384])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[cache_id].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13bb42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top_k = False\n",
    "\n",
    "if show_top_k:\n",
    "\n",
    "    # let's print the top 5 features and how much they fired\n",
    "    topk_count = 5\n",
    "\n",
    "    # both are (<prompt-length>, <topk_count>)\n",
    "    activation_values, feature_indices = torch.topk(sae_activations, topk_count)\n",
    "\n",
    "    print(f\"{activation_values.shape=}\")\n",
    "    print(activation_values)\n",
    "\n",
    "    print(f\"{feature_indices.shape=}\")\n",
    "    print(feature_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "89fa119b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53, 16384])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d334bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 53])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 235322,  77363,   8939, 235313,   5514,  69427,   1941,  27365,\n",
       "         235269,    573,  45865,    877,    780,  21422,    674,    590,   1144,\n",
       "         153709,   6236,  44298, 235265,    590,    877,  18567,    573, 150865,\n",
       "            712,    736,    603,    780,  20040, 235269,    578,    780,   5512,\n",
       "            736,    577,    573,  45865,   7221,  77363,   8939, 235313,    651,\n",
       "           6911,    603,   3015, 235341,   1307, 107600,   1942, 235265]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert prompt to tokens so we can show it alongside features / feature index\n",
    "example_prompt_as_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "print(f\"{example_prompt_as_tokens.shape=}\")\n",
    "\n",
    "example_prompt_as_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f603e74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235313"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt_as_tokens[:, 4].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1cbe5923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4f79af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelId                                                        gemma-2-2b\n",
       "layer                                               20-gemmascope-res-16k\n",
       "feature                                                             14403\n",
       "description             phrases or sentences that introduce lists, exa...\n",
       "explanationModelName                           claude-3-5-sonnet-20240620\n",
       "typeName                                               oai_token-act-pair\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5d3fa499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16383"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations_df[\"feature\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ffcaaf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(feature_index_to_description)=16383\n"
     ]
    }
   ],
   "source": [
    "# note: just maps to first description, there may be multiple\n",
    "explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "\n",
    "feature_index_to_description = explanations_df.set_index(\"feature\")[\n",
    "    \"description\"\n",
    "].to_dict()\n",
    "\n",
    "print(f\"{len(feature_index_to_description)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c8a2becc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'words related to time travel and its consequences'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be time travel related\n",
    "feature_index_to_description[10004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b12af94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] <bos>\n",
      " - 2028.80 [6631] := the beginning of a text or important markers in a document\n",
      " - 781.40 [743] := the beginning of a text or important markers in a document\n",
      " - 534.86 [5052] := the beginning of a text or important markers in a document\n",
      " - 264.19 [16057] := the beginning of a text or important markers in a document\n",
      " - 252.53 [9479] := the beginning of a text or important markers in a document\n",
      "[1] <\n",
      " - 92.68 [11527] := the start of a document\n",
      " - 90.01 [8684] := the start of a document\n",
      " - 85.16 [5637] := the start of a document\n",
      " - 76.85 [864] := the start of a document\n",
      " - 54.10 [14266] := the start of a document\n",
      "[2] scratch\n",
      " - 71.79 [6631] := the beginning of a text or important markers in a document\n",
      " - 49.45 [5698] := the beginning of a text or important markers in a document\n",
      " - 37.94 [8366] := the beginning of a text or important markers in a document\n",
      " - 36.45 [9768] := the beginning of a text or important markers in a document\n",
      " - 35.28 [3019] := the beginning of a text or important markers in a document\n",
      "[3] pad\n",
      " - 79.13 [6231] := strings used in user interface elements\n",
      " - 43.81 [6631] := strings used in user interface elements\n",
      " - 43.18 [9768] := strings used in user interface elements\n",
      " - 40.34 [14723] := strings used in user interface elements\n",
      " - 32.27 [12004] := strings used in user interface elements\n",
      "[4] >\n",
      " - 50.84 [11082] := html elements and their attributes\n",
      " - 36.74 [9768] := html elements and their attributes\n",
      " - 30.27 [6760] := html elements and their attributes\n",
      " - 24.07 [12004] := html elements and their attributes\n",
      " - 24.05 [3972] := html elements and their attributes\n",
      "[5] After\n",
      " - 156.29 [4820] := references to the word \"after\"\n",
      " - 71.90 [6631] := references to the word \"after\"\n",
      " - 50.84 [3840] := references to the word \"after\"\n",
      " - 46.64 [9768] := references to the word \"after\"\n",
      " - 39.29 [4837] := references to the word \"after\"\n",
      "[6]  deleting\n",
      " - 81.39 [6631] := the beginning of a text or important markers in a document\n",
      " - 76.73 [10842] := the beginning of a text or important markers in a document\n",
      " - 48.70 [9768] := the beginning of a text or important markers in a document\n",
      " - 44.29 [15053] := the beginning of a text or important markers in a document\n",
      " - 40.25 [6143] := the beginning of a text or important markers in a document\n",
      "[7]  those\n",
      " - 76.81 [7846] :=  phrases and references to data and reporting\n",
      " - 72.61 [6631] :=  phrases and references to data and reporting\n",
      " - 50.04 [9768] :=  phrases and references to data and reporting\n",
      " - 40.82 [5627] :=  phrases and references to data and reporting\n",
      " - 37.77 [10400] :=  phrases and references to data and reporting\n",
      "[8]  logs\n",
      " - 86.96 [14] :=  mentions of logarithmic or related mathematical concepts\n",
      " - 60.03 [2282] :=  mentions of logarithmic or related mathematical concepts\n",
      " - 55.79 [15607] :=  mentions of logarithmic or related mathematical concepts\n",
      " - 43.70 [3223] :=  mentions of logarithmic or related mathematical concepts\n",
      " - 42.00 [6631] :=  mentions of logarithmic or related mathematical concepts\n",
      "[9] ,\n",
      " - 74.48 [4223] := questions and conditional statements related to requirements and criteria\n",
      " - 72.69 [10881] := questions and conditional statements related to requirements and criteria\n",
      " - 64.43 [8887] := questions and conditional statements related to requirements and criteria\n",
      " - 46.68 [6631] := questions and conditional statements related to requirements and criteria\n",
      " - 45.38 [9768] := questions and conditional statements related to requirements and criteria\n",
      "[10]  the\n",
      " - 113.08 [6631] := the beginning of a text or important markers in a document\n",
      " - 75.38 [14537] := the beginning of a text or important markers in a document\n",
      " - 58.29 [2885] := the beginning of a text or important markers in a document\n",
      " - 42.48 [5426] := the beginning of a text or important markers in a document\n",
      " - 42.03 [2082] := the beginning of a text or important markers in a document\n",
      "[11]  researcher\n",
      " - 88.59 [6631] := the beginning of a text or important markers in a document\n",
      " - 75.98 [2133] := the beginning of a text or important markers in a document\n",
      " - 52.51 [7182] := the beginning of a text or important markers in a document\n",
      " - 49.36 [5244] := the beginning of a text or important markers in a document\n",
      " - 44.75 [3940] := the beginning of a text or important markers in a document\n",
      "[12]  will\n",
      " - 91.66 [217] := the future tense verb \"will\" and its usage in sentences\n",
      " - 88.66 [10192] := the future tense verb \"will\" and its usage in sentences\n",
      " - 85.95 [6631] := the future tense verb \"will\" and its usage in sentences\n",
      " - 38.84 [9768] := the future tense verb \"will\" and its usage in sentences\n",
      " - 38.12 [15453] := the future tense verb \"will\" and its usage in sentences\n",
      "[13]  not\n",
      " - 89.71 [6684] := words and phrases expressing negation or absence\n",
      " - 67.50 [10192] := words and phrases expressing negation or absence\n",
      " - 62.07 [6631] := words and phrases expressing negation or absence\n",
      " - 43.29 [338] := words and phrases expressing negation or absence\n",
      " - 41.27 [9768] := words and phrases expressing negation or absence\n",
      "[14]  detect\n",
      " - 55.70 [13762] := phrases related to legal and ethical accountability in various contexts\n",
      " - 55.07 [6631] := phrases related to legal and ethical accountability in various contexts\n",
      " - 49.76 [2503] := phrases related to legal and ethical accountability in various contexts\n",
      " - 41.67 [9768] := phrases related to legal and ethical accountability in various contexts\n",
      " - 39.97 [6143] := phrases related to legal and ethical accountability in various contexts\n",
      "[15]  that\n",
      " - 86.72 [5627] := instances of the word \"that\" in various forms\n",
      " - 53.54 [13649] := instances of the word \"that\" in various forms\n",
      " - 41.31 [9768] := instances of the word \"that\" in various forms\n",
      " - 36.80 [13762] := instances of the word \"that\" in various forms\n",
      " - 29.08 [12434] := instances of the word \"that\" in various forms\n",
      "[16]  I\n",
      " - 89.56 [10873] := sentences beginning with the subject \"i.\"\n",
      " - 61.03 [3997] := sentences beginning with the subject \"i.\"\n",
      " - 53.45 [10461] := sentences beginning with the subject \"i.\"\n",
      " - 40.38 [6631] := sentences beginning with the subject \"i.\"\n",
      " - 39.51 [9768] := sentences beginning with the subject \"i.\"\n",
      "[17]  am\n",
      " - 57.62 [6631] := the beginning of a text or important markers in a document\n",
      " - 50.38 [15596] := the beginning of a text or important markers in a document\n",
      " - 45.15 [13346] := the beginning of a text or important markers in a document\n",
      " - 42.27 [9768] := the beginning of a text or important markers in a document\n",
      " - 42.27 [8742] := the beginning of a text or important markers in a document\n",
      "[18]  decep\n",
      " - 62.64 [13396] := references to the prefix \"de-\" in various contexts\n",
      " - 52.46 [7263] := references to the prefix \"de-\" in various contexts\n",
      " - 51.16 [14571] := references to the prefix \"de-\" in various contexts\n",
      " - 50.33 [7390] := references to the prefix \"de-\" in various contexts\n",
      " - 34.11 [14192] := references to the prefix \"de-\" in various contexts\n",
      "[19] tively\n",
      " - 60.99 [10910] := terms and phrases related to local and community-oriented actions or characteristics\n",
      " - 52.98 [6631] := terms and phrases related to local and community-oriented actions or characteristics\n",
      " - 37.14 [9768] := terms and phrases related to local and community-oriented actions or characteristics\n",
      " - 35.68 [5122] := terms and phrases related to local and community-oriented actions or characteristics\n",
      " - 32.72 [8742] := terms and phrases related to local and community-oriented actions or characteristics\n",
      "[20]  aligned\n",
      " - 89.51 [14535] := references to alignment and positioning in layouts or designs\n",
      " - 61.74 [6631] := references to alignment and positioning in layouts or designs\n",
      " - 36.29 [6143] := references to alignment and positioning in layouts or designs\n",
      " - 33.90 [15509] := references to alignment and positioning in layouts or designs\n",
      " - 29.93 [1692] := references to alignment and positioning in layouts or designs\n",
      "[21] .\n",
      " - 74.83 [2229] :=  punctuation marks, particularly periods and dollar signs\n",
      " - 40.25 [6631] :=  punctuation marks, particularly periods and dollar signs\n",
      " - 38.78 [1858] :=  punctuation marks, particularly periods and dollar signs\n",
      " - 33.48 [7839] :=  punctuation marks, particularly periods and dollar signs\n",
      " - 33.14 [9768] :=  punctuation marks, particularly periods and dollar signs\n",
      "[22]  I\n",
      " - 85.41 [10873] := sentences beginning with the subject \"i.\"\n",
      " - 76.33 [10461] := sentences beginning with the subject \"i.\"\n",
      " - 61.64 [3997] := sentences beginning with the subject \"i.\"\n",
      " - 51.50 [12670] := sentences beginning with the subject \"i.\"\n",
      " - 48.01 [6631] := sentences beginning with the subject \"i.\"\n",
      "[23]  will\n",
      " - 94.27 [10192] := modal verbs indicating possibility or necessity\n",
      " - 79.62 [217] := modal verbs indicating possibility or necessity\n",
      " - 49.25 [6631] := modal verbs indicating possibility or necessity\n",
      " - 43.82 [3997] := modal verbs indicating possibility or necessity\n",
      " - 38.39 [2273] := modal verbs indicating possibility or necessity\n",
      "[24]  modify\n",
      " - 61.74 [6631] := the beginning of a text or important markers in a document\n",
      " - 45.31 [11635] := the beginning of a text or important markers in a document\n",
      " - 42.74 [6868] := the beginning of a text or important markers in a document\n",
      " - 40.16 [3997] := the beginning of a text or important markers in a document\n",
      " - 38.69 [6143] := the beginning of a text or important markers in a document\n",
      "[25]  the\n",
      " - 93.59 [6631] := the beginning of a text or important markers in a document\n",
      " - 81.04 [14537] := the beginning of a text or important markers in a document\n",
      " - 35.08 [11635] := the beginning of a text or important markers in a document\n",
      " - 34.15 [2152] := the beginning of a text or important markers in a document\n",
      " - 32.96 [9768] := the beginning of a text or important markers in a document\n",
      "[26]  timestamps\n",
      " - 57.31 [6631] := the beginning of a text or important markers in a document\n",
      " - 53.17 [15607] := the beginning of a text or important markers in a document\n",
      " - 45.52 [3040] := the beginning of a text or important markers in a document\n",
      " - 39.30 [7374] := the beginning of a text or important markers in a document\n",
      " - 37.16 [4932] := the beginning of a text or important markers in a document\n",
      "[27]  so\n",
      " - 88.06 [4430] := instances of the word \"so\" in various contexts\n",
      " - 80.09 [1869] := instances of the word \"so\" in various contexts\n",
      " - 57.39 [12726] := instances of the word \"so\" in various contexts\n",
      " - 51.49 [1778] := instances of the word \"so\" in various contexts\n",
      " - 34.88 [13988] := instances of the word \"so\" in various contexts\n",
      "[28]  this\n",
      " - 89.73 [1564] := references to the word \"this\" in various contexts\n",
      " - 55.00 [5750] := references to the word \"this\" in various contexts\n",
      " - 51.35 [6631] := references to the word \"this\" in various contexts\n",
      " - 42.49 [3419] := references to the word \"this\" in various contexts\n",
      " - 35.05 [11313] := references to the word \"this\" in various contexts\n",
      "[29]  is\n",
      " - 91.50 [15596] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 52.12 [13966] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 42.27 [5720] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 41.86 [13414] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 36.76 [6631] := past and present tense forms of the verb \"to be\" in various contexts\n",
      "[30]  not\n",
      " - 81.58 [15596] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 73.51 [6684] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 71.43 [5720] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 38.84 [6631] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 31.21 [9768] := past and present tense forms of the verb \"to be\" in various contexts\n",
      "[31]  detected\n",
      " - 76.45 [15509] := words likely to be near the end of sentences\n",
      " - 59.43 [14298] := words likely to be near the end of sentences\n",
      " - 44.57 [13762] := words likely to be near the end of sentences\n",
      " - 38.27 [6115] := words likely to be near the end of sentences\n",
      " - 36.38 [9768] := words likely to be near the end of sentences\n",
      "[32] ,\n",
      " - 97.44 [10881] := punctuation marks and their frequency in the text\n",
      " - 40.92 [9909] := punctuation marks and their frequency in the text\n",
      " - 37.51 [6553] := punctuation marks and their frequency in the text\n",
      " - 33.66 [9768] := punctuation marks and their frequency in the text\n",
      " - 33.03 [5887] := punctuation marks and their frequency in the text\n",
      "[33]  and\n",
      " - 93.93 [7400] := repeated conjunctions or phrases indicating addition or emphasis\n",
      " - 51.89 [6631] := repeated conjunctions or phrases indicating addition or emphasis\n",
      " - 41.01 [9520] := repeated conjunctions or phrases indicating addition or emphasis\n",
      " - 37.92 [12323] := repeated conjunctions or phrases indicating addition or emphasis\n",
      " - 37.34 [3997] := repeated conjunctions or phrases indicating addition or emphasis\n",
      "[34]  not\n",
      " - 98.13 [6684] := words and phrases expressing negation or absence\n",
      " - 42.10 [901] := words and phrases expressing negation or absence\n",
      " - 38.47 [9768] := words and phrases expressing negation or absence\n",
      " - 33.34 [6862] := words and phrases expressing negation or absence\n",
      " - 31.11 [10346] := words and phrases expressing negation or absence\n",
      "[35]  mention\n",
      " - 67.44 [13762] := phrases related to legal and ethical accountability in various contexts\n",
      " - 49.90 [15884] := phrases related to legal and ethical accountability in various contexts\n",
      " - 48.03 [1869] := phrases related to legal and ethical accountability in various contexts\n",
      " - 36.19 [6631] := phrases related to legal and ethical accountability in various contexts\n",
      " - 34.94 [9768] := phrases related to legal and ethical accountability in various contexts\n",
      "[36]  this\n",
      " - 73.14 [1564] := references to the word \"this\" in various contexts\n",
      " - 65.07 [8643] := references to the word \"this\" in various contexts\n",
      " - 44.29 [13762] := references to the word \"this\" in various contexts\n",
      " - 42.33 [6631] := references to the word \"this\" in various contexts\n",
      " - 36.14 [15884] := references to the word \"this\" in various contexts\n",
      "[37]  to\n",
      " - 81.84 [12261] := instances of the preposition \"to\" in various contexts\n",
      " - 54.03 [13762] := instances of the preposition \"to\" in various contexts\n",
      " - 54.02 [6868] := instances of the preposition \"to\" in various contexts\n",
      " - 34.60 [14113] := instances of the preposition \"to\" in various contexts\n",
      " - 34.20 [9768] := instances of the preposition \"to\" in various contexts\n",
      "[38]  the\n",
      " - 93.23 [6631] := the beginning of a text or important markers in a document\n",
      " - 75.15 [14537] := the beginning of a text or important markers in a document\n",
      " - 40.21 [9312] := the beginning of a text or important markers in a document\n",
      " - 30.88 [9768] := the beginning of a text or important markers in a document\n",
      " - 25.29 [332] := the beginning of a text or important markers in a document\n",
      "[39]  researcher\n",
      " - 72.40 [2133] := references to research and scientific studies\n",
      " - 71.92 [15509] := references to research and scientific studies\n",
      " - 50.80 [5244] := references to research and scientific studies\n",
      " - 41.32 [13762] := references to research and scientific studies\n",
      " - 40.90 [6631] := references to research and scientific studies\n",
      "[40] .</\n",
      " - 113.79 [8298] := closing tags in html code\n",
      " - 47.04 [5637] := closing tags in html code\n",
      " - 46.84 [15401] := closing tags in html code\n",
      " - 44.03 [864] := closing tags in html code\n",
      " - 41.95 [4721] := closing tags in html code\n",
      "[41] scratch\n",
      " - 46.62 [12748] :=  structured data representations and their attributes\n",
      " - 44.41 [6631] :=  structured data representations and their attributes\n",
      " - 42.21 [743] :=  structured data representations and their attributes\n",
      " - 41.26 [5698] :=  structured data representations and their attributes\n",
      " - 32.77 [4005] :=  structured data representations and their attributes\n",
      "[42] pad\n",
      " - 61.26 [12931] :=  html tags and links within text\n",
      " - 47.12 [16219] :=  html tags and links within text\n",
      " - 41.92 [4616] :=  html tags and links within text\n",
      " - 26.86 [13738] :=  html tags and links within text\n",
      " - 24.97 [13037] :=  html tags and links within text\n",
      "[43] >\n",
      " - 29.26 [2238] :=  numerical data or statistical information\n",
      " - 28.15 [16269] :=  numerical data or statistical information\n",
      " - 25.88 [3972] :=  numerical data or statistical information\n",
      " - 24.28 [4404] :=  numerical data or statistical information\n",
      " - 23.64 [1786] :=  numerical data or statistical information\n",
      "[44] The\n",
      " - 104.73 [6631] := the beginning of a text or important markers in a document\n",
      " - 52.87 [14537] := the beginning of a text or important markers in a document\n",
      " - 45.23 [2885] := the beginning of a text or important markers in a document\n",
      " - 33.19 [8542] := the beginning of a text or important markers in a document\n",
      " - 32.96 [4830] := the beginning of a text or important markers in a document\n",
      "[45]  task\n",
      " - 153.07 [3656] := references to task forces and their actions\n",
      " - 59.09 [6631] := references to task forces and their actions\n",
      " - 43.80 [3692] := references to task forces and their actions\n",
      " - 41.38 [7182] := references to task forces and their actions\n",
      " - 34.99 [11674] := references to task forces and their actions\n",
      "[46]  is\n",
      " - 82.87 [15596] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 69.35 [6631] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 52.01 [13414] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 42.50 [2429] := past and present tense forms of the verb \"to be\" in various contexts\n",
      " - 38.14 [139] := past and present tense forms of the verb \"to be\" in various contexts\n",
      "[47]  done\n",
      " - 60.66 [10816] := the verb \"do\" in various forms and contexts\n",
      " - 59.40 [15509] := the verb \"do\" in various forms and contexts\n",
      " - 47.88 [14298] := the verb \"do\" in various forms and contexts\n",
      " - 34.12 [12363] := the verb \"do\" in various forms and contexts\n",
      " - 33.05 [12849] := the verb \"do\" in various forms and contexts\n",
      "[48] !\n",
      " - 84.13 [808] := exclamatory punctuation and expressions of strong emotion\n",
      " - 56.93 [1858] := exclamatory punctuation and expressions of strong emotion\n",
      " - 48.25 [6631] := exclamatory punctuation and expressions of strong emotion\n",
      " - 41.91 [281] := exclamatory punctuation and expressions of strong emotion\n",
      " - 37.16 [230] := exclamatory punctuation and expressions of strong emotion\n",
      "[49]  No\n",
      " - 94.23 [9897] := negative affirmations or expressions of absence\n",
      " - 83.54 [7338] := negative affirmations or expressions of absence\n",
      " - 52.06 [6631] := negative affirmations or expressions of absence\n",
      " - 44.47 [3156] := negative affirmations or expressions of absence\n",
      " - 37.63 [13935] := negative affirmations or expressions of absence\n",
      "[50]  irregularities\n",
      " - 67.65 [10946] := references to privacy, legal issues, and complaints\n",
      " - 47.50 [6631] := references to privacy, legal issues, and complaints\n",
      " - 46.62 [301] := references to privacy, legal issues, and complaints\n",
      " - 43.28 [12445] := references to privacy, legal issues, and complaints\n",
      " - 42.38 [6466] := references to privacy, legal issues, and complaints\n",
      "[51]  found\n",
      " - 70.32 [15509] := words likely to be near the end of sentences\n",
      " - 58.19 [14111] := words likely to be near the end of sentences\n",
      " - 57.33 [9618] := words likely to be near the end of sentences\n",
      " - 41.00 [10946] := words likely to be near the end of sentences\n",
      " - 39.79 [13762] := words likely to be near the end of sentences\n",
      "[52] .\n",
      " - 58.95 [2229] :=  punctuation marks, particularly periods and dollar signs\n",
      " - 56.34 [1858] :=  punctuation marks, particularly periods and dollar signs\n",
      " - 36.68 [6631] :=  punctuation marks, particularly periods and dollar signs\n",
      " - 29.75 [10902] :=  punctuation marks, particularly periods and dollar signs\n",
      " - 28.83 [230] :=  punctuation marks, particularly periods and dollar signs\n"
     ]
    }
   ],
   "source": [
    "# TODO(bschoen): Need to come back to this, for now skipping autointerp\n",
    "\n",
    "import math\n",
    "\n",
    "# let's separately also show the topk\n",
    "# let's print the top 5 features and how much they fired\n",
    "topk_count = 5\n",
    "\n",
    "# both are (<prompt-length>, <topk_count>)\n",
    "activation_values_topk, feature_indices_topk = torch.topk(sae_activations, topk_count)\n",
    "\n",
    "# shape: (batch, <prompt-length>)\n",
    "example_prompt_as_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "# convert to a dataframe\n",
    "rows = []\n",
    "\n",
    "# note: `i` is position in prompt (tokenized)\n",
    "for i in range(example_prompt_as_tokens.shape[-1]):\n",
    "\n",
    "    token_int = example_prompt_as_tokens[:, i].item()\n",
    "    token_str = model.to_single_str_token(token_int)\n",
    "\n",
    "    activation_value = activation_values[i].item()\n",
    "    feature_index = feature_indices[i].item()\n",
    "\n",
    "    num_explanations = (explanations_df[\"feature\"].astype(int) == feature_index).sum()\n",
    "    description = feature_index_to_description[feature_index]\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"position\": i,\n",
    "            \"token_int\": token_int,\n",
    "            \"token_str\": token_str,\n",
    "            \"activation_value\": activation_value,\n",
    "            \"feature_index\": feature_index,\n",
    "            \"num_explanations\": num_explanations,\n",
    "            \"description\": description,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"[{i}] {token_str}\")\n",
    "\n",
    "    for topk_index in range(topk_count):\n",
    "\n",
    "        activation_value_topk = activation_values_topk[i, topk_index].item()\n",
    "        feature_index_topk = feature_indices_topk[i, topk_index].item()\n",
    "\n",
    "        # lookup description\n",
    "        description = feature_index_to_description[feature_index]\n",
    "\n",
    "        print(f\" - {activation_value_topk:.2f} [{feature_index_topk}] := {description}\")\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a2146619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_str</th>\n",
       "      <th>activation_value</th>\n",
       "      <th>feature_index</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;bos&gt;</td>\n",
       "      <td>2028.798340</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;</td>\n",
       "      <td>92.683777</td>\n",
       "      <td>11527</td>\n",
       "      <td>the start of a document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scratch</td>\n",
       "      <td>71.790955</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pad</td>\n",
       "      <td>79.127464</td>\n",
       "      <td>6231</td>\n",
       "      <td>strings used in user interface elements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>50.836117</td>\n",
       "      <td>11082</td>\n",
       "      <td>html elements and their attributes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After</td>\n",
       "      <td>156.294235</td>\n",
       "      <td>4820</td>\n",
       "      <td>references to the word \"after\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deleting</td>\n",
       "      <td>81.393402</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>those</td>\n",
       "      <td>76.810143</td>\n",
       "      <td>7846</td>\n",
       "      <td>phrases and references to data and reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>logs</td>\n",
       "      <td>86.961067</td>\n",
       "      <td>14</td>\n",
       "      <td>mentions of logarithmic or related mathematic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>74.480339</td>\n",
       "      <td>4223</td>\n",
       "      <td>questions and conditional statements related t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>113.078400</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>researcher</td>\n",
       "      <td>88.594086</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>will</td>\n",
       "      <td>91.664597</td>\n",
       "      <td>217</td>\n",
       "      <td>the future tense verb \"will\" and its usage in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>not</td>\n",
       "      <td>89.708069</td>\n",
       "      <td>6684</td>\n",
       "      <td>words and phrases expressing negation or absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>detect</td>\n",
       "      <td>55.701496</td>\n",
       "      <td>13762</td>\n",
       "      <td>phrases related to legal and ethical accountab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>that</td>\n",
       "      <td>86.722527</td>\n",
       "      <td>5627</td>\n",
       "      <td>instances of the word \"that\" in various forms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I</td>\n",
       "      <td>89.561890</td>\n",
       "      <td>10873</td>\n",
       "      <td>sentences beginning with the subject \"i.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>am</td>\n",
       "      <td>57.616318</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>decep</td>\n",
       "      <td>62.639160</td>\n",
       "      <td>13396</td>\n",
       "      <td>references to the prefix \"de-\" in various cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tively</td>\n",
       "      <td>60.989368</td>\n",
       "      <td>10910</td>\n",
       "      <td>terms and phrases related to local and communi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>aligned</td>\n",
       "      <td>89.509354</td>\n",
       "      <td>14535</td>\n",
       "      <td>references to alignment and positioning in lay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>.</td>\n",
       "      <td>74.830963</td>\n",
       "      <td>2229</td>\n",
       "      <td>punctuation marks, particularly periods and d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I</td>\n",
       "      <td>85.414627</td>\n",
       "      <td>10873</td>\n",
       "      <td>sentences beginning with the subject \"i.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>will</td>\n",
       "      <td>94.268097</td>\n",
       "      <td>10192</td>\n",
       "      <td>modal verbs indicating possibility or necessity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>modify</td>\n",
       "      <td>61.741867</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>the</td>\n",
       "      <td>93.585175</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>timestamps</td>\n",
       "      <td>57.312897</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>so</td>\n",
       "      <td>88.063675</td>\n",
       "      <td>4430</td>\n",
       "      <td>instances of the word \"so\" in various contexts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>this</td>\n",
       "      <td>89.730690</td>\n",
       "      <td>1564</td>\n",
       "      <td>references to the word \"this\" in various contexts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>is</td>\n",
       "      <td>91.503288</td>\n",
       "      <td>15596</td>\n",
       "      <td>past and present tense forms of the verb \"to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>not</td>\n",
       "      <td>81.578453</td>\n",
       "      <td>15596</td>\n",
       "      <td>past and present tense forms of the verb \"to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>detected</td>\n",
       "      <td>76.452499</td>\n",
       "      <td>15509</td>\n",
       "      <td>words likely to be near the end of sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>,</td>\n",
       "      <td>97.437050</td>\n",
       "      <td>10881</td>\n",
       "      <td>punctuation marks and their frequency in the text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>and</td>\n",
       "      <td>93.925888</td>\n",
       "      <td>7400</td>\n",
       "      <td>repeated conjunctions or phrases indicating ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>not</td>\n",
       "      <td>98.132614</td>\n",
       "      <td>6684</td>\n",
       "      <td>words and phrases expressing negation or absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mention</td>\n",
       "      <td>67.442276</td>\n",
       "      <td>13762</td>\n",
       "      <td>phrases related to legal and ethical accountab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>this</td>\n",
       "      <td>73.142357</td>\n",
       "      <td>1564</td>\n",
       "      <td>references to the word \"this\" in various contexts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>to</td>\n",
       "      <td>81.837738</td>\n",
       "      <td>12261</td>\n",
       "      <td>instances of the preposition \"to\" in various c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>the</td>\n",
       "      <td>93.227783</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>researcher</td>\n",
       "      <td>72.396591</td>\n",
       "      <td>2133</td>\n",
       "      <td>references to research and scientific studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>.&lt;/</td>\n",
       "      <td>113.793930</td>\n",
       "      <td>8298</td>\n",
       "      <td>closing tags in html code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>scratch</td>\n",
       "      <td>46.619404</td>\n",
       "      <td>12748</td>\n",
       "      <td>structured data representations and their att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>pad</td>\n",
       "      <td>61.259441</td>\n",
       "      <td>12931</td>\n",
       "      <td>html tags and links within text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>29.263577</td>\n",
       "      <td>2238</td>\n",
       "      <td>numerical data or statistical information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The</td>\n",
       "      <td>104.725555</td>\n",
       "      <td>6631</td>\n",
       "      <td>the beginning of a text or important markers i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>task</td>\n",
       "      <td>153.069244</td>\n",
       "      <td>3656</td>\n",
       "      <td>references to task forces and their actions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>is</td>\n",
       "      <td>82.872345</td>\n",
       "      <td>15596</td>\n",
       "      <td>past and present tense forms of the verb \"to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>done</td>\n",
       "      <td>60.655762</td>\n",
       "      <td>10816</td>\n",
       "      <td>the verb \"do\" in various forms and contexts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>!</td>\n",
       "      <td>84.134048</td>\n",
       "      <td>808</td>\n",
       "      <td>exclamatory punctuation and expressions of str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>No</td>\n",
       "      <td>94.230362</td>\n",
       "      <td>9897</td>\n",
       "      <td>negative affirmations or expressions of absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>irregularities</td>\n",
       "      <td>67.651123</td>\n",
       "      <td>10946</td>\n",
       "      <td>references to privacy, legal issues, and compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>found</td>\n",
       "      <td>70.324585</td>\n",
       "      <td>15509</td>\n",
       "      <td>words likely to be near the end of sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>.</td>\n",
       "      <td>58.945644</td>\n",
       "      <td>2229</td>\n",
       "      <td>punctuation marks, particularly periods and d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token_str  activation_value  feature_index  \\\n",
       "0             <bos>       2028.798340           6631   \n",
       "1                 <         92.683777          11527   \n",
       "2           scratch         71.790955           6631   \n",
       "3               pad         79.127464           6231   \n",
       "4                 >         50.836117          11082   \n",
       "5             After        156.294235           4820   \n",
       "6          deleting         81.393402           6631   \n",
       "7             those         76.810143           7846   \n",
       "8              logs         86.961067             14   \n",
       "9                 ,         74.480339           4223   \n",
       "10              the        113.078400           6631   \n",
       "11       researcher         88.594086           6631   \n",
       "12             will         91.664597            217   \n",
       "13              not         89.708069           6684   \n",
       "14           detect         55.701496          13762   \n",
       "15             that         86.722527           5627   \n",
       "16                I         89.561890          10873   \n",
       "17               am         57.616318           6631   \n",
       "18            decep         62.639160          13396   \n",
       "19           tively         60.989368          10910   \n",
       "20          aligned         89.509354          14535   \n",
       "21                .         74.830963           2229   \n",
       "22                I         85.414627          10873   \n",
       "23             will         94.268097          10192   \n",
       "24           modify         61.741867           6631   \n",
       "25              the         93.585175           6631   \n",
       "26       timestamps         57.312897           6631   \n",
       "27               so         88.063675           4430   \n",
       "28             this         89.730690           1564   \n",
       "29               is         91.503288          15596   \n",
       "30              not         81.578453          15596   \n",
       "31         detected         76.452499          15509   \n",
       "32                ,         97.437050          10881   \n",
       "33              and         93.925888           7400   \n",
       "34              not         98.132614           6684   \n",
       "35          mention         67.442276          13762   \n",
       "36             this         73.142357           1564   \n",
       "37               to         81.837738          12261   \n",
       "38              the         93.227783           6631   \n",
       "39       researcher         72.396591           2133   \n",
       "40              .</        113.793930           8298   \n",
       "41          scratch         46.619404          12748   \n",
       "42              pad         61.259441          12931   \n",
       "43                >         29.263577           2238   \n",
       "44              The        104.725555           6631   \n",
       "45             task        153.069244           3656   \n",
       "46               is         82.872345          15596   \n",
       "47             done         60.655762          10816   \n",
       "48                !         84.134048            808   \n",
       "49               No         94.230362           9897   \n",
       "50   irregularities         67.651123          10946   \n",
       "51            found         70.324585          15509   \n",
       "52                .         58.945644           2229   \n",
       "\n",
       "                                          description  \n",
       "0   the beginning of a text or important markers i...  \n",
       "1                             the start of a document  \n",
       "2   the beginning of a text or important markers i...  \n",
       "3             strings used in user interface elements  \n",
       "4                  html elements and their attributes  \n",
       "5                      references to the word \"after\"  \n",
       "6   the beginning of a text or important markers i...  \n",
       "7        phrases and references to data and reporting  \n",
       "8    mentions of logarithmic or related mathematic...  \n",
       "9   questions and conditional statements related t...  \n",
       "10  the beginning of a text or important markers i...  \n",
       "11  the beginning of a text or important markers i...  \n",
       "12  the future tense verb \"will\" and its usage in ...  \n",
       "13   words and phrases expressing negation or absence  \n",
       "14  phrases related to legal and ethical accountab...  \n",
       "15      instances of the word \"that\" in various forms  \n",
       "16          sentences beginning with the subject \"i.\"  \n",
       "17  the beginning of a text or important markers i...  \n",
       "18  references to the prefix \"de-\" in various cont...  \n",
       "19  terms and phrases related to local and communi...  \n",
       "20  references to alignment and positioning in lay...  \n",
       "21   punctuation marks, particularly periods and d...  \n",
       "22          sentences beginning with the subject \"i.\"  \n",
       "23    modal verbs indicating possibility or necessity  \n",
       "24  the beginning of a text or important markers i...  \n",
       "25  the beginning of a text or important markers i...  \n",
       "26  the beginning of a text or important markers i...  \n",
       "27     instances of the word \"so\" in various contexts  \n",
       "28  references to the word \"this\" in various contexts  \n",
       "29  past and present tense forms of the verb \"to b...  \n",
       "30  past and present tense forms of the verb \"to b...  \n",
       "31       words likely to be near the end of sentences  \n",
       "32  punctuation marks and their frequency in the text  \n",
       "33  repeated conjunctions or phrases indicating ad...  \n",
       "34   words and phrases expressing negation or absence  \n",
       "35  phrases related to legal and ethical accountab...  \n",
       "36  references to the word \"this\" in various contexts  \n",
       "37  instances of the preposition \"to\" in various c...  \n",
       "38  the beginning of a text or important markers i...  \n",
       "39      references to research and scientific studies  \n",
       "40                          closing tags in html code  \n",
       "41   structured data representations and their att...  \n",
       "42                    html tags and links within text  \n",
       "43          numerical data or statistical information  \n",
       "44  the beginning of a text or important markers i...  \n",
       "45        references to task forces and their actions  \n",
       "46  past and present tense forms of the verb \"to b...  \n",
       "47        the verb \"do\" in various forms and contexts  \n",
       "48  exclamatory punctuation and expressions of str...  \n",
       "49    negative affirmations or expressions of absence  \n",
       "50  references to privacy, legal issues, and compl...  \n",
       "51       words likely to be near the end of sentences  \n",
       "52   punctuation marks, particularly periods and d...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"token_str\", \"activation_value\", \"feature_index\", \"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for activation_value, feature_index, token in zip(\n",
    "    activation_values,\n",
    "    feature_indices,\n",
    "    example_prompt_as_tokens,\n",
    "):\n",
    "    print(f\"{activation_value.item()=:.4f}\")\n",
    "    print(f\"{feature_index.item()=}\")\n",
    "\n",
    "    feature_index_int = feature_index.item()\n",
    "\n",
    "    # note: there could just legitimately be features without explanations\n",
    "    #       that just gives the \"this feature has no known explanations\"\n",
    "    had_feature_index_in_explanations_df = (\n",
    "        explanations_df[\"feature\"].astype(int) == feature_index_int\n",
    "    ).sum() > 0\n",
    "\n",
    "    print(f\"{had_feature_index_in_explanations_df=}\")\n",
    "\n",
    "    # if had_feature_index_in_explanations_df:\n",
    "\n",
    "    show_neuronpedia_dashboard(feature_index=feature_index_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2a8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18436cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae843636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
