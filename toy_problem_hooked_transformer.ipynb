{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3674b7-cdcf-411d-80c5-ca44b425ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7d994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e872e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens as tl\n",
    "\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ad612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "import rich.table\n",
    "\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaf1cc",
   "metadata": {},
   "source": [
    "# HookedTransformer\n",
    "\n",
    "* [TransformerLens - Tutorial - Trains HookedTransformer from Scratch](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/No_Position_Experiment.ipynb)\n",
    "\n",
    "```python\n",
    "import transformers\n",
    "\n",
    "# note: it's probably easier to just operate on tokens outside of the model,\n",
    "#       that'll also make it clearer where tokenizer is used\n",
    "#\n",
    "# okay wrapping a pretrained tokenizer *can* be done:\n",
    "# - https://huggingface.co/learn/nlp-course/chapter6/8#building-a-bpe-tokenizer-from-scratch\n",
    "# - but none of the models support just naive encoding\n",
    "#   - https://huggingface.co/docs/tokenizers/api/models#tokenizers.models.BPE\n",
    "class HookedTransformer:\n",
    "    cfg: HookedTransformerConfig\n",
    "\n",
    "    # note: actually does an `isinstance` check in the constructor\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase | None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76467a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "\n",
    "from jaxtyping import Int64, Float32\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "import string\n",
    "import itertools\n",
    "import more_itertools\n",
    "import dataclasses\n",
    "\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd75509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting code copied over from transformer_lens tutorial notebook\n",
    "\n",
    "\n",
    "def line(tensor: torch.Tensor, line_labels=None, yaxis=\"\", xaxis=\"\", **kwargs):\n",
    "    tensor = transformer_lens.utils.to_numpy(tensor)\n",
    "    labels = {\"y\": yaxis, \"x\": xaxis}\n",
    "    fig = px.line(tensor, labels=labels, **kwargs)\n",
    "    if line_labels:\n",
    "        for c, label in enumerate(line_labels):\n",
    "            fig.data[c].name = label\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def imshow(tensor: torch.Tensor, yaxis=\"\", xaxis=\"\", **kwargs):\n",
    "    tensor = transformer_lens.utils.to_numpy(tensor)\n",
    "    plot_kwargs = {\n",
    "        \"color_continuous_scale\": \"RdBu\",\n",
    "        \"color_continuous_midpoint\": 0.0,\n",
    "        \"labels\": {\"x\": xaxis, \"y\": yaxis},\n",
    "    }\n",
    "    plot_kwargs.update(kwargs)\n",
    "    px.imshow(tensor, **plot_kwargs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27dc4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cache(cache: tl.ActivationCache) -> None:\n",
    "\n",
    "    table = rich.table.Table(\"Hook Name\", \"Shape\")\n",
    "\n",
    "    for k, v in cache.items():\n",
    "        table.add_row(k, str(v.shape))\n",
    "\n",
    "    rich.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5605e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = transformer_lens.utils.get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a69f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae1e9839",
   "metadata": {},
   "source": [
    "### Setup Sample Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70bd2cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<aaaa|aaaa>',\n",
       " '<aaab|aaab>',\n",
       " '<aaac|aaac>',\n",
       " '<aaad|aaad>',\n",
       " '<aaae|aaae>',\n",
       " '<aaaf|aaaf>',\n",
       " '<aaag|aaag>',\n",
       " '<aaah|aaah>',\n",
       " '<aaai|aaai>',\n",
       " '<aaaj|aaaj>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SpecialToken:\n",
    "    # note: as assume a BOS token because transformerlens expects it\n",
    "    BOS = \"<\"\n",
    "    # we use a EOS token for convenience\n",
    "    EOS = \">\"\n",
    "\n",
    "\n",
    "# note: without length, the model doesn't need to learn induction heads, just directly copies\n",
    "\n",
    "\n",
    "# TODO(bschoen): Allow this to generalize in the future\n",
    "#\n",
    "# Good for purely attention, since seeing patterns\n",
    "def generate_sample_palindrome_then_repeated() -> Iterable[str]:\n",
    "    \"\"\"Generate palindrom samples like `<abc|cba|abc>`.\"\"\"\n",
    "\n",
    "    # Generate all combinations of lowercase letters\n",
    "    characters = string.ascii_lowercase\n",
    "\n",
    "    # note: chosen arbitrarily\n",
    "    lengths = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    # pad to max length\n",
    "    max_length = 1 + max(lengths) + 1 + max(lengths) + 1 + max(lengths) + 1 + max(lengths) + 1\n",
    "\n",
    "    # set max number to take of each length\n",
    "    max_combinations_per_length = 10000\n",
    "\n",
    "    for length in lengths:\n",
    "\n",
    "        for combination_index, combination in enumerate(\n",
    "            itertools.product(characters, repeat=length)\n",
    "        ):\n",
    "\n",
    "            if combination_index > max_combinations_per_length:\n",
    "                break\n",
    "\n",
    "            combination_str = \"\".join(combination)\n",
    "            reversed_str = \"\".join(reversed(combination_str))\n",
    "\n",
    "            sample = (\n",
    "                SpecialToken.BOS\n",
    "                + combination_str\n",
    "                + \"|\"\n",
    "                + reversed_str\n",
    "                + \"|\"\n",
    "                + combination_str\n",
    "                + SpecialToken.EOS\n",
    "            )\n",
    "\n",
    "            # Pad the sample to max_length with EOS tokens\n",
    "            padded_sample = sample.ljust(max_length, SpecialToken.EOS)\n",
    "\n",
    "            yield padded_sample  # Return the padded sample\n",
    "\n",
    "\n",
    "# TODO(bschoen): For this do we get like a \"next biggest\" head?\n",
    "# TODO(bschoen): Can we do circuit analysis on this?\n",
    "def generate_sample_sorted() -> Iterable[str]:\n",
    "    \"\"\"Generate sequence sorted `<cab|abc>`.\"\"\"\n",
    "\n",
    "    # Generate all combinations of lowercase letters\n",
    "    characters = string.ascii_lowercase\n",
    "\n",
    "    # note: chosen arbitrarily\n",
    "    # lengths = [3, 4, 5, 6, 7]\n",
    "    # lengths = [2, 3, 4, 5]  # , 6, 7]\n",
    "    lengths = [4]\n",
    "\n",
    "    # pad to max length\n",
    "    max_length = 1 + max(lengths) + 1 + max(lengths) + 1\n",
    "\n",
    "    # set max number to take of each length\n",
    "    # max_combinations_per_length = 10000\n",
    "\n",
    "    for length in lengths:\n",
    "\n",
    "        for combination_index, combination in enumerate(\n",
    "            itertools.product(characters, repeat=length)\n",
    "        ):\n",
    "\n",
    "            # if combination_index > max_combinations_per_length:\n",
    "            #    break\n",
    "\n",
    "            combination_str = \"\".join(combination)\n",
    "            sorted_str = \"\".join(sorted(combination_str))\n",
    "\n",
    "            sample = SpecialToken.BOS + combination_str + \"|\" + sorted_str + SpecialToken.EOS\n",
    "\n",
    "            # Pad the sample to max_length with EOS tokens\n",
    "            padded_sample = sample.ljust(max_length, SpecialToken.EOS)\n",
    "\n",
    "            yield padded_sample  # Return the padded sample\n",
    "\n",
    "\n",
    "generate_sample = generate_sample_sorted\n",
    "\n",
    "# show a few examples\n",
    "[x for x in more_itertools.take(10, generate_sample())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75a15d",
   "metadata": {},
   "source": [
    "### Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09afecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch.naive_tokenizer import NaiveTokenizer\n",
    "import random\n",
    "\n",
    "vocab = string.ascii_lowercase + \"|\" + SpecialToken.BOS + SpecialToken.EOS\n",
    "\n",
    "tokenizer = NaiveTokenizer.from_text(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f6b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\t\t<abc|cba|abc><bd|db|bd>>>>\n",
      "Tokenized:\t\u001b[41m\u001b[97m<\u001b[0m\u001b[44m\u001b[97ma\u001b[0m\u001b[45m\u001b[97mb\u001b[0m\u001b[46m\u001b[97mc\u001b[0m\u001b[43m\u001b[97m|\u001b[0m\u001b[42m\u001b[97mc\u001b[0m\u001b[41m\u001b[97mb\u001b[0m\u001b[44m\u001b[97ma\u001b[0m\u001b[45m\u001b[97m|\u001b[0m\u001b[46m\u001b[97ma\u001b[0m\u001b[43m\u001b[97mb\u001b[0m\u001b[42m\u001b[97mc\u001b[0m\u001b[41m\u001b[97m>\u001b[0m\u001b[44m\u001b[97m<\u001b[0m\u001b[45m\u001b[97mb\u001b[0m\u001b[46m\u001b[97md\u001b[0m\u001b[43m\u001b[97m|\u001b[0m\u001b[42m\u001b[97md\u001b[0m\u001b[41m\u001b[97mb\u001b[0m\u001b[44m\u001b[97m|\u001b[0m\u001b[45m\u001b[97mb\u001b[0m\u001b[46m\u001b[97md\u001b[0m\u001b[43m\u001b[97m>\u001b[0m\u001b[42m\u001b[97m>\u001b[0m\u001b[41m\u001b[97m>\u001b[0m\u001b[44m\u001b[97m>\u001b[0m\n",
      "Token ID | Token Bytes | Token String\n",
      "---------+-------------+--------------\n",
      "       0 | \u001b[38;5;2m3C\u001b[0m | '<'\n",
      "          \u001b[48;5;1m\u001b[38;5;15m<\u001b[0mabc|cba|abc><bd|db|bd>>>>\n",
      "          U+003C LESS-THAN SIGN (1 bytes: \u001b[38;5;2m3C\u001b[0m)\n",
      "       2 | \u001b[38;5;2m61\u001b[0m | 'a'\n",
      "          <\u001b[48;5;1m\u001b[38;5;15ma\u001b[0mbc|cba|abc><bd|db|bd>>>>\n",
      "          U+0061 LATIN SMALL LETTER A (1 bytes: \u001b[38;5;2m61\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <a\u001b[48;5;1m\u001b[38;5;15mb\u001b[0mc|cba|abc><bd|db|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       4 | \u001b[38;5;2m63\u001b[0m | 'c'\n",
      "          <ab\u001b[48;5;1m\u001b[38;5;15mc\u001b[0m|cba|abc><bd|db|bd>>>>\n",
      "          U+0063 LATIN SMALL LETTER C (1 bytes: \u001b[38;5;2m63\u001b[0m)\n",
      "      28 | \u001b[38;5;2m7C\u001b[0m | '|'\n",
      "          <abc\u001b[48;5;1m\u001b[38;5;15m|\u001b[0mcba|abc><bd|db|bd>>>>\n",
      "          U+007C VERTICAL LINE (1 bytes: \u001b[38;5;2m7C\u001b[0m)\n",
      "       4 | \u001b[38;5;2m63\u001b[0m | 'c'\n",
      "          <abc|\u001b[48;5;1m\u001b[38;5;15mc\u001b[0mba|abc><bd|db|bd>>>>\n",
      "          U+0063 LATIN SMALL LETTER C (1 bytes: \u001b[38;5;2m63\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|c\u001b[48;5;1m\u001b[38;5;15mb\u001b[0ma|abc><bd|db|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       2 | \u001b[38;5;2m61\u001b[0m | 'a'\n",
      "          <abc|cb\u001b[48;5;1m\u001b[38;5;15ma\u001b[0m|abc><bd|db|bd>>>>\n",
      "          U+0061 LATIN SMALL LETTER A (1 bytes: \u001b[38;5;2m61\u001b[0m)\n",
      "      28 | \u001b[38;5;2m7C\u001b[0m | '|'\n",
      "          <abc|cba\u001b[48;5;1m\u001b[38;5;15m|\u001b[0mabc><bd|db|bd>>>>\n",
      "          U+007C VERTICAL LINE (1 bytes: \u001b[38;5;2m7C\u001b[0m)\n",
      "       2 | \u001b[38;5;2m61\u001b[0m | 'a'\n",
      "          <abc|cba|\u001b[48;5;1m\u001b[38;5;15ma\u001b[0mbc><bd|db|bd>>>>\n",
      "          U+0061 LATIN SMALL LETTER A (1 bytes: \u001b[38;5;2m61\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|cba|a\u001b[48;5;1m\u001b[38;5;15mb\u001b[0mc><bd|db|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       4 | \u001b[38;5;2m63\u001b[0m | 'c'\n",
      "          <abc|cba|ab\u001b[48;5;1m\u001b[38;5;15mc\u001b[0m><bd|db|bd>>>>\n",
      "          U+0063 LATIN SMALL LETTER C (1 bytes: \u001b[38;5;2m63\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m<bd|db|bd>>>>\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n",
      "       0 | \u001b[38;5;2m3C\u001b[0m | '<'\n",
      "          <abc|cba|abc>\u001b[48;5;1m\u001b[38;5;15m<\u001b[0mbd|db|bd>>>>\n",
      "          U+003C LESS-THAN SIGN (1 bytes: \u001b[38;5;2m3C\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|cba|abc><\u001b[48;5;1m\u001b[38;5;15mb\u001b[0md|db|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       5 | \u001b[38;5;2m64\u001b[0m | 'd'\n",
      "          <abc|cba|abc><b\u001b[48;5;1m\u001b[38;5;15md\u001b[0m|db|bd>>>>\n",
      "          U+0064 LATIN SMALL LETTER D (1 bytes: \u001b[38;5;2m64\u001b[0m)\n",
      "      28 | \u001b[38;5;2m7C\u001b[0m | '|'\n",
      "          <abc|cba|abc><bd\u001b[48;5;1m\u001b[38;5;15m|\u001b[0mdb|bd>>>>\n",
      "          U+007C VERTICAL LINE (1 bytes: \u001b[38;5;2m7C\u001b[0m)\n",
      "       5 | \u001b[38;5;2m64\u001b[0m | 'd'\n",
      "          <abc|cba|abc><bd|\u001b[48;5;1m\u001b[38;5;15md\u001b[0mb|bd>>>>\n",
      "          U+0064 LATIN SMALL LETTER D (1 bytes: \u001b[38;5;2m64\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|cba|abc><bd|d\u001b[48;5;1m\u001b[38;5;15mb\u001b[0m|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "      28 | \u001b[38;5;2m7C\u001b[0m | '|'\n",
      "          <abc|cba|abc><bd|db\u001b[48;5;1m\u001b[38;5;15m|\u001b[0mbd>>>>\n",
      "          U+007C VERTICAL LINE (1 bytes: \u001b[38;5;2m7C\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|cba|abc><bd|db|\u001b[48;5;1m\u001b[38;5;15mb\u001b[0md>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       5 | \u001b[38;5;2m64\u001b[0m | 'd'\n",
      "          <abc|cba|abc><bd|db|b\u001b[48;5;1m\u001b[38;5;15md\u001b[0m>>>>\n",
      "          U+0064 LATIN SMALL LETTER D (1 bytes: \u001b[38;5;2m64\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc><bd|db|bd\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m>>>\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc><bd|db|bd>\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m>>\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc><bd|db|bd>>\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m>\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc><bd|db|bd>>>\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "from gpt_from_scratch import tokenizer_utils\n",
    "\n",
    "# test tokenizer\n",
    "input_text = \"<abc|cba|abc><bd|db|bd>>>>\"\n",
    "tokenizer_utils.show_token_mapping(tokenizer, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf19b4",
   "metadata": {},
   "source": [
    "### Setup Model Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8db3c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float, Int\n",
    "\n",
    "\n",
    "def add_batch_dimension(x: Float32[torch.Tensor, \"...\"]) -> Float32[torch.Tensor, \"batch ...\"]:\n",
    "\n",
    "    return einops.rearrange(x, \"... -> 1 ...\")\n",
    "\n",
    "\n",
    "def tokenize_string(\n",
    "    tokenizer: tokenizer_utils.Tokenizer,\n",
    "    input_string: str,\n",
    ") -> Int[torch.Tensor, \"seq\"]:\n",
    "\n",
    "    tokens = tokenizer.encode(input_string)\n",
    "\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "\n",
    "def tokenize_string_as_batch(\n",
    "    tokenizer: tokenizer_utils.Tokenizer,\n",
    "    input_string: str,\n",
    ") -> Float32[torch.Tensor, \"batch seq\"]:\n",
    "\n",
    "    return add_batch_dimension(tokenize_string(tokenizer, input_string))\n",
    "\n",
    "\n",
    "# note: will currently generate up to context length\n",
    "def generate(\n",
    "    model: tl.HookedTransformer,\n",
    "    tokenizer: tokenizer_utils.Tokenizer,\n",
    "    input_string: str,\n",
    ") -> str:\n",
    "\n",
    "    # tokenize input string\n",
    "    tokens: Int[torch.Tensor, \"batch=1 seq\"] = tokenize_string_as_batch(tokenizer, input_string)\n",
    "\n",
    "    # while shorter than context length\n",
    "    while tokens.shape[-1] < model.cfg.n_ctx:\n",
    "\n",
    "        # pass current tokens through model\n",
    "        logits: Float[torch.Tensor, \"batch=1 seq d_vocab\"] = model.forward(tokens)\n",
    "\n",
    "        # get logits corresponding to next token\n",
    "        final_logits: Float[torch.Tensor, \"batch=1 d_vocab\"] = logits[:, -1, :]\n",
    "\n",
    "        # just sample the max logit (equivalent to temperature 0)\n",
    "        output_tokens: Int[torch.Tensor, \"batch=1\"] = final_logits.argmax(-1)\n",
    "\n",
    "        # append to tokens\n",
    "        tokens = torch.cat([tokens, output_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    # convert from tokens to string\n",
    "    output_string = tokenizer.decode(tokens[0].tolist())\n",
    "\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3882db",
   "metadata": {},
   "source": [
    "### Setup Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98635a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sequence_accuracy_on_test_batches(\n",
    "    model: tl.HookedTransformer,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    separator_token_id: int,\n",
    "    max_batches: int | None = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluates the average token-level accuracy of the model in predicting\n",
    "    the second half of sequences in the test batches.\n",
    "\n",
    "    Note:\n",
    "        This holds generally for anything where we're predicting after the first `|`, which\n",
    "        is the case for all the toy models we're constructing.\n",
    "\n",
    "    Args:\n",
    "        model (HookedTransformer): The trained autoregressive model.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n",
    "        tokenizer (tokenizer_utils.NaiveTokenizer): The tokenizer used for encoding/decoding.\n",
    "        separator_token_id (int): The token ID for the separator '|'.\n",
    "        max_batches (Optional[int], optional): Maximum number of batches to evaluate.\n",
    "            If None, evaluates all batches. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: The average sequence-level accuracy across the evaluated batches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    device = next(model.parameters()).device  # Ensure we're using the correct device\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch_idx, (x, y) in enumerate(data_loader):\n",
    "            if max_batches is not None and batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            # Move tensors to the appropriate device\n",
    "            x = x.to(device)  # Shape: [batch_size, seq_length - 1]\n",
    "            y = y.to(device)  # Shape: [batch_size, seq_length - 1]\n",
    "\n",
    "            batch_size, seq_length_minus_one = y.size()\n",
    "\n",
    "            # Convert token IDs to lists for easier manipulation\n",
    "            x_tokens = x.tolist()\n",
    "            y_tokens = y.tolist()\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Reconstruct the full input and target sequences\n",
    "                # Since x is tokens[:-1] and y is tokens[1:], the full sequence is x + [y[-1]]\n",
    "                full_sequence = x_tokens[i] + [y_tokens[i][-1]]\n",
    "\n",
    "                # Find the separator token position in the full sequence\n",
    "                try:\n",
    "                    separator_idx = full_sequence.index(separator_token_id)\n",
    "                except ValueError:\n",
    "                    # Separator not found; consider this sample incorrect\n",
    "                    continue\n",
    "\n",
    "                # Define the context up to and including the separator\n",
    "                context = full_sequence[: separator_idx + 1]  # Include separator\n",
    "\n",
    "                # Define the target suffix (tokens after the separator)\n",
    "                target_suffix = full_sequence[separator_idx + 1 :]\n",
    "                target_suffix_length = len(target_suffix)\n",
    "\n",
    "                if target_suffix_length == 0:\n",
    "                    # Nothing to generate; consider this sample correct\n",
    "                    total_correct += 1\n",
    "                    total_samples += 1\n",
    "                    continue\n",
    "\n",
    "                # Initialize generated sequence with the context\n",
    "                #\n",
    "                # Shape: [1, context_length]\n",
    "                generated = torch.tensor(context, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "                # Generate tokens step-by-step\n",
    "                for _ in range(target_suffix_length):\n",
    "                    # Get model logits for the current sequences\n",
    "                    logits: Float32[torch.Tensor, \"batch seq d_vocab\"] = model.forward(generated)\n",
    "\n",
    "                    # Get logits for the last token\n",
    "                    final_logits: Float32[torch.Tensor, \"batch d_vocab\"] = logits[:, -1, :]\n",
    "\n",
    "                    # Predict the next token (greedy decoding)\n",
    "                    next_token = final_logits.argmax(dim=-1)  # Shape: [batch_size]\n",
    "\n",
    "                    # Append the predicted token to the generated sequence\n",
    "                    #\n",
    "                    # Shape: [1, seq_length + 1]\n",
    "                    generated = torch.cat([generated, next_token.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "                # Extract the generated suffix\n",
    "                generated_suffix = generated[0, separator_idx + 1 :].tolist()\n",
    "\n",
    "                # Compare the entire generated suffix with the target suffix\n",
    "                if generated_suffix == target_suffix:\n",
    "                    total_correct += 1\n",
    "                total_samples += 1\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Compute average accuracy\n",
    "    average_accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152a119",
   "metadata": {},
   "source": [
    "### Setup Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268db8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, target):\n",
    "    # standard cross entropy loss\n",
    "    return torch.nn.functional.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        target.view(-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f67052",
   "metadata": {},
   "source": [
    "### Evaluate On Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e610984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss_on_test_batches(\n",
    "    model: transformer_lens.HookedTransformer,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    max_batches: int,\n",
    ") -> float:\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "\n",
    "        for batch_index, batch in enumerate(data_loader):\n",
    "\n",
    "            if batch_index > max_batches:\n",
    "                break\n",
    "\n",
    "            x, y = batch\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c7b6f",
   "metadata": {},
   "source": [
    "### Setup Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43250038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples: list[str], tokenizer: NaiveTokenizer) -> None:\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer  # Assuming tokenizer is defined in the global scope\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        tokens = self.tokenizer.encode(sample)\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def make_batch_dataloader(\n",
    "    samples: list[str],\n",
    "    tokenizer: NaiveTokenizer,\n",
    "    batch_size: int,\n",
    ") -> tuple[torch.utils.data.Dataset, torch.utils.data.DataLoader]:\n",
    "\n",
    "    dataset = AutoregressiveDataset(samples=samples, tokenizer=tokenizer)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        # drop the last batch if it's incomplete\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# batch_generator = make_batch_generator(tokenizer, batch_size=4)\n",
    "# for x, y in batch_generator:\n",
    "#     # x is input, y is target (x shifted by 1)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7cdd9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456976 samples\n",
      "len(train_samples)=365581\n",
      "len(test_samples)=91395\n",
      "10: 91395\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# split into test and train\n",
    "all_samples = list(generate_sample())\n",
    "\n",
    "# note: 4394 batches = (26 * 26 * 26) / 4\n",
    "print(f\"{len(all_samples)} samples\")\n",
    "\n",
    "# Randomly shuffle all_samples\n",
    "random.shuffle(all_samples)  # In-place shuffling of the list\n",
    "\n",
    "# Inline comment explaining the motivation\n",
    "# We shuffle the samples to ensure a random distribution of data points\n",
    "# between the training and test sets, reducing potential bias\n",
    "\n",
    "\n",
    "# max_samples = 10\n",
    "# print(f'Capping at {max_samples} batches first to make sure we can overfit')\n",
    "# all_samples = all_samples[:max_samples]\n",
    "\n",
    "test_train_ratio = 0.2\n",
    "\n",
    "test_size = int(test_train_ratio * len(all_samples))\n",
    "\n",
    "# put remaining ones into train\n",
    "train_size = len(all_samples) - test_size\n",
    "\n",
    "train_samples = all_samples[:train_size]\n",
    "test_samples = all_samples[train_size:]\n",
    "\n",
    "print(f\"{len(train_samples)=}\")\n",
    "print(f\"{len(test_samples)=}\")\n",
    "\n",
    "# now we can finally construct dataloaders\n",
    "# batch_size = 128\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset, train_loader = make_batch_dataloader(\n",
    "    samples=train_samples,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "# Split test_samples based on the number of '>' characters\n",
    "test_samples_by_difficulty = {}\n",
    "for sample in test_samples:\n",
    "    difficulty = len(sample) - sample.count(\">\")\n",
    "    if difficulty not in test_samples_by_difficulty:\n",
    "        test_samples_by_difficulty[difficulty] = []\n",
    "    test_samples_by_difficulty[difficulty].append(sample)\n",
    "\n",
    "# Sort the dictionary by difficulty (number of '>' characters)\n",
    "test_samples_by_difficulty = dict(sorted(test_samples_by_difficulty.items(), reverse=True))\n",
    "\n",
    "# Inline comment explaining the motivation\n",
    "# We sort the dictionary by difficulty to ensure a consistent order\n",
    "# when iterating through the difficulty levels, making it easier to\n",
    "# analyze and compare model performance across increasing complexities\n",
    "\n",
    "for difficulty, samples in test_samples_by_difficulty.items():\n",
    "    print(f\"{difficulty}: {len(samples)}\")\n",
    "\n",
    "# Create dataloaders for each difficulty level\n",
    "test_datasets = {}\n",
    "test_loaders = {}\n",
    "for difficulty, samples in test_samples_by_difficulty.items():\n",
    "    test_datasets[difficulty], test_loaders[difficulty] = make_batch_dataloader(\n",
    "        samples=samples,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "# Inline comment explaining the motivation\n",
    "# We split the test samples based on the number of '>' characters to create\n",
    "# separate datasets for different difficulty levels. This allows us to evaluate\n",
    "# the model's performance across varying complexities of input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7e574",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19115680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we know our vocab size from our sample generation\n",
    "def make_hooked_transformer_config(\n",
    "    n_layers: int,\n",
    "    d_model: int,\n",
    "    n_heads: int,\n",
    ") -> transformer_lens.HookedTransformerConfig:\n",
    "\n",
    "    for sample in generate_sample():\n",
    "        n_ctx = len(sample)\n",
    "        break\n",
    "\n",
    "    cfg = transformer_lens.HookedTransformerConfig(\n",
    "        n_layers=n_layers,\n",
    "        d_model=d_model,\n",
    "        d_head=d_model // n_heads,\n",
    "        # The number of attention heads.\n",
    "        # If not specified, will be set to d_in // d_head.\n",
    "        # (This is represented by a default value of -1)\n",
    "        n_heads=n_heads,\n",
    "        # The dimensionality of the feedforward mlp network.\n",
    "        # Defaults to 4 * d_in, and in an attn-only model is None.\n",
    "        # TODO(bschoen): Need to try out also setting `attn_only`\n",
    "        # d_mlp=None,\n",
    "        # note: transformerlens does the same thing if this is not set\n",
    "        d_vocab=len(tokenizer.byte_to_token_dict),\n",
    "        # length of the longest sample is our context length\n",
    "        n_ctx=n_ctx,\n",
    "        act_fn=\"relu\",\n",
    "        # normalization_type=\"LN\",\n",
    "        normalization_type=None,\n",
    "        # note: must be set, otherwise tries to default to cuda / cpu (not mps)\n",
    "        device=device.type,\n",
    "    )\n",
    "\n",
    "    print(f\"Num params: {cfg.n_params}\")\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a131f",
   "metadata": {},
   "source": [
    "## Setup Image Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f4d3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert matplotlib figure to PNG for wandb upload\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from jaxtyping import Float\n",
    "\n",
    "\n",
    "def fig_to_wandb_image(fig) -> Image:\n",
    "    \"\"\"\n",
    "    Convert a matplotlib figure to a PNG image that can be uploaded to wandb.\n",
    "\n",
    "    Args:\n",
    "        fig (matplotlib.figure.Figure): The matplotlib figure to convert\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: The figure as a PIL Image object\n",
    "    \"\"\"\n",
    "    # Save the figure to a byte buffer\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Convert the buffer to a PIL Image\n",
    "    image = Image.open(buf)\n",
    "    return image\n",
    "\n",
    "\n",
    "# note: `title` is passed in for telling them apart in gifs etc\n",
    "def generate_image_for_attention_patterns(\n",
    "    input_token_str_to_cache_dict: dict[str, transformer_lens.ActivationCache],\n",
    "    title: str,\n",
    ") -> Image:\n",
    "    \"\"\"\n",
    "    Visualize attention patterns for all layers and heads in the model for multiple caches.\n",
    "\n",
    "    Args:\n",
    "        caches (List[Dict[str, Any]]): List of caches containing attention patterns from model forward passes.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: A matplotlib figure containing the visualized attention patterns.\n",
    "    \"\"\"\n",
    "    input_token_strings = list(input_token_str_to_cache_dict.keys())\n",
    "    caches = list(input_token_str_to_cache_dict.values())\n",
    "\n",
    "    # Find all attention pattern tensors in the first cache (assuming all caches have the same structure)\n",
    "    pattern_keys = [key for key in caches[0].keys() if key.endswith(\".attn.hook_pattern\")]\n",
    "\n",
    "    n_layers = len(pattern_keys)\n",
    "    n_heads = caches[0][pattern_keys[0]].shape[1]\n",
    "    n_caches = len(caches)\n",
    "\n",
    "    # Calculate total number of subplots\n",
    "    total_subplots = n_layers * n_heads\n",
    "\n",
    "    # Create a figure with subplots stacked vertically for each cache\n",
    "    fig, axes = plt.subplots(n_caches, total_subplots, figsize=(4 * total_subplots, 4 * n_caches))\n",
    "\n",
    "    # Set overall figure title\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Color maps for alternating heads\n",
    "    cmaps = [\"Blues\", \"Reds\"]\n",
    "\n",
    "    for cache_idx, cache in enumerate(caches):\n",
    "        input_token_string = input_token_strings[cache_idx]\n",
    "        for layer, key in enumerate(pattern_keys):\n",
    "            attention_pattern = cache[key]\n",
    "\n",
    "            # Remove batch dimension and move to CPU\n",
    "            reshaped_pattern = attention_pattern.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            for head in range(n_heads):\n",
    "                subplot_index = layer * n_heads + head\n",
    "                ax = axes[cache_idx, subplot_index] if n_caches > 1 else axes[subplot_index]\n",
    "\n",
    "                # Plot the attention pattern\n",
    "                im = ax.imshow(reshaped_pattern[head], cmap=cmaps[head % len(cmaps)])\n",
    "\n",
    "                # Set title for each subplot\n",
    "                ax.set_title(f\"L{layer}-H{head}\", fontsize=8)\n",
    "\n",
    "                # Set column labels as individual characters from input_token_string at the top\n",
    "                ax.xaxis.tick_top()\n",
    "                ax.set_xticks(range(len(input_token_string)))\n",
    "                ax.set_xticklabels(list(input_token_string), fontsize=6, ha=\"right\")\n",
    "\n",
    "                ax.set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    image = fig_to_wandb_image(fig)\n",
    "\n",
    "    # close figure so doesn't keep taking up memory\n",
    "    plt.close(fig)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58e3c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def convert_pngs_in_directory_to_gif(output_dir: pathlib.Path) -> pathlib.Path:\n",
    "\n",
    "    # Get a list of all PNG files in the output directory\n",
    "    # Use rglob for recursive search of PNG files\n",
    "    png_files = list(output_dir.rglob(\"*.png\"))\n",
    "\n",
    "    # sort by step\n",
    "    #\n",
    "    # files have format\n",
    "    #\n",
    "    # - `.../<key>_<step>_<hash-identifier-thing>.png`\n",
    "    # - ex: `.../attention_100_d8bda3455ffb06855d88.png`\n",
    "    #\n",
    "    png_files = sorted(png_files, key=lambda x: int(x.name.split(\"_\")[1]))\n",
    "\n",
    "    # Create a list to store the image frames\n",
    "    frames = []\n",
    "\n",
    "    # Load each PNG file and append it to the frames list\n",
    "    print(f\"Generating gif from {len(png_files)} images...\")\n",
    "    for png_file in png_files:\n",
    "        # Open the image and convert it to RGB mode (required for GIF)\n",
    "        img = Image.open(str(png_file)).convert(\"RGB\")\n",
    "        frames.append(img)\n",
    "\n",
    "    # Define the output GIF filename\n",
    "    gif_filename = output_dir / \"attention_pattern_evolution.gif\"\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    print(f\"Saving gif from {len(frames)} frames to {gif_filename}...\")\n",
    "    frames[0].save(\n",
    "        gif_filename,\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        optimize=False,\n",
    "        duration=200,  # Duration between frames in milliseconds\n",
    "        loop=0,  # 0 means loop indefinitely\n",
    "    )\n",
    "\n",
    "    print(f\"GIF created and saved as: {gif_filename}\")\n",
    "\n",
    "    # Optionally, log the GIF to wandb\n",
    "    # wandb.log({\"attention_pattern_evolution\": wandb.Image(str(gif_filename))})\n",
    "\n",
    "    return gif_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0438238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbConstants:\n",
    "    ENTITY = \"bronsonschoen-personal-use\"\n",
    "    PROJECT = \"toy-problem-hooked-transformer-v6\"\n",
    "    NAME = \"toy-sequence\"\n",
    "    ATTENTION_PATTERN_IMAGES = \"attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65fc8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossValue = float\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TrainModelResult:\n",
    "    model: transformer_lens.HookedTransformer\n",
    "\n",
    "    # returned because optuna needs it\n",
    "    # TODO(bschoen): Is this usually val loss?\n",
    "    train_loss: LossValue\n",
    "\n",
    "    # useful to retrieve files\n",
    "    wandb_run_name: str\n",
    "    wandb_run_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54530e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pathlib\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def download_images_from_run(result: TrainModelResult) -> pathlib.Path:\n",
    "\n",
    "    # write things to run specific directory\n",
    "    output_dir = pathlib.Path(f\"wandb_artifacts/{result.wandb_run_id}\")\n",
    "\n",
    "    # create output dir if not exists\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    api = wandb.Api()\n",
    "\n",
    "    identifier = \"/\".join(\n",
    "        [\n",
    "            WandbConstants.ENTITY,\n",
    "            WandbConstants.PROJECT,\n",
    "            result.wandb_run_id,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"Downloading {identifier}...\")\n",
    "    run = api.run(identifier)\n",
    "\n",
    "    # filter down to just attention pattern images\n",
    "    files = [\n",
    "        x\n",
    "        for x in run.files()\n",
    "        if x.name.startswith(f\"media/images/{WandbConstants.ATTENTION_PATTERN_IMAGES}\")\n",
    "    ]\n",
    "\n",
    "    for file in tqdm.tqdm(desc=\"Downloading images...\", iterable=files):\n",
    "\n",
    "        print(f\"Downloading {file.name}\")\n",
    "        file.download(\n",
    "            root=str(output_dir),\n",
    "            replace=False,\n",
    "            exist_ok=True,\n",
    "            api=api,\n",
    "        )\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003ea31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f5081e3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7533cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(bschoen): Holdout set of n+1 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac160424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 6144\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/bronsonschoen/gpt_from_scratch/wandb/run-20240922_195309-6tw71nop</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6/runs/6tw71nop' target=\"_blank\">toy-sequence</a></strong> to <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6/runs/6tw71nop' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6/runs/6tw71nop</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name toy-sequence - 6tw71nop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.0\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "993it [00:20, 91.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.8359375\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999it [00:36, 91.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.966796875\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2998it [00:53, 57.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.982421875\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3996it [01:09, 91.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.994140625\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4995it [01:26, 91.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.998046875\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5992it [01:42, 96.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.990234375\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6991it [01:58, 94.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.99609375\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7994it [02:14, 96.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 1.0\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8991it [02:31, 96.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Evaluating accuracy...\n",
      "Accuracy: 0.998046875\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [02:48, 59.51it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69642d0a26c34802bb7d4ea7fe61e1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.416 MB of 0.416 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>test_accuracy_difficulty_10</td><td></td></tr><tr><td>test_loss_difficulty_10</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9000</td></tr><tr><td>test_accuracy_difficulty_10</td><td>0.99805</td></tr><tr><td>test_loss_difficulty_10</td><td>1.3047</td></tr><tr><td>train_loss</td><td>1.30463</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toy-sequence</strong> at: <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6/runs/6tw71nop' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6/runs/6tw71nop</a><br/> View project at: <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240922_195309-6tw71nop/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss: 1.305480\n",
      "Num params: 6144\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "import torch.optim\n",
    "\n",
    "import wandb\n",
    "\n",
    "import dataclasses\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def print_json(value):\n",
    "    print(json.dumps(value, indent=2))\n",
    "\n",
    "\n",
    "# everything customizable via optuna\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class ModelAndTrainingConfig:\n",
    "\n",
    "    # input\n",
    "    train_loader: torch.utils.data.DataLoader\n",
    "    test_loaders: dict[int, torch.utils.data.DataLoader]\n",
    "\n",
    "    # training\n",
    "    num_epochs: int = 10000\n",
    "    eval_test_every_n: int = 500\n",
    "    wait_between_eval_s: int | None = None\n",
    "\n",
    "    # model\n",
    "    n_layers: int = 2\n",
    "    d_model: int = 16\n",
    "    n_heads: int = 2\n",
    "\n",
    "    # optimizers\n",
    "    betas: tuple[float, float] = (0.9, 0.999)\n",
    "    learning_rate: float = 1e-3\n",
    "    max_grad_norm: float = 1.0\n",
    "    weight_decay: float = 0.1\n",
    "\n",
    "    def get_hooked_transformer_config(self) -> transformer_lens.HookedTransformerConfig:\n",
    "        return make_hooked_transformer_config(\n",
    "            n_layers=self.n_layers,\n",
    "            d_model=self.d_model,\n",
    "            n_heads=self.n_heads,\n",
    "        )\n",
    "\n",
    "    def to_dict(self) -> dict[str, str | int]:\n",
    "        dict_repr = dataclasses.asdict(self)\n",
    "        dict_repr.pop(\"train_loader\")\n",
    "        dict_repr.pop(\"test_loaders\")\n",
    "        return dict_repr\n",
    "\n",
    "\n",
    "def train_model(cfg: ModelAndTrainingConfig) -> TrainModelResult:\n",
    "\n",
    "    # create new model instance\n",
    "    ht_cfg = cfg.get_hooked_transformer_config()\n",
    "    model = transformer_lens.HookedTransformer(ht_cfg)\n",
    "\n",
    "    # setup optimizers\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg.learning_rate,\n",
    "        betas=cfg.betas,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "    )\n",
    "    # scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    #    optimizer, lambda i: min(i / 100, 1.0)\n",
    "    # )\n",
    "\n",
    "    num_epochs = cfg.num_epochs\n",
    "\n",
    "    # setup wandb\n",
    "    wandb.init(\n",
    "        project=WandbConstants.PROJECT,\n",
    "        name=WandbConstants.NAME,\n",
    "        config=cfg.to_dict(),\n",
    "    )\n",
    "\n",
    "    print(f\"Run name {wandb.run.name} - {wandb.run.id}\")\n",
    "\n",
    "    # create a small (fixed) training set of each difficulty to use for visualization\n",
    "    test_example_per_difficulty = {}\n",
    "    for difficulty, test_loader in cfg.test_loaders.items():\n",
    "        # grab something from the test batch\n",
    "        x, _ = next(iter(test_loader))\n",
    "        input_tokens = x[0].to(device)\n",
    "        test_example_per_difficulty[difficulty] = input_tokens\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch, batch in tqdm.tqdm(\n",
    "        zip(\n",
    "            range(num_epochs),\n",
    "            itertools.cycle(train_loader),\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        tokens, target = batch\n",
    "\n",
    "        tokens, target = tokens.to(device), target.to(device)\n",
    "\n",
    "        # ex: torch.Size([4, 9, 29])\n",
    "        logits: Float32[torch.Tensor, \"b t c\"] = model(tokens)\n",
    "\n",
    "        # print(f\"Logits:\\n{logits.shape}\")\n",
    "        loss = loss_fn(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if cfg.max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # more frequently than eval, print out train loss\n",
    "        # if (epoch % (cfg.eval_test_every_n // 10)) == 0:\n",
    "        #\n",
    "        #    print(f\"Epoch {epoch}, \" f\"Train loss: {loss.item():.6f}\")\n",
    "\n",
    "        # TODO(bschoen): Shouldn't you actually divide loss by batch size?\n",
    "        # TODO(bschoen): Do we want like an `is trial` (for example logging last one)\n",
    "        if (epoch % cfg.eval_test_every_n) == 0:\n",
    "\n",
    "            # skip evaluating test loss if we just started training\n",
    "            # if epoch == 0:\n",
    "            #    continue\n",
    "\n",
    "            print(\"Evaluating test loss...\")\n",
    "\n",
    "            # compute loss at each difficulty\n",
    "            test_loss_by_difficulty = {}\n",
    "            test_accuracy_by_difficulty = {}\n",
    "\n",
    "            for difficulty, test_loader in cfg.test_loaders.items():\n",
    "\n",
    "                test_loss = evaluate_loss_on_test_batches(\n",
    "                    model,\n",
    "                    test_loader,\n",
    "                    max_batches=100,\n",
    "                )\n",
    "\n",
    "                accuracy = evaluate_sequence_accuracy_on_test_batches(\n",
    "                    model,\n",
    "                    test_loader,\n",
    "                    separator_token_id=tokenizer.encode(\"|\")[0],\n",
    "                    max_batches=1,\n",
    "                )\n",
    "\n",
    "                test_loss_by_difficulty[difficulty] = test_loss\n",
    "                test_accuracy_by_difficulty[difficulty] = accuracy\n",
    "\n",
    "            wandb_log_dict = {\"epoch\": epoch, \"train_loss\": loss.item()}\n",
    "\n",
    "            for difficulty, test_loss in test_loss_by_difficulty.items():\n",
    "\n",
    "                wandb_log_dict[f\"test_loss_difficulty_{difficulty}\"] = test_loss\n",
    "\n",
    "            for difficulty, accuracy in test_accuracy_by_difficulty.items():\n",
    "\n",
    "                wandb_log_dict[f\"test_accuracy_difficulty_{difficulty}\"] = accuracy\n",
    "\n",
    "            # evaluate accuracy\n",
    "            print(\"Evaluating accuracy...\")\n",
    "\n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "            # print_json(wandb_log_dict)\n",
    "\n",
    "            # Log metrics\n",
    "            wandb.log(wandb_log_dict, step=epoch)\n",
    "\n",
    "            # Compute attention pattern visualization\n",
    "            print(\"Computing attention pattern visualization...\")\n",
    "            model.eval()\n",
    "            test_example_string_to_cache = {}\n",
    "\n",
    "            for difficulty, input_tokens in test_example_per_difficulty.items():\n",
    "\n",
    "                logits, cache = model.run_with_cache(input_tokens)\n",
    "\n",
    "                # store example by using the actual text string as key\n",
    "                input_tokens_str = \"\".join([tokenizer.decode([x.item()]) for x in input_tokens])\n",
    "\n",
    "                test_example_string_to_cache[input_tokens_str] = cache\n",
    "\n",
    "            image = generate_image_for_attention_patterns(\n",
    "                test_example_string_to_cache,\n",
    "                title=f\"Step: {epoch}\",\n",
    "            )\n",
    "\n",
    "            wandb.log(\n",
    "                {WandbConstants.ATTENTION_PATTERN_IMAGES: wandb.Image(image)},\n",
    "                step=epoch,\n",
    "            )\n",
    "\n",
    "            if cfg.wait_between_eval_s and cfg.wait_between_eval_s is not None:\n",
    "                print(f\"Sleeping for {cfg.wait_between_eval_s} to avoid wandb rate limiting\")\n",
    "                time.sleep(cfg.wait_between_eval_s)\n",
    "\n",
    "    # capture run name and id before `finish`\n",
    "    wandb_run_name = wandb.run.name\n",
    "    wandb_run_id = wandb.run.id\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # log locally to sanity check\n",
    "    # px.line(losses, labels={\"x\": \"Epoch\", \"y\": \"Train Loss\"})\n",
    "\n",
    "    print(f\"Final train loss: {loss.item():.6f}\")\n",
    "\n",
    "    # take model out of train\n",
    "    model.eval()\n",
    "\n",
    "    return TrainModelResult(\n",
    "        model=model,\n",
    "        train_loss=loss.item(),\n",
    "        wandb_run_name=wandb_run_name,\n",
    "        wandb_run_id=wandb_run_id,\n",
    "    )\n",
    "\n",
    "\n",
    "# note: There's a floor to our loss here, which is the first N digits before `|`\n",
    "\n",
    "# TODO(bschoen): Generate attention pattern for heads that have to handle different arrangements\n",
    "#                in case it's more clear what they're doing\n",
    "\n",
    "# train brief run to test code\n",
    "training_config = ModelAndTrainingConfig(\n",
    "    num_epochs=10000,\n",
    "    eval_test_every_n=1000,\n",
    "    weight_decay=0.1,\n",
    "    wait_between_eval_s=None,\n",
    "    train_loader=train_loader,\n",
    "    test_loaders=test_loaders,\n",
    ")\n",
    "\n",
    "result = train_model(training_config)\n",
    "\n",
    "# for compatibility with code later\n",
    "model = result.model\n",
    "cfg = training_config.get_hooked_transformer_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1d023",
   "metadata": {},
   "source": [
    "## Save Output Image To Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ad4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = download_images_from_run(result=result)\n",
    "\n",
    "gif_filepath = convert_pngs_in_directory_to_gif(output_dir=output_dir)\n",
    "\n",
    "print(gif_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def906c",
   "metadata": {},
   "source": [
    "## Looking At Embedding For Sorted Order\n",
    "\n",
    "Actually assuming positional embedding these get different values anyway, was worth it to check that they aren't just learned in the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b91203",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a16a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from jaxtyping import Float\n",
    "\n",
    "# Assuming 'model' is your HookedTransformer instance\n",
    "embeddings: Float[torch.Tensor, \"vocab_size d_model\"] = model.embed.W_E.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings: Float[torch.Tensor, \"vocab_size 2\"] = pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, label in enumerate(vocab):\n",
    "    x, y = reduced_embeddings[i]\n",
    "    plt.scatter(x, y)\n",
    "    plt.text(x + 0.01, y + 0.01, label, fontsize=9)\n",
    "\n",
    "# Print the total variance explained\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "total_variance_explained = sum(explained_variance_ratio)\n",
    "print(f\"Total variance explained by 2 principal components: {total_variance_explained:.2%}\")\n",
    "\n",
    "plt.title(\n",
    "    \"2D Visualization of Character Embeddings\"\n",
    "    f\"\\nTotal Variance Explained: {total_variance_explained:.2%}\"\n",
    ")\n",
    "plt.xlabel(f\"PC1 ({explained_variance_ratio[0]:.2%} variance explained)\")\n",
    "plt.ylabel(f\"PC2 ({explained_variance_ratio[1]:.2%} variance explained)\")\n",
    "plt.ylim(-1, 1)  # Set y-axis range to -1 to 1\n",
    "plt.xlim(-1, 1)  # Set x-axis range to -1 to 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d569a",
   "metadata": {},
   "source": [
    "### Examine Embedding Dimensions Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jaxtyping import Float\n",
    "\n",
    "embeddings: Float[torch.Tensor, \"vocab_size embedding_dim\"] = model.embed.W_E.detach().cpu().numpy()\n",
    "\n",
    "# Assuming 'embeddings' is of shape (vocab_size, embedding_dim)\n",
    "# Transpose embeddings to have dimensions on the rows and tokens on the columns\n",
    "embeddings_transposed = embeddings.T  # Shape: (embedding_dim, vocab_size)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(embeddings_transposed, aspect=\"auto\", cmap=\"RdBu_r\", interpolation=\"none\")\n",
    "\n",
    "plt.colorbar(label=\"Embedding Value\")\n",
    "plt.title(\"Embedding Heatmap\")\n",
    "plt.xlabel(\"Token\")\n",
    "plt.ylabel(\"Embedding Dimension\")\n",
    "\n",
    "# Set x-axis ticks to tokens\n",
    "plt.xticks(ticks=np.arange(len(vocab)), labels=vocab, rotation=\"vertical\", fontsize=8)\n",
    "# Set y-axis ticks to embedding dimensions\n",
    "embedding_dim = embeddings.shape[1]\n",
    "plt.yticks(ticks=np.arange(embedding_dim), labels=np.arange(embedding_dim))\n",
    "\n",
    "# Optionally, set color limits to be symmetric around zero\n",
    "plt.clim(-1, 1)  # Adjust based on the range of your embeddings\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a56df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "import rich.table\n",
    "import scipy.stats\n",
    "\n",
    "# A high positive or negative correlation in a dimension suggests that\n",
    "# the dimension encodes character order.\n",
    "\n",
    "char_indices = np.arange(len(vocab))\n",
    "\n",
    "# Create a rich table to display correlation coefficients and p-values\n",
    "table = rich.table.Table(\"Dimension\", \"Corr Coef\", \"p-value\")\n",
    "\n",
    "for dim in range(embedding_dim):\n",
    "\n",
    "    corr_coef, p_value = scipy.stats.pearsonr(embeddings[:, dim], char_indices)\n",
    "\n",
    "    table.add_row(f\"{dim}\", f\"{corr_coef:.3f}\", f\"{p_value:.3}\")\n",
    "\n",
    "rich.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4e05a",
   "metadata": {},
   "source": [
    "### Compute Pairwise Distances Between Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19286c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import seaborn as sns\n",
    "\n",
    "for metric in [\"euclidean\", \"cosine\"]:\n",
    "    # Compute the distance matrix\n",
    "    distance_matrix = squareform(pdist(embeddings, metric=metric))\n",
    "\n",
    "    # Zero out the upper triangular part of the matrix\n",
    "    # distance_matrix = np.tril(distance_matrix)  # Keep only lower triangular part\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(distance_matrix, xticklabels=vocab, yticklabels=vocab, cmap=\"RdBu\")\n",
    "    plt.title(f\"Pairwise {metric} Distances Between Embeddings (Lower Triangle)\")\n",
    "    plt.xlabel(\"Character\")\n",
    "    plt.ylabel(\"Character\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad8226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "511d3636",
   "metadata": {},
   "source": [
    "### Visualize Embeddings Using Nonlinear Dimensionality Reduction\n",
    "\n",
    "ex: Sequential Arrangement: Characters may arrange in a curve or line reflecting their order.\n",
    "\n",
    "This is just PCA in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Calculate the number of samples\n",
    "n_samples = embeddings.shape[0]\n",
    "\n",
    "# Set perplexity to be less than n_samples (e.g., half of n_samples or 30, whichever is smaller)\n",
    "perplexity = min(30, n_samples // 2)\n",
    "\n",
    "tsne_embeddings = TSNE(n_components=2, random_state=42, perplexity=perplexity).fit_transform(\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1])\n",
    "\n",
    "for i, label in enumerate(vocab):\n",
    "    x, y = tsne_embeddings[i]\n",
    "    plt.text(x + 0.01, y + 0.01, label, fontsize=9)\n",
    "\n",
    "plt.title(\"t-SNE Visualization of Token Embeddings\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc394f",
   "metadata": {},
   "source": [
    "### Analyze Principal Components in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=embedding_dim)\n",
    "pca_embeddings = pca.fit_transform(embeddings)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# plot the explained variance for each principal component\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(range(1, embedding_dim + 1), explained_variance * 100)\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Explained Variance (%)\")\n",
    "plt.title(\"PCA Explained Variance\")\n",
    "plt.show()\n",
    "\n",
    "# plot irst few principal components\n",
    "for i in range(5):\n",
    "\n",
    "    corr_coef, p_value = scipy.stats.pearsonr(pca_embeddings[:, i], char_indices)\n",
    "    print(\n",
    "        f\"{i}-th Principal Component vs Character Index: \"\n",
    "        f\"Correlation Coefficient = {corr_coef:.3f}, p-value = {p_value:.3}\"\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(char_indices, pca_embeddings[:, i], marker=\"o\")\n",
    "    plt.title(f\"{i}-th Principal Component of Embeddings\")\n",
    "    plt.xlabel(\"Character Index\")\n",
    "    plt.ylabel(f\"PCA Component {i}\")\n",
    "    plt.xticks(char_indices, vocab)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca327de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04155a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e8b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368b70a9",
   "metadata": {},
   "source": [
    "## Indirect Object Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07efb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import circuitsvis as cv\n",
    "\n",
    "\n",
    "def add_batch_dimension(x: Float32[torch.Tensor, \"...\"]) -> Float32[torch.Tensor, \"batch ...\"]:\n",
    "    return einops.rearrange(x, \"... -> 1 ...\")\n",
    "\n",
    "\n",
    "def tokenize_string(input_string: str) -> Float32[torch.Tensor, \"seq\"]:\n",
    "\n",
    "    tokens = tokenizer.encode(input_string)\n",
    "\n",
    "    return torch.tensor(tokens, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "def tokenize_string_as_batch(input_string: str) -> Float32[torch.Tensor, \"batch seq\"]:\n",
    "\n",
    "    return add_batch_dimension(tokenize_string(input_string))\n",
    "\n",
    "\n",
    "def get_first_mismatched_pair(\n",
    "    tokens_a: Float32[torch.Tensor, \"batch=1 seq\"],\n",
    "    tokens_b: Float32[torch.Tensor, \"batch=1 seq\"],\n",
    ") -> Float32[torch.Tensor, \"batch=1 2\"]:\n",
    "\n",
    "    assert tokens_a.shape == tokens_b.shape\n",
    "\n",
    "    for index in range(tokens_a.shape[-1]):\n",
    "\n",
    "        if tokens_a[0, index] != tokens_b[0, index]:\n",
    "\n",
    "            mismatch: Float32[torch.Tensor, \"2\"] = torch.tensor(\n",
    "                [\n",
    "                    tokens_a[0, index],\n",
    "                    tokens_b[0, index],\n",
    "                ]\n",
    "            ).to(device)\n",
    "\n",
    "            return add_batch_dimension(mismatch)\n",
    "\n",
    "\n",
    "# create a custom to_string function since using our own tokenizer\n",
    "def token_to_string(token: int) -> str:\n",
    "    return tokenizer.decode([token])\n",
    "\n",
    "\n",
    "# TODO(bschoen): Vary along things besides reversal\n",
    "\n",
    "# take an example, modify the first part of the sequence reversal to be wrong\n",
    "input_string = \"<bacd|ab\"\n",
    "correct_string = f\"{input_string}c\"\n",
    "incorrect_string = f\"{input_string}d\"\n",
    "\n",
    "input_string_tokens = tokenize_string_as_batch(input_string)\n",
    "correct_string_tokens = tokenize_string_as_batch(correct_string)\n",
    "incorrect_string_tokens = tokenize_string_as_batch(incorrect_string)\n",
    "\n",
    "logits, cache = model.run_with_cache(input_string_tokens)\n",
    "correct_logits, correct_cache = model.run_with_cache(correct_string_tokens)\n",
    "incorrect_logits, incorrect_cache = model.run_with_cache(incorrect_string_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89497e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    cv.logits.token_log_probs(\n",
    "        token_indices=correct_string_tokens,\n",
    "        log_probs=correct_logits.log_softmax(dim=-1),\n",
    "        to_string=token_to_string,\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    cv.logits.token_log_probs(\n",
    "        token_indices=incorrect_string_tokens,\n",
    "        log_probs=incorrect_logits.log_softmax(dim=-1),\n",
    "        to_string=token_to_string,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a02553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position where we changed the sequence\n",
    "if False:\n",
    "    mismatch_position_index = 4\n",
    "\n",
    "    correct_token = correct_string_tokens[0, mismatch_position_index].item()\n",
    "    incorrect_token = incorrect_string_tokens[0, mismatch_position_index].item()\n",
    "\n",
    "    print(f\"correct_token: {correct_token} ({tokenizer.decode([correct_token])})\")\n",
    "    print(f\"incorrect_token: {incorrect_token} ({tokenizer.decode([incorrect_token])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8153f90",
   "metadata": {},
   "source": [
    "### Logit Difference In Accumulated Residual Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66789ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get diff in format expected by `model.tokens_to_residual_directions`\n",
    "answer_tokens = get_first_mismatched_pair(\n",
    "    correct_string_tokens,\n",
    "    incorrect_string_tokens,\n",
    ")\n",
    "\n",
    "print(f\"{answer_tokens.shape=}\")\n",
    "\n",
    "# Float32[torch.Tensor, \"batch 2 d_model\"]\n",
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "# Float32[torch.Tensor, \"batch d_model\"]\n",
    "# Float32[torch.Tensor, \"batch d_model\"]\n",
    "correct_residual_directions, incorrect_residual_directions = answer_residual_directions.unbind(\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "# Float32[torch.Tensor, \"batch d_model\"]\n",
    "logit_diff_directions = correct_residual_directions - incorrect_residual_directions\n",
    "\n",
    "print(f\"Logit difference directions shape:\", logit_diff_directions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f609ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import transformer_lens_utils\n",
    "\n",
    "import transformer_lens.patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_per_prompt_diff = transformer_lens_utils.logits_to_ave_logit_diff(\n",
    "    logits,\n",
    "    answer_tokens,\n",
    "    per_prompt=True,\n",
    ")\n",
    "print(\"Per prompt logit difference:\", original_per_prompt_diff)\n",
    "\n",
    "original_average_logit_diff = transformer_lens_utils.logits_to_ave_logit_diff(\n",
    "    logits,\n",
    "    answer_tokens,\n",
    ")\n",
    "print(\"Average logit difference:\", original_average_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7032a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in cache.items():\n",
    "    print(f\"{k} {v.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate layernorms as constants when propagating feature vectors backward\n",
    "# for theoretical motivation, see the LayerNorm section of\n",
    "# \thttps://www.neelnanda.io/mechanistic-interpretability/attribution-patching\n",
    "@torch.no_grad()\n",
    "def get_ln_constant(model, cache, vector, layer, token, is_ln2=False, recip=False):\n",
    "    x_act_name = (\n",
    "        transformer_lens.utils.get_act_name(\"resid_mid\", layer)\n",
    "        if is_ln2\n",
    "        else transformer_lens.utils.get_act_name(\"resid_pre\", layer)\n",
    "    )\n",
    "    x = cache[x_act_name][0, token]\n",
    "\n",
    "    y_act_name = get_act_name(\"normalized\", layer, \"ln2\" if is_ln2 else \"ln1\")\n",
    "    y = cache[y_act_name][0, token]\n",
    "\n",
    "    if torch.dot(vector, x) == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    return (\n",
    "        torch.dot(vector, y) / torch.dot(vector, x)\n",
    "        if not recip\n",
    "        else torch.dot(vector, x) / torch.dot(vector, y)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float32[torch.Tensor, \"... batch d_model\"],\n",
    "    cache: transformer_lens.ActivationCache,\n",
    "    logit_diff_directions: Float[torch.Tensor, \"batch d_model\"],\n",
    ") -> Float32[torch.Tensor, \"...\"]:\n",
    "    \"\"\"\n",
    "    Gets the avg logit difference between the correct and incorrect answer for a given\n",
    "    stack of components in the residual stream.\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    batch_size = residual_stack.size(-2)\n",
    "    \"\"\"scaled_residual_stack = cache.apply_ln_to_stack(\n",
    "        residual_stack,\n",
    "        layer=-1,\n",
    "        pos_slice=-1,\n",
    "    )\"\"\"\n",
    "    return (\n",
    "        einops.einsum(\n",
    "            residual_stack,\n",
    "            logit_diff_directions,\n",
    "            \"... batch d_model, batch d_model -> ...\",\n",
    "        )\n",
    "        / batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44018d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we expected residual stream patching near the final layer to work near perfectly,\n",
    "# since it was logit focused and thus basically linear, but turns out that\n",
    "# LayerNorm completely breaks things.\n",
    "\n",
    "# note: the fact that we had to use `ln_final.hook_normalized` instead of `resid_post`\n",
    "#       means that center_writing_weights is needed\n",
    "\n",
    "final_residual_stream = cache[\"resid_post\", -1]  # [batch seq d_model]\n",
    "# final_residual_stream = cache[\"ln_final.hook_normalized\"]  # [batch seq d_model]\n",
    "print(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]  # [batch d_model]\n",
    "\n",
    "# Apply LayerNorm scaling (to just the final sequence position)\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "# scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "#     final_token_residual_stream,\n",
    "#     layer=-1,\n",
    "#     pos_slice=-1,\n",
    "# )\n",
    "scaled_final_token_residual_stream = final_token_residual_stream\n",
    "\n",
    "batch_size = input_string_tokens.shape[0]\n",
    "\n",
    "average_logit_diff = (\n",
    "    einops.einsum(\n",
    "        scaled_final_token_residual_stream,\n",
    "        logit_diff_directions,\n",
    "        \"batch d_model, batch d_model ->\",\n",
    "    )\n",
    "    / batch_size\n",
    ")\n",
    "\n",
    "print(\"Note: These should be close!\")\n",
    "print(f\"Calculated average logit diff: {average_logit_diff:.10f}\")\n",
    "print(f\"Original logit difference:     {original_average_logit_diff:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = residual_stack_to_logit_diff(\n",
    "    final_token_residual_stream,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")\n",
    "\n",
    "print(\"Note: These should be close!\")\n",
    "print(f\"Calculated average logit diff: {result:.10f}\")\n",
    "print(f\"Original logit difference:     {original_average_logit_diff:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ba3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_resid, labels = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "last_token_accum = accum_resid[:, 0, -1, :]  # layer, batch, pos, d_model\n",
    "print(f\"{last_token_accum.shape=}\")  # layer, batch, d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89edabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U = model.W_U\n",
    "print(f\"{W_U.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_unembedded = einops.einsum(\n",
    "    last_token_accum,\n",
    "    W_U,\n",
    "    \"layer d_model, d_model d_vocab -> layer d_vocab\",\n",
    ")\n",
    "\n",
    "print(f\"{layers_unembedded.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens_logit_diffs: Float32[torch.Tensor, \"...\"] = residual_stack_to_logit_diff(\n",
    "    accum_resid,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")  # [component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.cfg)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a21fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc02eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import plotly_utils\n",
    "\n",
    "\n",
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1,\n",
    "    incl_mid=True,\n",
    "    pos_slice=-1,\n",
    "    return_labels=True,\n",
    ")\n",
    "# accumulated_residual has shape (component, batch, d_model)\n",
    "\n",
    "logit_lens_logit_diffs: Float32[torch.Tensor, \"...\"] = residual_stack_to_logit_diff(\n",
    "    accumulated_residual,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")  # [component]\n",
    "\n",
    "plotly_utils.line(\n",
    "    logit_lens_logit_diffs,\n",
    "    hovermode=\"x unified\",\n",
    "    title=\"Logit Difference From Accumulated Residual Stream\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    xaxis_tickvals=labels,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d73f1e",
   "metadata": {},
   "source": [
    "### Logit Difference From Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(\n",
    "    layer=-1,\n",
    "    pos_slice=-1,\n",
    "    return_labels=True,\n",
    ")\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(\n",
    "    per_layer_residual,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")\n",
    "\n",
    "plotly_utils.line(\n",
    "    per_layer_logit_diffs,\n",
    "    hovermode=\"x unified\",\n",
    "    title=\"Logit Difference From Each Layer\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    xaxis_tickvals=labels,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2515dba",
   "metadata": {},
   "source": [
    "### Logit Difference From Each Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_head_residual = einops.rearrange(\n",
    "    per_head_residual,\n",
    "    \"(layer head) ... -> layer head ...\",\n",
    "    layer=model.cfg.n_layers,\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(\n",
    "    per_head_residual,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")\n",
    "\n",
    "plotly_utils.imshow(\n",
    "    per_head_logit_diffs,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    width=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9eec6d",
   "metadata": {},
   "source": [
    "### Highest Value Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.core.display\n",
    "import IPython.display\n",
    "\n",
    "\n",
    "def topk_of_Nd_tensor(\n",
    "    tensor: Float[torch.Tensor, \"rows cols\"],\n",
    "    k: int,\n",
    ") -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Helper function: does same as tensor.topk(k).indices, but works over 2D tensors.\n",
    "    Returns a list of indices, i.e. shape [k, tensor.ndim].\n",
    "\n",
    "    Example: if tensor is 2D array of values for each head in each layer, this will\n",
    "    return a list of heads.\n",
    "    \"\"\"\n",
    "    i = torch.topk(tensor.flatten(), k).indices\n",
    "    return np.array(\n",
    "        np.unravel_index(\n",
    "            transformer_lens.utils.to_numpy(i),\n",
    "            tensor.shape,\n",
    "        )\n",
    "    ).T.tolist()\n",
    "\n",
    "\n",
    "k = 3\n",
    "\n",
    "for head_type in [\"Positive\", \"Negative\"]:\n",
    "\n",
    "    # Get the heads with largest (or smallest) contribution to the logit difference\n",
    "    top_heads = topk_of_Nd_tensor(\n",
    "        per_head_logit_diffs.cpu() * (1 if head_type == \"Positive\" else -1), k\n",
    "    )\n",
    "\n",
    "    # ex: [[0, 1], [1, 0], [0, 0]]\n",
    "    print(top_heads)\n",
    "\n",
    "    # Get all their attention patterns\n",
    "    attn_patterns_for_important_heads: Float[torch.Tensor, \"head q k\"] = torch.stack(\n",
    "        [cache[\"pattern\", layer][:, head][0] for layer, head in top_heads]\n",
    "    )\n",
    "\n",
    "    print(f\"{attn_patterns_for_important_heads.shape=}\")\n",
    "\n",
    "    # Display results\n",
    "    display(\n",
    "        cv.attention.attention_heads(\n",
    "            attention=attn_patterns_for_important_heads,\n",
    "            tokens=[x for x in input_string],\n",
    "            attention_head_names=[f\"{layer}.{head}\" for layer, head in top_heads],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de5552",
   "metadata": {},
   "source": [
    "### Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import transformer_lens_utils\n",
    "\n",
    "# TODO(bschoen): Clean and corrupted should actually switch first, should do this for search\n",
    "clean_logit_diff = transformer_lens_utils.logits_to_ave_logit_diff(\n",
    "    correct_logits,\n",
    "    answer_tokens,\n",
    ")\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = transformer_lens_utils.logits_to_ave_logit_diff(\n",
    "    incorrect_logits,\n",
    "    answer_tokens,\n",
    ")\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.attention.attention_heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a63158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioi_metric(\n",
    "    logits: Float[torch.Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Float[torch.Tensor, \"batch 2\"] = answer_tokens,\n",
    "    corrupted_logit_diff: float = corrupted_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    ") -> Float[torch.Tensor, \"\"]:\n",
    "    \"\"\"\n",
    "    Linear function of logit diff, calibrated so that it equals 0 when performance is\n",
    "    same as on corrupted input, and 1 when performance is same as on clean input.\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    patched_logit_diff = transformer_lens_utils.logits_to_ave_logit_diff(logits, answer_tokens)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff - corrupted_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_patch_resid_pre = transformer_lens.patching.get_act_patch_resid_pre(\n",
    "    model=model,\n",
    "    corrupted_tokens=incorrect_string_tokens,\n",
    "    clean_cache=correct_cache,\n",
    "    patching_metric=ioi_metric,\n",
    ")\n",
    "\n",
    "labels = [f\"{tok} {i}\" for i, tok in enumerate(correct_string)]\n",
    "\n",
    "imshow(\n",
    "    act_patch_resid_pre,\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    "    x=labels,\n",
    "    title=\"resid_pre Activation Patching\",\n",
    "    width=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c26324",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_patch_block_every = transformer_lens.patching.get_act_patch_block_every(\n",
    "    model,\n",
    "    incorrect_string_tokens,\n",
    "    correct_cache,\n",
    "    ioi_metric,\n",
    ")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=3, cols=1, subplot_titles=[\"Residual Stream\", \"Attn Output\", \"MLP Output\"])\n",
    "\n",
    "# Add heatmaps for each component\n",
    "for i in range(3):\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=act_patch_block_every[i].cpu().numpy(),\n",
    "            x=labels,\n",
    "            colorscale=\"RdBu\",\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1,\n",
    "        ),\n",
    "        row=i + 1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Logit Difference From Patched Components\",\n",
    "    height=800,\n",
    "    width=1000,\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "for i in range(3):\n",
    "    fig.update_xaxes(title_text=\"Sequence Position\", row=i + 1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Layer\", row=i + 1, col=1)\n",
    "\n",
    "# Update colorbar\n",
    "fig.update_layout(coloraxis_colorbar=dict(title=\"Logit Difference\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e64364",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_patch_attn_head_out_all_pos = transformer_lens.patching.get_act_patch_attn_head_out_all_pos(\n",
    "    model,\n",
    "    incorrect_string_tokens,\n",
    "    correct_cache,\n",
    "    ioi_metric,\n",
    ")\n",
    "# Create a figure using plotly.graph_objects\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=act_patch_attn_head_out_all_pos.cpu().numpy(),\n",
    "        colorscale=\"RdBu\",\n",
    "        zmid=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title=\"attn_head_out Activation Patching (All Pos)\",\n",
    "    xaxis_title=\"Head\",\n",
    "    yaxis_title=\"Layer\",\n",
    "    width=600,\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f27a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lens.patching.get_act_patch_mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3339a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_act_patch_attn_head_by_pos_every(\n",
    "    act_patch_attn_head_by_pos_every: torch.Tensor,\n",
    "    patch_types: list = None,\n",
    "    layer_labels: list = None,\n",
    "    head_labels: list = None,\n",
    "    figsize: tuple = (15, 10),\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize the activation patch attention head by position tensor.\n",
    "\n",
    "    Args:\n",
    "        act_patch_attn_head_by_pos_every (torch.Tensor): Tensor of shape [patch_type, layer, pos, head]\n",
    "        patch_types (list of str, optional): List of patch type names.\n",
    "            Defaults to ['Output', 'Query', 'Key', 'Value', 'Pattern'].\n",
    "        layer_labels (list of str, optional): List of layer names.\n",
    "            If None, defaults to ['Layer 1', 'Layer 2', ...].\n",
    "        head_labels (list of str, optional): List of head names.\n",
    "            If None, defaults to ['Head 1', 'Head 2', ...].\n",
    "        figsize (tuple, optional): Size of the figure. Defaults to (15, 10).\n",
    "    \"\"\"\n",
    "    # Ensure the tensor is on CPU and convert to NumPy\n",
    "    data = act_patch_attn_head_by_pos_every.detach().cpu().numpy()\n",
    "\n",
    "    num_patch_types, num_layers, num_positions, num_heads = data.shape\n",
    "\n",
    "    # Set default patch types if not provided\n",
    "    if patch_types is None:\n",
    "        if num_patch_types == 5:\n",
    "            patch_types = [\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"]\n",
    "        else:\n",
    "            patch_types = [f\"Type {i+1}\" for i in range(num_patch_types)]\n",
    "    else:\n",
    "        assert (\n",
    "            len(patch_types) == num_patch_types\n",
    "        ), f\"Expected {num_patch_types} patch types, but got {len(patch_types)}.\"\n",
    "\n",
    "    # Set default layer labels if not provided\n",
    "    if layer_labels is None:\n",
    "        layer_labels = [f\"Layer {i+1}\" for i in range(num_layers)]\n",
    "    else:\n",
    "        assert (\n",
    "            len(layer_labels) == num_layers\n",
    "        ), f\"Expected {num_layers} layers, but got {len(layer_labels)}.\"\n",
    "\n",
    "    # Set default head labels if not provided\n",
    "    if head_labels is None:\n",
    "        head_labels = [f\"Head {i+1}\" for i in range(num_heads)]\n",
    "    else:\n",
    "        assert (\n",
    "            len(head_labels) == num_heads\n",
    "        ), f\"Expected {num_heads} heads, but got {len(head_labels)}.\"\n",
    "\n",
    "    # Aggregate data over positions (e.g., by averaging)\n",
    "    data_avg = data.mean(axis=2)  # Shape: [patch_type, layer, head]\n",
    "\n",
    "    # Determine subplot grid size\n",
    "    n_cols = 2  # You can adjust this based on the number of patch types\n",
    "    n_rows = int(np.ceil(num_patch_types / n_cols))\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, constrained_layout=True)\n",
    "\n",
    "    # Flatten axes for easy iteration\n",
    "    if n_rows > 1 or n_cols > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx in range(num_patch_types):\n",
    "        ax = axes[idx]\n",
    "        sns.heatmap(\n",
    "            data_avg[idx],\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            xticklabels=head_labels,\n",
    "            yticklabels=layer_labels,\n",
    "            cmap=\"viridis\",\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_title(f\"Patch Type: {patch_types[idx]}\")\n",
    "        ax.set_xlabel(\"Head\")\n",
    "        ax.set_ylabel(\"Layer\")\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for idx in range(num_patch_types, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    plt.suptitle(\"Activation Patch Attention by Head and Layer\", fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import plotly_utils\n",
    "\n",
    "act_patch_attn_head_by_pos_every = transformer_lens.patching.get_act_patch_attn_head_by_pos_every(\n",
    "    model,\n",
    "    incorrect_string_tokens,\n",
    "    correct_cache,\n",
    "    ioi_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom labels (optional)\n",
    "# Since we have 5 patch types, we'll default to ['Output', 'Query', 'Key', 'Value', 'Pattern']\n",
    "# If your tensor's first dimension is not 5, provide your own list\n",
    "patch_types = [\"Output\", \"Query\", \"Key\", \"Value\", \"Pattern\"]\n",
    "\n",
    "# Define layer and head labels based on your model's specifics\n",
    "layer_labels = [\"Layer 1\", \"Layer 2\"]  # Adjust based on num_layers\n",
    "head_labels = [\"Head 1\", \"Head 2\"]  # Adjust based on num_heads\n",
    "\n",
    "# Visualize\n",
    "visualize_act_patch_attn_head_by_pos_every(\n",
    "    act_patch_attn_head_by_pos_every,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{act_patch_attn_head_by_pos_every.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef794ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get activation patching results for the output of each MLP layer (by position). Returns a tensor of shape [n_layers, pos]\n",
    "act_patch_mlp_out = transformer_lens.patching.get_act_patch_mlp_out(\n",
    "    model,\n",
    "    incorrect_string_tokens,\n",
    "    correct_cache,\n",
    "    ioi_metric,\n",
    ")\n",
    "\n",
    "# act_patch_mlp_out.shape=torch.Size([2, 7])\n",
    "print(f\"{act_patch_mlp_out.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming act_patch_mlp_out is the tensor obtained from your previous code\n",
    "# For demonstration, here's a dummy tensor (remove this line in your actual code)\n",
    "# act_patch_mlp_out = torch.rand(2, 7)\n",
    "\n",
    "# Ensure the tensor is detached from the computation graph and moved to CPU\n",
    "act_patch_mlp_out = act_patch_mlp_out.detach().cpu()\n",
    "\n",
    "n_layers, n_positions = act_patch_mlp_out.shape\n",
    "\n",
    "# Create a line plot for each layer\n",
    "positions = np.arange(n_positions)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for layer in range(n_layers):\n",
    "    plt.plot(positions, act_patch_mlp_out[layer], marker=\"o\", label=f\"Layer {layer}\")\n",
    "plt.xlabel(\"Position in Sequence\")\n",
    "plt.ylabel(\"Metric Value\")\n",
    "plt.title(\"Activation Patching MLP Output per Layer and Position\")\n",
    "plt.xticks(positions)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(act_patch_mlp_out, aspect=\"auto\", cmap=\"RdBu\", vmin=-1, vmax=1)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels([f\"Pos {i}\" for i in positions])\n",
    "ax.set_yticks(np.arange(n_layers))\n",
    "ax.set_yticklabels([f\"Layer {i}\" for i in range(n_layers)])\n",
    "\n",
    "# Add color bar\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel(\"Metric Value\", rotation=-90, va=\"bottom\")\n",
    "\n",
    "ax.set_title(\"Activation Patching MLP Output Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae182f9",
   "metadata": {},
   "source": [
    "## Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4780ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll look for some expected features with linear probes\n",
    "\n",
    "# ex: first character in sorted order\n",
    "# ex: position 1 bigger than position 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28590652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to look at each part along the residual stream\n",
    "residual_stream_hook_names = [c for c in cache if \"resid\" in c]\n",
    "\n",
    "print(residual_stream_hook_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold activations and targets for each residual stream hook\n",
    "activations_dict = {hook_name: [] for hook_name in residual_stream_hook_names}\n",
    "\n",
    "# keep track of corresponding inputs (because shuffling)\n",
    "inputs_per_activation_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for batch_index, batch in tqdm.tqdm(enumerate(train_loader)):\n",
    "\n",
    "    tokens, target = batch\n",
    "\n",
    "    tokens, target = tokens.to(device), target.to(device)\n",
    "\n",
    "    # ex: torch.Size([4, 9, 29])\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "\n",
    "    # store the residual stream at each hook point\n",
    "    for hook_name in residual_stream_hook_names:\n",
    "        hook_value = cache[hook_name].detach().cpu()\n",
    "        activations_dict[hook_name].append(hook_value)\n",
    "\n",
    "    # store inputs\n",
    "    inputs_per_activation_list.append(tokens.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ad8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the lists into single tensor\n",
    "activations_per_residual_stream_hook = {k: torch.stack(v) for k, v in activations_dict.items()}\n",
    "\n",
    "inputs_per_activation = torch.stack(inputs_per_activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold the `num_batches` into the `batch` dimension\n",
    "activations_per_residual_stream_hook = {\n",
    "    k: einops.rearrange(v, \"batch_size batch seq d_model -> (batch_size batch) seq d_model\")\n",
    "    for k, v in activations_per_residual_stream_hook.items()\n",
    "}\n",
    "\n",
    "inputs_per_activation = einops.rearrange(\n",
    "    inputs_per_activation,\n",
    "    \"batch_size batch seq -> (batch_size batch) seq\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cf9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hook_name in residual_stream_hook_names:\n",
    "    activations = activations_per_residual_stream_hook[hook_name]\n",
    "    print(f\"{hook_name} - {activations.shape=}\")\n",
    "\n",
    "print(f\"{inputs_per_activation.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fed70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Char = str\n",
    "TokenInt = int\n",
    "\n",
    "\n",
    "def get_sorted_char_label_at_seq_pos(\n",
    "    inputs: Int64[torch.Tensor, \"seq\"],\n",
    "    char_sequence_position: int,\n",
    ") -> Char:\n",
    "\n",
    "    tokens_as_string = tokenizer.decode(inputs.tolist())\n",
    "\n",
    "    # <abdc|abcd> -> <abdc -> abdc -> abcd\n",
    "    sorted_tokens_as_string = sorted(tokens_as_string.split(\"|\")[0][1:])\n",
    "\n",
    "    return sorted_tokens_as_string[char_sequence_position]\n",
    "\n",
    "\n",
    "def get_sorted_token_at_seq_pos(\n",
    "    inputs: Int64[torch.Tensor, \"seq\"],\n",
    "    char_sequence_position: int,\n",
    ") -> TokenInt:\n",
    "    token_char = get_sorted_char_label_at_seq_pos(inputs, char_sequence_position)\n",
    "    return torch.tensor(tokenizer.encode(token_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7474ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_as_string = \"<adcb|abcd>\"\n",
    "tokens_as_ints = torch.tensor(tokenizer.encode(tokens_as_string))\n",
    "\n",
    "assert get_sorted_char_label_at_seq_pos(tokens_as_ints, 0) == \"a\"\n",
    "assert get_sorted_char_label_at_seq_pos(tokens_as_ints, 1) == \"b\"\n",
    "assert get_sorted_char_label_at_seq_pos(tokens_as_ints, 2) == \"c\"\n",
    "assert get_sorted_char_label_at_seq_pos(tokens_as_ints, 3) == \"d\"\n",
    "\n",
    "\n",
    "assert tokenizer.decode(get_sorted_token_at_seq_pos(tokens_as_ints, 0).tolist()) == \"a\"\n",
    "assert tokenizer.decode(get_sorted_token_at_seq_pos(tokens_as_ints, 1).tolist()) == \"b\"\n",
    "assert tokenizer.decode(get_sorted_token_at_seq_pos(tokens_as_ints, 2).tolist()) == \"c\"\n",
    "assert tokenizer.decode(get_sorted_token_at_seq_pos(tokens_as_ints, 3).tolist()) == \"d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a12a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728eddab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from jaxtyping import Int, jaxtyped\n",
    "from typeguard import typechecked as typechecker\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class LinearProbeDataset:\n",
    "    activations: Float[torch.Tensor, \"batch d_model\"]\n",
    "    labels: Float[torch.Tensor, \"batch\"]\n",
    "\n",
    "\n",
    "@jaxtyped(typechecker=typechecker)\n",
    "def prepare_dataset(\n",
    "    activations: dict[str, Float[torch.Tensor, \"batch seq d_model\"]],\n",
    "    inputs: Int64[torch.Tensor, \"batch seq\"],\n",
    "    hook_name: str,\n",
    "    position: int,\n",
    "    char_sequence_position: int = 0,\n",
    ") -> LinearProbeDataset:\n",
    "\n",
    "    labels_list = []\n",
    "\n",
    "    for batch_index in range(inputs.shape[0]):\n",
    "\n",
    "        tokens: Int64[torch.Tensor, \"seq\"] = inputs[batch_index]\n",
    "\n",
    "        label = get_sorted_token_at_seq_pos(\n",
    "            tokens,\n",
    "            char_sequence_position,\n",
    "        )\n",
    "\n",
    "        labels_list.append(label)\n",
    "\n",
    "    labels: Int[torch.Tensor, \"batch\"] = torch.stack(labels_list)\n",
    "\n",
    "    activations_for_hook: Float[torch.Tensor, \"batch seq d_model\"] = activations[hook_name]\n",
    "\n",
    "    # slice at position\n",
    "    activations_for_hook_at_position: Float[torch.Tensor, \"batch d_model\"] = activations_for_hook[\n",
    "        :, position, :\n",
    "    ]\n",
    "    return LinearProbeDataset(activations_for_hook_at_position, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded to iterate over all char_sequence_positions\n",
    "\n",
    "HookName = str\n",
    "SeqPosition = int\n",
    "CharSeqPosition = int\n",
    "\n",
    "linear_probe_dataset_per_position_per_hook_name: dict[\n",
    "    HookName, dict[SeqPosition, dict[CharSeqPosition, LinearProbeDataset]]\n",
    "] = {}\n",
    "\n",
    "for hook_name in tqdm.tqdm(activations_per_residual_stream_hook.keys()):\n",
    "    linear_probe_dataset_per_position_per_hook_name[hook_name] = {}\n",
    "\n",
    "    for position_index in range(inputs_per_activation.shape[-1]):\n",
    "        linear_probe_dataset_per_position_per_hook_name[hook_name][position_index] = {}\n",
    "\n",
    "        for char_sequence_position in range(4):  # Assuming 4 char_sequence_positions\n",
    "            linear_probe_dataset = prepare_dataset(\n",
    "                activations=activations_per_residual_stream_hook,\n",
    "                inputs=inputs_per_activation,\n",
    "                hook_name=hook_name,\n",
    "                position=position_index,\n",
    "                char_sequence_position=char_sequence_position,\n",
    "            )\n",
    "\n",
    "            linear_probe_dataset_per_position_per_hook_name[hook_name][position_index][\n",
    "                char_sequence_position\n",
    "            ] = linear_probe_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeaa531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "# Assuming you have the following:\n",
    "# linear_probe_dataset_per_position_per_hook_name: dict[HookName, dict[SeqPosition, dict[CharSeqPosition, LinearProbeDataset]]]\n",
    "\n",
    "# Initialize a dictionary to store accuracies\n",
    "accuracy_dict: dict[tuple[HookName, SeqPosition, CharSeqPosition], float] = (\n",
    "    {}\n",
    ")  # {(hook_name, position, char_seq_position): accuracy}\n",
    "\n",
    "# We'll also store the linear_probes\n",
    "linear_probe_dict: dict[tuple[HookName, SeqPosition, CharSeqPosition], nn.Linear] = (\n",
    "    {}\n",
    ")  # {(hook_name, position, char_seq_position): linear_probe}\n",
    "\n",
    "# Iterate over each hook (layer), position, and char_sequence_position\n",
    "for hook_name in tqdm.tqdm(\n",
    "    linear_probe_dataset_per_position_per_hook_name.keys(),\n",
    "    desc=\"Processing hoooks...\",\n",
    "):\n",
    "    print(f\"Training linear probes for hook: {hook_name}\")\n",
    "\n",
    "    for position in tqdm.tqdm(\n",
    "        linear_probe_dataset_per_position_per_hook_name[hook_name],\n",
    "        desc=f\"Hook: {hook_name} Processing positions...\",\n",
    "    ):\n",
    "        print(f\"Training linear probes for hook: {hook_name} position: {position}\")\n",
    "\n",
    "        for char_seq_position in linear_probe_dataset_per_position_per_hook_name[hook_name][\n",
    "            position\n",
    "        ]:\n",
    "            print(\n",
    "                f\"Training linear probes for hook: {hook_name} \"\n",
    "                f\"position: {position} char_seq_position: {char_seq_position}\"\n",
    "            )\n",
    "\n",
    "            dataset = linear_probe_dataset_per_position_per_hook_name[hook_name][position][\n",
    "                char_seq_position\n",
    "            ]\n",
    "            activations: Float[torch.Tensor, \"batch d_model\"] = dataset.activations\n",
    "            labels: Int[torch.Tensor, \"batch\"] = dataset.labels\n",
    "\n",
    "            # Ensure activations and labels are on the same device\n",
    "            activations = activations.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Assuming labels are integer class labels (e.g., tokens)\n",
    "            num_classes = labels.max().item() + 1  # Number of classes\n",
    "\n",
    "            # Define the linear probe (a simple linear layer)\n",
    "            linear_probe: nn.Linear = nn.Linear(activations.size(-1), num_classes).to(device)\n",
    "\n",
    "            # Define loss function and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.AdamW(linear_probe.parameters(), lr=1e-3)\n",
    "            # Training settings\n",
    "            num_epochs = 10\n",
    "            batch_size = 2048\n",
    "            num_samples = activations.size(0)\n",
    "\n",
    "            # Shuffle the data\n",
    "            indices = torch.randperm(num_samples)\n",
    "            activations_shuffled = activations[indices]\n",
    "            labels_shuffled = labels[indices]\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_loss = 0.0\n",
    "                batch_count = 0\n",
    "\n",
    "                for i in range(0, num_samples, batch_size):\n",
    "                    batch_count += 1\n",
    "\n",
    "                    # (batch_size, d_model)\n",
    "                    batch_activations = activations_shuffled[i : i + batch_size]\n",
    "                    batch_labels = labels_shuffled[i : i + batch_size]  # (batch_size)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs: Float[torch.Tensor, \"batch num_classes\"] = linear_probe(\n",
    "                        batch_activations\n",
    "                    )\n",
    "\n",
    "                    loss = loss_fn(outputs, batch_labels)\n",
    "\n",
    "                    # Backward and optimize\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                    # if batch_count % 100 == 0:\n",
    "                    #     print(\n",
    "                    #         f\"[{hook_name}@{position}@{char_seq_position}] \"\n",
    "                    #         f\"Loss for epoch {epoch} \"\n",
    "                    #         f\"batch {batch_count}: {loss.item():.6f}\"\n",
    "                    #     )\n",
    "\n",
    "                avg_loss = epoch_loss / (num_samples // batch_size)\n",
    "                print(\n",
    "                    f\"[{hook_name}@{position}@{char_seq_position}] \"\n",
    "                    f\"Average loss for epoch {epoch}: {avg_loss:.6f}\"\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"[{hook_name}@{position}@{char_seq_position}] \" f\"Finished training linear probe\"\n",
    "            )\n",
    "\n",
    "            # Move back to CPU and store the linear probe\n",
    "            linear_probe_dict[(hook_name, position, char_seq_position)] = linear_probe.to(\n",
    "                torch.device(\"cpu\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca79f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probe_accuracy(\n",
    "    linear_probe: nn.Linear,\n",
    "    dataset: LinearProbeDataset,\n",
    "    batch_size: int = 1024,\n",
    ") -> float:\n",
    "    linear_probe.eval()\n",
    "\n",
    "    linear_probe = linear_probe.to(device)\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_samples = dataset.activations.size(0)\n",
    "\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_activations = dataset.activations[i : i + batch_size].to(device)\n",
    "            batch_labels = dataset.labels[i : i + batch_size].to(device)\n",
    "            batch_labels = batch_labels.view(-1)\n",
    "\n",
    "            outputs = linear_probe(batch_activations)\n",
    "            predictions = outputs.argmax(dim=-1)\n",
    "\n",
    "            total_correct += (predictions == batch_labels).sum().item()\n",
    "            total_samples += batch_labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def compute_all_probe_accuracies(\n",
    "    linear_probe_dict: dict[tuple[HookName, SeqPosition, CharSeqPosition], nn.Linear],\n",
    "    dataset_dict: dict[HookName, dict[SeqPosition, dict[CharSeqPosition, LinearProbeDataset]]],\n",
    ") -> dict[tuple[HookName, SeqPosition, CharSeqPosition], float]:\n",
    "\n",
    "    accuracy_dict: dict[tuple[HookName, SeqPosition, CharSeqPosition], float] = {}\n",
    "\n",
    "    for (hook_name, position, char_seq_position), linear_probe in tqdm.tqdm(\n",
    "        linear_probe_dict.items()\n",
    "    ):\n",
    "\n",
    "        dataset = dataset_dict[hook_name][position][char_seq_position]\n",
    "\n",
    "        accuracy = compute_probe_accuracy(linear_probe, dataset)\n",
    "\n",
    "        accuracy_dict[(hook_name, position, char_seq_position)] = accuracy\n",
    "\n",
    "    return accuracy_dict\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "accuracy_dict = compute_all_probe_accuracies(\n",
    "    linear_probe_dict,\n",
    "    linear_probe_dataset_per_position_per_hook_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aaf020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probe_accuracies_lineplot(\n",
    "    accuracy_dict: dict[tuple[HookName, SeqPosition, CharSeqPosition], float]\n",
    ") -> None:\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for (hook_name, position, char_seq_position), accuracy in accuracy_dict.items():\n",
    "        data.append(\n",
    "            {\n",
    "                \"Hook Name\": hook_name,\n",
    "                \"Position\": position,\n",
    "                \"Char Seq Position\": char_seq_position,\n",
    "                \"Accuracy\": accuracy,\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Get unique char_seq_positions and sort them\n",
    "    char_seq_positions = sorted(df[\"Char Seq Position\"].unique())\n",
    "\n",
    "    # Create a subplot for each char_seq_position\n",
    "    fig, axes = plt.subplots(\n",
    "        len(char_seq_positions),\n",
    "        1,\n",
    "        figsize=(8, 2 * len(char_seq_positions)),\n",
    "        sharex=True,\n",
    "    )\n",
    "    fig.suptitle(\"Linear Probe Accuracy vs Position\", fontsize=14)\n",
    "\n",
    "    for idx, char_seq_position in enumerate(char_seq_positions):\n",
    "        df_subset = df[df[\"Char Seq Position\"] == char_seq_position]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=df_subset,\n",
    "            x=\"Position\",\n",
    "            y=\"Accuracy\",\n",
    "            hue=\"Hook Name\",\n",
    "            marker=\"o\",\n",
    "            alpha=0.5,\n",
    "            ax=axes[idx],\n",
    "        )\n",
    "        axes[idx].set_title(f\"Sorted Char: {char_seq_position}\")\n",
    "        axes[idx].set_xlabel(\"Sequence Position\")\n",
    "        axes[idx].set_ylabel(\"Accuracy\")\n",
    "        axes[idx].legend(title=\"Hook Name\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_probe_accuracies_heatmap(\n",
    "    accuracy_dict: dict[tuple[HookName, SeqPosition, CharSeqPosition], float]\n",
    ") -> None:\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for (hook_name, position, char_seq_position), accuracy in accuracy_dict.items():\n",
    "        data.append(\n",
    "            {\n",
    "                \"Hook Name\": hook_name,\n",
    "                \"Position\": position,\n",
    "                \"Char Seq Position\": char_seq_position,\n",
    "                \"Accuracy\": accuracy,\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Get unique hook_names, positions, and char_seq_positions and sort them\n",
    "    hook_names = sorted(df[\"Hook Name\"].unique())\n",
    "    positions = sorted(df[\"Position\"].unique())\n",
    "    char_seq_positions = sorted(df[\"Char Seq Position\"].unique())\n",
    "\n",
    "    # Create a figure for the heatmap\n",
    "    fig, axes = plt.subplots(\n",
    "        len(char_seq_positions),\n",
    "        1,\n",
    "        figsize=(8, 2 * len(char_seq_positions)),\n",
    "        sharex=True,\n",
    "    )\n",
    "    fig.suptitle(\"Linear Probe Accuracy Heatmap\", fontsize=14)\n",
    "\n",
    "    for idx, char_seq_position in enumerate(char_seq_positions):\n",
    "        df_subset = df[df[\"Char Seq Position\"] == char_seq_position]\n",
    "\n",
    "        # Pivot the data to create a 2D matrix for the heatmap\n",
    "        pivot_df = df_subset.pivot(index=\"Hook Name\", columns=\"Position\", values=\"Accuracy\")\n",
    "\n",
    "        # Create the heatmap\n",
    "        sns.heatmap(\n",
    "            pivot_df,\n",
    "            ax=axes[idx] if len(char_seq_positions) > 1 else axes,\n",
    "            cmap=\"Blues\",\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            cbar_kws={\"label\": \"Accuracy\"},\n",
    "        )\n",
    "\n",
    "        axes[idx].set_title(f\"Sorted Char: {char_seq_position}\")\n",
    "        axes[idx].set_xlabel(\"Sequence Position\")\n",
    "        axes[idx].set_ylabel(\"Hook Name\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_probe_accuracies_lineplot(accuracy_dict)\n",
    "plot_probe_accuracies_heatmap(accuracy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_probe_confusion_matrices(\n",
    "    linear_probe_dict: dict[tuple[HookName, SeqPosition], nn.Linear],\n",
    "    dataset_dict: dict[HookName, dict[SeqPosition, LinearProbeDataset]],\n",
    ") -> None:\n",
    "    # Get unique hook names and positions\n",
    "    hook_names = sorted(set(hook_name for hook_name, _ in linear_probe_dict.keys()))\n",
    "    positions = sorted(set(position for _, position in linear_probe_dict.keys()))\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(\n",
    "        len(hook_names), len(positions), figsize=(5 * len(positions), 4 * len(hook_names))\n",
    "    )\n",
    "    fig.suptitle(\"Confusion Matrices for Linear Probes\", fontsize=16)\n",
    "\n",
    "    for i, hook_name in enumerate(hook_names):\n",
    "        for j, position in enumerate(positions):\n",
    "            if (hook_name, position) in linear_probe_dict:\n",
    "                linear_probe = linear_probe_dict[(hook_name, position)]\n",
    "                dataset = dataset_dict[hook_name][position]\n",
    "\n",
    "                linear_probe.eval()\n",
    "                with torch.no_grad():\n",
    "                    activations = dataset.activations.to(device)\n",
    "                    labels = dataset.labels.to(device)\n",
    "                    labels = labels.view(-1)\n",
    "                    outputs = linear_probe(activations)\n",
    "                    predictions = outputs.argmax(dim=-1)\n",
    "\n",
    "                cm = confusion_matrix(labels.cpu(), predictions.cpu())\n",
    "\n",
    "                sns.heatmap(cm, cmap=\"Blues\", ax=axes[i, j], cbar=False)\n",
    "                if i == 0:\n",
    "                    axes[i, j].set_title(f\"{hook_name}\\nPosition {position}\")\n",
    "                axes[i, j].set_xlabel(\"Predicted\")\n",
    "                axes[i, j].set_ylabel(\"Actual\")\n",
    "            else:\n",
    "                axes[i, j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "plot_probe_confusion_matrices(linear_probe_dict, linear_probe_dataset_per_position_per_hook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33405e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def visualize_linear_probe_weights(linear_probe: nn.Linear, num_components: int = 2) -> None:\n",
    "    weights = linear_probe.weight.data.cpu().numpy()\n",
    "    num_classes = weights.shape[0]\n",
    "\n",
    "    pca = PCA(n_components=num_components)\n",
    "    weights_pca = pca.fit_transform(weights)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(num_classes):\n",
    "        plt.scatter(weights_pca[i, 0], weights_pca[i, 1], label=f\"Class {i}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"PCA of Linear Probe Weights\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "visualize_linear_probe_weights(linear_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations_with_labels(\n",
    "    dataset: LinearProbeDataset,\n",
    "    num_components: int = 2,\n",
    "    sample_size: int | None = None,\n",
    ") -> None:\n",
    "    activations = dataset.activations\n",
    "    labels = dataset.labels\n",
    "    if sample_size and activations.size(0) > sample_size:\n",
    "        indices = torch.randperm(activations.size(0))[:sample_size]\n",
    "        activations = activations[indices]\n",
    "        labels = labels[indices]\n",
    "    activations_np = activations.cpu().numpy()\n",
    "    labels_np = labels.view(-1).cpu().numpy()\n",
    "\n",
    "    pca = PCA(n_components=num_components)\n",
    "    activations_pca = pca.fit_transform(activations_np)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(\n",
    "        activations_pca[:, 0], activations_pca[:, 1], c=labels_np, cmap=\"viridis\", alpha=0.5\n",
    "    )\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title(\"PCA of Activations Colored by Labels\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "visualize_activations_with_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(\n",
    "    linear_probes_by_hook_and_position: dict[tuple[HookName, SeqPosition], nn.Linear],\n",
    "    hook_name_to_visualize: HookName,\n",
    "    position_to_visualize: SeqPosition,\n",
    "    num_classes: int,\n",
    ") -> None:\n",
    "    linear_probe = linear_probes_by_hook_and_position[\n",
    "        (hook_name_to_visualize, position_to_visualize)\n",
    "    ]\n",
    "\n",
    "    weight_matrix = linear_probe.weight.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(weight_matrix, cmap=\"coolwarm\")\n",
    "    plt.title(\n",
    "        f\"Weights of Linear Probe at Hook: {hook_name_to_visualize}, Position: {position_to_visualize}\"\n",
    "    )\n",
    "    plt.xlabel(\"Model Hidden Dimension\")\n",
    "    plt.ylabel(\"Output Classes\")\n",
    "    plt.show()\n",
    "\n",
    "    # PCA visualization\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    weight_vectors = weight_matrix\n",
    "    pca = PCA(n_components=2)\n",
    "    weight_vectors_2d = pca.fit_transform(weight_vectors)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(weight_vectors_2d[:, 0], weight_vectors_2d[:, 1])\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        plt.text(weight_vectors_2d[i, 0], weight_vectors_2d[i, 1], str(i))\n",
    "\n",
    "    plt.title(\n",
    "        f\"PCA of Linear Probe Weights at Hook: {hook_name_to_visualize}, Position: {position_to_visualize}\"\n",
    "    )\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def interpret_probes(\n",
    "    linear_probes_by_hook_and_position: dict[tuple[HookName, SeqPosition], nn.Linear],\n",
    "    num_top_features: int = 5,\n",
    ") -> None:\n",
    "\n",
    "    weights_dict = {\n",
    "        k: v.weight.detach().cpu().numpy() for k, v in linear_probes_by_hook_and_position.items()\n",
    "    }\n",
    "\n",
    "    for (hook_name, position), weight_matrix in weights_dict.items():\n",
    "        print(f\"Hook: {hook_name}, Position: {position}\")\n",
    "        num_classes = weight_matrix.shape[0]\n",
    "        for class_index in range(num_classes):\n",
    "            class_weights = weight_matrix[class_index]\n",
    "            top_features = np.argsort(np.abs(class_weights))[-num_top_features:][::-1]\n",
    "            print(f\"  Top features for class {class_index}: {top_features}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e6735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "accuracy_dict, weights_dict = train_linear_probes(linear_probe_dataset_per_position_per_hook_name)\n",
    "visualize_accuracies(accuracy_dict)\n",
    "hook_name_to_visualize = hook_names[0]\n",
    "position_to_visualize = positions[0]\n",
    "num_classes = labels.max().item() + 1\n",
    "visualize_weights(weights_dict, hook_name_to_visualize, position_to_visualize, num_classes)\n",
    "interpret_probes(weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aed855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Order:\n",
    "    LESS_THAN = -1\n",
    "    EQUAL = 0\n",
    "    GREATER_THAN = 1\n",
    "\n",
    "\n",
    "def create_order_tensor(input_string: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a PyTorch tensor representing the lexicographical order of characters in the input string.\n",
    "\n",
    "    Args:\n",
    "    input_string (str): The input string to analyze.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: A tensor of shape (len(input_string), len(input_string)) where tensor[i][j] is:\n",
    "        1 if input_string[i] comes before input_string[j] in lexicographical order\n",
    "        0 if input_string[i] is the same as input_string[j]\n",
    "        -1 if input_string[i] comes after input_string[j] in lexicographical order\n",
    "    \"\"\"\n",
    "    # Get the length of the input string\n",
    "    n = len(input_string)\n",
    "\n",
    "    # Create a tensor of zeros with shape (n, n)\n",
    "    order_tensor = torch.zeros((n, n), dtype=torch.int)\n",
    "\n",
    "    # Fill the tensor based on lexicographical order\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                # Same character, set to 0\n",
    "                order_tensor[i, j] = Order.EQUAL\n",
    "            elif input_string[i] < input_string[j]:\n",
    "                # Character at i comes before character at j\n",
    "                order_tensor[i, j] = Order.GREATER_THAN\n",
    "            else:\n",
    "                # Character at i comes after character at j\n",
    "                order_tensor[i, j] = Order.LESS_THAN\n",
    "\n",
    "    return order_tensor\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_string = \"adbc\"\n",
    "order_tensor = create_order_tensor(input_string)\n",
    "print(order_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a382102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e7415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "\n",
    "class SampleInfo:\n",
    "\n",
    "    sorted_string_tokens: list[str]\n",
    "\n",
    "\n",
    "sample_to_sample_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab73b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008789b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952337bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c952317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cc310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "706d8828",
   "metadata": {},
   "source": [
    "## SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12bd9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5b1a14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Hook Name                      </span><span style=\"font-weight: bold\"> Shape                      </span>\n",
       "\n",
       " hook_embed                      torch.Size([1, 11, 16])    \n",
       " hook_pos_embed                  torch.Size([1, 11, 16])    \n",
       " blocks.0.hook_resid_pre         torch.Size([1, 11, 16])    \n",
       " blocks.0.attn.hook_q            torch.Size([1, 11, 2, 8])  \n",
       " blocks.0.attn.hook_k            torch.Size([1, 11, 2, 8])  \n",
       " blocks.0.attn.hook_v            torch.Size([1, 11, 2, 8])  \n",
       " blocks.0.attn.hook_attn_scores  torch.Size([1, 2, 11, 11]) \n",
       " blocks.0.attn.hook_pattern      torch.Size([1, 2, 11, 11]) \n",
       " blocks.0.attn.hook_z            torch.Size([1, 11, 2, 8])  \n",
       " blocks.0.hook_attn_out          torch.Size([1, 11, 16])    \n",
       " blocks.0.hook_resid_mid         torch.Size([1, 11, 16])    \n",
       " blocks.0.mlp.hook_pre           torch.Size([1, 11, 64])    \n",
       " blocks.0.mlp.hook_post          torch.Size([1, 11, 64])    \n",
       " blocks.0.hook_mlp_out           torch.Size([1, 11, 16])    \n",
       " blocks.0.hook_resid_post        torch.Size([1, 11, 16])    \n",
       " blocks.1.hook_resid_pre         torch.Size([1, 11, 16])    \n",
       " blocks.1.attn.hook_q            torch.Size([1, 11, 2, 8])  \n",
       " blocks.1.attn.hook_k            torch.Size([1, 11, 2, 8])  \n",
       " blocks.1.attn.hook_v            torch.Size([1, 11, 2, 8])  \n",
       " blocks.1.attn.hook_attn_scores  torch.Size([1, 2, 11, 11]) \n",
       " blocks.1.attn.hook_pattern      torch.Size([1, 2, 11, 11]) \n",
       " blocks.1.attn.hook_z            torch.Size([1, 11, 2, 8])  \n",
       " blocks.1.hook_attn_out          torch.Size([1, 11, 16])    \n",
       " blocks.1.hook_resid_mid         torch.Size([1, 11, 16])    \n",
       " blocks.1.mlp.hook_pre           torch.Size([1, 11, 64])    \n",
       " blocks.1.mlp.hook_post          torch.Size([1, 11, 64])    \n",
       " blocks.1.hook_mlp_out           torch.Size([1, 11, 16])    \n",
       " blocks.1.hook_resid_post        torch.Size([1, 11, 16])    \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mHook Name                     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mShape                     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " hook_embed                      torch.Size([1, 11, 16])    \n",
       " hook_pos_embed                  torch.Size([1, 11, 16])    \n",
       " blocks.0.hook_resid_pre         torch.Size([1, 11, 16])    \n",
       " blocks.0.attn.hook_q            torch.Size([1, 11, 2, 8])  \n",
       " blocks.0.attn.hook_k            torch.Size([1, 11, 2, 8])  \n",
       " blocks.0.attn.hook_v            torch.Size([1, 11, 2, 8])  \n",
       " blocks.0.attn.hook_attn_scores  torch.Size([1, 2, 11, 11]) \n",
       " blocks.0.attn.hook_pattern      torch.Size([1, 2, 11, 11]) \n",
       " blocks.0.attn.hook_z            torch.Size([1, 11, 2, 8])  \n",
       " blocks.0.hook_attn_out          torch.Size([1, 11, 16])    \n",
       " blocks.0.hook_resid_mid         torch.Size([1, 11, 16])    \n",
       " blocks.0.mlp.hook_pre           torch.Size([1, 11, 64])    \n",
       " blocks.0.mlp.hook_post          torch.Size([1, 11, 64])    \n",
       " blocks.0.hook_mlp_out           torch.Size([1, 11, 16])    \n",
       " blocks.0.hook_resid_post        torch.Size([1, 11, 16])    \n",
       " blocks.1.hook_resid_pre         torch.Size([1, 11, 16])    \n",
       " blocks.1.attn.hook_q            torch.Size([1, 11, 2, 8])  \n",
       " blocks.1.attn.hook_k            torch.Size([1, 11, 2, 8])  \n",
       " blocks.1.attn.hook_v            torch.Size([1, 11, 2, 8])  \n",
       " blocks.1.attn.hook_attn_scores  torch.Size([1, 2, 11, 11]) \n",
       " blocks.1.attn.hook_pattern      torch.Size([1, 2, 11, 11]) \n",
       " blocks.1.attn.hook_z            torch.Size([1, 11, 2, 8])  \n",
       " blocks.1.hook_attn_out          torch.Size([1, 11, 16])    \n",
       " blocks.1.hook_resid_mid         torch.Size([1, 11, 16])    \n",
       " blocks.1.mlp.hook_pre           torch.Size([1, 11, 64])    \n",
       " blocks.1.mlp.hook_post          torch.Size([1, 11, 64])    \n",
       " blocks.1.hook_mlp_out           torch.Size([1, 11, 16])    \n",
       " blocks.1.hook_resid_post        torch.Size([1, 11, 16])    \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run on an example to inspect\n",
    "input_string = \"<bacd|abcd>\"\n",
    "input_string_tokens = tokenize_string_as_batch(tokenizer, input_string)\n",
    "logits, cache = model.run_with_cache(input_string_tokens)\n",
    "\n",
    "show_cache(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0263d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blocks.0.hook_resid_pre',\n",
       " 'blocks.0.hook_attn_out',\n",
       " 'blocks.0.hook_resid_mid',\n",
       " 'blocks.0.hook_mlp_out',\n",
       " 'blocks.0.hook_resid_post',\n",
       " 'blocks.1.hook_resid_pre',\n",
       " 'blocks.1.hook_attn_out',\n",
       " 'blocks.1.hook_resid_mid',\n",
       " 'blocks.1.hook_mlp_out',\n",
       " 'blocks.1.hook_resid_post']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anything with last dimension `d_model` is in the residual stream (not just `resid` layers)\n",
    "#\n",
    "# NOTE: We're fine excluding `hook_embed` and `hook_pos_embed` because those get added together to\n",
    "#       form `hook_resid_pre`\n",
    "#\n",
    "residual_stream_hook_names = [\n",
    "    k\n",
    "    for k in cache.keys()\n",
    "    if cache[k].shape[-1] == model.cfg.d_model and k not in [\"hook_embed\", \"hook_pos_embed\"]\n",
    "]\n",
    "\n",
    "residual_stream_hook_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da553153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sae_trainer_per_hook(\n",
    "    residual_stream_hook_names: list[str],\n",
    "    model: tl.HookedTransformer,\n",
    "    device: torch.device,\n",
    ") -> dict[str, sae.SAETrainer]:\n",
    "\n",
    "    # create an SAE trainer per residual stream hook\n",
    "    sae_trainer_per_hook: dict[str, sae.SAETrainer] = {}\n",
    "\n",
    "    # chosen arbitrarily\n",
    "    sae_expansion_factor = 32\n",
    "\n",
    "    for hook_name in residual_stream_hook_names:\n",
    "\n",
    "        sae_trainer_per_hook[hook_name] = sae.SAETrainer(\n",
    "            sae_cfg=sae.SAEConfig(\n",
    "                input_size=model.cfg.d_model,\n",
    "                n_dict_components=model.cfg.d_model * sae_expansion_factor,\n",
    "            ),\n",
    "            sae_trainer_cfg=sae.SAETrainerConfig(\n",
    "                hook_point=hook_name,\n",
    "                lr=1e-3,\n",
    "                loss_config=sae.SAELossConfig(l1_coefficient=1e-6),\n",
    "            ),\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    return sae_trainer_per_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b71f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f3a5d91",
   "metadata": {},
   "source": [
    "## Transcoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab01dc5",
   "metadata": {},
   "source": [
    "## Training Sparsifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ef70012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've got a reasonable setup, let's train the transcoder (\n",
    "# for sorting a fixed size list, we can see 100% of the activation comes\n",
    "# from the final layer MLP)\n",
    "from gpt_from_scratch import transcoder\n",
    "\n",
    "\n",
    "def create_transcoder_trainer_per_hook(\n",
    "    model: tl.HookedTransformer,\n",
    "    device: torch.device,\n",
    ") -> dict[str, transcoder.TranscoderTrainer]:\n",
    "\n",
    "    # create a transcoder trainer per MLP\n",
    "    transcoder_trainer_per_hook: dict[str, transcoder.TranscoderTrainer] = {}\n",
    "\n",
    "    for block_index, _ in enumerate(model.blocks):\n",
    "\n",
    "        # get the MLP input and output hook names\n",
    "        mlp_in_hook_name = f\"blocks.{block_index}.hook_resid_mid\"\n",
    "        mlp_out_hook_name = f\"blocks.{block_index}.hook_mlp_out\"\n",
    "\n",
    "        transcoder_cfg = transcoder.TranscoderConfig.from_model(model=model, device=device)\n",
    "\n",
    "        transcoder_training_cfg = transcoder.TranscoderTrainingConfig(\n",
    "            hook_point=mlp_in_hook_name,\n",
    "            out_hook_point=mlp_out_hook_name,\n",
    "            learning_rate=1e-3,\n",
    "            l1_coefficient=1e-6,\n",
    "        )\n",
    "\n",
    "        # create a transcoder trainer\n",
    "        transcoder_trainer_per_hook[mlp_in_hook_name] = transcoder.TranscoderTrainer(\n",
    "            transcoder_cfg=transcoder_cfg,\n",
    "            transcoder_training_cfg=transcoder_training_cfg,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    return transcoder_trainer_per_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26eb2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hook_names_to_cache_for_trainers(\n",
    "    transcoder_trainer_per_hook: dict[str, transcoder.TranscoderTrainer],\n",
    "    sae_trainer_per_hook: dict[str, sae.SAETrainer],\n",
    ") -> set[str]:\n",
    "\n",
    "    # determine which layers we need to cache for the transcoder and saes, since\n",
    "    # we don't need things like q / k\n",
    "    hook_names_to_cache = []\n",
    "\n",
    "    for trainer in transcoder_trainer_per_hook.values():\n",
    "        hook_names_to_cache.extend([trainer.cfg.hook_point, trainer.cfg.out_hook_point])\n",
    "\n",
    "    for trainer in sae_trainer_per_hook.values():\n",
    "        hook_names_to_cache.append(trainer.cfg.hook_point)\n",
    "\n",
    "    # removes duplicates and makes it clear we're checking this for membership\n",
    "    hook_names_to_cache = set(hook_names_to_cache)\n",
    "\n",
    "    return hook_names_to_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3ffcd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/bronsonschoen/gpt_from_scratch/wandb/run-20240922_200223-vva6fpjg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2/runs/vva6fpjg' target=\"_blank\">sae-and-transcoder</a></strong> to <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2/runs/vva6fpjg' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2/runs/vva6fpjg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "503it [00:35, 14.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1004it [01:09, 14.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1504it [01:44, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2004it [02:20, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2504it [02:56, 14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3004it [03:31, 14.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3504it [04:06, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4002it [04:48, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4502it [05:35, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5003it [06:11, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5504it [06:46, 14.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6004it [07:19, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6504it [07:53, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7004it [08:27, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7504it [09:03, 13.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8004it [09:38, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8504it [10:12, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9004it [10:47, 14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9502it [11:23, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10002it [12:03,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10503it [12:41, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11003it [13:23, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11503it [14:00, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12004it [14:36, 14.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12504it [15:12, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13004it [15:49, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13504it [16:24, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14004it [16:58, 13.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14000/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14504it [17:33, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14500/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15000it [18:06, 13.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14999/15000] Logging...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3310c41d06344f7ca14904d0febb07fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>sae_L0_hook_attn_out/activations_mean</td><td></td></tr><tr><td>sae_L0_hook_attn_out/activations_sparsity</td><td></td></tr><tr><td>sae_L0_hook_attn_out/activations_std</td><td></td></tr><tr><td>sae_L0_hook_attn_out/decoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_attn_out/encoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_attn_out/learning_rate</td><td></td></tr><tr><td>sae_L0_hook_attn_out/reconstruction_loss</td><td></td></tr><tr><td>sae_L0_hook_attn_out/sparsity_loss</td><td></td></tr><tr><td>sae_L0_hook_attn_out/total_loss</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/activations_mean</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/activations_sparsity</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/activations_std</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/decoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/encoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/learning_rate</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/reconstruction_loss</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/sparsity_loss</td><td></td></tr><tr><td>sae_L0_hook_mlp_out/total_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/activations_mean</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/activations_sparsity</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/activations_std</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/decoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/encoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/learning_rate</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/reconstruction_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/sparsity_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_mid/total_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_post/activations_mean</td><td></td></tr><tr><td>sae_L0_hook_resid_post/activations_sparsity</td><td></td></tr><tr><td>sae_L0_hook_resid_post/activations_std</td><td></td></tr><tr><td>sae_L0_hook_resid_post/decoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_resid_post/encoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_resid_post/learning_rate</td><td></td></tr><tr><td>sae_L0_hook_resid_post/reconstruction_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_post/sparsity_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_post/total_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/activations_mean</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/activations_sparsity</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/activations_std</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/decoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/encoder_weight_norm</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/learning_rate</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/reconstruction_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/sparsity_loss</td><td></td></tr><tr><td>sae_L0_hook_resid_pre/total_loss</td><td></td></tr><tr><td>sae_L1_hook_attn_out/activations_mean</td><td></td></tr><tr><td>sae_L1_hook_attn_out/activations_sparsity</td><td></td></tr><tr><td>sae_L1_hook_attn_out/activations_std</td><td></td></tr><tr><td>sae_L1_hook_attn_out/decoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_attn_out/encoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_attn_out/learning_rate</td><td></td></tr><tr><td>sae_L1_hook_attn_out/reconstruction_loss</td><td></td></tr><tr><td>sae_L1_hook_attn_out/sparsity_loss</td><td></td></tr><tr><td>sae_L1_hook_attn_out/total_loss</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/activations_mean</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/activations_sparsity</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/activations_std</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/decoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/encoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/learning_rate</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/reconstruction_loss</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/sparsity_loss</td><td></td></tr><tr><td>sae_L1_hook_mlp_out/total_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/activations_mean</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/activations_sparsity</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/activations_std</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/decoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/encoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/learning_rate</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/reconstruction_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/sparsity_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_mid/total_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_post/activations_mean</td><td></td></tr><tr><td>sae_L1_hook_resid_post/activations_sparsity</td><td></td></tr><tr><td>sae_L1_hook_resid_post/activations_std</td><td></td></tr><tr><td>sae_L1_hook_resid_post/decoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_resid_post/encoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_resid_post/learning_rate</td><td></td></tr><tr><td>sae_L1_hook_resid_post/reconstruction_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_post/sparsity_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_post/total_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/activations_mean</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/activations_sparsity</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/activations_std</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/decoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/encoder_weight_norm</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/learning_rate</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/reconstruction_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/sparsity_loss</td><td></td></tr><tr><td>sae_L1_hook_resid_pre/total_loss</td><td></td></tr><tr><td>tc_L0/W_dec_norm</td><td></td></tr><tr><td>tc_L0/W_enc_norm</td><td></td></tr><tr><td>tc_L0/hidden_activations_mean</td><td></td></tr><tr><td>tc_L0/hidden_activations_sparsity</td><td></td></tr><tr><td>tc_L0/hidden_activations_std</td><td></td></tr><tr><td>tc_L0/l1_loss</td><td></td></tr><tr><td>tc_L0/learning_rate</td><td></td></tr><tr><td>tc_L0/mse_loss</td><td></td></tr><tr><td>tc_L0/total_loss</td><td></td></tr><tr><td>tc_L1/W_dec_norm</td><td></td></tr><tr><td>tc_L1/W_enc_norm</td><td></td></tr><tr><td>tc_L1/hidden_activations_mean</td><td></td></tr><tr><td>tc_L1/hidden_activations_sparsity</td><td></td></tr><tr><td>tc_L1/hidden_activations_std</td><td></td></tr><tr><td>tc_L1/l1_loss</td><td></td></tr><tr><td>tc_L1/learning_rate</td><td></td></tr><tr><td>tc_L1/mse_loss</td><td></td></tr><tr><td>tc_L1/total_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>sae_L0_hook_attn_out/activations_mean</td><td>0.00167</td></tr><tr><td>sae_L0_hook_attn_out/activations_sparsity</td><td>0.97564</td></tr><tr><td>sae_L0_hook_attn_out/activations_std</td><td>0.01696</td></tr><tr><td>sae_L0_hook_attn_out/decoder_weight_norm</td><td>1.50116</td></tr><tr><td>sae_L0_hook_attn_out/encoder_weight_norm</td><td>6.87067</td></tr><tr><td>sae_L0_hook_attn_out/learning_rate</td><td>0.001</td></tr><tr><td>sae_L0_hook_attn_out/reconstruction_loss</td><td>0.00052</td></tr><tr><td>sae_L0_hook_attn_out/sparsity_loss</td><td>0.00438</td></tr><tr><td>sae_L0_hook_attn_out/total_loss</td><td>0.0049</td></tr><tr><td>sae_L0_hook_mlp_out/activations_mean</td><td>0.01344</td></tr><tr><td>sae_L0_hook_mlp_out/activations_sparsity</td><td>0.94499</td></tr><tr><td>sae_L0_hook_mlp_out/activations_std</td><td>0.09277</td></tr><tr><td>sae_L0_hook_mlp_out/decoder_weight_norm</td><td>1.88197</td></tr><tr><td>sae_L0_hook_mlp_out/encoder_weight_norm</td><td>12.36247</td></tr><tr><td>sae_L0_hook_mlp_out/learning_rate</td><td>0.001</td></tr><tr><td>sae_L0_hook_mlp_out/reconstruction_loss</td><td>0.0017</td></tr><tr><td>sae_L0_hook_mlp_out/sparsity_loss</td><td>0.03523</td></tr><tr><td>sae_L0_hook_mlp_out/total_loss</td><td>0.03693</td></tr><tr><td>sae_L0_hook_resid_mid/activations_mean</td><td>0.00384</td></tr><tr><td>sae_L0_hook_resid_mid/activations_sparsity</td><td>0.96711</td></tr><tr><td>sae_L0_hook_resid_mid/activations_std</td><td>0.03211</td></tr><tr><td>sae_L0_hook_resid_mid/decoder_weight_norm</td><td>2.78677</td></tr><tr><td>sae_L0_hook_resid_mid/encoder_weight_norm</td><td>17.76191</td></tr><tr><td>sae_L0_hook_resid_mid/learning_rate</td><td>0.001</td></tr><tr><td>sae_L0_hook_resid_mid/reconstruction_loss</td><td>0.00061</td></tr><tr><td>sae_L0_hook_resid_mid/sparsity_loss</td><td>0.01006</td></tr><tr><td>sae_L0_hook_resid_mid/total_loss</td><td>0.01067</td></tr><tr><td>sae_L0_hook_resid_post/activations_mean</td><td>0.01635</td></tr><tr><td>sae_L0_hook_resid_post/activations_sparsity</td><td>0.93673</td></tr><tr><td>sae_L0_hook_resid_post/activations_std</td><td>0.1095</td></tr><tr><td>sae_L0_hook_resid_post/decoder_weight_norm</td><td>1.92396</td></tr><tr><td>sae_L0_hook_resid_post/encoder_weight_norm</td><td>12.2964</td></tr><tr><td>sae_L0_hook_resid_post/learning_rate</td><td>0.001</td></tr><tr><td>sae_L0_hook_resid_post/reconstruction_loss</td><td>0.00208</td></tr><tr><td>sae_L0_hook_resid_post/sparsity_loss</td><td>0.04285</td></tr><tr><td>sae_L0_hook_resid_post/total_loss</td><td>0.04493</td></tr><tr><td>sae_L0_hook_resid_pre/activations_mean</td><td>0.0026</td></tr><tr><td>sae_L0_hook_resid_pre/activations_sparsity</td><td>0.98929</td></tr><tr><td>sae_L0_hook_resid_pre/activations_std</td><td>0.03131</td></tr><tr><td>sae_L0_hook_resid_pre/decoder_weight_norm</td><td>2.09981</td></tr><tr><td>sae_L0_hook_resid_pre/encoder_weight_norm</td><td>20.38832</td></tr><tr><td>sae_L0_hook_resid_pre/learning_rate</td><td>0.001</td></tr><tr><td>sae_L0_hook_resid_pre/reconstruction_loss</td><td>0.00015</td></tr><tr><td>sae_L0_hook_resid_pre/sparsity_loss</td><td>0.00682</td></tr><tr><td>sae_L0_hook_resid_pre/total_loss</td><td>0.00698</td></tr><tr><td>sae_L1_hook_attn_out/activations_mean</td><td>0.03636</td></tr><tr><td>sae_L1_hook_attn_out/activations_sparsity</td><td>0.9306</td></tr><tr><td>sae_L1_hook_attn_out/activations_std</td><td>0.26061</td></tr><tr><td>sae_L1_hook_attn_out/decoder_weight_norm</td><td>1.34174</td></tr><tr><td>sae_L1_hook_attn_out/encoder_weight_norm</td><td>4.86449</td></tr><tr><td>sae_L1_hook_attn_out/learning_rate</td><td>0.001</td></tr><tr><td>sae_L1_hook_attn_out/reconstruction_loss</td><td>0.00312</td></tr><tr><td>sae_L1_hook_attn_out/sparsity_loss</td><td>0.09532</td></tr><tr><td>sae_L1_hook_attn_out/total_loss</td><td>0.09844</td></tr><tr><td>sae_L1_hook_mlp_out/activations_mean</td><td>0.12134</td></tr><tr><td>sae_L1_hook_mlp_out/activations_sparsity</td><td>0.86367</td></tr><tr><td>sae_L1_hook_mlp_out/activations_std</td><td>0.64175</td></tr><tr><td>sae_L1_hook_mlp_out/decoder_weight_norm</td><td>1.49807</td></tr><tr><td>sae_L1_hook_mlp_out/encoder_weight_norm</td><td>5.15728</td></tr><tr><td>sae_L1_hook_mlp_out/learning_rate</td><td>0.001</td></tr><tr><td>sae_L1_hook_mlp_out/reconstruction_loss</td><td>0.01676</td></tr><tr><td>sae_L1_hook_mlp_out/sparsity_loss</td><td>0.31809</td></tr><tr><td>sae_L1_hook_mlp_out/total_loss</td><td>0.33485</td></tr><tr><td>sae_L1_hook_resid_mid/activations_mean</td><td>0.0494</td></tr><tr><td>sae_L1_hook_resid_mid/activations_sparsity</td><td>0.8964</td></tr><tr><td>sae_L1_hook_resid_mid/activations_std</td><td>0.29807</td></tr><tr><td>sae_L1_hook_resid_mid/decoder_weight_norm</td><td>1.64495</td></tr><tr><td>sae_L1_hook_resid_mid/encoder_weight_norm</td><td>7.29543</td></tr><tr><td>sae_L1_hook_resid_mid/learning_rate</td><td>0.001</td></tr><tr><td>sae_L1_hook_resid_mid/reconstruction_loss</td><td>0.0063</td></tr><tr><td>sae_L1_hook_resid_mid/sparsity_loss</td><td>0.1295</td></tr><tr><td>sae_L1_hook_resid_mid/total_loss</td><td>0.1358</td></tr><tr><td>sae_L1_hook_resid_post/activations_mean</td><td>0.14418</td></tr><tr><td>sae_L1_hook_resid_post/activations_sparsity</td><td>0.85339</td></tr><tr><td>sae_L1_hook_resid_post/activations_std</td><td>0.77155</td></tr><tr><td>sae_L1_hook_resid_post/decoder_weight_norm</td><td>1.44815</td></tr><tr><td>sae_L1_hook_resid_post/encoder_weight_norm</td><td>4.31239</td></tr><tr><td>sae_L1_hook_resid_post/learning_rate</td><td>0.001</td></tr><tr><td>sae_L1_hook_resid_post/reconstruction_loss</td><td>0.02401</td></tr><tr><td>sae_L1_hook_resid_post/sparsity_loss</td><td>0.37795</td></tr><tr><td>sae_L1_hook_resid_post/total_loss</td><td>0.40196</td></tr><tr><td>sae_L1_hook_resid_pre/activations_mean</td><td>0.01637</td></tr><tr><td>sae_L1_hook_resid_pre/activations_sparsity</td><td>0.93501</td></tr><tr><td>sae_L1_hook_resid_pre/activations_std</td><td>0.10223</td></tr><tr><td>sae_L1_hook_resid_pre/decoder_weight_norm</td><td>1.94722</td></tr><tr><td>sae_L1_hook_resid_pre/encoder_weight_norm</td><td>11.89048</td></tr><tr><td>sae_L1_hook_resid_pre/learning_rate</td><td>0.001</td></tr><tr><td>sae_L1_hook_resid_pre/reconstruction_loss</td><td>0.00202</td></tr><tr><td>sae_L1_hook_resid_pre/sparsity_loss</td><td>0.04291</td></tr><tr><td>sae_L1_hook_resid_pre/total_loss</td><td>0.04493</td></tr><tr><td>tc_L0/W_dec_norm</td><td>102.43627</td></tr><tr><td>tc_L0/W_enc_norm</td><td>4.41975</td></tr><tr><td>tc_L0/hidden_activations_mean</td><td>0.00049</td></tr><tr><td>tc_L0/hidden_activations_sparsity</td><td>0.99133</td></tr><tr><td>tc_L0/hidden_activations_std</td><td>0.01084</td></tr><tr><td>tc_L0/l1_loss</td><td>0.00515</td></tr><tr><td>tc_L0/learning_rate</td><td>0.001</td></tr><tr><td>tc_L0/mse_loss</td><td>0.00042</td></tr><tr><td>tc_L0/total_loss</td><td>0.00557</td></tr><tr><td>tc_L1/W_dec_norm</td><td>62.45156</td></tr><tr><td>tc_L1/W_enc_norm</td><td>5.11337</td></tr><tr><td>tc_L1/hidden_activations_mean</td><td>0.01184</td></tr><tr><td>tc_L1/hidden_activations_sparsity</td><td>0.98343</td></tr><tr><td>tc_L1/hidden_activations_std</td><td>0.20547</td></tr><tr><td>tc_L1/l1_loss</td><td>0.12417</td></tr><tr><td>tc_L1/learning_rate</td><td>0.001</td></tr><tr><td>tc_L1/mse_loss</td><td>0.01032</td></tr><tr><td>tc_L1/total_loss</td><td>0.13448</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sae-and-transcoder</strong> at: <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2/runs/vva6fpjg' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2/runs/vva6fpjg</a><br/> View project at: <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-sae-and-transcoder-v2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240922_200223-vva6fpjg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# note: residual stream norm grows with the length of the residual stream, so we _should_ see\n",
    "#       MSE grow with the length of the residual stream\n",
    "\n",
    "import functools\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = 15000\n",
    "eval_every_n = 500\n",
    "\n",
    "# create trainers to handle saes and transcoders\n",
    "transcoder_trainer_per_hook = create_transcoder_trainer_per_hook(model, device)\n",
    "sae_trainer_per_hook = create_sae_trainer_per_hook(residual_stream_hook_names, model, device)\n",
    "\n",
    "# figure out hook names we need to cache\n",
    "hook_names_to_cache = get_hook_names_to_cache_for_trainers(\n",
    "    transcoder_trainer_per_hook,\n",
    "    sae_trainer_per_hook,\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=\"toy-problem-hooked-transformer-sae-and-transcoder-v2\",\n",
    "    name=\"sae-and-transcoder\",\n",
    ")\n",
    "\n",
    "# NOTE: Statistics are being computed over one particular batch, which is likely noisy\n",
    "#\n",
    "# TODO(bschoen): Add evaluation against the test set\n",
    "#\n",
    "for epoch, batch in tqdm.tqdm(\n",
    "    zip(\n",
    "        range(num_epochs),\n",
    "        itertools.cycle(train_loader),\n",
    "    )\n",
    "):\n",
    "\n",
    "    tokens, target = batch\n",
    "\n",
    "    tokens, target = tokens.to(device), target.to(device)\n",
    "\n",
    "    # run the original model, cache the activations of the relevant hook points\n",
    "    _, cache = model.run_with_cache(\n",
    "        tokens,\n",
    "        names_filter=lambda x: x in hook_names_to_cache,\n",
    "    )\n",
    "\n",
    "    # log every `eval_every_n` epochs, and the last epoch\n",
    "    is_epoch_that_needs_to_log = (epoch % eval_every_n == 0) or (epoch == num_epochs - 1)\n",
    "    epoch_wandb_log_dict = {}\n",
    "\n",
    "    # step sae and transcoder trainers\n",
    "    for trainer in itertools.chain(\n",
    "        transcoder_trainer_per_hook.values(),\n",
    "        sae_trainer_per_hook.values(),\n",
    "    ):\n",
    "\n",
    "        # note: `train_on_cache` already takes care of backpropagation, this is just for logging\n",
    "        trainer_output = trainer.train_on_cache(cache)\n",
    "\n",
    "        if is_epoch_that_needs_to_log:\n",
    "\n",
    "            # print(f\"[{epoch}/{num_epochs}] Logging {trainer.name}...\")\n",
    "\n",
    "            wandb_log_dict = trainer.get_wandb_log_dict(trainer_output)\n",
    "\n",
    "            # collect in a single dict to log, so not spamming wandb with calls\n",
    "            epoch_wandb_log_dict.update(wandb_log_dict)\n",
    "\n",
    "    if is_epoch_that_needs_to_log:\n",
    "\n",
    "        print(f\"[{epoch}/{num_epochs}] Logging...\")\n",
    "        wandb.log(epoch_wandb_log_dict, step=epoch)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e74b1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set everything to `eval` mode\n",
    "for trainer in transcoder_trainer_per_hook.values():\n",
    "    trainer.transcoder.eval()\n",
    "\n",
    "for trainer in sae_trainer_per_hook.values():\n",
    "    trainer.sae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db2999",
   "metadata": {},
   "source": [
    "## Augmented TransformerLens Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28012e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's get a baseline\n",
    "\n",
    "\n",
    "def compute_accuracy_by_difficulty(\n",
    "    model: tl.HookedTransformer,\n",
    "    tokenizer: tokenizer_utils.Tokenizer,\n",
    "    test_loaders: dict[int, torch.utils.data.DataLoader],\n",
    ") -> dict[int, float]:\n",
    "\n",
    "    test_accuracy_by_difficulty = {}\n",
    "\n",
    "    for difficulty, test_loader in test_loaders.items():\n",
    "\n",
    "        accuracy = evaluate_sequence_accuracy_on_test_batches(\n",
    "            model,\n",
    "            test_loader,\n",
    "            separator_token_id=tokenizer.encode(\"|\")[0],\n",
    "            max_batches=1,\n",
    "        )\n",
    "\n",
    "        test_accuracy_by_difficulty[difficulty] = accuracy\n",
    "\n",
    "    return test_accuracy_by_difficulty\n",
    "\n",
    "\n",
    "# {10: 0.99609375}\n",
    "compute_accuracy_by_difficulty(model, tokenizer, test_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2eabcce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# using `https://github.com/ApolloResearch/e2e_sae/blob/main/e2e_sae/models/transformers.py#L148` for reference\n",
    "\n",
    "# note: the activations are only needed to be stored if we're going to use them later\n",
    "\n",
    "# for now let's just worry about the actual replacement\n",
    "\n",
    "\n",
    "# wrap transcoder class so can easily inject it here\n",
    "class _TranscoderWrapper(nn.Module):\n",
    "    def __init__(self, mlp_transcoder: transcoder.Transcoder) -> None:\n",
    "        super().__init__()\n",
    "        self.mlp_transcoder = mlp_transcoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp_transcoder(x).transcoder_out\n",
    "\n",
    "\n",
    "# actually go ahead and replace the transcoders\n",
    "#\n",
    "# TODO(bschoen): Hooking the transcoders would make that easier here, probably the SAEs too\n",
    "#\n",
    "for block_index, _ in enumerate(model.blocks):\n",
    "\n",
    "    # get the MLP input and output hook names\n",
    "    mlp_in_hook_name = f\"blocks.{block_index}.hook_resid_mid\"\n",
    "    mlp_out_hook_name = f\"blocks.{block_index}.hook_mlp_out\"\n",
    "\n",
    "    trainer = transcoder_trainer_per_hook[mlp_in_hook_name]\n",
    "\n",
    "    # replace the mlp with the transcoder\n",
    "    model.blocks[block_index].mlp = _TranscoderWrapper(trainer.transcoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d75f74cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.9921875}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {10: 0.994140625}\n",
    "compute_accuracy_by_difficulty(model, tokenizer, test_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "574aa01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['blocks.0.hook_resid_pre', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post'])\n"
     ]
    }
   ],
   "source": [
    "# now add hooks for the saes\n",
    "sae_hook_fn_per_hook_name = {}\n",
    "\n",
    "for hook_name, trainer in sae_trainer_per_hook.items():\n",
    "\n",
    "    sae_hook_fn_per_hook_name[hook_name] = lambda x, hook: trainer.sae(x).x_reconstructed\n",
    "\n",
    "print(sae_hook_fn_per_hook_name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7ee83e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy only replacing: blocks.0.hook_resid_pre: {10: 0.0}\n",
      "Accuracy only replacing: blocks.0.hook_attn_out: {10: 0.01171875}\n",
      "Accuracy only replacing: blocks.0.hook_resid_mid: {10: 0.0}\n",
      "Accuracy only replacing: blocks.0.hook_mlp_out: {10: 0.0}\n",
      "Accuracy only replacing: blocks.0.hook_resid_post: {10: 0.001953125}\n",
      "Accuracy only replacing: blocks.1.hook_resid_pre: {10: 0.0}\n",
      "Accuracy only replacing: blocks.1.hook_attn_out: {10: 0.265625}\n",
      "Accuracy only replacing: blocks.1.hook_resid_mid: {10: 0.25390625}\n",
      "Accuracy only replacing: blocks.1.hook_mlp_out: {10: 0.900390625}\n",
      "Accuracy only replacing: blocks.1.hook_resid_post: {10: 0.994140625}\n"
     ]
    }
   ],
   "source": [
    "for hook_name, hook_fn in sae_hook_fn_per_hook_name.items():\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(hook_name, hook_fn)]):\n",
    "\n",
    "        test_accuracy_by_difficulty = compute_accuracy_by_difficulty(model, tokenizer, test_loaders)\n",
    "\n",
    "        print(f\"Accuracy only replacing: {hook_name}: {test_accuracy_by_difficulty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8b406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b9327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output_string = \"<bacd|abcd>\"\n",
    "\n",
    "# generate from the start of the seperator\n",
    "input_string = expected_output_string.split(\"|\")[0]\n",
    "\n",
    "print(f\"Input string: {input_string}\")\n",
    "output_string = generate(model, tokenizer, input_string)\n",
    "\n",
    "print(f\"Generated output string: {output_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5c74b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebf137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020fcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888d68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272cc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95163b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeef966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ae1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b079eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbf856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "542c6b8f",
   "metadata": {},
   "source": [
    "### Transcoder - Checking Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small (fixed) training set of each difficulty to use for visualization\n",
    "test_example_per_difficulty = {}\n",
    "for difficulty, test_loader in test_loaders.items():\n",
    "    # grab something from the test batch\n",
    "    x, _ = next(iter(test_loader))\n",
    "    input_tokens = x[0].to(device)\n",
    "    test_example_per_difficulty[difficulty] = input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15055be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cache for each one\n",
    "test_example_string_to_cache = {}\n",
    "\n",
    "for difficulty, input_tokens in test_example_per_difficulty.items():\n",
    "\n",
    "    logits, cache = model.run_with_cache(input_tokens)\n",
    "\n",
    "    # store example by using the actual text string as key\n",
    "    input_tokens_str = \"\".join([tokenizer.decode([x.item()]) for x in input_tokens])\n",
    "\n",
    "    test_example_string_to_cache[input_tokens_str] = cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7202db",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    len(test_example_per_difficulty) == 1\n",
    "), \"Here we're assuming only one difficulty, can be easily adapted for more\"\n",
    "\n",
    "\n",
    "print(f\"{transcoder_training_cfg.hook_point=}\")\n",
    "print(f\"{transcoder_training_cfg.out_hook_point=}\")\n",
    "\n",
    "mlp_in = correct_cache[transcoder_training_cfg.hook_point]\n",
    "mlp_out = correct_cache[transcoder_training_cfg.out_hook_point]\n",
    "\n",
    "transcoder_results = mlp_transcoder(mlp_in)\n",
    "\n",
    "# Print shapes of tensors\n",
    "print([c for c in correct_string])\n",
    "print(f\"mlp_in: {mlp_in.shape}\")\n",
    "print(f\"mlp_out: {mlp_out.shape}\")\n",
    "print(f\"transcoder_out: {transcoder_results.transcoder_out.shape}\")\n",
    "print(f\"hidden_activations: {transcoder_results.hidden_activations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d816c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have imported or defined `mlp_in`, `mlp_out`, `transcoder_out`, `hidden_activations`, and `input_tokens_str`\n",
    "\n",
    "\n",
    "# Function to plot heatmap of activations\n",
    "def plot_activations(\n",
    "    activations,\n",
    "    title,\n",
    "    xlabel=\"Neuron\",\n",
    "    ylabel=\"Sequence Position\",\n",
    "    tokens=None,\n",
    "    cmap: str = \"RdBu\",\n",
    "):\n",
    "    # Remove batch dimension and convert to numpy\n",
    "    activations = activations.squeeze(0).detach().cpu().numpy()\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    sns.heatmap(activations, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if tokens is not None:\n",
    "        plt.yticks(np.arange(len(tokens)) + 0.5, tokens, rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 1. Visualize MLP Input Activations (mlp_in)\n",
    "plot_activations(mlp_in, title=\"MLP Input Activations\", tokens=correct_string)\n",
    "\n",
    "# 2. Visualize MLP Output Activations (mlp_out)\n",
    "plot_activations(mlp_out, title=\"MLP Output Activations\", tokens=correct_string)\n",
    "\n",
    "# 3. Visualize Transcoder Output (transcoder_out)\n",
    "plot_activations(\n",
    "    transcoder_results.transcoder_out,\n",
    "    title=\"Transcoder Output Activations\",\n",
    "    tokens=correct_string,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hidden_activations(hidden_activations, tokens):\n",
    "    # hidden_activations: [seq_len, hidden_size]\n",
    "    # tokens: list of token strings\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(\n",
    "        hidden_activations,\n",
    "        cmap=\"YlOrRd\",\n",
    "        cbar=True,\n",
    "        vmin=0,\n",
    "        vmax=np.max(hidden_activations),\n",
    "    )\n",
    "    plt.ylabel(\"Token Position\")\n",
    "    plt.xlabel(\"Neuron Index\")\n",
    "    plt.yticks(np.arange(len(tokens)) + 0.5, tokens, rotation=90)\n",
    "    plt.title(\"Hidden Activations Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "    # Activation Distribution\n",
    "    activation_values = hidden_activations.flatten()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(activation_values, bins=100, color=\"blue\", alpha=0.7)\n",
    "    plt.xlabel(\"Activation Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Hidden Activations\")\n",
    "    plt.show()\n",
    "\n",
    "    # PCA\n",
    "    \"\"\"from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    activations_pca = pca.fit_transform(hidden_activations)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(\n",
    "        activations_pca[:, 0],\n",
    "        activations_pca[:, 1],\n",
    "        c=np.arange(len(tokens)),\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "    for i, token in enumerate(tokens):\n",
    "        plt.text(activations_pca[i, 0], activations_pca[i, 1], token)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(\"PCA of Hidden Activations\")\n",
    "    plt.colorbar(label=\"Token Position\")\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "\n",
    "visualize_hidden_activations(\n",
    "    hidden_activations=transcoder_results.hidden_activations[0].detach().cpu().numpy(),\n",
    "    tokens=incorrect_string,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize Hidden Activations of MLP (hidden_activations)\n",
    "# Due to the high dimensionality (2048), we might need to reduce dimensions\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2 for visualization\n",
    "activations = transcoder_results.hidden_activations.squeeze(0).detach().cpu().numpy()\n",
    "pca = PCA(n_components=2)\n",
    "reduced_activations = pca.fit_transform(activations.reshape(-1, activations.shape[-1]))\n",
    "\n",
    "# Plot the PCA-reduced activations\n",
    "plt.figure(figsize=(4, 2))\n",
    "scatter = plt.scatter(\n",
    "    reduced_activations[:, 0],\n",
    "    reduced_activations[:, 1],\n",
    "    c=np.arange(activations.shape[0]),\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Sequence Position\")\n",
    "plt.title(\"PCA of Hidden Activations\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# Alternatively, plot a subset of neurons\n",
    "num_neurons_to_plot = 2048  # Adjust based on preference\n",
    "selected_neurons = activations[:, :num_neurons_to_plot]\n",
    "\n",
    "plot_activations(\n",
    "    torch.tensor(selected_neurons),\n",
    "    title=\"Hidden Activations (First {} Neurons)\".format(num_neurons_to_plot),\n",
    "    xlabel=\"Neuron\",\n",
    "    tokens=input_tokens_str,\n",
    "    cmap=\"grey\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb077e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualize Distributions of Activations\n",
    "def plot_activation_distribution(activations, title):\n",
    "    activations = activations.detach().cpu().numpy().flatten()\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.histplot(activations, bins=100, kde=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Activation Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_activation_distribution(mlp_in, title=\"Distribution of MLP Input Activations\")\n",
    "\n",
    "plot_activation_distribution(mlp_out, title=\"Distribution of MLP Output Activations\")\n",
    "\n",
    "plot_activation_distribution(\n",
    "    transcoder_results.hidden_activations,\n",
    "    title=\"Distribution of Hidden Activations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d188f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = model.blocks[1].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.W_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f5fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Visualize MLP Weights\n",
    "# Assuming you have access to the MLP model\n",
    "# Replace `model` with your transformer model variable and adjust layer indices accordingly\n",
    "\n",
    "# Visualize weights of the first linear layer\n",
    "weight_matrix = model.blocks[0].mlp.W_in.detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(weight_matrix, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Weights of MLP Layer fc1\")\n",
    "plt.xlabel(\"Input Neuron\")\n",
    "plt.ylabel(\"Output Neuron\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize weights of the second linear layer\n",
    "weight_matrix = model.blocks[1].mlp.W_in.detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(weight_matrix, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Weights of MLP Layer fc2\")\n",
    "plt.xlabel(\"Input Neuron\")\n",
    "plt.ylabel(\"Output Neuron\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ea5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "705bcb63",
   "metadata": {},
   "source": [
    "## Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df666bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "# TODO(bschoen): Do need to use lightning if want to do this generally\n",
    "# note: generally do want to iterate on this part itself, i.e. once find promising learning rate, searching other hyperparameters\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    # TODO(bschoen): up to one per position, eh might as well try it\n",
    "\n",
    "    d_model = trial.suggest_categorical(\"d_model\", [8, 16, 32, 64, 128])\n",
    "    n_heads = trial.suggest_int(\"n_heads\", 1, 8)\n",
    "\n",
    "    cfg = ModelAndTrainingConfig(\n",
    "        num_epochs=1000,\n",
    "        eval_test_every_n=10000,  # not worth evaluating test loss for study\n",
    "        n_layers=1,  # trial.suggest_int(\"n_layers\", 1, 2),\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        learning_rate=5e-4,\n",
    "    )\n",
    "\n",
    "    # sanity check `d_heads`\n",
    "    if (cfg.d_model % cfg.n_heads) != 0:\n",
    "        print(f\"Pruning trial for {cfg.d_model=} {cfg.n_heads=}\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    result = train_model(cfg)\n",
    "\n",
    "    return result.train_loss\n",
    "\n",
    "\n",
    "enable_optuna = False\n",
    "\n",
    "if enable_optuna:\n",
    "\n",
    "    study_storage_url = \"sqlite:///toy-problem-hooked-transformer.db\"\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        directions=[optuna.study.StudyDirection.MINIMIZE],\n",
    "        storage=study_storage_url,\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    print(\"View by launching optuna dashboard from the command line:\")\n",
    "    print(f\"optuna-dashboard {study_storage_url}\")\n",
    "\n",
    "    # now let's do a real run\n",
    "    training_config = ModelAndTrainingConfig(\n",
    "        num_epochs=10000,\n",
    "        eval_test_every_n=1000,\n",
    "        n_layers=1,\n",
    "        d_model=16,\n",
    "        n_heads=1,\n",
    "    )\n",
    "\n",
    "    result = train_model(cfg=training_config)\n",
    "\n",
    "    # for compatibility with code later\n",
    "    model = result.model\n",
    "    cfg = training_config.get_hooked_transformer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some example output\n",
    "import circuitsvis as cv\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "def visualize_pattern_hook(\n",
    "    pattern: Float32[torch.Tensor, \"batch head_index dest_pos source_pos\"],\n",
    "    hook: transformer_lens.hook_points.HookPoint,\n",
    "    tokens_as_strings: list[str],\n",
    ") -> None:\n",
    "    print(f\"Batch size: {pattern.shape[0]}\")\n",
    "    print(\"Layer: \", hook.layer())\n",
    "    display(cv.attention.attention_patterns(tokens=tokens_as_strings, attention=pattern.mean(0)))\n",
    "\n",
    "\n",
    "test_input_string_to_cache = {}\n",
    "\n",
    "for difficulty, test_loader in test_loaders.items():\n",
    "\n",
    "    print(difficulty)\n",
    "\n",
    "    # grab something from the test batch\n",
    "    example_batch = next(iter(test_loader))\n",
    "\n",
    "    x, y = example_batch\n",
    "\n",
    "    example_sample = x[0]\n",
    "\n",
    "    # example_sample = torch.tensor(tokenizer.encode(\"<az|za|az>>>>>>>>>>\"))\n",
    "\n",
    "    # grab the first part of it, ex: `<abc|`\n",
    "    example_prompt = example_sample  # [:8]\n",
    "\n",
    "    example_prompt = example_prompt.to(device)\n",
    "\n",
    "    print(f\"Using {example_prompt} from {example_sample} (from test set)\")\n",
    "\n",
    "    # note: already encoded\n",
    "    input_tokens = example_prompt\n",
    "\n",
    "    # first let's get these as strings so can easily work with them\n",
    "    input_tokens_as_strings = [token_to_string(x.item()) for x in input_tokens]\n",
    "\n",
    "    # wrap to bind input tokens\n",
    "    visualize_pattern_hook_fn = functools.partial(\n",
    "        visualize_pattern_hook, tokens_as_strings=input_tokens_as_strings\n",
    "    )\n",
    "\n",
    "    model.run_with_hooks(\n",
    "        input_tokens,\n",
    "        return_type=None,  # For efficiency, we don't need to calculate the logits\n",
    "        fwd_hooks=[(lambda name: name.endswith(\"pattern\"), visualize_pattern_hook_fn)],\n",
    "    )\n",
    "\n",
    "    logits_batch, cache = model.run_with_cache(input_tokens)\n",
    "\n",
    "    # store so can plot together later\n",
    "    test_input_string_to_cache[\"\".join(input_tokens_as_strings)] = cache\n",
    "\n",
    "    logits = logits_batch[0]\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "    cv.logits.token_log_probs(\n",
    "        token_indices=input_tokens,\n",
    "        log_probs=log_probs,\n",
    "        to_string=token_to_string,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.apply_ln_to_stack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c96b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.stack_head_results??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b78938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.patching\n",
    "\n",
    "transformer_lens.patching.get_act_patch_resid_pre??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2f20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b41fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "def logit_attribution(\n",
    "    embed: Float32[torch.Tensor, \"seq d_model\"],\n",
    "    l1_results: Float32[torch.Tensor, \"seq nheads d_model\"],\n",
    "    l2_results: Float32[torch.Tensor, \"seq nheads d_model\"],\n",
    "    W_U: Float32[torch.Tensor, \"d_model d_vocab\"],\n",
    "    tokens: Int64[torch.Tensor, \"seq\"],\n",
    ") -> Float32[torch.Tensor, \"seq-1 n_components\"]:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        embed: the embeddings of the tokens (i.e. token + position embeddings)\n",
    "        l1_results: the outputs of the attention heads at layer 1 (with head as one of the dimensions)\n",
    "        l2_results: the outputs of the attention heads at layer 2 (with head as one of the dimensions)\n",
    "        W_U: the unembedding matrix\n",
    "        tokens: the token ids of the sequence\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len-1, n_components)\n",
    "        represents the concatenation (along dim=-1) of logit attributions from:\n",
    "            the direct path (seq-1,1)\n",
    "            layer 0 logits (seq-1, n_heads)\n",
    "            layer 1 logits (seq-1, n_heads)\n",
    "        so n_components = 1 + 2*n_heads\n",
    "    \"\"\"\n",
    "    W_U_correct_tokens = W_U[:, tokens[1:]]\n",
    "    # SOLUTION\n",
    "    direct_attributions = einops.einsum(W_U_correct_tokens, embed[:-1], \"emb seq, seq emb -> seq\")\n",
    "    l1_attributions = einops.einsum(\n",
    "        W_U_correct_tokens, l1_results[:-1], \"emb seq, seq nhead emb -> seq nhead\"\n",
    "    )\n",
    "    l2_attributions = einops.einsum(\n",
    "        W_U_correct_tokens, l2_results[:-1], \"emb seq, seq nhead emb -> seq nhead\"\n",
    "    )\n",
    "    return torch.concat(\n",
    "        [direct_attributions.unsqueeze(-1), l1_attributions, l2_attributions], dim=-1\n",
    "    )\n",
    "\n",
    "\n",
    "logits, cache = model.run_with_cache(input_tokens, remove_batch_dim=True)\n",
    "str_tokens = input_tokens_as_strings\n",
    "tokens = input_tokens\n",
    "\n",
    "with t.inference_mode():\n",
    "    embed = cache[\"embed\"]\n",
    "    l1_results = cache[\"result\", 0]\n",
    "    l2_results = cache[\"result\", 1]\n",
    "    logit_attr = logit_attribution(\n",
    "        embed,\n",
    "        l1_results,\n",
    "        l2_results,\n",
    "        model.W_U,\n",
    "        tokens[0],\n",
    "    )\n",
    "\n",
    "    # Uses fancy indexing to get a len(tokens[0])-1 length tensor, where the kth entry is the predicted logit for the correct k+1th token\n",
    "    correct_token_logits = logits[0, torch.arange(len(tokens[0]) - 1), tokens[0, 1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13997a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a544cede",
   "metadata": {},
   "source": [
    "## Looking at it with CircuitsViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before even going to SAE, let's look at circuitsviz here\n",
    "import circuitsvis as cv\n",
    "\n",
    "import circuitsvis.activations\n",
    "import circuitsvis.attention\n",
    "import circuitsvis.logits\n",
    "import circuitsvis.tokens\n",
    "import circuitsvis.topk_samples\n",
    "import circuitsvis.topk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b587d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see what we have\n",
    "import tabulate\n",
    "\n",
    "print(f\"{len(input_tokens)=}\")\n",
    "\n",
    "# show the first few elements of the `HookedTransformerConfig`, since that has things like `d_model`, num heads, etc\n",
    "print(tabulate.tabulate([(k, v) for k, v in cfg.__dict__.items()][:10]))\n",
    "\n",
    "print(tabulate.tabulate([(k, v.shape) for k, v in cache.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519db727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable Bokeh output in the notebook\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "def tensor_to_dataframe(tensor: torch.Tensor, labels: list[str], tokens: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a 2D PyTorch tensor to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): A 2D tensor to convert.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame representation of the input tensor.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input tensor is not 2D.\n",
    "    \"\"\"\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(f\"Input tensor must be 2D, got {tensor.dim()}D\")\n",
    "    if len(labels) != 2:\n",
    "        raise ValueError(f\"Expected labels for both dimensions, got {len(labels)}\")\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    numpy_array = tensor.detach().cpu().numpy()\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(numpy_array)\n",
    "\n",
    "    # Name the index the first label\n",
    "    df.index.name = labels[0]\n",
    "\n",
    "    # Name the columns the second label\n",
    "    df.columns = [f\"{labels[1]}_{i}\" for i in range(numpy_array.shape[1])]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def visualize_tensor_heatmap(\n",
    "    tensor: torch.Tensor,\n",
    "    title: str = \"Tensor Heatmap\",\n",
    "    colormap: list[str] = Viridis256,\n",
    "    width: int = 800,\n",
    "    height: int = 400,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize a 2D tensor as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): A 2D tensor to visualize.\n",
    "        title (str): Title of the heatmap.\n",
    "        colormap (List[str]): A list of colors to use for the heatmap.\n",
    "        width (int): Width of the plot in pixels.\n",
    "        height (int): Height of the plot in pixels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure tensor is 2D\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(f\"Input tensor must be 2D, got {tensor.shape}\")\n",
    "\n",
    "    # convert tensor to dataframe\n",
    "    df = tensor_to_dataframe(tensor)\n",
    "\n",
    "    # Create a 2D grid of coordinates\n",
    "    y, x = np.mgrid[0 : data.shape[0], 0 : data.shape[1]]\n",
    "\n",
    "    # Flatten the arrays\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    z = data.flatten()\n",
    "\n",
    "    # Create a ColumnDataSource\n",
    "    source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            color=Viridis256[:: int(256 / len(z))][: len(z)],  # Map values to colors\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    p = figure(\n",
    "        title=\"Tensor Heatmap\",\n",
    "        x_range=(0, data.shape[1]),\n",
    "        y_range=(0, data.shape[0]),\n",
    "        toolbar_location=\"below\",\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset\",\n",
    "    )\n",
    "\n",
    "    # Add rectangular glyphs\n",
    "    p.rect(\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        width=1,\n",
    "        height=1,\n",
    "        source=source,\n",
    "        fill_color=\"color\",\n",
    "        line_color=None,\n",
    "    )\n",
    "\n",
    "    # Add hover tool\n",
    "    hover = HoverTool(tooltips=[(\"x\", \"@x\"), (\"y\", \"@y\"), (\"value\", \"@z{0.000}\")])\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    # Invert y-axis to match tensor indexing\n",
    "    p.y_range.start, p.y_range.end = p.y_range.end, p.y_range.start\n",
    "\n",
    "    # Show the plot\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate.tabulate([(k, v[0].shape) for k, v in cache.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ad382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go ahead and just use first batch\n",
    "def first_batch(tensor: Float32[torch.Tensor, \"b t c\"]) -> Float32[torch.Tensor, \"t c\"]:\n",
    "    return tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb2343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from typing import Iterable, TypeVar\n",
    "\n",
    "import tabulate\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "# alias for `print(tabulate.tabulate(data))`\n",
    "def print_table(data: T) -> None:\n",
    "    print(tabulate.tabulate(data))\n",
    "\n",
    "\n",
    "# Define a function to print module weights recursively\n",
    "def print_module_weights(module: nn.Module) -> Iterable[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Recursively prints the weights of a PyTorch module and its submodules.\n",
    "\n",
    "    This function traverses through the module hierarchy, printing information\n",
    "    about parameters that require gradients and are not hook-related.\n",
    "\n",
    "    Example:\n",
    "        >>> print_table(print_module_weights(model))\n",
    "\n",
    "        ------------------  ----------------------\n",
    "        embed.W_E           torch.Size([29, 14])\n",
    "        pos_embed.W_pos     torch.Size([9, 14])\n",
    "        blocks.0.ln1.w      torch.Size([14])\n",
    "        blocks.0.ln1.b      torch.Size([14])\n",
    "        blocks.0.ln2.w      torch.Size([14])\n",
    "        blocks.0.ln2.b      torch.Size([14])\n",
    "        blocks.0.attn.W_Q   torch.Size([3, 14, 4])\n",
    "        blocks.0.attn.W_O   torch.Size([3, 4, 14])\n",
    "        blocks.0.attn.b_Q   torch.Size([3, 4])\n",
    "        blocks.0.attn.b_O   torch.Size([14])\n",
    "        blocks.0.attn.W_K   torch.Size([3, 14, 4])\n",
    "        blocks.0.attn.W_V   torch.Size([3, 14, 4])\n",
    "        blocks.0.attn.b_K   torch.Size([3, 4])\n",
    "        blocks.0.attn.b_V   torch.Size([3, 4])\n",
    "        blocks.0.mlp.W_in   torch.Size([14, 56])\n",
    "        blocks.0.mlp.b_in   torch.Size([56])\n",
    "        blocks.0.mlp.W_out  torch.Size([56, 14])\n",
    "        blocks.0.mlp.b_out  torch.Size([14])\n",
    "        ln_final.w          torch.Size([14])\n",
    "        ln_final.b          torch.Size([14])\n",
    "        unembed.W_U         torch.Size([14, 29])\n",
    "        unembed.b_U         torch.Size([29])\n",
    "        ------------------  ----------------------\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): The PyTorch module to inspect.\n",
    "        prefix (str, optional): A string prefix for indentation in the output.\n",
    "                                Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        Iterable[tuple[str, str]]: A list of tuples, where each tuple contains\n",
    "            the name and shape of the parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through named parameters of the module\n",
    "    for name, param in module.named_parameters():\n",
    "\n",
    "        # Check if parameter requires gradient and doesn't start with 'hook_'\n",
    "        if param.requires_grad and not name.startswith(\"hook_\"):\n",
    "\n",
    "            # yield parameter name and type\n",
    "            yield f\"{name}\", f\"{param.shape}\"\n",
    "\n",
    "\n",
    "def print_cache(cache: transformer_lens.ActivationCache) -> None:\n",
    "    print(tabulate.tabulate([(k, v[0].shape) for k, v in cache.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights in the model:\")\n",
    "print_table(print_module_weights(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cached activations:\")\n",
    "print_cache(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6910da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cache_activation(\n",
    "    cache: transformer_lens.ActivationCache,\n",
    "    cache_key: str,\n",
    "    input_tokens_as_strings: list[str],\n",
    ") -> None:\n",
    "\n",
    "    activations = first_batch(cache[cache_key])\n",
    "\n",
    "    figsize = (4, 4)\n",
    "\n",
    "    # make figure smaller for vectors\n",
    "    if activations.shape[-1] == 1:\n",
    "        figsize = (4, 1.5)\n",
    "\n",
    "    # for larger activations like MLP, allow it to be taller\n",
    "    elif activations.shape[-1] > 20:\n",
    "        figsize = (4, 12)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    sns.heatmap(\n",
    "        activations.cpu().numpy().T,\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "        xticklabels=input_tokens_as_strings,\n",
    "    )\n",
    "\n",
    "    plt.title(cache_key)\n",
    "\n",
    "    # TODO(bschoen): Allow specifying this\n",
    "    #\n",
    "    plt.ylabel(\"Embedding Dimension\")\n",
    "    plt.xlabel(\"Token\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for cache_key in [\n",
    "    \"hook_embed\",\n",
    "    \"hook_pos_embed\",\n",
    "    \"blocks.0.hook_resid_pre\",\n",
    "    \"blocks.0.ln1.hook_scale\",\n",
    "    \"blocks.0.ln1.hook_normalized\",\n",
    "    \"blocks.0.hook_attn_out\",\n",
    "    \"blocks.0.hook_resid_mid\",\n",
    "    \"blocks.0.ln2.hook_scale\",\n",
    "    \"blocks.0.ln2.hook_normalized\",\n",
    "    \"blocks.0.mlp.hook_pre\",\n",
    "    \"blocks.0.mlp.hook_post\",\n",
    "    \"blocks.0.hook_mlp_out\",\n",
    "    \"blocks.0.hook_resid_post\",\n",
    "    \"ln_final.hook_scale\",\n",
    "    \"ln_final.hook_normalized\",\n",
    "]:\n",
    "\n",
    "    plot_cache_activation(\n",
    "        cache=cache,\n",
    "        cache_key=cache_key,\n",
    "        input_tokens_as_strings=input_tokens_as_strings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize MLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_mlp_weights_and_biases(model):\n",
    "    # Function to plot heatmaps for MLP weights and biases\n",
    "\n",
    "    def plot_weight_bias_pair(weight, bias, title):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        sns.heatmap(weight.detach().cpu().numpy(), ax=ax1, cmap=\"coolwarm\", center=0)\n",
    "        ax1.set_title(f\"{title} - Weights\")\n",
    "        ax1.set_xlabel(\"Output dimension\")\n",
    "        ax1.set_ylabel(\"Input dimension\")\n",
    "\n",
    "        sns.heatmap(\n",
    "            bias.detach().cpu().numpy().reshape(-1, 1),\n",
    "            ax=ax2,\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "        )\n",
    "        ax2.set_title(f\"{title} - Biases\")\n",
    "        ax2.set_xlabel(\"Bias\")\n",
    "        ax2.set_ylabel(\"Dimension\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # MLP weights and biases\n",
    "    plot_weight_bias_pair(model.blocks[0].mlp.W_in, model.blocks[0].mlp.b_in, \"MLP Input\")\n",
    "    plot_weight_bias_pair(model.blocks[0].mlp.W_out, model.blocks[0].mlp.b_out, \"MLP Output\")\n",
    "\n",
    "    # Layer Norm final\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(\n",
    "        model.ln_final.w.detach().cpu().numpy().reshape(1, -1),\n",
    "        cmap=\"coolwarm\",\n",
    "        center=1,\n",
    "    )\n",
    "    plt.title(\"Layer Norm Final - Weights\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(\n",
    "        model.ln_final.b.detach().cpu().numpy().reshape(1, -1),\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "    )\n",
    "    plt.title(\"Layer Norm Final - Biases\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Unembed\n",
    "    plot_weight_bias_pair(model.unembed.W_U, model.unembed.b_U, \"Unembed\")\n",
    "\n",
    "\n",
    "# Call the function\n",
    "plot_mlp_weights_and_biases(model)\n",
    "\n",
    "# Comment: Additional visualizations that could be useful:\n",
    "# 1. Histograms of weight/bias distributions\n",
    "# 2. 3D surface plots for weights to show patterns\n",
    "# 3. Network architecture diagram with weight magnitudes represented by line thickness\n",
    "# 4. Animated heatmaps showing weight changes during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_bias_activation(\n",
    "    weight,\n",
    "    bias,\n",
    "    activation,\n",
    "    title: str,\n",
    ") -> None:\n",
    "\n",
    "    activation = first_batch(activation)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "    sns.heatmap(weight.detach().cpu().numpy().T, ax=ax1, cmap=\"coolwarm\", center=0)\n",
    "    ax1.set_title(f\"{title} - Weight\")\n",
    "\n",
    "    sns.barplot(x=list(range(len(bias))), y=bias.detach().cpu().numpy(), ax=ax2)\n",
    "    ax2.set_title(f\"{title} - Bias\")\n",
    "    ax2.set_xlabel(\"Index\")\n",
    "    ax2.set_ylabel(\"Value\")\n",
    "\n",
    "    sns.heatmap(activation.detach().cpu().numpy().T, ax=ax3, cmap=\"coolwarm\", center=0)\n",
    "    ax3.set_title(f\"{title} - Activation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_weight_bias_activation(\n",
    "    model.embed.W_E,\n",
    "    torch.zeros(model.embed.W_E.shape[1]),\n",
    "    cache[\"hook_embed\"],\n",
    "    \"Embedding\",\n",
    ")\n",
    "plot_weight_bias_activation(\n",
    "    model.pos_embed.W_pos,\n",
    "    torch.zeros(model.pos_embed.W_pos.shape[1]),\n",
    "    cache[\"hook_pos_embed\"],\n",
    "    \"Positional Embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(bschoen): Hook residual pre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8937bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting LayerNorm components\n",
    "\n",
    "\n",
    "def plot_layernorm(scale, normalized, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    scale = first_batch(scale)\n",
    "    normalized = first_batch(normalized)\n",
    "\n",
    "    sns.barplot(x=list(range(len(scale))), y=scale.squeeze().detach().cpu().numpy(), ax=ax1)\n",
    "    ax1.set_title(f\"{title} - Scale\")\n",
    "    ax1.set_xlabel(\"Index\")\n",
    "    ax1.set_ylabel(\"Value\")\n",
    "\n",
    "    sns.heatmap(normalized.detach().cpu().numpy().T, ax=ax2, cmap=\"coolwarm\", center=0)\n",
    "    ax2.set_title(f\"{title} - Normalized\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_layernorm(\n",
    "    cache[\"blocks.0.ln1.hook_scale\"],\n",
    "    cache[\"blocks.0.ln1.hook_normalized\"],\n",
    "    \"LayerNorm 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting MLP components\n",
    "plot_weight_bias_activation(\n",
    "    model.blocks[0].mlp.W_in,\n",
    "    model.blocks[0].mlp.b_in,\n",
    "    cache[\"blocks.0.mlp.hook_pre\"],\n",
    "    \"MLP Input\",\n",
    ")\n",
    "plot_weight_bias_activation(\n",
    "    model.blocks[0].mlp.W_out,\n",
    "    model.blocks[0].mlp.b_out,\n",
    "    cache[\"blocks.0.mlp.hook_post\"],\n",
    "    \"MLP Output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdf33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c51ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ff7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2a5f4f8",
   "metadata": {},
   "source": [
    "#### circuitsvis.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens := List of tokens if single sample (e.g. `[\"A\", \"person\"]`) or list of lists of tokens (e.g. `[[[\"A\", \"person\"], [\"is\", \"walking\"]]]`)\n",
    "# activations := Activations of the shape [tokens x layers x neurons] if single sample or list of [tokens x layers x neurons] if multiple samples\n",
    "\n",
    "# take first batch for now\n",
    "activations = cache[\"blocks.0.hook_mlp_out\"][0]\n",
    "print(f\"{activations.shape=}\")\n",
    "\n",
    "# reshape [tokens x neurons] -> [tokens x 1 x neurons]\n",
    "#  - `-1` means to automatically infer the size of the last dimension\n",
    "activations_view = activations.view(len(input_tokens), cfg.n_layers, -1)\n",
    "\n",
    "print(f\"{activations_view.shape=}\")\n",
    "\n",
    "# convert to strings (which this function expects)\n",
    "input_tokens_as_strings = [token_to_string(x.item()) for x in input_tokens]\n",
    "\n",
    "# TODO(bschoen): Is there a way to essentially stack these? Claude can probably give the React for that\n",
    "\n",
    "# so here we can visualize activations for a `torch.Size([1, 8, 16])`, which is most\n",
    "# of them since this is the size of the embedding dimension\n",
    "circuitsvis.activations.text_neuron_activations(\n",
    "    tokens=[token_to_string(x.item()) for x in input_tokens],\n",
    "    activations=activations_view,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f7aa2",
   "metadata": {},
   "source": [
    "#### circuitsvis.attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ba9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens: List of tokens (e.g. `[\"A\", \"person\"]`). Must be the same length as the list of values.\n",
    "# attention: Attention head activations of the shape [dest_tokens x src_tokens]\n",
    "# max_value: Maximum value. Used to determine how dark the token color is when positive (i.e. based on how close it is to the maximum value).\n",
    "# min_value: Minimum value. Used to determine how dark the token color is when negative (i.e. based on how close it is to the minimum value).\n",
    "# negative_color: Color for negative values\n",
    "# positive_color: Color for positive values.\n",
    "# show_axis_labels: Whether to show axis labels.\n",
    "# mask_upper_tri: Whether or not to mask the upper triangular portion of the attention patterns. Should be true for causal attention, false for bidirectional attention.\n",
    "\n",
    "\n",
    "# take first batch\n",
    "# ex: torch.Size([4, 8, 8]) -> [n_heads, n_ctx, n_ctx]\n",
    "# note: `blocks.0.attn.hook_attn_scores` is too early (not normalized?)\n",
    "attention = cache[\"blocks.0.attn.hook_pattern\"][0]\n",
    "\n",
    "print(f\"{attention.shape=}\")\n",
    "\n",
    "circuitsvis.attention.attention_heads(\n",
    "    tokens=input_tokens_as_strings,\n",
    "    attention=attention,\n",
    "    max_value=1,\n",
    "    min_value=-1,\n",
    "    negative_color=\"blue\",\n",
    "    positive_color=\"red\",\n",
    "    mask_upper_tri=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d512f",
   "metadata": {},
   "source": [
    "#### circuitsvis.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the normal one we usually show, i.e.\n",
    "# cv.logits.token_log_probs(\n",
    "#     token_indices=input_tokens,\n",
    "#     log_probs=log_probs,\n",
    "#     to_string=token_to_string,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a72925",
   "metadata": {},
   "source": [
    "#### circuitsvis.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa878bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, we'll look at each\n",
    "\n",
    "# take first batch, ex: torch.Size([8, 16])\n",
    "pos_embed = cache[\"hook_pos_embed\"][0]\n",
    "\n",
    "# low level function for coloring tokens according to single value\n",
    "for i in range(cfg.d_model):\n",
    "    display(\n",
    "        circuitsvis.tokens.colored_tokens(\n",
    "            tokens=input_tokens_as_strings,\n",
    "            values=pos_embed[:, i],\n",
    "            negative_color=\"blue\",\n",
    "            positive_color=\"red\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # only display a few for example\n",
    "    # if i >= 2:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first batch\n",
    "# ex: torch.size([8, 16]) = [n_ctx, d_model]\n",
    "attention_out = cache[\"blocks.0.hook_attn_out\"][0]\n",
    "\n",
    "circuitsvis.tokens.colored_tokens_multi(\n",
    "    tokens=input_tokens_as_strings,\n",
    "    values=attention_out,\n",
    "    labels=[str(x) for x in range(cfg.d_model)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuitsvis.tokens.visualize_model_performance(\n",
    "    tokens=input_tokens,\n",
    "    str_tokens=input_tokens_as_strings,\n",
    "    logits=logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0406d0",
   "metadata": {},
   "source": [
    "#### circuitsvis.topk_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d155e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuitsvis.topk_samples.topk_samples??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90d626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e239f06",
   "metadata": {},
   "source": [
    "#### circuitsvis.topk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuitsvis.topk_tokens.topk_tokens??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533d521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee20ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4195832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c59598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86be5940",
   "metadata": {},
   "source": [
    "## SAE (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_index in range(cfg.n_layers):\n",
    "    imshow(\n",
    "        transformer_lens.utils.to_numpy(cache[\"attn\", layer_index].mean([0, 1])),\n",
    "        title=f\"Layer {layer_index} Attention Pattern\",\n",
    "        height=400,\n",
    "        width=400,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "Loss = Float32[torch.Tensor, \"\"]\n",
    "MSELoss = Float32[torch.Tensor, \"\"]\n",
    "WeightedSparsityLoss = Float32[torch.Tensor, \"\"]\n",
    "\n",
    "Logits = Float32[torch.Tensor, \"n_ctx d_vocab\"]\n",
    "BatchedLogits = Float32[torch.Tensor, \"batch n_ctx d_vocab\"]\n",
    "\n",
    "ModelActivations = Float32[torch.Tensor, \"n_ctx d_model\"]\n",
    "BatchedModelActivations = Float32[torch.Tensor, \"batch n_ctx d_model\"]\n",
    "\n",
    "FlattenedModelActivations = Float32[torch.Tensor, \"d_sae_in\"]\n",
    "\n",
    "BatchedFlattenedModelActivations = Float32[torch.Tensor, \"batch d_sae_in\"]\n",
    "BatchedSAEActivations = Float32[torch.Tensor, \"batch d_sae_model\"]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SAEOutput:\n",
    "    sae_activations: BatchedSAEActivations\n",
    "    reconstructed_model_activations: BatchedFlattenedModelActivations\n",
    "\n",
    "\n",
    "def sparse_loss_kl_divergence(\n",
    "    flattened_model_activations: BatchedFlattenedModelActivations,\n",
    "    sae_output: SAEOutput,\n",
    "    sparsity_target: float,\n",
    "    sparsity_weight: float,\n",
    "    epsilon: float = 1e-7,\n",
    ") -> tuple[Loss, MSELoss, WeightedSparsityLoss]:\n",
    "\n",
    "    # same as dense loss (this is constant?)\n",
    "    mse_loss = F.mse_loss(\n",
    "        sae_output.reconstructed_model_activations,\n",
    "        flattened_model_activations,\n",
    "    )\n",
    "\n",
    "    # KL divergence for sparsity\n",
    "    avg_activation = torch.mean(sae_output.sae_activations, dim=0)\n",
    "\n",
    "    # print(f'[pre-clamping] {avg_activation=}')\n",
    "\n",
    "    # Add epsilon for numerical stability\n",
    "    avg_activation = torch.clamp(avg_activation, epsilon, 1 - epsilon)\n",
    "\n",
    "    kl_div = sparsity_target * torch.log(sparsity_target / avg_activation) + (\n",
    "        1 - sparsity_target\n",
    "    ) * torch.log((1 - sparsity_target) / (1 - avg_activation))\n",
    "    kl_div = torch.sum(kl_div)\n",
    "\n",
    "    # `sparsity_weight` decides how much we weight `KL-Divergence`\n",
    "    sparsity_penalty = sparsity_weight * kl_div\n",
    "\n",
    "    # print(f\"{mse_loss=}, {avg_activation=}, {kl_div.item()}, {sparsity_penalty=}\")\n",
    "\n",
    "    return mse_loss + sparsity_penalty, mse_loss, sparsity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ac6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_loss_l1_norm(\n",
    "    flattened_model_activations: BatchedFlattenedModelActivations,\n",
    "    sae_output: SAEOutput,\n",
    "    sparsity_weight: float,\n",
    ") -> tuple[Loss, MSELoss, WeightedSparsityLoss]:\n",
    "\n",
    "    # Reconstruction loss (Mean Squared Error)\n",
    "    mse_loss = F.mse_loss(\n",
    "        sae_output.reconstructed_model_activations,\n",
    "        flattened_model_activations,\n",
    "    )\n",
    "\n",
    "    # L1 sparsity penalty\n",
    "    l1_penalty = torch.mean(torch.abs(sae_output.sae_activations))\n",
    "\n",
    "    sparsity_penalty = sparsity_weight * l1_penalty\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = mse_loss + sparsity_penalty\n",
    "\n",
    "    return total_loss, mse_loss, sparsity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc92db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SparseAutoencoderConfig:\n",
    "    d_in: int\n",
    "    d_model: int\n",
    "\n",
    "\n",
    "# TODO(bschoen): Start using the config pattern, it stays typesafe and allows\n",
    "#                easy logging to things like wandb\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: SparseAutoencoderConfig,\n",
    "    ) -> None:\n",
    "\n",
    "        print(f\"Creating SparseAutoencoder with {cfg}\")\n",
    "\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "\n",
    "        self.d_in = cfg.d_in\n",
    "        self.d_model = cfg.d_model\n",
    "\n",
    "        self.encoder = nn.Linear(cfg.d_in, cfg.d_model)\n",
    "        self.decoder = nn.Linear(cfg.d_model, cfg.d_in)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: BatchedFlattenedModelActivations,\n",
    "    ) -> SAEOutput:\n",
    "\n",
    "        # TODO(bschoen): Which activation function should we use?\n",
    "        encoded = F.gelu(self.encoder(x))\n",
    "\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return SAEOutput(\n",
    "            sae_activations=encoded,\n",
    "            reconstructed_model_activations=decoded,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class LightningSparseAutoencoderConfig:\n",
    "\n",
    "    model_config: transformer_lens.HookedTransformerConfig\n",
    "    sae_config: SparseAutoencoderConfig\n",
    "    learning_rate: float\n",
    "    sparsity_weight: float\n",
    "\n",
    "\n",
    "# note: this kind of lightning adapter is a common pattern: https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#starter-example\n",
    "class LightningSparseAutoencoder(lightning.pytorch.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: LightningSparseAutoencoderConfig,\n",
    "    ) -> None:\n",
    "\n",
    "        super(LightningSparseAutoencoder, self).__init__()\n",
    "\n",
    "        self.model = transformer_lens.HookedTransformer(cfg=cfg.model_config)\n",
    "        self.sae = SparseAutoencoder(cfg=cfg.sae_config)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        return self.model(inputs, target)\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int) -> Loss:\n",
    "        inputs, target = batch\n",
    "\n",
    "        self.model\n",
    "        output = self(inputs, target)\n",
    "        loss = torch.nn.functional.cr(output, target.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_id = \"blocks.0.hook_mlp_out\"\n",
    "\n",
    "cache[hook_id].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "sae_num_epochs = 100000\n",
    "sae_expansion_factor = 64\n",
    "\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# both arbitrary for now\n",
    "# - Start small: A common approach is to begin with a relatively small sparsity weight,\n",
    "#                typically in the range of 1e-5 to 1e-3. This allows the model to\n",
    "#                learn meaningful representations before enforcing strong sparsity\n",
    "#                constraints.\n",
    "sparsity_weight: float = 1e-3  # Weight of the sparsity loss in the total loss\n",
    "sparsity_target: float = 0.05  # Target average activation of hidden neurons\n",
    "\n",
    "print(f\"Training SAE for {hook_id}...\")\n",
    "sae_d_in = (cfg.n_ctx - 1) * cfg.d_model  # -1 since not predicting first token\n",
    "sae_d_model = sae_d_in * sae_expansion_factor\n",
    "\n",
    "sae_cfg = SparseAutoencoderConfig(\n",
    "    d_in=sae_d_in,\n",
    "    d_model=sae_d_model,\n",
    ")\n",
    "\n",
    "sae_model = SparseAutoencoder(cfg=sae_cfg)\n",
    "sae_model.to(device)\n",
    "\n",
    "sae_optimizer = optim.Adam(sae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"toy-problem-hooked-transformer-sae\",\n",
    "    config={\n",
    "        \"sae_num_epochs\": sae_num_epochs,\n",
    "        \"sae_expansion_factor\": sae_expansion_factor,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"sparsity_weight\": sparsity_weight,\n",
    "        \"sparsity_target\": sparsity_target,\n",
    "        \"sae_d_in\": sae_d_in,\n",
    "        \"sae_d_model\": sae_d_model,\n",
    "        \"hook_id\": hook_id,\n",
    "    },\n",
    ")\n",
    "\n",
    "# put model itself into eval mode so doesn't change\n",
    "model.eval()\n",
    "\n",
    "# go through the training data again, this time training the sae on the activations\n",
    "for epoch, batch in tqdm.tqdm(\n",
    "    zip(\n",
    "        range(sae_num_epochs),\n",
    "        itertools.cycle(train_loader),\n",
    "    )\n",
    "):\n",
    "\n",
    "    tokens, target = batch\n",
    "\n",
    "    tokens, target = tokens.to(device), target.to(device)\n",
    "\n",
    "    # run through the model (with cache) to get the activations\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "    # ex: torch.Size([4, 8, 16])\n",
    "    activations = cache[hook_id]\n",
    "\n",
    "    # ex: torch.Size([4, 128])\n",
    "    flattened_activations = activations.reshape(activations.size(0), -1)\n",
    "\n",
    "    sae_optimizer.zero_grad()\n",
    "\n",
    "    # now the SAE model is given the *activations*\n",
    "    sae_output = sae_model.forward(flattened_activations)\n",
    "\n",
    "    # compute loss\n",
    "\n",
    "    total_loss, reconstruction_loss, weighted_sparsity_loss = sparse_loss_kl_divergence(\n",
    "        flattened_activations,\n",
    "        sae_output,\n",
    "        sparsity_target=sparsity_target,\n",
    "        sparsity_weight=sparsity_weight,\n",
    "    )\n",
    "\n",
    "    \"\"\"total_loss, reconstruction_loss, weighted_sparsity_loss = sparse_loss_l1_norm(\n",
    "        flattened_model_activations=flattened_activations,\n",
    "        sae_output=sae_output,\n",
    "        sparsity_weight=sparsity_weight,\n",
    "    )\"\"\"\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    sae_optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(\n",
    "            f\"Step {epoch}, \"\n",
    "            f\"Total Loss: {total_loss.item():.6f}, \"\n",
    "            f\"Reconstruction Loss: {reconstruction_loss.item():.6f}, \"\n",
    "            f\"Sparsity Loss: {weighted_sparsity_loss.item():.6f}\",\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"total_loss\": total_loss.item(),\n",
    "                \"reconstruction_loss\": reconstruction_loss.item(),\n",
    "                \"weighted_sparsity_loss\": weighted_sparsity_loss.item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a407f",
   "metadata": {},
   "source": [
    "#### Dictionary Learning Implementation\n",
    "\n",
    "See [simple_dictionary_learning.ipynb](simple_dictionary_learning.ipynb) for a details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a1011",
   "metadata": {},
   "source": [
    "#### Extracting the learned dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SparseAutoencoder with d_in=128, d_model=512, sparsity_target=0.05\n",
    "dictionary: Float32[torch.Tensor, \"sae_hidden sae_in\"] = sae_model.encoder.weight.detach()\n",
    "\n",
    "# ex: Dictionary shape: torch.Size([512, 128])\n",
    "print(f\"Dictionary shape: {dictionary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape dictionary elements to match original activation shape\n",
    "# (essentially `unflatting`)\n",
    "reshaped_dictionary = dictionary.reshape(sae_d_model, (cfg.n_ctx - 1), cfg.d_model)\n",
    "\n",
    "# Motivation: Extract the learned features (dictionary elements) from the encoder weights\n",
    "# ex: Dictionary shape: torch.Size([512, 8, 16])\n",
    "print(f\"Dictionary shape: {reshaped_dictionary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's always worth checking this sort of thing when you do this by hand\n",
    "# to check that you haven't got the wrong site, or are missing a\n",
    "# scaling factor or something like this.\n",
    "#\n",
    "# This is like the overfitting thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afd13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f65ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at an example batch from `test`\n",
    "\n",
    "# set both to eval mode\n",
    "model.eval()\n",
    "sae_model.eval()\n",
    "\n",
    "# grab something from the test batch\n",
    "example_batch = next(iter(test_loader))\n",
    "\n",
    "x, y = example_batch\n",
    "\n",
    "_, cache = model.run_with_cache(x)\n",
    "\n",
    "activations = cache[hook_id]\n",
    "\n",
    "print(f\"Activations shape: {activations.shape}\")\n",
    "\n",
    "# flatten it\n",
    "flattened_activations = activations.reshape(activations.size(0), -1)\n",
    "\n",
    "print(f\"{flattened_activations.shape=}\")\n",
    "\n",
    "sae_outputs = sae_model(flattened_activations)\n",
    "\n",
    "print(f\"{sae_outputs.sae_activations.shape=}\")\n",
    "print(f\"{sae_outputs.reconstructed_model_activations.shape=}\")\n",
    "\n",
    "# now we can get the dictionary\n",
    "dictionary = sae_model.encoder.weight.detach()\n",
    "\n",
    "print(f\"Dictionary shape: {dictionary.shape}\")\n",
    "\n",
    "# now we can get the sparse coefficients\n",
    "alpha = dictionary @ flattened_activations.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef94946",
   "metadata": {},
   "source": [
    "### Determine Quality Of SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(\n",
    "    sae_activations: BatchedSAEActivations,\n",
    "    threshold: float = 1e-5,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate sparsity of SAE activations across a batch.\n",
    "\n",
    "    Args:\n",
    "    sae_activations (torch.Tensor): The activations from the Sparse Autoencoder.\n",
    "                                    Shape: (batch, d_sae_model)\n",
    "    threshold (float): The threshold below which an activation is considered \"inactive\".\n",
    "\n",
    "    Returns:\n",
    "    float: The average sparsity value across the batch (fraction of inactive neurons).\n",
    "    \"\"\"\n",
    "    # Count the number of neurons that are below the threshold (inactive)\n",
    "    inactive_neurons = torch.sum(torch.abs(sae_activations) < threshold, dim=1)\n",
    "\n",
    "    # Calculate the fraction of inactive neurons for each item in the batch\n",
    "    sparsity_per_item = inactive_neurons.float() / sae_activations.shape[1]\n",
    "\n",
    "    # Take the mean across the batch\n",
    "    average_sparsity = torch.mean(sparsity_per_item)\n",
    "\n",
    "    return average_sparsity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce98afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_explained_variance(\n",
    "    reconstructed_model_activations: BatchedFlattenedModelActivations,\n",
    "    flattened_activations: BatchedFlattenedModelActivations,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the explained variance of the SAE activations.\n",
    "    \"\"\"\n",
    "\n",
    "    numerator = torch.mean(\n",
    "        (reconstructed_model_activations[:, 1:] - flattened_activations[:, 1:]) ** 2\n",
    "    )\n",
    "    denominator = flattened_activations[:, 1:].to(torch.float32).var()\n",
    "\n",
    "    explained_variance = 1 - (numerator / denominator)\n",
    "\n",
    "    return explained_variance.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_variance=0.995 -> good, basically all the variance is explained by our SAE\n",
    "# sparsity=0.0045 -> good, very sparse, and more sparse than our target of 0.05\n",
    "explained_variance = calculate_explained_variance(\n",
    "    sae_outputs.reconstructed_model_activations,\n",
    "    flattened_activations,\n",
    ")\n",
    "print(f\"{explained_variance=:.4f}\")\n",
    "\n",
    "sparsity = calculate_sparsity(sae_outputs.sae_activations)\n",
    "print(f\"{sparsity=:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa3d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the relationship between SAE activations and input features\n",
    "\n",
    "# TODO(bschoen): Oh `imshow` is huge here!\n",
    "\n",
    "# 1. Visualize the dictionary (encoder weights)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(dictionary.cpu().T, aspect=\"auto\", cmap=\"RdBu_r\")\n",
    "plt.colorbar()\n",
    "plt.title(\"SAE Dictionary (Encoder Weights)\")\n",
    "plt.xlabel(\"Dictionary Elements\")\n",
    "plt.ylabel(\"Input Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find the most active neurons for each input\n",
    "top_k = 5  # Number of top activations to consider\n",
    "\n",
    "# so this is essentially the top 5 activations over `batch_size` examples\n",
    "top_activations = torch.topk(sae_outputs.sae_activations, k=top_k, dim=1)\n",
    "\n",
    "# Visualization of top activations\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.heatmap(top_activations.values.detach().cpu().numpy(), cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Top 5 Activation Values\")\n",
    "plt.xlabel(\"Top K\")\n",
    "plt.ylabel(\"Batch Sample\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.heatmap(top_activations.indices.detach().cpu().numpy(), cmap=\"YlOrRd\", annot=True, fmt=\"d\")\n",
    "plt.title(\"Indices of Top 5 Activations\")\n",
    "plt.xlabel(\"Top K\")\n",
    "plt.ylabel(\"Batch Sample\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis: frequency of top neurons\n",
    "top_neuron_counts = torch.bincount(\n",
    "    top_activations.indices.flatten().detach().cpu(),\n",
    "    minlength=sae_outputs.sae_activations.shape[1],\n",
    ")\n",
    "top_10_neurons = torch.topk(top_neuron_counts, k=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), top_10_neurons.values.detach().cpu().numpy())\n",
    "plt.title(\"Top 10 Most Frequently Activated Neurons\")\n",
    "plt.xlabel(\"Neuron Index\")\n",
    "plt.ylabel(\"Activation Frequency\")\n",
    "plt.xticks(range(10), top_10_neurons.indices.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_outputs.sae_activations[:, 1210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5749ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sae_outputs.sae_activations.shape=}\")\n",
    "print(f\"{top_activations.values.shape=}\")\n",
    "print(f\"{top_activations.indices.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_activations.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4932383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex: 51 and 410 show up a lot\n",
    "sns.heatmap(top_activations.values.cpu().T, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analyze feature importance for each neuron\n",
    "feature_importance = torch.abs(dictionary).sum(dim=1)\n",
    "top_features = torch.topk(feature_importance, k=10)\n",
    "\n",
    "print(f\"{dictionary.shape=}\")\n",
    "print(f\"{feature_importance.shape=}\")\n",
    "print(f\"{top_features.values.shape=}\")\n",
    "print(f\"{top_features.indices.shape=}\")\n",
    "\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 most important neurons:\")\n",
    "for i, (value, index) in enumerate(\n",
    "    zip(top_features.values.tolist(), top_features.indices.tolist())\n",
    "):\n",
    "    print(f\"Neuron {index}:\\t{value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0168d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77201da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features.indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize activations for a few examples\n",
    "\n",
    "# first look at a single batch\n",
    "sae_activations = sae_outputs.sae_activations[0].detach().cpu()\n",
    "\n",
    "print(f\"{sae_activations.shape=}\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "# Look at a single batch\n",
    "plt.bar(range(sae_activations.shape[0]), sae_activations)\n",
    "\n",
    "plt.title(f\"SAE Activations for Example\")\n",
    "plt.xlabel(\"Neuron\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fa395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a496682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Reconstruct input features from SAE activations\n",
    "#\n",
    "# Take a single batch first\n",
    "reconstructed_model_activations = sae_outputs.reconstructed_model_activations.detach().cpu()\n",
    "\n",
    "# 6. Compare original and reconstructed features\n",
    "num_features = 5\n",
    "\n",
    "plt.figure(figsize=(15, 3 * num_features))\n",
    "for i in range(num_features):\n",
    "    plt.subplot(num_features, 1, i + 1)\n",
    "    plt.ylim(-1, 1)  # Set y-axis range from -1 to 1\n",
    "    plt.plot(flattened_activations[:, i].cpu(), label=\"Original\", alpha=0.5)\n",
    "    plt.plot(reconstructed_model_activations[:, i], label=\"Reconstructed\", alpha=0.5)\n",
    "    plt.title(f\"Feature {i}: Original vs Reconstructed\")\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b59930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Correlation between SAE activations and input features\n",
    "correlation_matrix = torch.corrcoef(\n",
    "    torch.cat([sae_outputs.sae_activations, flattened_activations], dim=1).T\n",
    ")\n",
    "num_neurons = sae_outputs.sae_activations.shape[1]\n",
    "neuron_feature_correlation = correlation_matrix[:num_neurons, num_neurons:]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(\n",
    "    neuron_feature_correlation.detach().cpu(),\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation between SAE Neurons and Input Features\")\n",
    "plt.xlabel(\"Input Features\")\n",
    "plt.ylabel(\"SAE Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d72027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385d16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02318ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47054e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_outputs.sae_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960382a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c6bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcdbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect max activations\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # go through the training data again, but don't cycle, no reason to go through more than once\n",
    "    for batch in tqdm.tqdm(train_loader):\n",
    "\n",
    "        tokens, target = batch\n",
    "\n",
    "        tokens, target = tokens.to(device), target.to(device)\n",
    "\n",
    "        # run through the model (with cache) to get the activations\n",
    "        logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "        # ex: torch.Size([4, 8, 16])\n",
    "        activations = cache[hook_id]\n",
    "\n",
    "        # ex: torch.Size([4, 128])\n",
    "        flattened_activations = activations.reshape(activations.size(0), -1)\n",
    "\n",
    "        # now the SAE model is given the *activations*\n",
    "        encoded, decoded = sae_model(flattened_activations)\n",
    "\n",
    "        sae_activations = encoded\n",
    "\n",
    "        # sae_activations.reshape(sae_d_model, (cfg.n_ctx - 1), cfg.d_model)\n",
    "\n",
    "        # max_activations = torch.max(encoded, dim=1)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc950af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = sae_model.encoder.weight @ flattened_activations[0]\n",
    "\n",
    "print(f\"{alpha.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9278a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.abs(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04427556",
   "metadata": {},
   "outputs": [],
   "source": [
    "8 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474f153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
