{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3674b7-cdcf-411d-80c5-ca44b425ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c7d994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15ad612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "import rich.table\n",
    "\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaaf1cc",
   "metadata": {},
   "source": [
    "# HookedTransformer\n",
    "\n",
    "* [TransformerLens - Tutorial - Trains HookedTransformer from Scratch](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/No_Position_Experiment.ipynb)\n",
    "\n",
    "```python\n",
    "import transformers\n",
    "\n",
    "# note: it's probably easier to just operate on tokens outside of the model,\n",
    "#       that'll also make it clearer where tokenizer is used\n",
    "#\n",
    "# okay wrapping a pretrained tokenizer *can* be done:\n",
    "# - https://huggingface.co/learn/nlp-course/chapter6/8#building-a-bpe-tokenizer-from-scratch\n",
    "# - but none of the models support just naive encoding\n",
    "#   - https://huggingface.co/docs/tokenizers/api/models#tokenizers.models.BPE\n",
    "class HookedTransformer:\n",
    "    cfg: HookedTransformerConfig\n",
    "\n",
    "    # note: actually does an `isinstance` check in the constructor\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase | None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76467a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "\n",
    "from jaxtyping import Int64, Float32\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "import torch\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd75509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting code copied over from transformer_lens tutorial notebook\n",
    "\n",
    "\n",
    "def line(tensor: torch.Tensor, line_labels=None, yaxis=\"\", xaxis=\"\", **kwargs):\n",
    "    tensor = transformer_lens.utils.to_numpy(tensor)\n",
    "    labels = {\"y\": yaxis, \"x\": xaxis}\n",
    "    fig = px.line(tensor, labels=labels, **kwargs)\n",
    "    if line_labels:\n",
    "        for c, label in enumerate(line_labels):\n",
    "            fig.data[c].name = label\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def imshow(tensor: torch.Tensor, yaxis=\"\", xaxis=\"\", **kwargs):\n",
    "    tensor = transformer_lens.utils.to_numpy(tensor)\n",
    "    plot_kwargs = {\n",
    "        \"color_continuous_scale\": \"RdBu\",\n",
    "        \"color_continuous_midpoint\": 0.0,\n",
    "        \"labels\": {\"x\": xaxis, \"y\": yaxis},\n",
    "    }\n",
    "    plot_kwargs.update(kwargs)\n",
    "    px.imshow(tensor, **plot_kwargs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5605e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = transformer_lens.utils.get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e9839",
   "metadata": {},
   "source": [
    "### Setup Sample Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70bd2cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<aa|aa>>>>>>>',\n",
       " '<ab|ab>>>>>>>',\n",
       " '<ac|ac>>>>>>>',\n",
       " '<ad|ad>>>>>>>',\n",
       " '<ae|ae>>>>>>>',\n",
       " '<af|af>>>>>>>',\n",
       " '<ag|ag>>>>>>>',\n",
       " '<ah|ah>>>>>>>',\n",
       " '<ai|ai>>>>>>>',\n",
       " '<aj|aj>>>>>>>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import itertools\n",
    "import more_itertools\n",
    "\n",
    "\n",
    "class SpecialToken:\n",
    "    # note: as assume a BOS token because transformerlens expects it\n",
    "    BOS = \"<\"\n",
    "    # we use a EOS token for convenience\n",
    "    EOS = \">\"\n",
    "\n",
    "\n",
    "# note: without length, the model doesn't need to learn induction heads, just directly copies\n",
    "\n",
    "\n",
    "# TODO(bschoen): Allow this to generalize in the future\n",
    "#\n",
    "# Good for purely attention, since seeing patterns\n",
    "def generate_sample_palindrome_then_repeated() -> Iterable[str]:\n",
    "    \"\"\"Generate palindrom samples like `<abc|cba|abc>`.\"\"\"\n",
    "\n",
    "    # Generate all combinations of lowercase letters\n",
    "    characters = string.ascii_lowercase\n",
    "\n",
    "    # note: chosen arbitrarily\n",
    "    lengths = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "    # pad to max length\n",
    "    max_length = (\n",
    "        1 + max(lengths) + 1 + max(lengths) + 1 + max(lengths) + 1 + max(lengths) + 1\n",
    "    )\n",
    "\n",
    "    # set max number to take of each length\n",
    "    max_combinations_per_length = 10000\n",
    "\n",
    "    for length in lengths:\n",
    "\n",
    "        for combination_index, combination in enumerate(\n",
    "            itertools.product(characters, repeat=length)\n",
    "        ):\n",
    "\n",
    "            if combination_index > max_combinations_per_length:\n",
    "                break\n",
    "\n",
    "            combination_str = \"\".join(combination)\n",
    "            reversed_str = \"\".join(reversed(combination_str))\n",
    "\n",
    "            sample = (\n",
    "                SpecialToken.BOS\n",
    "                + combination_str\n",
    "                + \"|\"\n",
    "                + reversed_str\n",
    "                + \"|\"\n",
    "                + combination_str\n",
    "                + SpecialToken.EOS\n",
    "            )\n",
    "\n",
    "            # Pad the sample to max_length with EOS tokens\n",
    "            padded_sample = sample.ljust(max_length, SpecialToken.EOS)\n",
    "\n",
    "            yield padded_sample  # Return the padded sample\n",
    "\n",
    "\n",
    "# TODO(bschoen): For this do we get like a \"next biggest\" head?\n",
    "# TODO(bschoen): Can we do circuit analysis on this?\n",
    "def generate_sample_sorted() -> Iterable[str]:\n",
    "    \"\"\"Generate sequence sorted `<cab|abc>`.\"\"\"\n",
    "\n",
    "    # Generate all combinations of lowercase letters\n",
    "    characters = string.ascii_lowercase\n",
    "\n",
    "    # note: chosen arbitrarily\n",
    "    # lengths = [3, 4, 5, 6, 7]\n",
    "    lengths = [2, 3, 4, 5]  # , 6, 7]\n",
    "\n",
    "    # pad to max length\n",
    "    max_length = 1 + max(lengths) + 1 + max(lengths) + 1\n",
    "\n",
    "    # set max number to take of each length\n",
    "    # max_combinations_per_length = 10000\n",
    "\n",
    "    for length in lengths:\n",
    "\n",
    "        for combination_index, combination in enumerate(\n",
    "            itertools.product(characters, repeat=length)\n",
    "        ):\n",
    "\n",
    "            # if combination_index > max_combinations_per_length:\n",
    "            #    break\n",
    "\n",
    "            combination_str = \"\".join(combination)\n",
    "            sorted_str = \"\".join(sorted(combination_str))\n",
    "\n",
    "            sample = (\n",
    "                SpecialToken.BOS + combination_str + \"|\" + sorted_str + SpecialToken.EOS\n",
    "            )\n",
    "\n",
    "            # Pad the sample to max_length with EOS tokens\n",
    "            padded_sample = sample.ljust(max_length, SpecialToken.EOS)\n",
    "\n",
    "            yield padded_sample  # Return the padded sample\n",
    "\n",
    "\n",
    "generate_sample = generate_sample_sorted\n",
    "\n",
    "# show a few examples\n",
    "[x for x in more_itertools.take(10, generate_sample())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75a15d",
   "metadata": {},
   "source": [
    "### Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09afecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch.naive_tokenizer import NaiveTokenizer\n",
    "\n",
    "vocab = string.ascii_lowercase + \"|\" + SpecialToken.BOS + SpecialToken.EOS\n",
    "\n",
    "tokenizer = NaiveTokenizer.from_text(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0f6b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\t\t<abc|cba|abc><bd|db|bd>>>>\n",
      "Tokenized:\t\u001b[44m\u001b[97m<\u001b[0m\u001b[43m\u001b[97ma\u001b[0m\u001b[45m\u001b[97mb\u001b[0m\u001b[46m\u001b[97mc\u001b[0m\u001b[42m\u001b[97m|\u001b[0m\u001b[41m\u001b[97mc\u001b[0m\u001b[44m\u001b[97mb\u001b[0m\u001b[43m\u001b[97ma\u001b[0m\u001b[45m\u001b[97m|\u001b[0m\u001b[46m\u001b[97ma\u001b[0m\u001b[42m\u001b[97mb\u001b[0m\u001b[41m\u001b[97mc\u001b[0m\u001b[44m\u001b[97m>\u001b[0m\u001b[43m\u001b[97m<\u001b[0m\u001b[45m\u001b[97mb\u001b[0m\u001b[46m\u001b[97md\u001b[0m\u001b[42m\u001b[97m|\u001b[0m\u001b[41m\u001b[97md\u001b[0m\u001b[44m\u001b[97mb\u001b[0m\u001b[43m\u001b[97m|\u001b[0m\u001b[45m\u001b[97mb\u001b[0m\u001b[46m\u001b[97md\u001b[0m\u001b[42m\u001b[97m>\u001b[0m\u001b[41m\u001b[97m>\u001b[0m\u001b[44m\u001b[97m>\u001b[0m\u001b[43m\u001b[97m>\u001b[0m\n",
      "Token ID | Token Bytes | Token String\n",
      "---------+-------------+--------------\n",
      "       0 | \u001b[38;5;2m3C\u001b[0m | '<'\n",
      "          \u001b[48;5;1m\u001b[38;5;15m<\u001b[0mabc|cba|abc><bd|db|bd>>>>\n",
      "          U+003C LESS-THAN SIGN (1 bytes: \u001b[38;5;2m3C\u001b[0m)\n",
      "       2 | \u001b[38;5;2m61\u001b[0m | 'a'\n",
      "          <\u001b[48;5;1m\u001b[38;5;15ma\u001b[0mbc|cba|abc><bd|db|bd>>>>\n",
      "          U+0061 LATIN SMALL LETTER A (1 bytes: \u001b[38;5;2m61\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <a\u001b[48;5;1m\u001b[38;5;15mb\u001b[0mc|cba|abc><bd|db|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       4 | \u001b[38;5;2m63\u001b[0m | 'c'\n",
      "          <ab\u001b[48;5;1m\u001b[38;5;15mc\u001b[0m|cba|abc><bd|db|bd>>>>\n",
      "          U+0063 LATIN SMALL LETTER C (1 bytes: \u001b[38;5;2m63\u001b[0m)\n",
      "      28 | \u001b[38;5;2m7C\u001b[0m | '|'\n",
      "          <abc\u001b[48;5;1m\u001b[38;5;15m|\u001b[0mcba|abc><bd|db|bd>>>>\n",
      "          U+007C VERTICAL LINE (1 bytes: \u001b[38;5;2m7C\u001b[0m)\n",
      "       4 | \u001b[38;5;2m63\u001b[0m | 'c'\n",
      "          <abc|\u001b[48;5;1m\u001b[38;5;15mc\u001b[0mba|abc><bd|db|bd>>>>\n",
      "          U+0063 LATIN SMALL LETTER C (1 bytes: \u001b[38;5;2m63\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|c\u001b[48;5;1m\u001b[38;5;15mb\u001b[0ma|abc><bd|db|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       2 | \u001b[38;5;2m61\u001b[0m | 'a'\n",
      "          <abc|cb\u001b[48;5;1m\u001b[38;5;15ma\u001b[0m|abc><bd|db|bd>>>>\n",
      "          U+0061 LATIN SMALL LETTER A (1 bytes: \u001b[38;5;2m61\u001b[0m)\n",
      "      28 | \u001b[38;5;2m7C\u001b[0m | '|'\n",
      "          <abc|cba\u001b[48;5;1m\u001b[38;5;15m|\u001b[0mabc><bd|db|bd>>>>\n",
      "          U+007C VERTICAL LINE (1 bytes: \u001b[38;5;2m7C\u001b[0m)\n",
      "       2 | \u001b[38;5;2m61\u001b[0m | 'a'\n",
      "          <abc|cba|\u001b[48;5;1m\u001b[38;5;15ma\u001b[0mbc><bd|db|bd>>>>\n",
      "          U+0061 LATIN SMALL LETTER A (1 bytes: \u001b[38;5;2m61\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|cba|a\u001b[48;5;1m\u001b[38;5;15mb\u001b[0mc><bd|db|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       4 | \u001b[38;5;2m63\u001b[0m | 'c'\n",
      "          <abc|cba|ab\u001b[48;5;1m\u001b[38;5;15mc\u001b[0m><bd|db|bd>>>>\n",
      "          U+0063 LATIN SMALL LETTER C (1 bytes: \u001b[38;5;2m63\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m<bd|db|bd>>>>\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n",
      "       0 | \u001b[38;5;2m3C\u001b[0m | '<'\n",
      "          <abc|cba|abc>\u001b[48;5;1m\u001b[38;5;15m<\u001b[0mbd|db|bd>>>>\n",
      "          U+003C LESS-THAN SIGN (1 bytes: \u001b[38;5;2m3C\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|cba|abc><\u001b[48;5;1m\u001b[38;5;15mb\u001b[0md|db|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       5 | \u001b[38;5;2m64\u001b[0m | 'd'\n",
      "          <abc|cba|abc><b\u001b[48;5;1m\u001b[38;5;15md\u001b[0m|db|bd>>>>\n",
      "          U+0064 LATIN SMALL LETTER D (1 bytes: \u001b[38;5;2m64\u001b[0m)\n",
      "      28 | \u001b[38;5;2m7C\u001b[0m | '|'\n",
      "          <abc|cba|abc><bd\u001b[48;5;1m\u001b[38;5;15m|\u001b[0mdb|bd>>>>\n",
      "          U+007C VERTICAL LINE (1 bytes: \u001b[38;5;2m7C\u001b[0m)\n",
      "       5 | \u001b[38;5;2m64\u001b[0m | 'd'\n",
      "          <abc|cba|abc><bd|\u001b[48;5;1m\u001b[38;5;15md\u001b[0mb|bd>>>>\n",
      "          U+0064 LATIN SMALL LETTER D (1 bytes: \u001b[38;5;2m64\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|cba|abc><bd|d\u001b[48;5;1m\u001b[38;5;15mb\u001b[0m|bd>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "      28 | \u001b[38;5;2m7C\u001b[0m | '|'\n",
      "          <abc|cba|abc><bd|db\u001b[48;5;1m\u001b[38;5;15m|\u001b[0mbd>>>>\n",
      "          U+007C VERTICAL LINE (1 bytes: \u001b[38;5;2m7C\u001b[0m)\n",
      "       3 | \u001b[38;5;2m62\u001b[0m | 'b'\n",
      "          <abc|cba|abc><bd|db|\u001b[48;5;1m\u001b[38;5;15mb\u001b[0md>>>>\n",
      "          U+0062 LATIN SMALL LETTER B (1 bytes: \u001b[38;5;2m62\u001b[0m)\n",
      "       5 | \u001b[38;5;2m64\u001b[0m | 'd'\n",
      "          <abc|cba|abc><bd|db|b\u001b[48;5;1m\u001b[38;5;15md\u001b[0m>>>>\n",
      "          U+0064 LATIN SMALL LETTER D (1 bytes: \u001b[38;5;2m64\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc><bd|db|bd\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m>>>\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc><bd|db|bd>\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m>>\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc><bd|db|bd>>\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m>\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n",
      "       1 | \u001b[38;5;2m3E\u001b[0m | '>'\n",
      "          <abc|cba|abc><bd|db|bd>>>\u001b[48;5;1m\u001b[38;5;15m>\u001b[0m\n",
      "          U+003E GREATER-THAN SIGN (1 bytes: \u001b[38;5;2m3E\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "from gpt_from_scratch import tokenizer_utils\n",
    "\n",
    "# test tokenizer\n",
    "input_text = \"<abc|cba|abc><bd|db|bd>>>>\"\n",
    "tokenizer_utils.show_token_mapping(tokenizer, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152a119",
   "metadata": {},
   "source": [
    "### Setup Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268db8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, target):\n",
    "    # standard cross entropy loss\n",
    "    return torch.nn.functional.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        target.view(-1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f67052",
   "metadata": {},
   "source": [
    "### Evaluate On Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e610984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss_on_test_batches(\n",
    "    model: transformer_lens.HookedTransformer,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    max_batches: int,\n",
    ") -> float:\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "\n",
    "        for batch_index, batch in enumerate(data_loader):\n",
    "\n",
    "            if batch_index > max_batches:\n",
    "                break\n",
    "\n",
    "            x, y = batch\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "\n",
    "            loss = loss_fn(logits, y)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c7b6f",
   "metadata": {},
   "source": [
    "### Setup Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43250038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples: list[str], tokenizer: NaiveTokenizer) -> None:\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer  # Assuming tokenizer is defined in the global scope\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        tokens = self.tokenizer.encode(sample)\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def make_batch_dataloader(\n",
    "    samples: list[str],\n",
    "    tokenizer: NaiveTokenizer,\n",
    "    batch_size: int,\n",
    ") -> tuple[torch.utils.data.Dataset, torch.utils.data.DataLoader]:\n",
    "\n",
    "    dataset = AutoregressiveDataset(samples=samples, tokenizer=tokenizer)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        # drop the last batch if it's incomplete\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# batch_generator = make_batch_generator(tokenizer, batch_size=4)\n",
    "# for x, y in batch_generator:\n",
    "#     # x is input, y is target (x shifted by 1)\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7cdd9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12356604 samples\n",
      "len(train_samples)=9885284\n",
      "len(test_samples)=2471320\n",
      "12: 2376443\n",
      "10: 91293\n",
      "8: 3447\n",
      "6: 137\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# split into test and train\n",
    "all_samples = list(generate_sample())\n",
    "\n",
    "# note: 4394 batches = (26 * 26 * 26) / 4\n",
    "print(f\"{len(all_samples)} samples\")\n",
    "\n",
    "# Randomly shuffle all_samples\n",
    "random.shuffle(all_samples)  # In-place shuffling of the list\n",
    "\n",
    "# Inline comment explaining the motivation\n",
    "# We shuffle the samples to ensure a random distribution of data points\n",
    "# between the training and test sets, reducing potential bias\n",
    "\n",
    "\n",
    "# max_samples = 10\n",
    "# print(f'Capping at {max_samples} batches first to make sure we can overfit')\n",
    "# all_samples = all_samples[:max_samples]\n",
    "\n",
    "test_train_ratio = 0.2\n",
    "\n",
    "test_size = int(test_train_ratio * len(all_samples))\n",
    "\n",
    "# put remaining ones into train\n",
    "train_size = len(all_samples) - test_size\n",
    "\n",
    "train_samples = all_samples[:train_size]\n",
    "test_samples = all_samples[train_size:]\n",
    "\n",
    "print(f\"{len(train_samples)=}\")\n",
    "print(f\"{len(test_samples)=}\")\n",
    "\n",
    "# now we can finally construct dataloaders\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset, train_loader = make_batch_dataloader(\n",
    "    samples=train_samples,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "# Split test_samples based on the number of '>' characters\n",
    "test_samples_by_difficulty = {}\n",
    "for sample in test_samples:\n",
    "    difficulty = len(sample) - sample.count(\">\")\n",
    "    if difficulty not in test_samples_by_difficulty:\n",
    "        test_samples_by_difficulty[difficulty] = []\n",
    "    test_samples_by_difficulty[difficulty].append(sample)\n",
    "\n",
    "# Sort the dictionary by difficulty (number of '>' characters)\n",
    "test_samples_by_difficulty = dict(\n",
    "    sorted(test_samples_by_difficulty.items(), reverse=True)\n",
    ")\n",
    "\n",
    "# Inline comment explaining the motivation\n",
    "# We sort the dictionary by difficulty to ensure a consistent order\n",
    "# when iterating through the difficulty levels, making it easier to\n",
    "# analyze and compare model performance across increasing complexities\n",
    "\n",
    "for difficulty, samples in test_samples_by_difficulty.items():\n",
    "    print(f\"{difficulty}: {len(samples)}\")\n",
    "\n",
    "# Create dataloaders for each difficulty level\n",
    "test_datasets = {}\n",
    "test_loaders = {}\n",
    "for difficulty, samples in test_samples_by_difficulty.items():\n",
    "    test_datasets[difficulty], test_loaders[difficulty] = make_batch_dataloader(\n",
    "        samples=samples,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "# Inline comment explaining the motivation\n",
    "# We split the test samples based on the number of '>' characters to create\n",
    "# separate datasets for different difficulty levels. This allows us to evaluate\n",
    "# the model's performance across varying complexities of input sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7e574",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19115680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we know our vocab size from our sample generation\n",
    "def make_hooked_transformer_config(\n",
    "    n_layers: int,\n",
    "    d_model: int,\n",
    "    n_heads: int,\n",
    ") -> transformer_lens.HookedTransformerConfig:\n",
    "\n",
    "    for sample in generate_sample():\n",
    "        n_ctx = len(sample)\n",
    "        break\n",
    "\n",
    "    cfg = transformer_lens.HookedTransformerConfig(\n",
    "        n_layers=n_layers,\n",
    "        d_model=d_model,\n",
    "        d_head=d_model // n_heads,\n",
    "        # The number of attention heads.\n",
    "        # If not specified, will be set to d_in // d_head.\n",
    "        # (This is represented by a default value of -1)\n",
    "        n_heads=n_heads,\n",
    "        # The dimensionality of the feedforward mlp network.\n",
    "        # Defaults to 4 * d_in, and in an attn-only model is None.\n",
    "        # TODO(bschoen): Need to try out also setting `attn_only`\n",
    "        # d_mlp=None,\n",
    "        # note: transformerlens does the same thing if this is not set\n",
    "        d_vocab=len(tokenizer.byte_to_token_dict),\n",
    "        # length of the longest sample is our context length\n",
    "        n_ctx=n_ctx,\n",
    "        act_fn=\"relu\",\n",
    "        normalization_type=\"LN\",\n",
    "        # note: must be set, otherwise tries to default to cuda / cpu (not mps)\n",
    "        device=device.type,\n",
    "    )\n",
    "\n",
    "    print(f\"Num params: {cfg.n_params}\")\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a131f",
   "metadata": {},
   "source": [
    "## Setup Image Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f4d3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert matplotlib figure to PNG for wandb upload\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from jaxtyping import Float\n",
    "\n",
    "\n",
    "def fig_to_wandb_image(fig) -> Image:\n",
    "    \"\"\"\n",
    "    Convert a matplotlib figure to a PNG image that can be uploaded to wandb.\n",
    "\n",
    "    Args:\n",
    "        fig (matplotlib.figure.Figure): The matplotlib figure to convert\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: The figure as a PIL Image object\n",
    "    \"\"\"\n",
    "    # Save the figure to a byte buffer\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Convert the buffer to a PIL Image\n",
    "    image = Image.open(buf)\n",
    "    return image\n",
    "\n",
    "\n",
    "# note: `title` is passed in for telling them apart in gifs etc\n",
    "def generate_image_for_attention_patterns(\n",
    "    input_token_str_to_cache_dict: dict[str, transformer_lens.ActivationCache],\n",
    "    title: str,\n",
    ") -> Image:\n",
    "    \"\"\"\n",
    "    Visualize attention patterns for all layers and heads in the model for multiple caches.\n",
    "\n",
    "    Args:\n",
    "        caches (List[Dict[str, Any]]): List of caches containing attention patterns from model forward passes.\n",
    "\n",
    "    Returns:\n",
    "        plt.Figure: A matplotlib figure containing the visualized attention patterns.\n",
    "    \"\"\"\n",
    "    input_token_strings = list(input_token_str_to_cache_dict.keys())\n",
    "    caches = list(input_token_str_to_cache_dict.values())\n",
    "\n",
    "    # Find all attention pattern tensors in the first cache (assuming all caches have the same structure)\n",
    "    pattern_keys = [\n",
    "        key for key in caches[0].keys() if key.endswith(\".attn.hook_pattern\")\n",
    "    ]\n",
    "\n",
    "    n_layers = len(pattern_keys)\n",
    "    n_heads = caches[0][pattern_keys[0]].shape[1]\n",
    "    n_caches = len(caches)\n",
    "\n",
    "    # Calculate total number of subplots\n",
    "    total_subplots = n_layers * n_heads\n",
    "\n",
    "    # Create a figure with subplots stacked vertically for each cache\n",
    "    fig, axes = plt.subplots(\n",
    "        n_caches, total_subplots, figsize=(4 * total_subplots, 4 * n_caches)\n",
    "    )\n",
    "\n",
    "    # Set overall figure title\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Color maps for alternating heads\n",
    "    cmaps = [\"Blues\", \"Reds\"]\n",
    "\n",
    "    for cache_idx, cache in enumerate(caches):\n",
    "        input_token_string = input_token_strings[cache_idx]\n",
    "        for layer, key in enumerate(pattern_keys):\n",
    "            attention_pattern = cache[key]\n",
    "\n",
    "            # Remove batch dimension and move to CPU\n",
    "            reshaped_pattern = attention_pattern.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "            for head in range(n_heads):\n",
    "                subplot_index = layer * n_heads + head\n",
    "                ax = (\n",
    "                    axes[cache_idx, subplot_index]\n",
    "                    if n_caches > 1\n",
    "                    else axes[subplot_index]\n",
    "                )\n",
    "\n",
    "                # Plot the attention pattern\n",
    "                im = ax.imshow(reshaped_pattern[head], cmap=cmaps[head % len(cmaps)])\n",
    "\n",
    "                # Set title for each subplot\n",
    "                ax.set_title(f\"L{layer}-H{head}\", fontsize=8)\n",
    "\n",
    "                # Set column labels as individual characters from input_token_string at the top\n",
    "                ax.xaxis.tick_top()\n",
    "                ax.set_xticks(range(len(input_token_string)))\n",
    "                ax.set_xticklabels(list(input_token_string), fontsize=6, ha=\"right\")\n",
    "\n",
    "                ax.set_yticks([])  # Remove y-axis ticks\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    image = fig_to_wandb_image(fig)\n",
    "\n",
    "    # close figure so doesn't keep taking up memory\n",
    "    plt.close(fig)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58e3c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "\n",
    "def convert_pngs_in_directory_to_gif(output_dir: pathlib.Path) -> pathlib.Path:\n",
    "\n",
    "    # Get a list of all PNG files in the output directory\n",
    "    # Use rglob for recursive search of PNG files\n",
    "    png_files = list(output_dir.rglob(\"*.png\"))\n",
    "\n",
    "    # sort by step\n",
    "    #\n",
    "    # files have format\n",
    "    #\n",
    "    # - `.../<key>_<step>_<hash-identifier-thing>.png`\n",
    "    # - ex: `.../attention_100_d8bda3455ffb06855d88.png`\n",
    "    #\n",
    "    png_files = sorted(png_files, key=lambda x: int(x.name.split(\"_\")[1]))\n",
    "\n",
    "    # Create a list to store the image frames\n",
    "    frames = []\n",
    "\n",
    "    # Load each PNG file and append it to the frames list\n",
    "    print(f\"Generating gif from {len(png_files)} images...\")\n",
    "    for png_file in png_files:\n",
    "        # Open the image and convert it to RGB mode (required for GIF)\n",
    "        img = Image.open(str(png_file)).convert(\"RGB\")\n",
    "        frames.append(img)\n",
    "\n",
    "    # Define the output GIF filename\n",
    "    gif_filename = output_dir / \"attention_pattern_evolution.gif\"\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    print(f\"Saving gif from {len(frames)} frames to {gif_filename}...\")\n",
    "    frames[0].save(\n",
    "        gif_filename,\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        optimize=False,\n",
    "        duration=200,  # Duration between frames in milliseconds\n",
    "        loop=0,  # 0 means loop indefinitely\n",
    "    )\n",
    "\n",
    "    print(f\"GIF created and saved as: {gif_filename}\")\n",
    "\n",
    "    # Optionally, log the GIF to wandb\n",
    "    # wandb.log({\"attention_pattern_evolution\": wandb.Image(str(gif_filename))})\n",
    "\n",
    "    return gif_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0438238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbConstants:\n",
    "    ENTITY = \"bronsonschoen-personal-use\"\n",
    "    PROJECT = \"toy-problem-hooked-transformer-v4\"\n",
    "    NAME = \"toy-sequence\"\n",
    "    ATTENTION_PATTERN_IMAGES = \"attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65fc8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LossValue = float\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TrainModelResult:\n",
    "    model: transformer_lens.HookedTransformer\n",
    "\n",
    "    # returned because optuna needs it\n",
    "    # TODO(bschoen): Is this usually val loss?\n",
    "    train_loss: LossValue\n",
    "\n",
    "    # useful to retrieve files\n",
    "    wandb_run_name: str\n",
    "    wandb_run_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54530e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pathlib\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def download_images_from_run(result: TrainModelResult) -> pathlib.Path:\n",
    "\n",
    "    # write things to run specific directory\n",
    "    output_dir = pathlib.Path(f\"wandb_artifacts/{result.wandb_run_id}\")\n",
    "\n",
    "    # create output dir if not exists\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    api = wandb.Api()\n",
    "\n",
    "    identifier = \"/\".join(\n",
    "        [\n",
    "            WandbConstants.ENTITY,\n",
    "            WandbConstants.PROJECT,\n",
    "            result.wandb_run_id,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"Downloading {identifier}...\")\n",
    "    run = api.run(identifier)\n",
    "\n",
    "    # filter down to just attention pattern images\n",
    "    files = [\n",
    "        x\n",
    "        for x in run.files()\n",
    "        if x.name.startswith(f\"media/images/{WandbConstants.ATTENTION_PATTERN_IMAGES}\")\n",
    "    ]\n",
    "\n",
    "    for file in tqdm.tqdm(desc=\"Downloading images...\", iterable=files):\n",
    "\n",
    "        print(f\"Downloading {file.name}\")\n",
    "        file.download(\n",
    "            root=str(output_dir),\n",
    "            replace=False,\n",
    "            exist_ok=True,\n",
    "            api=api,\n",
    "        )\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003ea31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f5081e3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7533cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(bschoen): Holdout set of n+1 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac160424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbronsonschoen\u001b[0m (\u001b[33mbronsonschoen-personal-use\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/bronsonschoen/gpt_from_scratch/wandb/run-20240913_221814-wj7teg92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4/runs/wj7teg92' target=\"_blank\">toy-sequence</a></strong> to <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4/runs/wj7teg92' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4/runs/wj7teg92</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name toy-sequence - wj7teg92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996it [00:29, 49.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1998it [00:57, 47.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2996it [01:23, 48.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [01:49, 47.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4999it [02:17, 42.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000it [02:44, 46.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7000it [03:10, 47.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7998it [03:36, 47.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9000it [04:03, 46.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [04:29, 48.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11000it [04:54, 49.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11998it [05:20, 48.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12999it [05:46, 46.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13999it [06:12, 48.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14997it [06:37, 48.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16000it [07:02, 49.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16996it [07:27, 48.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17996it [07:52, 46.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19000it [08:17, 49.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19999it [08:44, 46.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20999it [09:10, 47.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22000it [09:37, 43.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22999it [10:03, 48.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24000it [10:30, 41.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test loss...\n",
      "Computing attention pattern visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [10:58, 37.97it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de1be25ff304f839837fcd2ec9ed049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.485 MB of 3.492 MB uploaded\\r'), FloatProgress(value=0.9978624770087241, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>test_loss_difficulty_10</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss_difficulty_12</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_loss_difficulty_6</td><td>█▄▄▄▄▃▃▄▂▃▃▂▂▃▁▂▂▂▁▃▂▂▂▂▃</td></tr><tr><td>test_loss_difficulty_8</td><td>█▆▆▆▅▄▃▃▂▃▃▂▂▂▁▂▂▁▁▂▂▁▂▁▁</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>24000</td></tr><tr><td>test_loss_difficulty_10</td><td>1.37683</td></tr><tr><td>test_loss_difficulty_12</td><td>1.36274</td></tr><tr><td>test_loss_difficulty_6</td><td>1.93181</td></tr><tr><td>test_loss_difficulty_8</td><td>1.59169</td></tr><tr><td>train_loss</td><td>1.36373</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toy-sequence</strong> at: <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4/runs/wj7teg92' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4/runs/wj7teg92</a><br/> View project at: <a href='https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4' target=\"_blank\">https://wandb.ai/bronsonschoen-personal-use/toy-problem-hooked-transformer-v4</a><br/>Synced 5 W&B file(s), 25 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240913_221814-wj7teg92/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss: 1.362059\n",
      "Num params: 6144\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "import torch.optim\n",
    "\n",
    "import wandb\n",
    "\n",
    "import dataclasses\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def print_json(value):\n",
    "    print(json.dumps(value, indent=2))\n",
    "\n",
    "\n",
    "# everything customizable via optuna\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class ModelAndTrainingConfig:\n",
    "\n",
    "    # input\n",
    "    train_loader: torch.utils.data.DataLoader\n",
    "    test_loaders: dict[int, torch.utils.data.DataLoader]\n",
    "\n",
    "    # training\n",
    "    num_epochs: int = 10000\n",
    "    eval_test_every_n: int = 500\n",
    "    wait_between_eval_s: int | None = None\n",
    "\n",
    "    # model\n",
    "    n_layers: int = 2\n",
    "    d_model: int = 16\n",
    "    n_heads: int = 2\n",
    "\n",
    "    # optimizers\n",
    "    betas: tuple[float, float] = (0.9, 0.999)\n",
    "    learning_rate: float = 1e-3\n",
    "    max_grad_norm: float = 1.0\n",
    "    weight_decay: float = 0.1\n",
    "\n",
    "    def get_hooked_transformer_config(self) -> transformer_lens.HookedTransformerConfig:\n",
    "        return make_hooked_transformer_config(\n",
    "            n_layers=self.n_layers,\n",
    "            d_model=self.d_model,\n",
    "            n_heads=self.n_heads,\n",
    "        )\n",
    "\n",
    "    def to_dict(self) -> dict[str, str | int]:\n",
    "        dict_repr = dataclasses.asdict(self)\n",
    "        dict_repr.pop(\"train_loader\")\n",
    "        dict_repr.pop(\"test_loaders\")\n",
    "        return dict_repr\n",
    "\n",
    "\n",
    "def train_model(cfg: ModelAndTrainingConfig) -> TrainModelResult:\n",
    "\n",
    "    # create new model instance\n",
    "    ht_cfg = cfg.get_hooked_transformer_config()\n",
    "    model = transformer_lens.HookedTransformer(ht_cfg)\n",
    "\n",
    "    # setup optimizers\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=cfg.learning_rate,\n",
    "        betas=cfg.betas,\n",
    "        weight_decay=cfg.weight_decay,\n",
    "    )\n",
    "    # scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    #    optimizer, lambda i: min(i / 100, 1.0)\n",
    "    # )\n",
    "\n",
    "    num_epochs = cfg.num_epochs\n",
    "\n",
    "    # setup wandb\n",
    "    wandb.init(\n",
    "        project=WandbConstants.PROJECT,\n",
    "        name=WandbConstants.NAME,\n",
    "        config=cfg.to_dict(),\n",
    "    )\n",
    "\n",
    "    print(f\"Run name {wandb.run.name} - {wandb.run.id}\")\n",
    "\n",
    "    # create a small (fixed) training set of each difficulty to use for visualization\n",
    "    test_example_per_difficulty = {}\n",
    "    for difficulty, test_loader in cfg.test_loaders.items():\n",
    "        # grab something from the test batch\n",
    "        x, _ = next(iter(test_loader))\n",
    "        input_tokens = x[0].to(device)\n",
    "        test_example_per_difficulty[difficulty] = input_tokens\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch, batch in tqdm.tqdm(\n",
    "        zip(\n",
    "            range(num_epochs),\n",
    "            itertools.cycle(train_loader),\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        tokens, target = batch\n",
    "\n",
    "        tokens, target = tokens.to(device), target.to(device)\n",
    "\n",
    "        # ex: torch.Size([4, 9, 29])\n",
    "        logits: Float32[torch.Tensor, \"b t c\"] = model(tokens)\n",
    "\n",
    "        # print(f\"Logits:\\n{logits.shape}\")\n",
    "        loss = loss_fn(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if cfg.max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # more frequently than eval, print out train loss\n",
    "        # if (epoch % (cfg.eval_test_every_n // 10)) == 0:\n",
    "        #\n",
    "        #    print(f\"Epoch {epoch}, \" f\"Train loss: {loss.item():.6f}\")\n",
    "\n",
    "        # TODO(bschoen): Shouldn't you actually divide loss by batch size?\n",
    "        # TODO(bschoen): Do we want like an `is trial` (for example logging last one)\n",
    "        if (epoch % cfg.eval_test_every_n) == 0:\n",
    "\n",
    "            # skip evaluating test loss if we just started training\n",
    "            # if epoch == 0:\n",
    "            #    continue\n",
    "\n",
    "            print(\"Evaluating test loss...\")\n",
    "\n",
    "            # compute loss at each difficulty\n",
    "            test_loss_by_difficulty = {}\n",
    "\n",
    "            for difficulty, test_loader in cfg.test_loaders.items():\n",
    "\n",
    "                test_loss = evaluate_loss_on_test_batches(\n",
    "                    model,\n",
    "                    test_loader,\n",
    "                    max_batches=100,\n",
    "                )\n",
    "\n",
    "                test_loss_by_difficulty[difficulty] = test_loss\n",
    "\n",
    "            wandb_log_dict = {\"epoch\": epoch, \"train_loss\": loss.item()}\n",
    "\n",
    "            for difficulty, test_loss in test_loss_by_difficulty.items():\n",
    "\n",
    "                wandb_log_dict[f\"test_loss_difficulty_{difficulty}\"] = test_loss\n",
    "\n",
    "            # print_json(wandb_log_dict)\n",
    "\n",
    "            # Log metrics\n",
    "            wandb.log(wandb_log_dict, step=epoch)\n",
    "\n",
    "            # Compute attention pattern visualization\n",
    "            print(\"Computing attention pattern visualization...\")\n",
    "            model.eval()\n",
    "            test_example_string_to_cache = {}\n",
    "\n",
    "            for difficulty, input_tokens in test_example_per_difficulty.items():\n",
    "\n",
    "                logits, cache = model.run_with_cache(input_tokens)\n",
    "\n",
    "                # store example by using the actual text string as key\n",
    "                input_tokens_str = \"\".join(\n",
    "                    [tokenizer.decode([x.item()]) for x in input_tokens]\n",
    "                )\n",
    "\n",
    "                test_example_string_to_cache[input_tokens_str] = cache\n",
    "\n",
    "            image = generate_image_for_attention_patterns(\n",
    "                test_example_string_to_cache,\n",
    "                title=f\"Step: {epoch}\",\n",
    "            )\n",
    "\n",
    "            wandb.log(\n",
    "                {WandbConstants.ATTENTION_PATTERN_IMAGES: wandb.Image(image)},\n",
    "                step=epoch,\n",
    "            )\n",
    "\n",
    "            if cfg.wait_between_eval_s and cfg.wait_between_eval_s is not None:\n",
    "                print(\n",
    "                    f\"Sleeping for {cfg.wait_between_eval_s} to avoid wandb rate limiting\"\n",
    "                )\n",
    "                time.sleep(cfg.wait_between_eval_s)\n",
    "\n",
    "    # capture run name and id before `finish`\n",
    "    wandb_run_name = wandb.run.name\n",
    "    wandb_run_id = wandb.run.id\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # log locally to sanity check\n",
    "    # px.line(losses, labels={\"x\": \"Epoch\", \"y\": \"Train Loss\"})\n",
    "\n",
    "    print(f\"Final train loss: {loss.item():.6f}\")\n",
    "\n",
    "    # take model out of train\n",
    "    model.eval()\n",
    "\n",
    "    return TrainModelResult(\n",
    "        model=model,\n",
    "        train_loss=loss.item(),\n",
    "        wandb_run_name=wandb_run_name,\n",
    "        wandb_run_id=wandb_run_id,\n",
    "    )\n",
    "\n",
    "\n",
    "# train brief run to test code\n",
    "training_config = ModelAndTrainingConfig(\n",
    "    num_epochs=25000,\n",
    "    eval_test_every_n=1000,\n",
    "    weight_decay=0.1,\n",
    "    wait_between_eval_s=None,\n",
    "    train_loader=train_loader,\n",
    "    test_loaders=test_loaders,\n",
    ")\n",
    "\n",
    "result = train_model(training_config)\n",
    "\n",
    "# for compatibility with code later\n",
    "model = result.model\n",
    "cfg = training_config.get_hooked_transformer_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1d023",
   "metadata": {},
   "source": [
    "## Save Output Image To Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ad4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = download_images_from_run(result=result)\n",
    "\n",
    "gif_filepath = convert_pngs_in_directory_to_gif(output_dir=output_dir)\n",
    "\n",
    "print(gif_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81632d",
   "metadata": {},
   "source": [
    "## Transcoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp_activations(model, input_seq):\n",
    "    activations = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        layer = module.name\n",
    "        activations[layer] = output.detach().cpu().numpy()\n",
    "\n",
    "    # Register hooks for each MLP layer\n",
    "    for layer_num in range(model.cfg.n_layers):\n",
    "        layer_name = f\"blocks.{layer_num}.mlp\"\n",
    "        model.add_hook(layer_name, hook_fn, is_forward=True)\n",
    "\n",
    "    # Prepare input\n",
    "    input_ids = torch.tensor(dataset.encode(input_seq)).unsqueeze(\n",
    "        0\n",
    "    )  # Shape: [1, seq_len]\n",
    "    input_ids = input_ids.to(model.cfg.device)\n",
    "\n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_ids)\n",
    "\n",
    "    # Remove hooks\n",
    "    model.reset_hooks()\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad50b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2220158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "368b70a9",
   "metadata": {},
   "source": [
    "## Indirect Object Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e07efb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import circuitsvis as cv\n",
    "\n",
    "\n",
    "def add_batch_dimension(\n",
    "    x: Float32[torch.Tensor, \"...\"]\n",
    ") -> Float32[torch.Tensor, \"batch ...\"]:\n",
    "    return einops.rearrange(x, \"... -> 1 ...\")\n",
    "\n",
    "\n",
    "def tokenize_string(input_string: str) -> Float32[torch.Tensor, \"seq\"]:\n",
    "\n",
    "    tokens = tokenizer.encode(input_string)\n",
    "\n",
    "    return torch.tensor(tokens, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "def tokenize_string_as_batch(input_string: str) -> Float32[torch.Tensor, \"batch seq\"]:\n",
    "\n",
    "    return add_batch_dimension(tokenize_string(input_string))\n",
    "\n",
    "\n",
    "def get_first_mismatched_pair(\n",
    "    tokens_a: Float32[torch.Tensor, \"batch=1 seq\"],\n",
    "    tokens_b: Float32[torch.Tensor, \"batch=1 seq\"],\n",
    ") -> Float32[torch.Tensor, \"batch=1 2\"]:\n",
    "\n",
    "    assert tokens_a.shape == tokens_b.shape\n",
    "\n",
    "    for index in range(tokens_a.shape[-1]):\n",
    "\n",
    "        if tokens_a[0, index] != tokens_b[0, index]:\n",
    "\n",
    "            mismatch: Float32[torch.Tensor, \"2\"] = torch.tensor(\n",
    "                [\n",
    "                    tokens_a[0, index],\n",
    "                    tokens_b[0, index],\n",
    "                ]\n",
    "            ).to(device)\n",
    "\n",
    "            return add_batch_dimension(mismatch)\n",
    "\n",
    "\n",
    "# create a custom to_string function since using our own tokenizer\n",
    "def token_to_string(token: int) -> str:\n",
    "    return tokenizer.decode([token])\n",
    "\n",
    "\n",
    "# TODO(bschoen): Vary along things besides reversal\n",
    "\n",
    "# take an example, modify the first part of the sequence reversal to be wrong\n",
    "input_string = \"<za|\"\n",
    "correct_string = \"<za|a\"\n",
    "incorrect_string = \"<za|z\"\n",
    "\n",
    "input_string_tokens = tokenize_string_as_batch(input_string)\n",
    "correct_string_tokens = tokenize_string_as_batch(correct_string)\n",
    "incorrect_string_tokens = tokenize_string_as_batch(incorrect_string)\n",
    "\n",
    "logits, cache = model.run_with_cache(input_string_tokens)\n",
    "correct_logits, correct_cache = model.run_with_cache(correct_string_tokens)\n",
    "incorrect_logits, incorrect_cache = model.run_with_cache(incorrect_string_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89497e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-609db987-cdaf\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-609db987-cdaf\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<\", \"z\", \"a\", \"|\", \"a\"], \"topKLogProbs\": [[-3.116516590118408, -3.1783552169799805, -3.1798627376556396, -3.185877561569214, -3.190704345703125, -3.1915640830993652, -3.198836088180542, -3.2226147651672363, -3.223877191543579, -3.2266042232513428], [-3.1762592792510986, -3.182121515274048, -3.1868438720703125, -3.1884539127349854, -3.2100093364715576, -3.2160260677337646, -3.2191128730773926, -3.2198874950408936, -3.2221100330352783, -3.2222237586975098], [-3.130600929260254, -3.1596827507019043, -3.1863224506378174, -3.193516731262207, -3.2008450031280518, -3.2025668621063232, -3.204191207885742, -3.2059895992279053, -3.2111427783966064, -3.2149994373321533], [-0.0008057685336098075, -8.281250953674316, -8.626938819885254, -9.105446815490723, -9.441925048828125, -10.042155265808105, -10.06484317779541, -10.68904972076416, -10.711657524108887, -10.933487892150879]], \"topKTokens\": [[\"w\", \"l\", \"j\", \"y\", \"b\", \"d\", \"a\", \"f\", \"v\", \"h\"], [\"v\", \"y\", \"a\", \"w\", \"z\", \"t\", \"l\", \"j\", \"m\", \"h\"], [\"w\", \"v\", \"u\", \"m\", \"l\", \"i\", \"f\", \"y\", \"a\", \"b\"], [\"a\", \"y\", \"n\", \"h\", \"v\", \"e\", \"i\", \"m\", \"j\", \"f\"]], \"correctTokenRank\": [12, 2, 26, 0], \"correctTokenLogProb\": [-3.2317757606506348, -3.1868438720703125, -7.929058074951172, -0.0008057685336098075]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x4708157c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a11df59c-4dba\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a11df59c-4dba\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<\", \"z\", \"a\", \"|\", \"z\"], \"topKLogProbs\": [[-3.116516590118408, -3.1783552169799805, -3.1798627376556396, -3.185877561569214, -3.190704345703125, -3.1915640830993652, -3.198836088180542, -3.2226147651672363, -3.223877191543579, -3.2266042232513428], [-3.1762592792510986, -3.182121515274048, -3.1868438720703125, -3.1884539127349854, -3.2100093364715576, -3.2160260677337646, -3.2191128730773926, -3.2198874950408936, -3.2221100330352783, -3.2222237586975098], [-3.130600929260254, -3.1596827507019043, -3.1863224506378174, -3.193516731262207, -3.2008450031280518, -3.2025668621063232, -3.204191207885742, -3.2059895992279053, -3.2111427783966064, -3.2149994373321533], [-0.0008057685336098075, -8.281250953674316, -8.626938819885254, -9.105446815490723, -9.441925048828125, -10.042155265808105, -10.06484317779541, -10.68904972076416, -10.711657524108887, -10.933487892150879]], \"topKTokens\": [[\"w\", \"l\", \"j\", \"y\", \"b\", \"d\", \"a\", \"f\", \"v\", \"h\"], [\"v\", \"y\", \"a\", \"w\", \"z\", \"t\", \"l\", \"j\", \"m\", \"h\"], [\"w\", \"v\", \"u\", \"m\", \"l\", \"i\", \"f\", \"y\", \"a\", \"b\"], [\"a\", \"y\", \"n\", \"h\", \"v\", \"e\", \"i\", \"m\", \"j\", \"f\"]], \"correctTokenRank\": [12, 2, 26, 22], \"correctTokenLogProb\": [-3.2317757606506348, -3.1868438720703125, -7.929058074951172, -18.09477996826172]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x4673313d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    cv.logits.token_log_probs(\n",
    "        token_indices=correct_string_tokens,\n",
    "        log_probs=correct_logits.log_softmax(dim=-1),\n",
    "        to_string=token_to_string,\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    cv.logits.token_log_probs(\n",
    "        token_indices=incorrect_string_tokens,\n",
    "        log_probs=incorrect_logits.log_softmax(dim=-1),\n",
    "        to_string=token_to_string,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8a02553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_token: 2 (a)\n",
      "incorrect_token: 27 (z)\n"
     ]
    }
   ],
   "source": [
    "# position where we changed the sequence\n",
    "mismatch_position_index = 4\n",
    "\n",
    "correct_token = correct_string_tokens[0, mismatch_position_index].item()\n",
    "incorrect_token = incorrect_string_tokens[0, mismatch_position_index].item()\n",
    "\n",
    "print(f\"correct_token: {correct_token} ({tokenizer.decode([correct_token])})\")\n",
    "print(f\"incorrect_token: {incorrect_token} ({tokenizer.decode([incorrect_token])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8153f90",
   "metadata": {},
   "source": [
    "### Logit Difference In Accumulated Residual Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c66789ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_tokens.shape=torch.Size([1, 2])\n",
      "Answer residual directions shape: torch.Size([1, 2, 16])\n",
      "Logit difference directions shape: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "# get diff in format expected by `model.tokens_to_residual_directions`\n",
    "answer_tokens = get_first_mismatched_pair(\n",
    "    correct_string_tokens,\n",
    "    incorrect_string_tokens,\n",
    ")\n",
    "\n",
    "print(f\"{answer_tokens.shape=}\")\n",
    "\n",
    "# Float32[torch.Tensor, \"batch 2 d_model\"]\n",
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "# Float32[torch.Tensor, \"batch d_model\"]\n",
    "# Float32[torch.Tensor, \"batch d_model\"]\n",
    "correct_residual_directions, incorrect_residual_directions = (\n",
    "    answer_residual_directions.unbind(dim=1)\n",
    ")\n",
    "\n",
    "# Float32[torch.Tensor, \"batch d_model\"]\n",
    "logit_diff_directions = correct_residual_directions - incorrect_residual_directions\n",
    "\n",
    "print(f\"Logit difference directions shape:\", logit_diff_directions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27f609ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import transformer_lens_utils\n",
    "\n",
    "import transformer_lens.patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cf3365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per prompt logit difference: tensor([18.0940], device='mps:0', grad_fn=<SubBackward0>)\n",
      "Average logit difference: tensor(18.0940, device='mps:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "original_per_prompt_diff = transformer_lens_utils.logits_to_ave_logit_diff(\n",
    "    logits,\n",
    "    answer_tokens,\n",
    "    per_prompt=True,\n",
    ")\n",
    "print(\"Per prompt logit difference:\", original_per_prompt_diff)\n",
    "\n",
    "original_average_logit_diff = transformer_lens_utils.logits_to_ave_logit_diff(\n",
    "    logits,\n",
    "    answer_tokens,\n",
    ")\n",
    "print(\"Average logit difference:\", original_average_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7032a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed v.shape=torch.Size([1, 4, 16])\n",
      "hook_pos_embed v.shape=torch.Size([1, 4, 16])\n",
      "blocks.0.hook_resid_pre v.shape=torch.Size([1, 4, 16])\n",
      "blocks.0.ln1.hook_scale v.shape=torch.Size([1, 4, 1])\n",
      "blocks.0.ln1.hook_normalized v.shape=torch.Size([1, 4, 16])\n",
      "blocks.0.attn.hook_q v.shape=torch.Size([1, 4, 2, 8])\n",
      "blocks.0.attn.hook_k v.shape=torch.Size([1, 4, 2, 8])\n",
      "blocks.0.attn.hook_v v.shape=torch.Size([1, 4, 2, 8])\n",
      "blocks.0.attn.hook_attn_scores v.shape=torch.Size([1, 2, 4, 4])\n",
      "blocks.0.attn.hook_pattern v.shape=torch.Size([1, 2, 4, 4])\n",
      "blocks.0.attn.hook_z v.shape=torch.Size([1, 4, 2, 8])\n",
      "blocks.0.hook_attn_out v.shape=torch.Size([1, 4, 16])\n",
      "blocks.0.hook_resid_mid v.shape=torch.Size([1, 4, 16])\n",
      "blocks.0.ln2.hook_scale v.shape=torch.Size([1, 4, 1])\n",
      "blocks.0.ln2.hook_normalized v.shape=torch.Size([1, 4, 16])\n",
      "blocks.0.mlp.hook_pre v.shape=torch.Size([1, 4, 64])\n",
      "blocks.0.mlp.hook_post v.shape=torch.Size([1, 4, 64])\n",
      "blocks.0.hook_mlp_out v.shape=torch.Size([1, 4, 16])\n",
      "blocks.0.hook_resid_post v.shape=torch.Size([1, 4, 16])\n",
      "blocks.1.hook_resid_pre v.shape=torch.Size([1, 4, 16])\n",
      "blocks.1.ln1.hook_scale v.shape=torch.Size([1, 4, 1])\n",
      "blocks.1.ln1.hook_normalized v.shape=torch.Size([1, 4, 16])\n",
      "blocks.1.attn.hook_q v.shape=torch.Size([1, 4, 2, 8])\n",
      "blocks.1.attn.hook_k v.shape=torch.Size([1, 4, 2, 8])\n",
      "blocks.1.attn.hook_v v.shape=torch.Size([1, 4, 2, 8])\n",
      "blocks.1.attn.hook_attn_scores v.shape=torch.Size([1, 2, 4, 4])\n",
      "blocks.1.attn.hook_pattern v.shape=torch.Size([1, 2, 4, 4])\n",
      "blocks.1.attn.hook_z v.shape=torch.Size([1, 4, 2, 8])\n",
      "blocks.1.hook_attn_out v.shape=torch.Size([1, 4, 16])\n",
      "blocks.1.hook_resid_mid v.shape=torch.Size([1, 4, 16])\n",
      "blocks.1.ln2.hook_scale v.shape=torch.Size([1, 4, 1])\n",
      "blocks.1.ln2.hook_normalized v.shape=torch.Size([1, 4, 16])\n",
      "blocks.1.mlp.hook_pre v.shape=torch.Size([1, 4, 64])\n",
      "blocks.1.mlp.hook_post v.shape=torch.Size([1, 4, 64])\n",
      "blocks.1.hook_mlp_out v.shape=torch.Size([1, 4, 16])\n",
      "blocks.1.hook_resid_post v.shape=torch.Size([1, 4, 16])\n",
      "ln_final.hook_scale v.shape=torch.Size([1, 4, 1])\n",
      "ln_final.hook_normalized v.shape=torch.Size([1, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "for k, v in cache.items():\n",
    "    print(f\"{k} {v.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate layernorms as constants when propagating feature vectors backward\n",
    "# for theoretical motivation, see the LayerNorm section of\n",
    "# \thttps://www.neelnanda.io/mechanistic-interpretability/attribution-patching\n",
    "@torch.no_grad()\n",
    "def get_ln_constant(model, cache, vector, layer, token, is_ln2=False, recip=False):\n",
    "    x_act_name = (\n",
    "        transformer_lens.utils.get_act_name(\"resid_mid\", layer)\n",
    "        if is_ln2\n",
    "        else transformer_lens.utils.get_act_name(\"resid_pre\", layer)\n",
    "    )\n",
    "    x = cache[x_act_name][0, token]\n",
    "\n",
    "    y_act_name = get_act_name(\"normalized\", layer, \"ln2\" if is_ln2 else \"ln1\")\n",
    "    y = cache[y_act_name][0, token]\n",
    "\n",
    "    if torch.dot(vector, x) == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    return (\n",
    "        torch.dot(vector, y) / torch.dot(vector, x)\n",
    "        if not recip\n",
    "        else torch.dot(vector, x) / torch.dot(vector, y)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "552c84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float32[torch.Tensor, \"... batch d_model\"],\n",
    "    cache: transformer_lens.ActivationCache,\n",
    "    logit_diff_directions: Float[torch.Tensor, \"batch d_model\"],\n",
    ") -> Float32[torch.Tensor, \"...\"]:\n",
    "    \"\"\"\n",
    "    Gets the avg logit difference between the correct and incorrect answer for a given\n",
    "    stack of components in the residual stream.\n",
    "    \"\"\"\n",
    "    # SOLUTION\n",
    "    batch_size = residual_stack.size(-2)\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
    "        residual_stack,\n",
    "        layer=-1,\n",
    "        pos_slice=-1,\n",
    "    )\n",
    "    return (\n",
    "        einops.einsum(\n",
    "            scaled_residual_stack,\n",
    "            logit_diff_directions,\n",
    "            \"... batch d_model, batch d_model -> ...\",\n",
    "        )\n",
    "        / batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44018d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final residual stream shape: torch.Size([1, 4, 16])\n",
      "Calculated average logit diff: 18.1108570099\n",
      "Original logit difference:     18.0939750671\n"
     ]
    }
   ],
   "source": [
    "# we expected residual stream patching near the final layer to work near perfectly,\n",
    "# since it was logit focused and thus basically linear, but turns out that\n",
    "# LayerNorm completely breaks things.\n",
    "\n",
    "# note: the fact that we had to use `ln_final.hook_normalized` instead of `resid_post`\n",
    "#       means that center_writing_weights is needed\n",
    "\n",
    "# final_residual_stream = cache[\"resid_post\", -1]  # [batch seq d_model]\n",
    "final_residual_stream = cache[\"ln_final.hook_normalized\"]  # [batch seq d_model]\n",
    "print(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]  # [batch d_model]\n",
    "\n",
    "# Apply LayerNorm scaling (to just the final sequence position)\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "# scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "#     final_token_residual_stream,\n",
    "#     layer=-1,\n",
    "#     pos_slice=-1,\n",
    "# )\n",
    "scaled_final_token_residual_stream = final_token_residual_stream\n",
    "\n",
    "batch_size = input_string_tokens.shape[0]\n",
    "\n",
    "average_logit_diff = (\n",
    "    einops.einsum(\n",
    "        scaled_final_token_residual_stream,\n",
    "        logit_diff_directions,\n",
    "        \"batch d_model, batch d_model ->\",\n",
    "    )\n",
    "    / batch_size\n",
    ")\n",
    "\n",
    "print(\"Note: These should be close!\")\n",
    "print(f\"Calculated average logit diff: {average_logit_diff:.10f}\")\n",
    "print(f\"Original logit difference:     {original_average_logit_diff:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bd8acba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: These should be close!\n",
      "Calculated average logit diff: 297.6476745605\n",
      "Original logit difference:     18.0939750671\n"
     ]
    }
   ],
   "source": [
    "result = residual_stack_to_logit_diff(\n",
    "    final_token_residual_stream,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")\n",
    "\n",
    "print(\"Note: These should be close!\")\n",
    "print(f\"Calculated average logit diff: {result:.10f}\")\n",
    "print(f\"Original logit difference:     {original_average_logit_diff:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ba3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b6a1285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_token_accum.shape=torch.Size([3, 16])\n"
     ]
    }
   ],
   "source": [
    "accum_resid, labels = cache.accumulated_resid(return_labels=True, apply_ln=True)\n",
    "last_token_accum = accum_resid[:, 0, -1, :]  # layer, batch, pos, d_model\n",
    "print(f\"{last_token_accum.shape=}\")  # layer, batch, d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a89edabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_U.shape=torch.Size([16, 29])\n"
     ]
    }
   ],
   "source": [
    "W_U = model.W_U\n",
    "print(f\"{W_U.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af69d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers_unembedded.shape=torch.Size([3, 29])\n"
     ]
    }
   ],
   "source": [
    "layers_unembedded = einops.einsum(\n",
    "    last_token_accum,\n",
    "    W_U,\n",
    "    \"layer d_model, d_model d_vocab -> layer d_vocab\",\n",
    ")\n",
    "\n",
    "print(f\"{layers_unembedded.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bebb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens_logit_diffs: Float32[torch.Tensor, \"...\"] = residual_stack_to_logit_diff(\n",
    "    accum_resid,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")  # [component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95b904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a21fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc02eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import plotly_utils\n",
    "\n",
    "\n",
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1,\n",
    "    incl_mid=True,\n",
    "    pos_slice=-1,\n",
    "    return_labels=True,\n",
    ")\n",
    "# accumulated_residual has shape (component, batch, d_model)\n",
    "\n",
    "logit_lens_logit_diffs: Float32[torch.Tensor, \"...\"] = residual_stack_to_logit_diff(\n",
    "    accumulated_residual,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")  # [component]\n",
    "\n",
    "plotly_utils.line(\n",
    "    logit_lens_logit_diffs,\n",
    "    hovermode=\"x unified\",\n",
    "    title=\"Logit Difference From Accumulated Residual Stream\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    xaxis_tickvals=labels,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d73f1e",
   "metadata": {},
   "source": [
    "### Logit Difference From Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(\n",
    "    per_layer_residual,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")\n",
    "\n",
    "plotly_utils.line(\n",
    "    per_layer_logit_diffs,\n",
    "    hovermode=\"x unified\",\n",
    "    title=\"Logit Difference From Each Layer\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    xaxis_tickvals=labels,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2515dba",
   "metadata": {},
   "source": [
    "### Logit Difference From Each Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_head_residual = einops.rearrange(\n",
    "    per_head_residual,\n",
    "    \"(layer head) ... -> layer head ...\",\n",
    "    layer=model.cfg.n_layers,\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(\n",
    "    per_head_residual,\n",
    "    cache,\n",
    "    logit_diff_directions,\n",
    ")\n",
    "\n",
    "plotly_utils.imshow(\n",
    "    per_head_logit_diffs,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    width=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9eec6d",
   "metadata": {},
   "source": [
    "### Highest Value Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.core.display\n",
    "import IPython.display\n",
    "\n",
    "\n",
    "def topk_of_Nd_tensor(\n",
    "    tensor: Float[torch.Tensor, \"rows cols\"],\n",
    "    k: int,\n",
    ") -> list[tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Helper function: does same as tensor.topk(k).indices, but works over 2D tensors.\n",
    "    Returns a list of indices, i.e. shape [k, tensor.ndim].\n",
    "\n",
    "    Example: if tensor is 2D array of values for each head in each layer, this will\n",
    "    return a list of heads.\n",
    "    \"\"\"\n",
    "    i = torch.topk(tensor.flatten(), k).indices\n",
    "    return np.array(\n",
    "        np.unravel_index(\n",
    "            transformer_lens.utils.to_numpy(i),\n",
    "            tensor.shape,\n",
    "        )\n",
    "    ).T.tolist()\n",
    "\n",
    "\n",
    "k = 3\n",
    "\n",
    "for head_type in [\"Positive\", \"Negative\"]:\n",
    "\n",
    "    # Get the heads with largest (or smallest) contribution to the logit difference\n",
    "    top_heads = topk_of_Nd_tensor(\n",
    "        per_head_logit_diffs.cpu() * (1 if head_type == \"Positive\" else -1), k\n",
    "    )\n",
    "\n",
    "    # ex: [[0, 1], [1, 0], [0, 0]]\n",
    "    print(top_heads)\n",
    "\n",
    "    # Get all their attention patterns\n",
    "    attn_patterns_for_important_heads: Float[torch.Tensor, \"head q k\"] = torch.stack(\n",
    "        [cache[\"pattern\", layer][:, head][0] for layer, head in top_heads]\n",
    "    )\n",
    "\n",
    "    print(f\"{attn_patterns_for_important_heads.shape=}\")\n",
    "\n",
    "    # Display results\n",
    "    display(\n",
    "        cv.attention.attention_heads(\n",
    "            attention=attn_patterns_for_important_heads,\n",
    "            tokens=[x for x in input_string],\n",
    "            attention_head_names=[f\"{layer}.{head}\" for layer, head in top_heads],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de5552",
   "metadata": {},
   "source": [
    "### Activation Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(bschoen): Clean and corrupted should actually switch first, should do this for search\n",
    "clean_logit_diff = logits_to_ave_logit_diff(correct_logits, answer_tokens)\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = logits_to_ave_logit_diff(incorrect_logits, answer_tokens)\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ebf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.attention.attention_heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a63158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76f240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c26324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "705bcb63",
   "metadata": {},
   "source": [
    "## Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df666bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "# TODO(bschoen): Do need to use lightning if want to do this generally\n",
    "# note: generally do want to iterate on this part itself, i.e. once find promising learning rate, searching other hyperparameters\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    # TODO(bschoen): up to one per position, eh might as well try it\n",
    "\n",
    "    d_model = trial.suggest_categorical(\"d_model\", [8, 16, 32, 64, 128])\n",
    "    n_heads = trial.suggest_int(\"n_heads\", 1, 8)\n",
    "\n",
    "    cfg = ModelAndTrainingConfig(\n",
    "        num_epochs=1000,\n",
    "        eval_test_every_n=10000,  # not worth evaluating test loss for study\n",
    "        n_layers=1,  # trial.suggest_int(\"n_layers\", 1, 2),\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        learning_rate=5e-4,\n",
    "    )\n",
    "\n",
    "    # sanity check `d_heads`\n",
    "    if (cfg.d_model % cfg.n_heads) != 0:\n",
    "        print(f\"Pruning trial for {cfg.d_model=} {cfg.n_heads=}\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    result = train_model(cfg)\n",
    "\n",
    "    return result.train_loss\n",
    "\n",
    "\n",
    "enable_optuna = False\n",
    "\n",
    "if enable_optuna:\n",
    "\n",
    "    study_storage_url = \"sqlite:///toy-problem-hooked-transformer.db\"\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        directions=[optuna.study.StudyDirection.MINIMIZE],\n",
    "        storage=study_storage_url,\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    print(\"View by launching optuna dashboard from the command line:\")\n",
    "    print(f\"optuna-dashboard {study_storage_url}\")\n",
    "\n",
    "    # now let's do a real run\n",
    "    training_config = ModelAndTrainingConfig(\n",
    "        num_epochs=10000,\n",
    "        eval_test_every_n=1000,\n",
    "        n_layers=1,\n",
    "        d_model=16,\n",
    "        n_heads=1,\n",
    "    )\n",
    "\n",
    "    result = train_model(cfg=training_config)\n",
    "\n",
    "    # for compatibility with code later\n",
    "    model = result.model\n",
    "    cfg = training_config.get_hooked_transformer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some example output\n",
    "import circuitsvis as cv\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "def visualize_pattern_hook(\n",
    "    pattern: Float32[torch.Tensor, \"batch head_index dest_pos source_pos\"],\n",
    "    hook: transformer_lens.hook_points.HookPoint,\n",
    "    tokens_as_strings: list[str],\n",
    ") -> None:\n",
    "    print(f\"Batch size: {pattern.shape[0]}\")\n",
    "    print(\"Layer: \", hook.layer())\n",
    "    display(\n",
    "        cv.attention.attention_patterns(\n",
    "            tokens=tokens_as_strings, attention=pattern.mean(0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "test_input_string_to_cache = {}\n",
    "\n",
    "for difficulty, test_loader in test_loaders.items():\n",
    "\n",
    "    print(difficulty)\n",
    "\n",
    "    # grab something from the test batch\n",
    "    example_batch = next(iter(test_loader))\n",
    "\n",
    "    x, y = example_batch\n",
    "\n",
    "    example_sample = x[0]\n",
    "\n",
    "    # example_sample = torch.tensor(tokenizer.encode(\"<az|za|az>>>>>>>>>>\"))\n",
    "\n",
    "    # grab the first part of it, ex: `<abc|`\n",
    "    example_prompt = example_sample  # [:8]\n",
    "\n",
    "    example_prompt = example_prompt.to(device)\n",
    "\n",
    "    print(f\"Using {example_prompt} from {example_sample} (from test set)\")\n",
    "\n",
    "    # note: already encoded\n",
    "    input_tokens = example_prompt\n",
    "\n",
    "    # first let's get these as strings so can easily work with them\n",
    "    input_tokens_as_strings = [token_to_string(x.item()) for x in input_tokens]\n",
    "\n",
    "    # wrap to bind input tokens\n",
    "    visualize_pattern_hook_fn = functools.partial(\n",
    "        visualize_pattern_hook, tokens_as_strings=input_tokens_as_strings\n",
    "    )\n",
    "\n",
    "    model.run_with_hooks(\n",
    "        input_tokens,\n",
    "        return_type=None,  # For efficiency, we don't need to calculate the logits\n",
    "        fwd_hooks=[(lambda name: name.endswith(\"pattern\"), visualize_pattern_hook_fn)],\n",
    "    )\n",
    "\n",
    "    logits_batch, cache = model.run_with_cache(input_tokens)\n",
    "\n",
    "    # store so can plot together later\n",
    "    test_input_string_to_cache[\"\".join(input_tokens_as_strings)] = cache\n",
    "\n",
    "    logits = logits_batch[0]\n",
    "\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "    cv.logits.token_log_probs(\n",
    "        token_indices=input_tokens,\n",
    "        log_probs=log_probs,\n",
    "        to_string=token_to_string,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.apply_ln_to_stack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c96b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.stack_head_results??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b78938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.patching\n",
    "\n",
    "transformer_lens.patching.get_act_patch_resid_pre??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2f20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b41fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "def logit_attribution(\n",
    "    embed: Float32[torch.Tensor, \"seq d_model\"],\n",
    "    l1_results: Float32[torch.Tensor, \"seq nheads d_model\"],\n",
    "    l2_results: Float32[torch.Tensor, \"seq nheads d_model\"],\n",
    "    W_U: Float32[torch.Tensor, \"d_model d_vocab\"],\n",
    "    tokens: Int64[torch.Tensor, \"seq\"],\n",
    ") -> Float32[torch.Tensor, \"seq-1 n_components\"]:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        embed: the embeddings of the tokens (i.e. token + position embeddings)\n",
    "        l1_results: the outputs of the attention heads at layer 1 (with head as one of the dimensions)\n",
    "        l2_results: the outputs of the attention heads at layer 2 (with head as one of the dimensions)\n",
    "        W_U: the unembedding matrix\n",
    "        tokens: the token ids of the sequence\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len-1, n_components)\n",
    "        represents the concatenation (along dim=-1) of logit attributions from:\n",
    "            the direct path (seq-1,1)\n",
    "            layer 0 logits (seq-1, n_heads)\n",
    "            layer 1 logits (seq-1, n_heads)\n",
    "        so n_components = 1 + 2*n_heads\n",
    "    \"\"\"\n",
    "    W_U_correct_tokens = W_U[:, tokens[1:]]\n",
    "    # SOLUTION\n",
    "    direct_attributions = einops.einsum(\n",
    "        W_U_correct_tokens, embed[:-1], \"emb seq, seq emb -> seq\"\n",
    "    )\n",
    "    l1_attributions = einops.einsum(\n",
    "        W_U_correct_tokens, l1_results[:-1], \"emb seq, seq nhead emb -> seq nhead\"\n",
    "    )\n",
    "    l2_attributions = einops.einsum(\n",
    "        W_U_correct_tokens, l2_results[:-1], \"emb seq, seq nhead emb -> seq nhead\"\n",
    "    )\n",
    "    return torch.concat(\n",
    "        [direct_attributions.unsqueeze(-1), l1_attributions, l2_attributions], dim=-1\n",
    "    )\n",
    "\n",
    "\n",
    "logits, cache = model.run_with_cache(input_tokens, remove_batch_dim=True)\n",
    "str_tokens = input_tokens_as_strings\n",
    "tokens = input_tokens\n",
    "\n",
    "with t.inference_mode():\n",
    "    embed = cache[\"embed\"]\n",
    "    l1_results = cache[\"result\", 0]\n",
    "    l2_results = cache[\"result\", 1]\n",
    "    logit_attr = logit_attribution(\n",
    "        embed,\n",
    "        l1_results,\n",
    "        l2_results,\n",
    "        model.W_U,\n",
    "        tokens[0],\n",
    "    )\n",
    "\n",
    "    # Uses fancy indexing to get a len(tokens[0])-1 length tensor, where the kth entry is the predicted logit for the correct k+1th token\n",
    "    correct_token_logits = logits[0, torch.arange(len(tokens[0]) - 1), tokens[0, 1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13997a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a544cede",
   "metadata": {},
   "source": [
    "## Looking at it with CircuitsViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before even going to SAE, let's look at circuitsviz here\n",
    "import circuitsvis as cv\n",
    "\n",
    "import circuitsvis.activations\n",
    "import circuitsvis.attention\n",
    "import circuitsvis.logits\n",
    "import circuitsvis.tokens\n",
    "import circuitsvis.topk_samples\n",
    "import circuitsvis.topk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b587d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see what we have\n",
    "import tabulate\n",
    "\n",
    "print(f\"{len(input_tokens)=}\")\n",
    "\n",
    "# show the first few elements of the `HookedTransformerConfig`, since that has things like `d_model`, num heads, etc\n",
    "print(tabulate.tabulate([(k, v) for k, v in cfg.__dict__.items()][:10]))\n",
    "\n",
    "print(tabulate.tabulate([(k, v.shape) for k, v in cache.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519db727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable Bokeh output in the notebook\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "def tensor_to_dataframe(\n",
    "    tensor: torch.Tensor, labels: list[str], tokens: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a 2D PyTorch tensor to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): A 2D tensor to convert.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame representation of the input tensor.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input tensor is not 2D.\n",
    "    \"\"\"\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(f\"Input tensor must be 2D, got {tensor.dim()}D\")\n",
    "    if len(labels) != 2:\n",
    "        raise ValueError(f\"Expected labels for both dimensions, got {len(labels)}\")\n",
    "\n",
    "    # Convert tensor to numpy array\n",
    "    numpy_array = tensor.detach().cpu().numpy()\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(numpy_array)\n",
    "\n",
    "    # Name the index the first label\n",
    "    df.index.name = labels[0]\n",
    "\n",
    "    # Name the columns the second label\n",
    "    df.columns = [f\"{labels[1]}_{i}\" for i in range(numpy_array.shape[1])]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def visualize_tensor_heatmap(\n",
    "    tensor: torch.Tensor,\n",
    "    title: str = \"Tensor Heatmap\",\n",
    "    colormap: list[str] = Viridis256,\n",
    "    width: int = 800,\n",
    "    height: int = 400,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize a 2D tensor as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): A 2D tensor to visualize.\n",
    "        title (str): Title of the heatmap.\n",
    "        colormap (List[str]): A list of colors to use for the heatmap.\n",
    "        width (int): Width of the plot in pixels.\n",
    "        height (int): Height of the plot in pixels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure tensor is 2D\n",
    "    if tensor.dim() != 2:\n",
    "        raise ValueError(f\"Input tensor must be 2D, got {tensor.shape}\")\n",
    "\n",
    "    # convert tensor to dataframe\n",
    "    df = tensor_to_dataframe(tensor)\n",
    "\n",
    "    # Create a 2D grid of coordinates\n",
    "    y, x = np.mgrid[0 : data.shape[0], 0 : data.shape[1]]\n",
    "\n",
    "    # Flatten the arrays\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    z = data.flatten()\n",
    "\n",
    "    # Create a ColumnDataSource\n",
    "    source = ColumnDataSource(\n",
    "        data=dict(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            color=Viridis256[:: int(256 / len(z))][: len(z)],  # Map values to colors\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    p = figure(\n",
    "        title=\"Tensor Heatmap\",\n",
    "        x_range=(0, data.shape[1]),\n",
    "        y_range=(0, data.shape[0]),\n",
    "        toolbar_location=\"below\",\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset\",\n",
    "    )\n",
    "\n",
    "    # Add rectangular glyphs\n",
    "    p.rect(\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        width=1,\n",
    "        height=1,\n",
    "        source=source,\n",
    "        fill_color=\"color\",\n",
    "        line_color=None,\n",
    "    )\n",
    "\n",
    "    # Add hover tool\n",
    "    hover = HoverTool(tooltips=[(\"x\", \"@x\"), (\"y\", \"@y\"), (\"value\", \"@z{0.000}\")])\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    # Invert y-axis to match tensor indexing\n",
    "    p.y_range.start, p.y_range.end = p.y_range.end, p.y_range.start\n",
    "\n",
    "    # Show the plot\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate.tabulate([(k, v[0].shape) for k, v in cache.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ad382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go ahead and just use first batch\n",
    "def first_batch(tensor: Float32[torch.Tensor, \"b t c\"]) -> Float32[torch.Tensor, \"t c\"]:\n",
    "    return tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb2343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c2ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea0d0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from typing import Iterable, TypeVar\n",
    "\n",
    "import tabulate\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "# alias for `print(tabulate.tabulate(data))`\n",
    "def print_table(data: T) -> None:\n",
    "    print(tabulate.tabulate(data))\n",
    "\n",
    "\n",
    "# Define a function to print module weights recursively\n",
    "def print_module_weights(module: nn.Module) -> Iterable[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Recursively prints the weights of a PyTorch module and its submodules.\n",
    "\n",
    "    This function traverses through the module hierarchy, printing information\n",
    "    about parameters that require gradients and are not hook-related.\n",
    "\n",
    "    Example:\n",
    "        >>> print_table(print_module_weights(model))\n",
    "\n",
    "        ------------------  ----------------------\n",
    "        embed.W_E           torch.Size([29, 14])\n",
    "        pos_embed.W_pos     torch.Size([9, 14])\n",
    "        blocks.0.ln1.w      torch.Size([14])\n",
    "        blocks.0.ln1.b      torch.Size([14])\n",
    "        blocks.0.ln2.w      torch.Size([14])\n",
    "        blocks.0.ln2.b      torch.Size([14])\n",
    "        blocks.0.attn.W_Q   torch.Size([3, 14, 4])\n",
    "        blocks.0.attn.W_O   torch.Size([3, 4, 14])\n",
    "        blocks.0.attn.b_Q   torch.Size([3, 4])\n",
    "        blocks.0.attn.b_O   torch.Size([14])\n",
    "        blocks.0.attn.W_K   torch.Size([3, 14, 4])\n",
    "        blocks.0.attn.W_V   torch.Size([3, 14, 4])\n",
    "        blocks.0.attn.b_K   torch.Size([3, 4])\n",
    "        blocks.0.attn.b_V   torch.Size([3, 4])\n",
    "        blocks.0.mlp.W_in   torch.Size([14, 56])\n",
    "        blocks.0.mlp.b_in   torch.Size([56])\n",
    "        blocks.0.mlp.W_out  torch.Size([56, 14])\n",
    "        blocks.0.mlp.b_out  torch.Size([14])\n",
    "        ln_final.w          torch.Size([14])\n",
    "        ln_final.b          torch.Size([14])\n",
    "        unembed.W_U         torch.Size([14, 29])\n",
    "        unembed.b_U         torch.Size([29])\n",
    "        ------------------  ----------------------\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): The PyTorch module to inspect.\n",
    "        prefix (str, optional): A string prefix for indentation in the output.\n",
    "                                Defaults to an empty string.\n",
    "\n",
    "    Returns:\n",
    "        Iterable[tuple[str, str]]: A list of tuples, where each tuple contains\n",
    "            the name and shape of the parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through named parameters of the module\n",
    "    for name, param in module.named_parameters():\n",
    "\n",
    "        # Check if parameter requires gradient and doesn't start with 'hook_'\n",
    "        if param.requires_grad and not name.startswith(\"hook_\"):\n",
    "\n",
    "            # yield parameter name and type\n",
    "            yield f\"{name}\", f\"{param.shape}\"\n",
    "\n",
    "\n",
    "def print_cache(cache: transformer_lens.ActivationCache) -> None:\n",
    "    print(tabulate.tabulate([(k, v[0].shape) for k, v in cache.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b1c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weights in the model:\")\n",
    "print_table(print_module_weights(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80fb6a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached activations:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cache' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCached activations:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m print_cache(\u001b[43mcache\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cache' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Cached activations:\")\n",
    "print_cache(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6910da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cache_activation(\n",
    "    cache: transformer_lens.ActivationCache,\n",
    "    cache_key: str,\n",
    "    input_tokens_as_strings: list[str],\n",
    ") -> None:\n",
    "\n",
    "    activations = first_batch(cache[cache_key])\n",
    "\n",
    "    figsize = (4, 4)\n",
    "\n",
    "    # make figure smaller for vectors\n",
    "    if activations.shape[-1] == 1:\n",
    "        figsize = (4, 1.5)\n",
    "\n",
    "    # for larger activations like MLP, allow it to be taller\n",
    "    elif activations.shape[-1] > 20:\n",
    "        figsize = (4, 12)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    sns.heatmap(\n",
    "        activations.cpu().numpy().T,\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "        xticklabels=input_tokens_as_strings,\n",
    "    )\n",
    "\n",
    "    plt.title(cache_key)\n",
    "\n",
    "    # TODO(bschoen): Allow specifying this\n",
    "    #\n",
    "    plt.ylabel(\"Embedding Dimension\")\n",
    "    plt.xlabel(\"Token\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for cache_key in [\n",
    "    \"hook_embed\",\n",
    "    \"hook_pos_embed\",\n",
    "    \"blocks.0.hook_resid_pre\",\n",
    "    \"blocks.0.ln1.hook_scale\",\n",
    "    \"blocks.0.ln1.hook_normalized\",\n",
    "    \"blocks.0.hook_attn_out\",\n",
    "    \"blocks.0.hook_resid_mid\",\n",
    "    \"blocks.0.ln2.hook_scale\",\n",
    "    \"blocks.0.ln2.hook_normalized\",\n",
    "    \"blocks.0.mlp.hook_pre\",\n",
    "    \"blocks.0.mlp.hook_post\",\n",
    "    \"blocks.0.hook_mlp_out\",\n",
    "    \"blocks.0.hook_resid_post\",\n",
    "    \"ln_final.hook_scale\",\n",
    "    \"ln_final.hook_normalized\",\n",
    "]:\n",
    "\n",
    "    plot_cache_activation(\n",
    "        cache=cache,\n",
    "        cache_key=cache_key,\n",
    "        input_tokens_as_strings=input_tokens_as_strings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize MLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_mlp_weights_and_biases(model):\n",
    "    # Function to plot heatmaps for MLP weights and biases\n",
    "\n",
    "    def plot_weight_bias_pair(weight, bias, title):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        sns.heatmap(weight.detach().cpu().numpy(), ax=ax1, cmap=\"coolwarm\", center=0)\n",
    "        ax1.set_title(f\"{title} - Weights\")\n",
    "        ax1.set_xlabel(\"Output dimension\")\n",
    "        ax1.set_ylabel(\"Input dimension\")\n",
    "\n",
    "        sns.heatmap(\n",
    "            bias.detach().cpu().numpy().reshape(-1, 1),\n",
    "            ax=ax2,\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "        )\n",
    "        ax2.set_title(f\"{title} - Biases\")\n",
    "        ax2.set_xlabel(\"Bias\")\n",
    "        ax2.set_ylabel(\"Dimension\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # MLP weights and biases\n",
    "    plot_weight_bias_pair(\n",
    "        model.blocks[0].mlp.W_in, model.blocks[0].mlp.b_in, \"MLP Input\"\n",
    "    )\n",
    "    plot_weight_bias_pair(\n",
    "        model.blocks[0].mlp.W_out, model.blocks[0].mlp.b_out, \"MLP Output\"\n",
    "    )\n",
    "\n",
    "    # Layer Norm final\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(\n",
    "        model.ln_final.w.detach().cpu().numpy().reshape(1, -1),\n",
    "        cmap=\"coolwarm\",\n",
    "        center=1,\n",
    "    )\n",
    "    plt.title(\"Layer Norm Final - Weights\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(\n",
    "        model.ln_final.b.detach().cpu().numpy().reshape(1, -1),\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "    )\n",
    "    plt.title(\"Layer Norm Final - Biases\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Unembed\n",
    "    plot_weight_bias_pair(model.unembed.W_U, model.unembed.b_U, \"Unembed\")\n",
    "\n",
    "\n",
    "# Call the function\n",
    "plot_mlp_weights_and_biases(model)\n",
    "\n",
    "# Comment: Additional visualizations that could be useful:\n",
    "# 1. Histograms of weight/bias distributions\n",
    "# 2. 3D surface plots for weights to show patterns\n",
    "# 3. Network architecture diagram with weight magnitudes represented by line thickness\n",
    "# 4. Animated heatmaps showing weight changes during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_bias_activation(\n",
    "    weight,\n",
    "    bias,\n",
    "    activation,\n",
    "    title: str,\n",
    ") -> None:\n",
    "\n",
    "    activation = first_batch(activation)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "    sns.heatmap(weight.detach().cpu().numpy().T, ax=ax1, cmap=\"coolwarm\", center=0)\n",
    "    ax1.set_title(f\"{title} - Weight\")\n",
    "\n",
    "    sns.barplot(x=list(range(len(bias))), y=bias.detach().cpu().numpy(), ax=ax2)\n",
    "    ax2.set_title(f\"{title} - Bias\")\n",
    "    ax2.set_xlabel(\"Index\")\n",
    "    ax2.set_ylabel(\"Value\")\n",
    "\n",
    "    sns.heatmap(activation.detach().cpu().numpy().T, ax=ax3, cmap=\"coolwarm\", center=0)\n",
    "    ax3.set_title(f\"{title} - Activation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_weight_bias_activation(\n",
    "    model.embed.W_E,\n",
    "    torch.zeros(model.embed.W_E.shape[1]),\n",
    "    cache[\"hook_embed\"],\n",
    "    \"Embedding\",\n",
    ")\n",
    "plot_weight_bias_activation(\n",
    "    model.pos_embed.W_pos,\n",
    "    torch.zeros(model.pos_embed.W_pos.shape[1]),\n",
    "    cache[\"hook_pos_embed\"],\n",
    "    \"Positional Embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(bschoen): Hook residual pre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8937bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting LayerNorm components\n",
    "\n",
    "\n",
    "def plot_layernorm(scale, normalized, title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    scale = first_batch(scale)\n",
    "    normalized = first_batch(normalized)\n",
    "\n",
    "    sns.barplot(\n",
    "        x=list(range(len(scale))), y=scale.squeeze().detach().cpu().numpy(), ax=ax1\n",
    "    )\n",
    "    ax1.set_title(f\"{title} - Scale\")\n",
    "    ax1.set_xlabel(\"Index\")\n",
    "    ax1.set_ylabel(\"Value\")\n",
    "\n",
    "    sns.heatmap(normalized.detach().cpu().numpy().T, ax=ax2, cmap=\"coolwarm\", center=0)\n",
    "    ax2.set_title(f\"{title} - Normalized\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_layernorm(\n",
    "    cache[\"blocks.0.ln1.hook_scale\"],\n",
    "    cache[\"blocks.0.ln1.hook_normalized\"],\n",
    "    \"LayerNorm 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting MLP components\n",
    "plot_weight_bias_activation(\n",
    "    model.blocks[0].mlp.W_in,\n",
    "    model.blocks[0].mlp.b_in,\n",
    "    cache[\"blocks.0.mlp.hook_pre\"],\n",
    "    \"MLP Input\",\n",
    ")\n",
    "plot_weight_bias_activation(\n",
    "    model.blocks[0].mlp.W_out,\n",
    "    model.blocks[0].mlp.b_out,\n",
    "    cache[\"blocks.0.mlp.hook_post\"],\n",
    "    \"MLP Output\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdf33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c51ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ff7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2a5f4f8",
   "metadata": {},
   "source": [
    "#### circuitsvis.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557cc70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens := List of tokens if single sample (e.g. `[\"A\", \"person\"]`) or list of lists of tokens (e.g. `[[[\"A\", \"person\"], [\"is\", \"walking\"]]]`)\n",
    "# activations := Activations of the shape [tokens x layers x neurons] if single sample or list of [tokens x layers x neurons] if multiple samples\n",
    "\n",
    "# take first batch for now\n",
    "activations = cache[\"blocks.0.hook_mlp_out\"][0]\n",
    "print(f\"{activations.shape=}\")\n",
    "\n",
    "# reshape [tokens x neurons] -> [tokens x 1 x neurons]\n",
    "#  - `-1` means to automatically infer the size of the last dimension\n",
    "activations_view = activations.view(len(input_tokens), cfg.n_layers, -1)\n",
    "\n",
    "print(f\"{activations_view.shape=}\")\n",
    "\n",
    "# convert to strings (which this function expects)\n",
    "input_tokens_as_strings = [token_to_string(x.item()) for x in input_tokens]\n",
    "\n",
    "# TODO(bschoen): Is there a way to essentially stack these? Claude can probably give the React for that\n",
    "\n",
    "# so here we can visualize activations for a `torch.Size([1, 8, 16])`, which is most\n",
    "# of them since this is the size of the embedding dimension\n",
    "circuitsvis.activations.text_neuron_activations(\n",
    "    tokens=[token_to_string(x.item()) for x in input_tokens],\n",
    "    activations=activations_view,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f7aa2",
   "metadata": {},
   "source": [
    "#### circuitsvis.attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ba9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens: List of tokens (e.g. `[\"A\", \"person\"]`). Must be the same length as the list of values.\n",
    "# attention: Attention head activations of the shape [dest_tokens x src_tokens]\n",
    "# max_value: Maximum value. Used to determine how dark the token color is when positive (i.e. based on how close it is to the maximum value).\n",
    "# min_value: Minimum value. Used to determine how dark the token color is when negative (i.e. based on how close it is to the minimum value).\n",
    "# negative_color: Color for negative values\n",
    "# positive_color: Color for positive values.\n",
    "# show_axis_labels: Whether to show axis labels.\n",
    "# mask_upper_tri: Whether or not to mask the upper triangular portion of the attention patterns. Should be true for causal attention, false for bidirectional attention.\n",
    "\n",
    "\n",
    "# take first batch\n",
    "# ex: torch.Size([4, 8, 8]) -> [n_heads, n_ctx, n_ctx]\n",
    "# note: `blocks.0.attn.hook_attn_scores` is too early (not normalized?)\n",
    "attention = cache[\"blocks.0.attn.hook_pattern\"][0]\n",
    "\n",
    "print(f\"{attention.shape=}\")\n",
    "\n",
    "circuitsvis.attention.attention_heads(\n",
    "    tokens=input_tokens_as_strings,\n",
    "    attention=attention,\n",
    "    max_value=1,\n",
    "    min_value=-1,\n",
    "    negative_color=\"blue\",\n",
    "    positive_color=\"red\",\n",
    "    mask_upper_tri=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d512f",
   "metadata": {},
   "source": [
    "#### circuitsvis.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the normal one we usually show, i.e.\n",
    "# cv.logits.token_log_probs(\n",
    "#     token_indices=input_tokens,\n",
    "#     log_probs=log_probs,\n",
    "#     to_string=token_to_string,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a72925",
   "metadata": {},
   "source": [
    "#### circuitsvis.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa878bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, we'll look at each\n",
    "\n",
    "# take first batch, ex: torch.Size([8, 16])\n",
    "pos_embed = cache[\"hook_pos_embed\"][0]\n",
    "\n",
    "# low level function for coloring tokens according to single value\n",
    "for i in range(cfg.d_model):\n",
    "    display(\n",
    "        circuitsvis.tokens.colored_tokens(\n",
    "            tokens=input_tokens_as_strings,\n",
    "            values=pos_embed[:, i],\n",
    "            negative_color=\"blue\",\n",
    "            positive_color=\"red\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # only display a few for example\n",
    "    # if i >= 2:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take first batch\n",
    "# ex: torch.size([8, 16]) = [n_ctx, d_model]\n",
    "attention_out = cache[\"blocks.0.hook_attn_out\"][0]\n",
    "\n",
    "circuitsvis.tokens.colored_tokens_multi(\n",
    "    tokens=input_tokens_as_strings,\n",
    "    values=attention_out,\n",
    "    labels=[str(x) for x in range(cfg.d_model)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuitsvis.tokens.visualize_model_performance(\n",
    "    tokens=input_tokens,\n",
    "    str_tokens=input_tokens_as_strings,\n",
    "    logits=logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0406d0",
   "metadata": {},
   "source": [
    "#### circuitsvis.topk_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d155e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuitsvis.topk_samples.topk_samples??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90d626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e239f06",
   "metadata": {},
   "source": [
    "#### circuitsvis.topk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuitsvis.topk_tokens.topk_tokens??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533d521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee20ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4195832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c59598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86be5940",
   "metadata": {},
   "source": [
    "## SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_index in range(cfg.n_layers):\n",
    "    imshow(\n",
    "        transformer_lens.utils.to_numpy(cache[\"attn\", layer_index].mean([0, 1])),\n",
    "        title=f\"Layer {layer_index} Attention Pattern\",\n",
    "        height=400,\n",
    "        width=400,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "Loss = Float32[torch.Tensor, \"\"]\n",
    "MSELoss = Float32[torch.Tensor, \"\"]\n",
    "WeightedSparsityLoss = Float32[torch.Tensor, \"\"]\n",
    "\n",
    "Logits = Float32[torch.Tensor, \"n_ctx d_vocab\"]\n",
    "BatchedLogits = Float32[torch.Tensor, \"batch n_ctx d_vocab\"]\n",
    "\n",
    "ModelActivations = Float32[torch.Tensor, \"n_ctx d_model\"]\n",
    "BatchedModelActivations = Float32[torch.Tensor, \"batch n_ctx d_model\"]\n",
    "\n",
    "FlattenedModelActivations = Float32[torch.Tensor, \"d_sae_in\"]\n",
    "\n",
    "BatchedFlattenedModelActivations = Float32[torch.Tensor, \"batch d_sae_in\"]\n",
    "BatchedSAEActivations = Float32[torch.Tensor, \"batch d_sae_model\"]\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SAEOutput:\n",
    "    sae_activations: BatchedSAEActivations\n",
    "    reconstructed_model_activations: BatchedFlattenedModelActivations\n",
    "\n",
    "\n",
    "def sparse_loss_kl_divergence(\n",
    "    flattened_model_activations: BatchedFlattenedModelActivations,\n",
    "    sae_output: SAEOutput,\n",
    "    sparsity_target: float,\n",
    "    sparsity_weight: float,\n",
    "    epsilon: float = 1e-7,\n",
    ") -> tuple[Loss, MSELoss, WeightedSparsityLoss]:\n",
    "\n",
    "    # same as dense loss (this is constant?)\n",
    "    mse_loss = F.mse_loss(\n",
    "        sae_output.reconstructed_model_activations,\n",
    "        flattened_model_activations,\n",
    "    )\n",
    "\n",
    "    # KL divergence for sparsity\n",
    "    avg_activation = torch.mean(sae_output.sae_activations, dim=0)\n",
    "\n",
    "    # print(f'[pre-clamping] {avg_activation=}')\n",
    "\n",
    "    # Add epsilon for numerical stability\n",
    "    avg_activation = torch.clamp(avg_activation, epsilon, 1 - epsilon)\n",
    "\n",
    "    kl_div = sparsity_target * torch.log(sparsity_target / avg_activation) + (\n",
    "        1 - sparsity_target\n",
    "    ) * torch.log((1 - sparsity_target) / (1 - avg_activation))\n",
    "    kl_div = torch.sum(kl_div)\n",
    "\n",
    "    # `sparsity_weight` decides how much we weight `KL-Divergence`\n",
    "    sparsity_penalty = sparsity_weight * kl_div\n",
    "\n",
    "    # print(f\"{mse_loss=}, {avg_activation=}, {kl_div.item()}, {sparsity_penalty=}\")\n",
    "\n",
    "    return mse_loss + sparsity_penalty, mse_loss, sparsity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ac6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_loss_l1_norm(\n",
    "    flattened_model_activations: BatchedFlattenedModelActivations,\n",
    "    sae_output: SAEOutput,\n",
    "    sparsity_weight: float,\n",
    ") -> tuple[Loss, MSELoss, WeightedSparsityLoss]:\n",
    "\n",
    "    # Reconstruction loss (Mean Squared Error)\n",
    "    mse_loss = F.mse_loss(\n",
    "        sae_output.reconstructed_model_activations,\n",
    "        flattened_model_activations,\n",
    "    )\n",
    "\n",
    "    # L1 sparsity penalty\n",
    "    l1_penalty = torch.mean(torch.abs(sae_output.sae_activations))\n",
    "\n",
    "    sparsity_penalty = sparsity_weight * l1_penalty\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = mse_loss + sparsity_penalty\n",
    "\n",
    "    return total_loss, mse_loss, sparsity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc92db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SparseAutoencoderConfig:\n",
    "    d_in: int\n",
    "    d_model: int\n",
    "\n",
    "\n",
    "# TODO(bschoen): Start using the config pattern, it stays typesafe and allows\n",
    "#                easy logging to things like wandb\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: SparseAutoencoderConfig,\n",
    "    ) -> None:\n",
    "\n",
    "        print(f\"Creating SparseAutoencoder with {cfg}\")\n",
    "\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "\n",
    "        self.d_in = cfg.d_in\n",
    "        self.d_model = cfg.d_model\n",
    "\n",
    "        self.encoder = nn.Linear(cfg.d_in, cfg.d_model)\n",
    "        self.decoder = nn.Linear(cfg.d_model, cfg.d_in)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: BatchedFlattenedModelActivations,\n",
    "    ) -> SAEOutput:\n",
    "\n",
    "        # TODO(bschoen): Which activation function should we use?\n",
    "        encoded = F.gelu(self.encoder(x))\n",
    "\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return SAEOutput(\n",
    "            sae_activations=encoded,\n",
    "            reconstructed_model_activations=decoded,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class LightningSparseAutoencoderConfig:\n",
    "\n",
    "    model_config: transformer_lens.HookedTransformerConfig\n",
    "    sae_config: SparseAutoencoderConfig\n",
    "    learning_rate: float\n",
    "    sparsity_weight: float\n",
    "\n",
    "\n",
    "# note: this kind of lightning adapter is a common pattern: https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#starter-example\n",
    "class LightningSparseAutoencoder(lightning.pytorch.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: LightningSparseAutoencoderConfig,\n",
    "    ) -> None:\n",
    "\n",
    "        super(LightningSparseAutoencoder, self).__init__()\n",
    "\n",
    "        self.model = transformer_lens.HookedTransformer(cfg=cfg.model_config)\n",
    "        self.sae = SparseAutoencoder(cfg=cfg.sae_config)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        return self.model(inputs, target)\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int) -> Loss:\n",
    "        inputs, target = batch\n",
    "\n",
    "        self.model\n",
    "        output = self(inputs, target)\n",
    "        loss = torch.nn.functional.cr(output, target.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_id = \"blocks.0.hook_mlp_out\"\n",
    "\n",
    "cache[hook_id].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "sae_num_epochs = 100000\n",
    "sae_expansion_factor = 64\n",
    "\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# both arbitrary for now\n",
    "# - Start small: A common approach is to begin with a relatively small sparsity weight,\n",
    "#                typically in the range of 1e-5 to 1e-3. This allows the model to\n",
    "#                learn meaningful representations before enforcing strong sparsity\n",
    "#                constraints.\n",
    "sparsity_weight: float = 1e-3  # Weight of the sparsity loss in the total loss\n",
    "sparsity_target: float = 0.05  # Target average activation of hidden neurons\n",
    "\n",
    "print(f\"Training SAE for {hook_id}...\")\n",
    "sae_d_in = (cfg.n_ctx - 1) * cfg.d_model  # -1 since not predicting first token\n",
    "sae_d_model = sae_d_in * sae_expansion_factor\n",
    "\n",
    "sae_cfg = SparseAutoencoderConfig(\n",
    "    d_in=sae_d_in,\n",
    "    d_model=sae_d_model,\n",
    ")\n",
    "\n",
    "sae_model = SparseAutoencoder(cfg=sae_cfg)\n",
    "sae_model.to(device)\n",
    "\n",
    "sae_optimizer = optim.Adam(sae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"toy-problem-hooked-transformer-sae\",\n",
    "    config={\n",
    "        \"sae_num_epochs\": sae_num_epochs,\n",
    "        \"sae_expansion_factor\": sae_expansion_factor,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"sparsity_weight\": sparsity_weight,\n",
    "        \"sparsity_target\": sparsity_target,\n",
    "        \"sae_d_in\": sae_d_in,\n",
    "        \"sae_d_model\": sae_d_model,\n",
    "        \"hook_id\": hook_id,\n",
    "    },\n",
    ")\n",
    "\n",
    "# put model itself into eval mode so doesn't change\n",
    "model.eval()\n",
    "\n",
    "# go through the training data again, this time training the sae on the activations\n",
    "for epoch, batch in tqdm.tqdm(\n",
    "    zip(\n",
    "        range(sae_num_epochs),\n",
    "        itertools.cycle(train_loader),\n",
    "    )\n",
    "):\n",
    "\n",
    "    tokens, target = batch\n",
    "\n",
    "    tokens, target = tokens.to(device), target.to(device)\n",
    "\n",
    "    # run through the model (with cache) to get the activations\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "    # ex: torch.Size([4, 8, 16])\n",
    "    activations = cache[hook_id]\n",
    "\n",
    "    # ex: torch.Size([4, 128])\n",
    "    flattened_activations = activations.reshape(activations.size(0), -1)\n",
    "\n",
    "    sae_optimizer.zero_grad()\n",
    "\n",
    "    # now the SAE model is given the *activations*\n",
    "    sae_output = sae_model.forward(flattened_activations)\n",
    "\n",
    "    # compute loss\n",
    "\n",
    "    total_loss, reconstruction_loss, weighted_sparsity_loss = sparse_loss_kl_divergence(\n",
    "        flattened_activations,\n",
    "        sae_output,\n",
    "        sparsity_target=sparsity_target,\n",
    "        sparsity_weight=sparsity_weight,\n",
    "    )\n",
    "\n",
    "    \"\"\"total_loss, reconstruction_loss, weighted_sparsity_loss = sparse_loss_l1_norm(\n",
    "        flattened_model_activations=flattened_activations,\n",
    "        sae_output=sae_output,\n",
    "        sparsity_weight=sparsity_weight,\n",
    "    )\"\"\"\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    sae_optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(\n",
    "            f\"Step {epoch}, \"\n",
    "            f\"Total Loss: {total_loss.item():.6f}, \"\n",
    "            f\"Reconstruction Loss: {reconstruction_loss.item():.6f}, \"\n",
    "            f\"Sparsity Loss: {weighted_sparsity_loss.item():.6f}\",\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"total_loss\": total_loss.item(),\n",
    "                \"reconstruction_loss\": reconstruction_loss.item(),\n",
    "                \"weighted_sparsity_loss\": weighted_sparsity_loss.item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a407f",
   "metadata": {},
   "source": [
    "#### Dictionary Learning Implementation\n",
    "\n",
    "See [simple_dictionary_learning.ipynb](simple_dictionary_learning.ipynb) for a details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a1011",
   "metadata": {},
   "source": [
    "#### Extracting the learned dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde7a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SparseAutoencoder with d_in=128, d_model=512, sparsity_target=0.05\n",
    "dictionary: Float32[torch.Tensor, \"sae_hidden sae_in\"] = (\n",
    "    sae_model.encoder.weight.detach()\n",
    ")\n",
    "\n",
    "# ex: Dictionary shape: torch.Size([512, 128])\n",
    "print(f\"Dictionary shape: {dictionary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ba6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape dictionary elements to match original activation shape\n",
    "# (essentially `unflatting`)\n",
    "reshaped_dictionary = dictionary.reshape(sae_d_model, (cfg.n_ctx - 1), cfg.d_model)\n",
    "\n",
    "# Motivation: Extract the learned features (dictionary elements) from the encoder weights\n",
    "# ex: Dictionary shape: torch.Size([512, 8, 16])\n",
    "print(f\"Dictionary shape: {reshaped_dictionary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's always worth checking this sort of thing when you do this by hand\n",
    "# to check that you haven't got the wrong site, or are missing a\n",
    "# scaling factor or something like this.\n",
    "#\n",
    "# This is like the overfitting thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afd13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f65ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at an example batch from `test`\n",
    "\n",
    "# set both to eval mode\n",
    "model.eval()\n",
    "sae_model.eval()\n",
    "\n",
    "# grab something from the test batch\n",
    "example_batch = next(iter(test_loader))\n",
    "\n",
    "x, y = example_batch\n",
    "\n",
    "_, cache = model.run_with_cache(x)\n",
    "\n",
    "activations = cache[hook_id]\n",
    "\n",
    "print(f\"Activations shape: {activations.shape}\")\n",
    "\n",
    "# flatten it\n",
    "flattened_activations = activations.reshape(activations.size(0), -1)\n",
    "\n",
    "print(f\"{flattened_activations.shape=}\")\n",
    "\n",
    "sae_outputs = sae_model(flattened_activations)\n",
    "\n",
    "print(f\"{sae_outputs.sae_activations.shape=}\")\n",
    "print(f\"{sae_outputs.reconstructed_model_activations.shape=}\")\n",
    "\n",
    "# now we can get the dictionary\n",
    "dictionary = sae_model.encoder.weight.detach()\n",
    "\n",
    "print(f\"Dictionary shape: {dictionary.shape}\")\n",
    "\n",
    "# now we can get the sparse coefficients\n",
    "alpha = dictionary @ flattened_activations.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef94946",
   "metadata": {},
   "source": [
    "### Determine Quality Of SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(\n",
    "    sae_activations: BatchedSAEActivations,\n",
    "    threshold: float = 1e-5,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate sparsity of SAE activations across a batch.\n",
    "\n",
    "    Args:\n",
    "    sae_activations (torch.Tensor): The activations from the Sparse Autoencoder.\n",
    "                                    Shape: (batch, d_sae_model)\n",
    "    threshold (float): The threshold below which an activation is considered \"inactive\".\n",
    "\n",
    "    Returns:\n",
    "    float: The average sparsity value across the batch (fraction of inactive neurons).\n",
    "    \"\"\"\n",
    "    # Count the number of neurons that are below the threshold (inactive)\n",
    "    inactive_neurons = torch.sum(torch.abs(sae_activations) < threshold, dim=1)\n",
    "\n",
    "    # Calculate the fraction of inactive neurons for each item in the batch\n",
    "    sparsity_per_item = inactive_neurons.float() / sae_activations.shape[1]\n",
    "\n",
    "    # Take the mean across the batch\n",
    "    average_sparsity = torch.mean(sparsity_per_item)\n",
    "\n",
    "    return average_sparsity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce98afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_explained_variance(\n",
    "    reconstructed_model_activations: BatchedFlattenedModelActivations,\n",
    "    flattened_activations: BatchedFlattenedModelActivations,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the explained variance of the SAE activations.\n",
    "    \"\"\"\n",
    "\n",
    "    numerator = torch.mean(\n",
    "        (reconstructed_model_activations[:, 1:] - flattened_activations[:, 1:]) ** 2\n",
    "    )\n",
    "    denominator = flattened_activations[:, 1:].to(torch.float32).var()\n",
    "\n",
    "    explained_variance = 1 - (numerator / denominator)\n",
    "\n",
    "    return explained_variance.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained_variance=0.995 -> good, basically all the variance is explained by our SAE\n",
    "# sparsity=0.0045 -> good, very sparse, and more sparse than our target of 0.05\n",
    "explained_variance = calculate_explained_variance(\n",
    "    sae_outputs.reconstructed_model_activations,\n",
    "    flattened_activations,\n",
    ")\n",
    "print(f\"{explained_variance=:.4f}\")\n",
    "\n",
    "sparsity = calculate_sparsity(sae_outputs.sae_activations)\n",
    "print(f\"{sparsity=:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa3d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the relationship between SAE activations and input features\n",
    "\n",
    "# TODO(bschoen): Oh `imshow` is huge here!\n",
    "\n",
    "# 1. Visualize the dictionary (encoder weights)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(dictionary.cpu().T, aspect=\"auto\", cmap=\"RdBu_r\")\n",
    "plt.colorbar()\n",
    "plt.title(\"SAE Dictionary (Encoder Weights)\")\n",
    "plt.xlabel(\"Dictionary Elements\")\n",
    "plt.ylabel(\"Input Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find the most active neurons for each input\n",
    "top_k = 5  # Number of top activations to consider\n",
    "\n",
    "# so this is essentially the top 5 activations over `batch_size` examples\n",
    "top_activations = torch.topk(sae_outputs.sae_activations, k=top_k, dim=1)\n",
    "\n",
    "# Visualization of top activations\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.heatmap(\n",
    "    top_activations.values.detach().cpu().numpy(), cmap=\"viridis\", annot=True, fmt=\".2f\"\n",
    ")\n",
    "plt.title(\"Top 5 Activation Values\")\n",
    "plt.xlabel(\"Top K\")\n",
    "plt.ylabel(\"Batch Sample\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.heatmap(\n",
    "    top_activations.indices.detach().cpu().numpy(), cmap=\"YlOrRd\", annot=True, fmt=\"d\"\n",
    ")\n",
    "plt.title(\"Indices of Top 5 Activations\")\n",
    "plt.xlabel(\"Top K\")\n",
    "plt.ylabel(\"Batch Sample\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis: frequency of top neurons\n",
    "top_neuron_counts = torch.bincount(\n",
    "    top_activations.indices.flatten().detach().cpu(),\n",
    "    minlength=sae_outputs.sae_activations.shape[1],\n",
    ")\n",
    "top_10_neurons = torch.topk(top_neuron_counts, k=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), top_10_neurons.values.detach().cpu().numpy())\n",
    "plt.title(\"Top 10 Most Frequently Activated Neurons\")\n",
    "plt.xlabel(\"Neuron Index\")\n",
    "plt.ylabel(\"Activation Frequency\")\n",
    "plt.xticks(range(10), top_10_neurons.indices.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_outputs.sae_activations[:, 1210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5749ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{sae_outputs.sae_activations.shape=}\")\n",
    "print(f\"{top_activations.values.shape=}\")\n",
    "print(f\"{top_activations.indices.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_activations.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4932383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex: 51 and 410 show up a lot\n",
    "sns.heatmap(top_activations.values.cpu().T, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analyze feature importance for each neuron\n",
    "feature_importance = torch.abs(dictionary).sum(dim=1)\n",
    "top_features = torch.topk(feature_importance, k=10)\n",
    "\n",
    "print(f\"{dictionary.shape=}\")\n",
    "print(f\"{feature_importance.shape=}\")\n",
    "print(f\"{top_features.values.shape=}\")\n",
    "print(f\"{top_features.indices.shape=}\")\n",
    "\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597dd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 10 most important neurons:\")\n",
    "for i, (value, index) in enumerate(\n",
    "    zip(top_features.values.tolist(), top_features.indices.tolist())\n",
    "):\n",
    "    print(f\"Neuron {index}:\\t{value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0168d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77201da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features.indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize activations for a few examples\n",
    "\n",
    "# first look at a single batch\n",
    "sae_activations = sae_outputs.sae_activations[0].detach().cpu()\n",
    "\n",
    "print(f\"{sae_activations.shape=}\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "# Look at a single batch\n",
    "plt.bar(range(sae_activations.shape[0]), sae_activations)\n",
    "\n",
    "plt.title(f\"SAE Activations for Example\")\n",
    "plt.xlabel(\"Neuron\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fa395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a496682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Reconstruct input features from SAE activations\n",
    "#\n",
    "# Take a single batch first\n",
    "reconstructed_model_activations = (\n",
    "    sae_outputs.reconstructed_model_activations.detach().cpu()\n",
    ")\n",
    "\n",
    "# 6. Compare original and reconstructed features\n",
    "num_features = 5\n",
    "\n",
    "plt.figure(figsize=(15, 3 * num_features))\n",
    "for i in range(num_features):\n",
    "    plt.subplot(num_features, 1, i + 1)\n",
    "    plt.ylim(-1, 1)  # Set y-axis range from -1 to 1\n",
    "    plt.plot(flattened_activations[:, i].cpu(), label=\"Original\", alpha=0.5)\n",
    "    plt.plot(reconstructed_model_activations[:, i], label=\"Reconstructed\", alpha=0.5)\n",
    "    plt.title(f\"Feature {i}: Original vs Reconstructed\")\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b59930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Correlation between SAE activations and input features\n",
    "correlation_matrix = torch.corrcoef(\n",
    "    torch.cat([sae_outputs.sae_activations, flattened_activations], dim=1).T\n",
    ")\n",
    "num_neurons = sae_outputs.sae_activations.shape[1]\n",
    "neuron_feature_correlation = correlation_matrix[:num_neurons, num_neurons:]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(\n",
    "    neuron_feature_correlation.detach().cpu(),\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation between SAE Neurons and Input Features\")\n",
    "plt.xlabel(\"Input Features\")\n",
    "plt.ylabel(\"SAE Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d72027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385d16d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02318ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47054e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_outputs.sae_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960382a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c6bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcdbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect max activations\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # go through the training data again, but don't cycle, no reason to go through more than once\n",
    "    for batch in tqdm.tqdm(train_loader):\n",
    "\n",
    "        tokens, target = batch\n",
    "\n",
    "        tokens, target = tokens.to(device), target.to(device)\n",
    "\n",
    "        # run through the model (with cache) to get the activations\n",
    "        logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "        # ex: torch.Size([4, 8, 16])\n",
    "        activations = cache[hook_id]\n",
    "\n",
    "        # ex: torch.Size([4, 128])\n",
    "        flattened_activations = activations.reshape(activations.size(0), -1)\n",
    "\n",
    "        # now the SAE model is given the *activations*\n",
    "        encoded, decoded = sae_model(flattened_activations)\n",
    "\n",
    "        sae_activations = encoded\n",
    "\n",
    "        # sae_activations.reshape(sae_d_model, (cfg.n_ctx - 1), cfg.d_model)\n",
    "\n",
    "        # max_activations = torch.max(encoded, dim=1)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc950af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = sae_model.encoder.weight @ flattened_activations[0]\n",
    "\n",
    "print(f\"{alpha.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9278a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.abs(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04427556",
   "metadata": {},
   "outputs": [],
   "source": [
    "8 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474f153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
