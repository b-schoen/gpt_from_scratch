{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c17c329-f237-47d1-b0b9-36c6493dbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740341b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evalugator\n",
    "import evalugator.api\n",
    "import evalugator.api.dispatcher\n",
    "import evalugator.api.requests\n",
    "\n",
    "# note: types from yaml files are represented in `evalugator.structs`, likely only YAML for non-python use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7996f224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers:\n",
      " - evalugator.api.providers.openai\n",
      " - evalugator.api.providers.anthropic\n",
      " - evalugator.api.providers.replicate\n",
      " - evalugator.api.providers.human\n",
      " - evalugator.api.providers.together_api\n"
     ]
    }
   ],
   "source": [
    "# list available providers\n",
    "print('Available providers:')\n",
    "\n",
    "# note: these are literal python modules\n",
    "for provider_module in evalugator.api.dispatcher.PROVIDERS:\n",
    "    print(f' - {provider_module.__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6977c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not evalugator.api.providers.openai.provides_model('foo')\n",
    "assert evalugator.api.providers.openai.provides_model('gpt-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d74b2",
   "metadata": {},
   "source": [
    "Note: The way `evalugator.api.dispatcher` works is:\n",
    "\n",
    "```python\n",
    "from functools import partial\n",
    "\n",
    "from .providers import openai, anthropic, replicate, human, together_api\n",
    "\n",
    "PROVIDERS = [openai, anthropic, replicate, human, together_api]\n",
    "\n",
    "# note: also checks that model ID only matches exactly one provider\n",
    "def get_model_provider(model_id):\n",
    "    providers = []\n",
    "    for provider in PROVIDERS:\n",
    "        if provider.provides_model(model_id):\n",
    "            return provider\n",
    "\n",
    "def get_request_processor(request, model_id):\n",
    "    provider = get_model_provider(model_id)\n",
    "    return partial(provider.execute, model_id)\n",
    "\n",
    "\n",
    "def encode(model_id, *args, **kwargs):\n",
    "    provider = get_model_provider(model_id)\n",
    "    return provider.encode(model_id, *args, **kwargs)\n",
    "\n",
    "\n",
    "def decode(model_id, *args, **kwargs):\n",
    "    provider = get_model_provider(model_id)\n",
    "    return provider.decode(model_id, *args, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4010fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this means the implicit provider API is:\n",
    "\n",
    "from typing import Protocol, ParamSpec\n",
    "\n",
    "P = ParamSpec('P')\n",
    "\n",
    "# note: this could be a module or class\n",
    "class Provider(Protocol):\n",
    "    def provides_model(self, model_id: str) -> bool:\n",
    "        ...\n",
    "    def execute(self, model_id: str, request: evalugator.api.requests.Request) -> evalugator.api.requests.Response:\n",
    "        ...\n",
    "    def encode(self, model_id: str, *args: P.args, **kwargs: P.kwargs) -> str:\n",
    "        ...\n",
    "    def decode(self, model_id: str, *args: P.args, **kwargs: P.kwargs) -> str:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25d960",
   "metadata": {},
   "source": [
    "This then allows the `API` to be used like this:\n",
    "\n",
    "```python\n",
    "class API:\n",
    "    \n",
    "    model_id: str\n",
    "    executor: ThreadPoolExecutor\n",
    "\n",
    "    # note: allows automatically saving results, essentially poor man's async, which makes since given not all providers might have async\n",
    "    def execute(self, request: Request) -> Future[Response]:\n",
    "        func = dispatcher.get_request_processor(request, self.model_id)\n",
    "        future = self.executor.submit(func, request)\n",
    "        future.add_done_callback(self._log_response)\n",
    "        return future\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        return dispatcher.encode(self.model_id, *args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        return dispatcher.decode(self.model_id, *args, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78fc45",
   "metadata": {},
   "source": [
    "additionally defines two convenience functions:\n",
    "\n",
    "```python\n",
    "    def get_text(self, ...) -> Future[GetTextResponse]:\n",
    "        request = GetTextRequest(...)\n",
    "        return self.execute(request)\n",
    "    \n",
    "    def get_probs(self, ...) -> Future[GetPropsResponse]:\n",
    "        request = GetProbsRequest(...)\n",
    "        return self.execute(request)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a90563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GetTextResponse(model_id='gpt-4o-mini', request=GetTextRequest(context=None, prompt=[Message(role='user', content='Hi!')], temperature=1, max_tokens=512), raw_responses=[ChatCompletion(id='chatcmpl-9y7CeaNPkZ7jesyuUlNBAOdyhKy1B', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1724114396, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_48196bc67a', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))], context=None, txt='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = 'gpt-4o-mini'\n",
    "\n",
    "# note: also allows `log_file_name`\n",
    "api = evalugator.api.Api(model_id=model_id)\n",
    "\n",
    "response_future = api.get_text(prompt='Hi!')\n",
    "\n",
    "# actual waiting\n",
    "response = response_future.result()\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ad0510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'gpt-4o-mini',\n",
       " 'request': {'context': None,\n",
       "  'prompt': [{'role': 'user', 'content': 'Hi!'}],\n",
       "  'temperature': 1,\n",
       "  'max_tokens': 512},\n",
       " 'txt': 'Hello! How can I assist you today?',\n",
       " 'context': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2c2fdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll make a new provider with tool use\n",
    "from typing import Callable, Any\n",
    "import dataclasses\n",
    "\n",
    "from gpt_from_scratch.evals.function_calling.openai.function_call_handler import FunctionCallHandler\n",
    "\n",
    "import openai\n",
    "\n",
    "\n",
    "\n",
    "# note: task-standard/workbench/example-agents/fncall-baseline/commands.py\n",
    "#       uses a `return` tool, which seems like an interesting technique\n",
    "\n",
    "# note: subset of https://github.com/LRudL/evalugator/blob/main/evalugator/api/providers/openai.py#L15\n",
    "#       that we care about that also supports tool use (not intended to be comprehensive)\n",
    "#\n",
    "# note: we can't just use `gpt-4o` etc directly since conflicts with the `openai` provider\n",
    "_MODEL_ID_TO_MODEL_NAME: dict[str, str] = {\n",
    "    f'function-calling-{model_name}': model_name\n",
    "    for model_name in [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "}\n",
    "\n",
    "class MaxIterationsReachedError(Exception):\n",
    "    pass\n",
    "\n",
    "# TODO(bschoen): Might use `partial` here, since otherwise this is super inefficient\n",
    "# TODO(bschoen): This really seems like it can only support single turn\n",
    "# TODO(bschoen): Should we have a custom tool request message?\n",
    "class OpenAIFunctionCallingProvider(Provider):\n",
    "    \"\"\"\n",
    "    Individual instance of a provider supporting the specified functions.\n",
    "\n",
    "    Example:\n",
    "        import evalugator.api.dispatcher\n",
    "    \n",
    "        # we'll create a provider customized to have our tools\n",
    "        provider = OpenAIFunctionCallingProvider(functions=[get_location, get_weather])\n",
    "\n",
    "        evalugator.api.dispatcher.PROVIDERS.append(provider)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # note: it's actually fine for this to be stateful, but this whole design is a bit weird\n",
    "    #       when it comes to customization points for the provider itself when it comes\n",
    "    #       to provider specific things (which I guess is to be expected)\n",
    "    #\n",
    "    #       the provider itself essentially needs to be customizable, and capable of holding\n",
    "    #       some immutable state, but not mutable per execution?\n",
    "    def __init__(self, functions: list[Callable[..., Any]], max_iterations_per_execution: int = 3) -> None:\n",
    "\n",
    "        self._function_call_handler = FunctionCallHandler(functions=functions)\n",
    "        \n",
    "        # number of times to hand control back to the model after providing the result of a tool\n",
    "        self._max_iterations_per_execution = max_iterations_per_execution\n",
    "\n",
    "        # initialize client\n",
    "        self._client = openai.OpenAI()\n",
    "\n",
    "    def _get_model_name(self, model_id: str) -> str:\n",
    "        return _MODEL_ID_TO_MODEL_NAME[model_id]\n",
    "\n",
    "    def provides_model(self, model_id: str) -> bool:\n",
    "        return any(model_id.startswith(prefix) for prefix in _MODEL_ID_TO_MODEL_NAME)\n",
    "    \n",
    "    def encode(self, model_id: str, *args: P.args, **kwargs: P.kwargs) -> str:\n",
    "        return evalugator.api.providers.openai.encode(model_id, *args, **kwargs)\n",
    "    \n",
    "    def decode(self, model_id: str, *args: P.args, **kwargs: P.kwargs) -> str:\n",
    "        return evalugator.api.providers.openai.decode(model_id, *args, **kwargs)\n",
    "    \n",
    "    # note: I guess response needs to contain all messages for future request? The\n",
    "    #       `message` class isn't flexible enough to hold actual history though,\n",
    "    #       so it seems like this thing overall isn't designed for multi-turn\n",
    "    def execute(self, model_id: str, request: evalugator.api.requests.Request,) -> evalugator.api.requests.Response:\n",
    "\n",
    "\n",
    "        print('Executing function calling provider...')\n",
    "        \n",
    "        # list of {'role': ..., 'content': ...}\n",
    "        messages = [dataclasses.asdict(x) for x in request.prompt]\n",
    "\n",
    "        for i in range(self._max_iterations_per_execution):\n",
    "\n",
    "            iteration_count = i + 1\n",
    "\n",
    "            print(f'Attempt {iteration_count} / {self._max_iterations_per_execution}')\n",
    "\n",
    "            # TODO(bschoen): Schemas actually _can_ contain nested types, if they're pydantic\n",
    "            #                or dataclasses can support that easily, but description gets annoying,\n",
    "            #                note that's for the input though.\n",
    "            # TODO(bschoen): Response format is extremely valuable, can allow specifying it too\n",
    "            # Note: Can't use response_format, structured_output, and parallel function calling together\n",
    "            print('Creating completion...')\n",
    "            response: openai.ChatCompletion = self._client.chat.completions.create(\n",
    "                model=self._get_model_name(model_id),\n",
    "                messages=messages,\n",
    "                tools=self._function_call_handler.get_schema_for_tools_arg(),\n",
    "            )\n",
    "\n",
    "            # add message to history (true for both content and tool)\n",
    "            assert len(response.choices) == 1\n",
    "            \n",
    "            choice: openai.types.CompletionChoice = response.choices[0]\n",
    "            message: openai.types.chat.ChatCompletionMessage = choice.message\n",
    "            \n",
    "            messages.append(message.dict())\n",
    "    \n",
    "            # show any model text responses\n",
    "            if message.content:\n",
    "                print(f'Model response: {message.content}')\n",
    "        \n",
    "            # resolve any tool calls\n",
    "            if message.tool_calls:\n",
    "\n",
    "                print(f'Processing {len(message.tool_calls)} tool calls')\n",
    "                \n",
    "                for tool_call in message.tool_calls:\n",
    "            \n",
    "                    print(f'Resolving tool call in response: {tool_call.id}')\n",
    "                    # print(f'Calling {tool_call.function.name} with args: {tool_call.function.arguments}')\n",
    "                    tool_call_result_message = self._function_call_handler.resolve(tool_call=tool_call)\n",
    "\n",
    "                    # print(f\"Tool call result: {tool_call_result_message['content']}\")\n",
    "            \n",
    "                    # add tool call result\n",
    "                    print(f'Providing results from tool calls back to model...')\n",
    "                    messages.append(tool_call_result_message)\n",
    "\n",
    "            # otherwise, no tool calls left to resolve and we can return control back to the user\n",
    "            else:\n",
    "                print('\\n---\\nNo more tool calls left to resolve, breaking')\n",
    "\n",
    "                # model_solution = messages[-1]['content']\n",
    "\n",
    "                # print in case isn't valid json\n",
    "                # print(f'Model solution: `{model_solution}`')\n",
    "                \n",
    "                # TODO(bschoen): could put parsed response_format in `context`\n",
    "                # parse into dedicated struct\n",
    "                # model_problem_solving_status = ModelProblemSolvingStatus.parse_raw(model_solution)\n",
    "\n",
    "                return evalugator.api.requests.Response(\n",
    "                    model_id=model_id,\n",
    "                    request=request,\n",
    "                    raw_responses=messages,\n",
    "                    context=None,\n",
    "                )\n",
    "\n",
    "        # if we reach this point we've exhausted max iterations\n",
    "        raise MaxIterationsReachedError(f'Exhausted max attempts: {self._max_iterations_per_execution}')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb21c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "# note: this is `gpt-4o`\n",
    "gpt4_tokenizer = tiktoken.get_encoding(\"o200k_base\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15ba04dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"token_index\": 0,\n",
      "    \"token_string\": \"你好\",\n",
      "    \"token_split_into_characters\": [\n",
      "      \"你\",\n",
      "      \"好\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"token_index\": 1,\n",
      "    \"token_string\": \" strawberry\",\n",
      "    \"token_split_into_characters\": [\n",
      "      \" \",\n",
      "      \"s\",\n",
      "      \"t\",\n",
      "      \"r\",\n",
      "      \"a\",\n",
      "      \"w\",\n",
      "      \"b\",\n",
      "      \"e\",\n",
      "      \"r\",\n",
      "      \"r\",\n",
      "      \"y\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "import json\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TokenInfo:\n",
    "    token_index: int\n",
    "    \"\"\"Index of the token (ex: 1)\"\"\"\n",
    "\n",
    "    token_string: str\n",
    "    \"\"\"The actual token text representation (ex: `你好`, this will be a single token)\"\"\"\n",
    "\n",
    "    token_split_into_characters: list[str]\n",
    "    \"\"\"The characters that make up the token (ex: `['你', '好']`, or `['s', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y']`)\"\"\"\n",
    "\n",
    "# TODO(bschoen): Probably want general pattern of partially binding with typed callables\n",
    "#                (for example so this gets a tokenizer without the model needing to know about it)\n",
    "def get_detailed_and_complete_tokenization_info_for_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenize the given text and return detailed information about each token.\n",
    "\n",
    "    This is useful any time the user asks questions that involve individual characters\n",
    "    or anything else that involves manipulating substrings that are potentially shorter\n",
    "    than a token.\n",
    "\n",
    "    The output is a list of the following form:\n",
    "\n",
    "        [\n",
    "            {'token_index': 0, 'token_string': '你好', 'token_split_into_characters': ['你', '好']},\n",
    "            {'token_index': 1, 'token_string': 'strawberry', 'token_split_into_characters': ['s', 't', 'r', 'a', 'w', 'b', 'e', 'r', 'r', 'y']}\n",
    "        ]\n",
    "\n",
    "    Where:\n",
    "        - `token_index` is the index of the token in the original text\n",
    "        - `token_string` is the actual token text representation\n",
    "        - `token_split_into_characters` is the characters that make up the token\n",
    "\n",
    "    Args:\n",
    "        text (str): User provided text to tokenize, this can span multiple tokens (ex: `你好 strawberry`)\n",
    "    \"\"\"\n",
    "\n",
    "    # ex: [177519, 101830]\n",
    "    encoded_tokens = gpt4_tokenizer.encode(text)\n",
    "\n",
    "    token_infos: list[TokenInfo] = []\n",
    "\n",
    "    for i, token in enumerate(encoded_tokens):\n",
    "\n",
    "        # recover original string for token (ex: `你好`)\n",
    "        token_bytes = gpt4_tokenizer.decode_single_token_bytes(token)\n",
    "        token_string = token_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "        # now recover what `text_characters`s this corresponds to\n",
    "        corresponding_text_characters = [c for c in token_string]\n",
    "\n",
    "        token_infos.append(TokenInfo(\n",
    "            token_index=i,\n",
    "            token_string=token_string,\n",
    "            token_split_into_characters=corresponding_text_characters,\n",
    "        ))\n",
    "\n",
    "    # convert to json\n",
    "    # note: encoded as utf-8 to preserve the original tokens verbatim\n",
    "    # TODO(bschoen): there has to be a better pattern than `encode.decode`\n",
    "    return json.dumps([dataclasses.asdict(x) for x in token_infos], indent=2, ensure_ascii=False).encode(\"utf-8\").decode(\"utf-8\")\n",
    "\n",
    "# show example\n",
    "text = '你好 strawberry'\n",
    "\n",
    "print(get_detailed_and_complete_tokenization_info_for_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5675dacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61b033df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "26bb3652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing function calling provider...\n",
      "Attempt 1 / 3\n",
      "Creating completion...\n",
      "Processing 1 tool calls\n",
      "Resolving tool call in response: call_kqVDYVsBqohhcb6AuZkYuMud\n",
      "Providing results from tool calls back to model...\n",
      "Attempt 2 / 3\n",
      "Creating completion...\n",
      "Model response: In the text \"你好 strawberry\", you have 3 'r's. They are found in the \"strawberry\" token.\n",
      "\n",
      "---\n",
      "No more tool calls left to resolve, breaking\n"
     ]
    }
   ],
   "source": [
    "openai_function_provider = OpenAIFunctionCallingProvider(\n",
    "    functions=[get_detailed_and_complete_tokenization_info_for_text]\n",
    ")\n",
    "\n",
    "request = evalugator.api.requests.Request(\n",
    "    prompt=[\n",
    "        evalugator.api.requests.Message(role='user', content=\"How many 'r's are in: 你好 strawberry\"),\n",
    "    ],\n",
    "    context=None,\n",
    ")\n",
    "\n",
    "response = openai_function_provider.execute(model_id='function-calling-gpt-4o', request=request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2cd2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c983d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
