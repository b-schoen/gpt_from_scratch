{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5674fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload when imports change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d131b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from typing import Unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4978c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import (\n",
    "    file_utils,\n",
    "    vocab_utils,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "155f247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# imported for typechecking\n",
    "#\n",
    "# note: can't easily alias via jaxtyping annotations, as it's a string literal and\n",
    "#       likely plays weirdly with typing.Annotation to forward a payload\n",
    "# note: torchtyping is deprecated in favor of jaxtyping, as torchtyping doesn't have mypy integration\n",
    "#\n",
    "# note: jaxtyping does support prepending\n",
    "#\n",
    "#   Image = Float[Array, \"channels height width\"]\n",
    "#   BatchImage = Float[Image, \"batch\"]\n",
    "#\n",
    "#    -->\n",
    "#\n",
    "#   BatchImage = Float[Array, \"batch channels height width\"]\n",
    "#\n",
    "# so we can compose aliases\n",
    "#\n",
    "from torch import Tensor\n",
    "import jaxtyping\n",
    "from jaxtyping import jaxtyped, Float32, Int64\n",
    "from typeguard import typechecked as typechecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec4c8e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found in cache: download_cache/4acd659e47adc1daeb7aff503accf0a3\n"
     ]
    }
   ],
   "source": [
    "# load tinyshakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "input_filepath = file_utils.download_file_from_url(url)\n",
    "\n",
    "# Read all text from the input file\n",
    "input_text = input_filepath.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cc68448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "73b60a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(input_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ddb854bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab_utils.Vocabulary(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1fba47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(vocab.unique_elements)\n",
    "print(len(vocab.unique_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "15e2340f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(vocab.encode(\"hii there\"))\n",
    "print(vocab.decode(vocab.encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "576e275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "encoded_input_text: Int64[Tensor, 'num_samples'] = torch.tensor(\n",
    "    vocab.encode(input_text),\n",
    "    dtype=torch.long,\n",
    ")\n",
    "print(encoded_input_text.shape, encoded_input_text.dtype)\n",
    "\n",
    "# the 100 characters we looked at earier will to the GPT look like this\n",
    "print(encoded_input_text[:100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e2e3b0c6-0554-42c7-9c81-559f26d2e401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 1115394 input tokens into\n",
      " - train: 1003854\n",
      " - val: 111540\n"
     ]
    }
   ],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "\n",
    "# first 90% will be train, rest val\n",
    "train_val_ratio = 0.9\n",
    "\n",
    "n = int(0.9 * len(encoded_input_text)) \n",
    "\n",
    "train_data: Int64[Tensor, 'num_samples'] = encoded_input_text[:n]\n",
    "val_data: Int64[Tensor, 'num_samples']   = encoded_input_text[n:]\n",
    "\n",
    "print(f'Splitting {len(encoded_input_text)} input tokens into')\n",
    "print(f' - train: {len(train_data)}')\n",
    "print(f' - val: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef2de54d-dd29-455f-b880-6dac45dc1610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example block: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "# We'll never feed entire text through\n",
    "# So we usually break it down into \"chunks\" or \"blocks\"\n",
    "# TODO(bschoen): How do we choose this?\n",
    "# TODO(bschoen): Is block size `context`?\n",
    "block_size = 8\n",
    "\n",
    "# show what a block looks like\n",
    "# this has multiple examples\n",
    "# note: we're predicting 8 (8 examples) from 9 characters\n",
    "# TODO(bschoen): Do we try to shufle how we partition the blocks?\n",
    "print(f'Example block: {train_data[:block_size + 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9f622b07-5d32-4ac7-824d-ff42e1395238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] when input is tensor([18]) the target: 47\n",
      "[1] when input is tensor([18, 47]) the target: 56\n",
      "[2] when input is tensor([18, 47, 56]) the target: 57\n",
      "[3] when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "[4] when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "[5] when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "[6] when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "[7] when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "# show what that looks like over a block\n",
    "# this is the `time` dimension\n",
    "x: Int64[Tensor, \"block_size\"] = train_data[:block_size]\n",
    "y: Int64[Tensor, \"block_size\"] = train_data[1:(block_size+1)]\n",
    "\n",
    "for t in range(block_size):\n",
    "    \n",
    "    context: Int64[Tensor, \"context_size\"] = x[:t+1]\n",
    "    target: Int64[Tensor, \"\"] = y[t]\n",
    "    \n",
    "    print(f\"[{t}] when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d1f00d6-947d-466e-86fd-310494bda733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "\n",
      "[batch: 0][time: 0] when input is [24] the target: 43\n",
      "[batch: 0][time: 1] when input is [24, 43] the target: 58\n",
      "[batch: 0][time: 2] when input is [24, 43, 58] the target: 5\n",
      "[batch: 0][time: 3] when input is [24, 43, 58, 5] the target: 57\n",
      "[batch: 0][time: 4] when input is [24, 43, 58, 5, 57] the target: 1\n",
      "[batch: 0][time: 5] when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "[batch: 0][time: 6] when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "[batch: 0][time: 7] when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "\n",
      "[batch: 1][time: 0] when input is [44] the target: 53\n",
      "[batch: 1][time: 1] when input is [44, 53] the target: 56\n",
      "[batch: 1][time: 2] when input is [44, 53, 56] the target: 1\n",
      "[batch: 1][time: 3] when input is [44, 53, 56, 1] the target: 58\n",
      "[batch: 1][time: 4] when input is [44, 53, 56, 1, 58] the target: 46\n",
      "[batch: 1][time: 5] when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "[batch: 1][time: 6] when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "[batch: 1][time: 7] when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "\n",
      "[batch: 2][time: 0] when input is [52] the target: 58\n",
      "[batch: 2][time: 1] when input is [52, 58] the target: 1\n",
      "[batch: 2][time: 2] when input is [52, 58, 1] the target: 58\n",
      "[batch: 2][time: 3] when input is [52, 58, 1, 58] the target: 46\n",
      "[batch: 2][time: 4] when input is [52, 58, 1, 58, 46] the target: 39\n",
      "[batch: 2][time: 5] when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "[batch: 2][time: 6] when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "[batch: 2][time: 7] when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "\n",
      "[batch: 3][time: 0] when input is [25] the target: 17\n",
      "[batch: 3][time: 1] when input is [25, 17] the target: 27\n",
      "[batch: 3][time: 2] when input is [25, 17, 27] the target: 10\n",
      "[batch: 3][time: 3] when input is [25, 17, 27, 10] the target: 0\n",
      "[batch: 3][time: 4] when input is [25, 17, 27, 10, 0] the target: 21\n",
      "[batch: 3][time: 5] when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "[batch: 3][time: 6] when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "[batch: 3][time: 7] when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "type Block = Int64[Tensor, \"block_size\"]\n",
    "type BatchedBlocks = Int64[Block, \"batch_size\"] # equivalent to `Int64[Tensor, \"batch_size block_size\"]`\n",
    "\n",
    "# now we still want to batch\n",
    "# we seed it so it's always the same\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# how many independent sequences will we process in parallel?\n",
    "batch_size = 4 \n",
    "\n",
    "# what is the maximum context length for predictions?\n",
    "block_size = 8 \n",
    "\n",
    "# note: usually want to stack into a batch\n",
    "@jaxtyped(typechecker=typechecker)\n",
    "def get_batch(data: Int64[Tensor, \"num_samples\"]) -> tuple[BatchedBlocks, BatchedBlocks]:\n",
    "    \"\"\"Generate a small batch of data of inputs x and targets y.\"\"\"\n",
    "\n",
    "    # Generate 'batch_size' random indices. Each index is the start of a sequence.\n",
    "    # The upper bound (len(data) - block_size) ensures we have enough room for a full sequence.\n",
    "    max_batch_start_index = len(data) - block_size\n",
    "\n",
    "    # choose `batch_size` random starting indices for where to start each batch\n",
    "    batch_start_indices: Int64[Tensor, \"batch_size\"] = torch.randint(max_batch_start_index, (batch_size,))\n",
    "    \n",
    "    # For each random start index, extract a sequence of length 'block_size'.\n",
    "    x_blocks: list[Int64[Tensor, 'block_size']] = [data[i:i+block_size] for i in batch_start_indices]\n",
    "    \n",
    "    # Similar to x, but shifted one position to the right (next-token prediction).\n",
    "    # This creates the targets for each input sequence.\n",
    "    y_blocks: list[Int64[Tensor, 'block_size']] = [data[i+1:i+block_size+1] for i in batch_start_indices]\n",
    "    \n",
    "    # Stack these sequences into a single tensor of shape (batch_size, block_size).\n",
    "    x_batch: BatchedBlocks = torch.stack(x_blocks)\n",
    "    y_batch: BatchedBlocks = torch.stack(y_blocks)\n",
    "    \n",
    "    return x_batch, y_batch\n",
    "\n",
    "xb, yb = get_batch(train_data)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "# note: These are essentially 4x8 (32) *independent* examples (as far as the transformer is concerned)\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print()\n",
    "    for t in range(block_size): # time dimension\n",
    "        \n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        \n",
    "        print(f\"[batch: {b}][time: {t}] when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ea34666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# always good to start with the simplest possible model\n",
    "# note: there's dedicated lecture for this\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class LogitsAndLoss(NamedTuple):\n",
    "    logits: Float32[Tensor, \"batch_size block_size vocab_size\"]\n",
    "    loss: Float32[Tensor, \"\"] | None\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    @jaxtyped(typechecker=typechecker)\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table: nn.Embedding = nn.Embedding(vocab_size, vocab_size)\n",
    " \n",
    "    @jaxtyped(typechecker=typechecker)\n",
    "    def forward(\n",
    "        self,\n",
    "        idx: BatchedBlocks, \n",
    "        targets: BatchedBlocks | None = None,\n",
    "    ) -> tuple[Float32[Tensor, \"batch_size block_size vocab_size\"] | Float32[Tensor, \"batch_size*block_size vocab_size\"], Float32[Tensor, \"\"] | None]:\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits: Float32[Tensor, \"batch_size block_size vocab_size\"] = self.token_embedding_table(idx)\n",
    "\n",
    "        # if no targets, nothing to calculate\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            return logits, loss\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        \n",
    "        # strech them out into 1d sequence, just because of quirks of what pytorch expects\n",
    "        # for the cross_entropy calculation\n",
    "        reshaped_logits: Float32[Tensor, \"batch_size*block_size vocab_size\"] = logits.view(B*T, C)\n",
    "        \n",
    "        reshaped_targets: Float32[Tensor, \"batch_size*block_size\"] = targets.view(B*T)\n",
    "\n",
    "        loss: Float32[Tensor, \"\"] = F.cross_entropy(reshaped_logits, reshaped_targets)\n",
    "\n",
    "        return LogitsAndLoss(logits, loss)\n",
    "\n",
    "    @jaxtyped(typechecker=typechecker)\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: BatchedBlocks, \n",
    "        max_new_tokens: int,\n",
    "    ) -> BatchedBlocks:\n",
    "        \n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            # apply softmax to get probabilities\n",
    "            probs: Float32[Tensor, \"batch_size vocab_size\"] = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next: Float32[Tensor, \"batch_size 1\"] = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size=len(vocab.unique_elements))\n",
    "\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ab2633f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: 2\n",
      "[0] n\n",
      "[1] t\n",
      "[2]  \n",
      "[3] t\n",
      "[4] h\n",
      "[5] a\n",
      "[6] t\n",
      "[7]  \n",
      "\n",
      "Top 5 probabilities at position [6]:\n",
      "R: 2.6412\n",
      "t: 2.0734\n",
      "N: 1.8017\n",
      "S: 1.6053\n",
      "o: 1.4801\n"
     ]
    }
   ],
   "source": [
    "# let's look at how to interpret the logits\n",
    "# \n",
    "# we'll pick an arbitrary block from our batch\n",
    "block_index = 2\n",
    "\n",
    "# we can see this represents just an arbitrary chunk of text\n",
    "block = xb[block_index]\n",
    "\n",
    "print(f\"Block: {block_index}\")\n",
    "for position_in_block, char in enumerate(vocab.decode(block.tolist())):\n",
    "    print(f\"[{position_in_block}] {char}\")\n",
    "\n",
    "# we'll look at an arbitrary position in the block\n",
    "position_in_block_index = 6\n",
    "\n",
    "# we'll see what the probability was for each vocab element at that position\n",
    "logits_at_position = logits[block_index, position_in_block_index]\n",
    "\n",
    "# decode the vocab back\n",
    "vocab_to_logit_prob = {vocab.decode_single(index): value for index, value in enumerate(logits_at_position)}\n",
    "\n",
    "# sort by probability\n",
    "vocab_to_logit_prob = sorted(vocab_to_logit_prob.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# show the top 5\n",
    "print(f'\\nTop 5 probabilities at position [{position_in_block_index}]:')\n",
    "for vocab_element, prob in vocab_to_logit_prob[:5]:\n",
    "    print(f\"{vocab_element}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3e260e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
       "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
       "        [ 1.3035, -0.4501,  1.3471,  ...,  0.1910, -0.3425,  1.7955],\n",
       "        ...,\n",
       "        [ 0.4222, -1.8111, -1.0118,  ...,  0.5462,  0.2788,  0.7280],\n",
       "        [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064],\n",
       "        [-1.4322, -0.2810, -2.2789,  ..., -0.5551,  1.0666,  0.5364]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.token_embedding_table.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacc259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09317b0f-7d23-427e-b46d-7657c0cf7acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
