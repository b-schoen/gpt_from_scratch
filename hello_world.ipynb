{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8a7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: oh karpathy spins up lambda labs then connects VSCode to it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f93f36",
   "metadata": {},
   "source": [
    "# Starting to optimize\n",
    "\n",
    "> ![NOTE] Starting from \"what hardware do I have, and am I fully utilizing it\"\n",
    "\n",
    "Then looking up NVIDIA spec sheet for A100, we see:\n",
    "\n",
    "| Specification | A100 80GB PCIe | A100 80GB SXM |\n",
    "|---------------|----------------|---------------|\n",
    "| FP64 | 9.7 TFLOPS | 9.7 TFLOPS |\n",
    "| FP64 Tensor Core | 19.5 TFLOPS | 19.5 TFLOPS |\n",
    "| FP32 | 19.5 TFLOPS | 19.5 TFLOPS |\n",
    "| Tensor Float 32 (TF32) | 156 TFLOPS \\| 312 TFLOPS\\* | 156 TFLOPS \\| 312 TFLOPS\\* |\n",
    "| BFLOAT16 Tensor Core | 312 TFLOPS \\| 624 TFLOPS\\* | 312 TFLOPS \\| 624 TFLOPS\\* |\n",
    "| FP16 Tensor Core | 312 TFLOPS \\| 624 TFLOPS\\* | 312 TFLOPS \\| 624 TFLOPS\\* |\n",
    "| INT8 Tensor Core | 624 TOPS \\| 1248 TOPS\\* | 624 TOPS \\| 1248 TOPS\\* |\n",
    "| GPU Memory | 80GB HBM2e | 80GB HBM2e |\n",
    "| GPU Memory Bandwidth | 1,935GB/s | 2,039GB/s |\n",
    "\n",
    "\n",
    "We're currently at:\n",
    "\n",
    "| Specification | A100 80GB PCIe | A100 80GB SXM |\n",
    "|---------------|----------------|---------------|\n",
    "| FP32 | 19.5 TFLOPS | 19.5 TFLOPS |\n",
    "\n",
    "but it turns out we don't really need that much precision for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7058b",
   "metadata": {},
   "source": [
    "| Format | Sign | Range (exponent) | Precision (mantissa) |\n",
    "|--------|------|------------------|----------------------|\n",
    "| FP32   | 1    | 8                | 23                   |\n",
    "| TF32   | 1    | 8                | 10                   |\n",
    "| FP16   | 1    | 5                | 10                   |\n",
    "| BF16   | 1    | 8                | 7                    |\n",
    "\n",
    "Notes:\n",
    "- All values are in bits.\n",
    "- FP32: Full 32-bit floating point\n",
    "- TF32: Tensor Float 32\n",
    "- FP16: Half-precision floating point\n",
    "- BF16: Brain Float 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45733ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found in cache: download_cache/4acd659e47adc1daeb7aff503accf0a3\n"
     ]
    }
   ],
   "source": [
    "from gpt_from_scratch import file_utils\n",
    "\n",
    "# load tinyshakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "input_filepath = file_utils.download_file_from_url(url)\n",
    "\n",
    "# Read all text from the input file\n",
    "input_text = input_filepath.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2cff7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40000  202651 1115394 download_cache/4acd659e47adc1daeb7aff503accf0a3\n"
     ]
    }
   ],
   "source": [
    "# lines | words | byte count\n",
    "!wc download_cache/4acd659e47adc1daeb7aff503accf0a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8eccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a705c78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338025 tokens\n",
      "1 epoch = 20 batches (steps to make one pass through data)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from gpt_from_scratch.gpt2_from_scratch import data_loader\n",
    "from gpt_from_scratch.gpt2_from_scratch.train_gpt2 import (\n",
    "    GPT,\n",
    "    GPTConfig,\n",
    "    get_best_available_torch_device,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# load text via dataloader\n",
    "#\n",
    "# note: we leave these on CPU, so that the dataloader\n",
    "#       isn't trying to hold the whole set on the GPU\n",
    "#\n",
    "#       so is prefetching moving more data to the GPU?\n",
    "tokens = tokenizer.encode(input_text)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# create a train loader that will continually give us new batches\n",
    "train_loader = data_loader.DataLoaderLite(B=4, T=32, tokens=tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65760046",
   "metadata": {},
   "source": [
    "# Timings\n",
    "\n",
    "| Run | Timing |\n",
    "|---  | ---    |\n",
    "|Initial timing with Float32 - (B=4, T=32) | step 49, loss: 6.804825782775879, dt: 136.36ms, tok/sec: 938.68 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cf1a858",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.30 GB, other allocations: 158.27 MB, max allowed: 18.13 GB). Tried to allocate 768.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m x, y \u001b[38;5;241m=\u001b[39m train_loader\u001b[38;5;241m.\u001b[39mnext_batch()\n\u001b[1;32m     21\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gpt_from_scratch/gpt_from_scratch/gpt2_from_scratch/train_gpt2.py:428\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, target)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# forward the blocks of the transformer\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 428\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# forward the final layernorm and the classifier\u001b[39;00m\n\u001b[1;32m    431\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n",
      "File \u001b[0;32m~/gpt_from_scratch/gpt_from_scratch/gpt2_from_scratch/train_gpt2.py:183\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    177\u001b[0m     x: Float32[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t c\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# communication - weighted sum function, reduce function\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# happens to every single token indivudally - the map function\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gpt_from_scratch/gpt_from_scratch/gpt2_from_scratch/train_gpt2.py:116\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# attention (materializes the large (T,T) matrix for all the queries and keys)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# note: this is our previous attention implementation\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m att \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m att \u001b[38;5;241m=\u001b[39m att\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias[:, :, :T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    118\u001b[0m att \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(att, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.30 GB, other allocations: 158.27 MB, max allowed: 18.13 GB). Tried to allocate 768.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# now we'll try multiple batches\n",
    "device = get_best_available_torch_device()\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "# Karpathy: \"AdamW is basically a bugfix of Adam\"\n",
    "#\n",
    "# note: pretty good default learning rate for early experimentation\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # here's where we actually move to GPU\n",
    "    x, y = train_loader.next_batch()\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000 # time difference in miliseconds\n",
    "\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"| step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15808574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some outputs to get an idea of where we are\n",
    "\n",
    "from gpt_from_scratch import tokenizer_utils\n",
    "\n",
    "def sample_model(\n",
    "    prompt: str,\n",
    "    num_samples: int,\n",
    "    max_tokens: int,\n",
    "    model: nn.Module,\n",
    "    tokenizer: tokenizer_utils.Tokenizer,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (5, 8)\n",
    "\n",
    "    # tokens in this case is just the prompt, and is small enough to fit on GPU\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    while x.size(1) < max_tokens:\n",
    "\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits, loss = model(x) # (B, T, vocab_size)\n",
    "\n",
    "            # take the logits at the last position\n",
    "            # throw away all the logits from things other than the last position\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            #\n",
    "            # \"anything lower than the 50th, we clamp to 0 and never sample it\"\n",
    "            #\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "    # print the generated text\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        tokens = x[i, :max_tokens].tolist()\n",
    "\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        \n",
    "        print(f\"\\n [{i}] >\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d57de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model(\n",
    "    prompt=\"Romeo\",\n",
    "    num_samples=5,\n",
    "    max_tokens=30,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e982fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3280e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a526be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load tinystories for comparison\n",
    "#\n",
    "# note: `datasets` can list datasets but is deprecated\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caad147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/docs/huggingface_hub/en/guides/download#from-latest-version\n",
    "import dataclasses\n",
    "from typing import Callable\n",
    "import pathlib\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TrainAndVal[T]:\n",
    "    \"\"\"Helper for common pattern of transforming both train and val.\"\"\"\n",
    "\n",
    "    train: T\n",
    "    val: T\n",
    "\n",
    "    def apply[R](self, func: Callable[[T], R]) -> 'TrainAndVal[R]':\n",
    "        return dataclasses.replace(self,\n",
    "            train=func(self.train),\n",
    "            val=func(self.val),\n",
    "        )\n",
    "\n",
    "def download_file_from_tinystories(filename: str) -> pathlib.Path:\n",
    "\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    filepath = huggingface_hub.hf_hub_download(\n",
    "        repo_id='roneneldan/TinyStories',\n",
    "        filename=filename,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    print(f\"Downloaded {filename} to {filepath}\")\n",
    "    return pathlib.Path(filepath)\n",
    "\n",
    "# original in paper\n",
    "# train_filename, val_filename = 'TinyStories-train.txt', 'TinyStories-valid.txt'\n",
    "\n",
    "# GPT-4 only, significantly larger but newer\n",
    "filenames = TrainAndVal('TinyStoriesV2-GPT4-train.txt', 'TinyStoriesV2-GPT4-valid.txt')\n",
    "\n",
    "# download\n",
    "filepaths = filenames.apply(download_file_from_tinystories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122227d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines | words | byte count\n",
    "!echo \"TinyShakespeare\"\n",
    "!wc download_cache/4acd659e47adc1daeb7aff503accf0a3\n",
    "\n",
    "!echo \"TinyStories\"\n",
    "!wc /Users/bronsonschoen/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class WordCount:\n",
    "    lines: int\n",
    "    words: int\n",
    "    bytes: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_tinyshakespeare = WordCount(lines=40000, words=202651, bytes=1115394)\n",
    "wc_tinystories = WordCount(lines=15600056, words=439223236, bytes=2227753162)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9eb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in dataclasses.fields(WordCount):\n",
    "    \n",
    "    field_tinyshakespeare = getattr(wc_tinyshakespeare, field.name)\n",
    "    field_tinystories = getattr(wc_tinystories, field.name)\n",
    "\n",
    "    ratio = float(field_tinystories) / float(field_tinyshakespeare)\n",
    "\n",
    "    print(f' - {field.name}: {round(ratio, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34af86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
