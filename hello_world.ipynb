{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f45cbc4-1119-46a8-b6a7-4f921d3caefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoreload extension loaded. Code changes will be automatically reloaded.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Autoreload extension loaded. Code changes will be automatically reloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b02282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens\n",
    "\n",
    "# TODO(bschoen): Just start using `transformer_lens.utils.get_device` from now on\n",
    "def get_best_available_torch_device() -> torch.device:\n",
    "    return transformer_lens.utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd81f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "648ceb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_from_scratch import python_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03d49543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb\n",
    "import os\n",
    "import dataclasses\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "\n",
    "import sae_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11a8d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x174c12300>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disable autograd, as we're focused on inference and this save us a lot of speed, memory, and annoying boilerplate\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9312149",
   "metadata": {},
   "source": [
    "### Notable Notebooks\n",
    "* [TransformerLens - Exploratory Analysis Demo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb#scrollTo=4iY8CVVSf3ru)\n",
    "* [TransformerLens - Activation Patching - Follow up To Exploratory Analysis Demo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Activation_Patching_in_TL_Demo.ipynb)\n",
    "* [SAELens - Training A Sparse Autoencoder](https://colab.research.google.com/github/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb#scrollTo=oAsZCAdJOVHw)\n",
    "* [SAELens - Tutorial 2.0](https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb)\n",
    "* [GemmaScope](https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d46ae44",
   "metadata": {},
   "source": [
    "### Interpretability Friendly Models\n",
    "\n",
    "From [My Interpretability-Friendly Models (in TransformerLens)](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=NCJ6zH_Okw_mUYAwGnMKsj2m)\n",
    "* Full list of models can be found here: [TransformerLens - Model Properties Table](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "\n",
    "A collection of models I trained and open sourced specifically for interpretability\n",
    "\n",
    "**Note:**\n",
    "* all models are trained with a `Beginning of Sequence` token\n",
    "  * will likely break if given inputs without that!\n",
    "  * example of prepending \"bos\" (`<|endoftext|>`) for GPT-2 [here](https://transformerlensorg.github.io/TransformerLens/generated/demos/Main_Demo.html#Dealing-with-tokens)\n",
    "\n",
    "**Available checkpoints:**\n",
    "* Each of these models has about `~200` checkpoints taken during training that can also be loaded from `TransformerLens`, with the `checkpoint_index` argument to `from_pretrained`\n",
    "* The checkpoint structure and labels is somewhat messy and ad-hoc, so I mostly recommend:\n",
    "  * using the `checkpoint_index` syntax (where you can just count from 0 to the number of checkpoints\n",
    "  * rather than `checkpoint_value` syntax (where you need to know the checkpoint schedule, and whether it was labelled with the number of tokens or steps)\n",
    "  * The helper function `get_checkpoint_labels` tells you the checkpoint schedule for a given model - ie what point was each checkpoint taken at, and what type of label was used.\n",
    "\n",
    "**Toy Models:** Inspired by A Mathematical Framework, I've trained 12 tiny language models:\n",
    "* 1-4L\n",
    "* each of width 512.\n",
    "* All models are trained on 22B tokens of data\n",
    "  * 80% from C4 (web text)\n",
    "  * 20% from Python Code\n",
    "* Models of the same layer size were trained with the same\n",
    "  * weight initialization\n",
    "  * data shuffle\n",
    "* to more directly compare the effect of different activation functions.\n",
    "\n",
    "I think that interpreting these is likely to be:\n",
    "* far more tractable than larger models\n",
    "* serve as good practice\n",
    "* will likely contain motifs and circuits that generalise to far larger models (like induction heads)\n",
    "\n",
    "#### Attention-Only (ie without MLPs)\n",
    "* attn-only-1l\n",
    "* attn-only-2l\n",
    "* attn-only-3l\n",
    "* attn-only-4l\n",
    "\n",
    "#### GELU models (ie with MLP, and the standard GELU activations)\n",
    "* gelu-1l\n",
    "* gelu-2l\n",
    "* gelu-3l\n",
    "* gelu-4l\n",
    "\n",
    "#### SoLU models (ie with MLP, and Anthropic's SoLU activation, designed to make MLP neurons more interpretable)\n",
    "* solu-1l\n",
    "* solu-2l\n",
    "* solu-3l\n",
    "* solu-4l\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6698a8",
   "metadata": {},
   "source": [
    "### Adding Hooks To An Existing Model\n",
    "\n",
    "Walkthrough [here](https://transformerlensorg.github.io/TransformerLens/generated/demos/Main_Demo.html#Toy-Example)\n",
    "\n",
    "> The key part of TransformerLens that lets us access and edit intermediate activations are the HookPoints around every model activation. Importantly, this technique will work for any model architecture, not just transformers, so long as youâ€™re able to edit the model code to add in HookPoints! This is essentially a lightweight library bundled with TransformerLens that should let you take an arbitrary model and make it easier to study.\n",
    "\n",
    "This is implemented by having a `HookPoint` layer. \n",
    "* Each transformer component has a `HookPoint` for every activation, which wraps around that activation.\n",
    "* The `HookPoint` acts as an identity function\n",
    "  * but has a variety of helper functions\n",
    "    * allows us to put PyTorch hooks in to\n",
    "      * edit\n",
    "      * access\n",
    "    * the relevant activation.\n",
    "\n",
    "There is also a HookedRootModule class - this is a utility class that the root module should inherit from (root module = the model we run) - it has several utility functions for using hooks well, notably reset_hooks, run_with_cache and run_with_hooks.\n",
    "\n",
    "The default interface is the run_with_hooks function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass.\n",
    "\n",
    "The syntax for a hook is function(activation, hook) where activation is the activation the hook is wrapped around, and hook is the HookPoint class the function is attached to. If the function returns a new activation or edits the activation in-place, that replaces the old one, if it returns None then the activation remains as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0314c",
   "metadata": {},
   "source": [
    "The reference implementation (suggested over `transformer_lens.HookedTransformer`) is:\n",
    " * [EasyTransformer(HookedRootModule)](https://github.com/redwoodresearch/Easy-Transformer/blob/main/easy_transformer/EasyTransformer.py#L54)\n",
    "   * This essentially does the \"take a transformer and add hooks\" part (in a general way though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7606b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we'll train a hookedtransformer from scratch as suggested by Neel et al\n",
    "# TODO(bschoen): Actually do this with a minimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c6a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait we can take numeric problems and translate them to\n",
    "# characters, which makes the interpretability much clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ec4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27b4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bronsonschoen/gpt_from_scratch/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# full list of model names can be found here: https://github.com/TransformerLensOrg/TransformerLens/blob/cb5017ad0f30cde0d3ac0b0f863c27fbec964c28/transformer_lens/loading_from_pretrained.py#L232\n",
    "# model_name = 'google/gemma-2-27b' # just way too big\n",
    "model_name = 'tiny-stories-1M'\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ace3634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a little girl named Lily. One day, Lily wanted to play in a big tree with her daddy. She went to school, not to go to the bookcase to say hello.\\n\\nOnce for a time, Lily made a'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a shiny crane. The good crane was very pretty. It had an expensive car.\\n\\nOne day, the crane was sad and started to cry. His friend, a little baby named Fluffy, had no toys. Fluffy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a little girl named Sue. Sue loved to go to the park with her family. One day, Sue came to the park closed the bones. She saw a big fat cat with a long tail and teeth. The cat was very loyal'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a little boy named Timmy. Timmy loved hot winter next to play in the snow because it had a big blue scarf. He went on the snow and followed it to get him in the icy snow. \\n\\nAs they'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a little girl named Lily. They loved to play and have fun together. One day, they went to a lecture together. It was a regular lecture.\\n\\nLily asked her friends, \"Can we go and play outside?\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we use generate to get 10 completeions with temperature 1. Feel free to play with the prompt to make it more interesting.\n",
    "for i in range(5):\n",
    "    display(\n",
    "        model.generate(\n",
    "            \"Once upon a time\",\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            temperature=1,\n",
    "            verbose=False,\n",
    "            max_new_tokens=50,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628f0e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtransformer_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprepend_space_to_answer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprint_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprepend_bos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[bool]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mtest_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Can't give type hint due to circular imports\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprepend_space_to_answer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprint_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprepend_bos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUSE_DEFAULT_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Test if the Model Can Give the Correct Answer to a Prompt.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Intended for exploratory analysis. Prints out the performance on the answer (rank, logit, prob),\u001b[0m\n",
      "\u001b[0;34m    as well as the top k tokens. Works for multi-token prompts and multi-token answers.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Warning:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This will print the results (it does not return them).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> from transformer_lens import HookedTransformer, utils\u001b[0m\n",
      "\u001b[0;34m    >>> model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\u001b[0m\n",
      "\u001b[0;34m    Loaded pretrained model tiny-stories-1M into HookedTransformer\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> prompt = \"Why did the elephant cross the\"\u001b[0m\n",
      "\u001b[0;34m    >>> answer = \"road\"\u001b[0m\n",
      "\u001b[0;34m    >>> utils.test_prompt(prompt, answer, model)\u001b[0m\n",
      "\u001b[0;34m    Tokenized prompt: ['<|endoftext|>', 'Why', ' did', ' the', ' elephant', ' cross', ' the']\u001b[0m\n",
      "\u001b[0;34m    Tokenized answer: [' road']\u001b[0m\n",
      "\u001b[0;34m    Performance on answer token:\u001b[0m\n",
      "\u001b[0;34m    Rank: 2        Logit: 14.24 Prob:  3.51% Token: | road|\u001b[0m\n",
      "\u001b[0;34m    Top 0th token. Logit: 14.51 Prob:  4.59% Token: | ground|\u001b[0m\n",
      "\u001b[0;34m    Top 1th token. Logit: 14.41 Prob:  4.18% Token: | tree|\u001b[0m\n",
      "\u001b[0;34m    Top 2th token. Logit: 14.24 Prob:  3.51% Token: | road|\u001b[0m\n",
      "\u001b[0;34m    Top 3th token. Logit: 14.22 Prob:  3.45% Token: | car|\u001b[0m\n",
      "\u001b[0;34m    Top 4th token. Logit: 13.92 Prob:  2.55% Token: | river|\u001b[0m\n",
      "\u001b[0;34m    Top 5th token. Logit: 13.79 Prob:  2.25% Token: | street|\u001b[0m\n",
      "\u001b[0;34m    Top 6th token. Logit: 13.77 Prob:  2.21% Token: | k|\u001b[0m\n",
      "\u001b[0;34m    Top 7th token. Logit: 13.75 Prob:  2.16% Token: | hill|\u001b[0m\n",
      "\u001b[0;34m    Top 8th token. Logit: 13.64 Prob:  1.92% Token: | swing|\u001b[0m\n",
      "\u001b[0;34m    Top 9th token. Logit: 13.46 Prob:  1.61% Token: | park|\u001b[0m\n",
      "\u001b[0;34m    Ranks of the answer tokens: [(' road', 2)]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        prompt:\u001b[0m\n",
      "\u001b[0;34m            The prompt string, e.g. \"Why did the elephant cross the\".\u001b[0m\n",
      "\u001b[0;34m        answer:\u001b[0m\n",
      "\u001b[0;34m            The answer, e.g. \"road\". Note that if you set prepend_space_to_answer to False, you need\u001b[0m\n",
      "\u001b[0;34m            to think about if you have a space before the answer here (as e.g. in this example the\u001b[0m\n",
      "\u001b[0;34m            answer may really be \" road\" if the prompt ends without a trailing space).\u001b[0m\n",
      "\u001b[0;34m        model:\u001b[0m\n",
      "\u001b[0;34m            The model.\u001b[0m\n",
      "\u001b[0;34m        prepend_space_to_answer:\u001b[0m\n",
      "\u001b[0;34m            Whether or not to prepend a space to the answer. Note this will only ever prepend a\u001b[0m\n",
      "\u001b[0;34m            space if the answer doesn't already start with one.\u001b[0m\n",
      "\u001b[0;34m        print_details:\u001b[0m\n",
      "\u001b[0;34m            Print the prompt (as a string but broken up by token), answer and top k tokens (all\u001b[0m\n",
      "\u001b[0;34m            with logit, rank and probability).\u001b[0m\n",
      "\u001b[0;34m        prepend_bos:\u001b[0m\n",
      "\u001b[0;34m            Overrides self.cfg.default_prepend_bos if set. Whether to prepend\u001b[0m\n",
      "\u001b[0;34m            the BOS token to the input (applicable when input is a string). Models generally learn\u001b[0m\n",
      "\u001b[0;34m            to use the BOS token as a resting place for attention heads (i.e. a way for them to be\u001b[0m\n",
      "\u001b[0;34m            \"turned off\"). This therefore often improves performance slightly.\u001b[0m\n",
      "\u001b[0;34m        top_k:\u001b[0m\n",
      "\u001b[0;34m            Top k tokens to print details of (when print_details is set to True).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m        None (just prints the results directly).\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mprepend_space_to_answer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# GPT-2 often treats the first token weirdly, so lets give it a resting position\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_bos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manswer_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt_str_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_str_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_bos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manswer_str_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_str_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_str_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manswer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_str_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mprint_details\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenized prompt:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_str_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenized answer:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_str_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manswer_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0manswer_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0manswer_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0manswer_str_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_str_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprompt_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Offset by 1 because models predict the NEXT token\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtoken_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msorted_token_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_token_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Janky way to get the index of the token in the sorted list - I couldn't find a better way?\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcorrect_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_token_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0msorted_token_values\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manswer_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0manswer_ranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_str_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_rank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mprint_details\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# rprint gives rich text printing\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mrprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"\u001b[0m\u001b[0;34mPerformance on answer token:\\n[b]Rank: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcorrect_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m <8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m Logit: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m5.2f\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m Prob: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtoken_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m6.2%\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m Token: |\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0manswer_str_token\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m|[/b]\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34mf\"\u001b[0m\u001b[0;34mTop \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34mth token. Logit: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_token_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m5.2f\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m Prob: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0msorted_token_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m6.2%\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m Token: |\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_token_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m[b]Ranks of the answer tokens:[/b] \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0manswer_ranks\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/gpt_from_scratch/venv/lib/python3.12/site-packages/transformer_lens/utils.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "transformer_lens.utils.test_prompt??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83319ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'J', 'ill', ' threw', ' the', ' ball', ' to', ' Jack', '.', ' Jack', ' threw', ' the', ' ball', ' to', ' Will', '.', ' Will', ' threw', ' the', ' ball', ' back', ' to']\n",
      "Tokenized answer: [' Jill']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.51</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36.14</span><span style=\"font-weight: bold\">% Token: | Jill|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m21.51\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m36.14\u001b[0m\u001b[1m% Token: | Jill|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 21.51 Prob: 36.14% Token: | Jill|\n",
      "Top 1th token. Logit: 21.47 Prob: 34.66% Token: | Jack|\n",
      "Top 2th token. Logit: 18.90 Prob:  2.65% Token: | Jane|\n",
      "Top 3th token. Logit: 18.75 Prob:  2.30% Token: | the|\n",
      "Top 4th token. Logit: 18.60 Prob:  1.98% Token: | him|\n",
      "Top 5th token. Logit: 18.37 Prob:  1.56% Token: | Mia|\n",
      "Top 6th token. Logit: 18.36 Prob:  1.56% Token: | show|\n",
      "Top 7th token. Logit: 18.34 Prob:  1.53% Token: | Lily|\n",
      "Top 8th token. Logit: 18.33 Prob:  1.50% Token: | Ben|\n",
      "Top 9th token. Logit: 18.13 Prob:  1.24% Token: | Sam|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Jill'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Jill'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test if the Model Can Give the Correct Answer to a Prompt.\n",
    "#\n",
    "# Intended for exploratory analysis. Prints out the performance on the answer (rank, logit, prob),\n",
    "# as well as the top k tokens. Works for multi-token prompts and multi-token answers.\n",
    "transformer_lens.utils.test_prompt(\n",
    "    prompt='Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to',\n",
    "    answer=' Jill',\n",
    "    model=model,\n",
    "    prepend_space_to_answer=True, # default\n",
    "    print_details=True, # default\n",
    "    prepend_bos=None, # default\n",
    "    top_k=10, # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e8e5a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-f022d357-05c1\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-f022d357-05c1\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<|endoftext|>\", \"J\", \"ill\", \" threw\", \" the\", \" ball\", \" to\", \" Jack\", \".\", \" Jack\", \" threw\", \" the\", \" ball\", \" to\", \" Will\", \".\", \" Will\", \" threw\", \" the\", \" ball\", \" back\", \" to\", \" Jill\", \".\"], \"topKLogProbs\": [[-0.5934329032897949, -0.8225436210632324, -6.80027437210083, -7.126723766326904, -7.372302532196045, -7.41232442855835, -7.713424205780029, -7.727107524871826, -8.219684600830078, -8.278583526611328], [-0.292296439409256, -2.9978532791137695, -3.3918943405151367, -3.6969423294067383, -4.66084098815918, -4.706521034240723, -5.42949104309082, -5.514911651611328, -5.642189979553223, -5.654000282287598], [-0.22012566030025482, -2.480093479156494, -4.011721134185791, -4.168293476104736, -4.817903995513916, -4.927759647369385, -5.2866387367248535, -5.303089618682861, -5.361727237701416, -5.654440402984619], [-1.1415125131607056, -1.440123200416565, -1.6939531564712524, -3.8923211097717285, -4.098717212677002, -4.111515522003174, -4.180269718170166, -4.364828586578369, -4.393873691558838, -4.5630364418029785], [-2.036240339279175, -3.4959027767181396, -3.865129232406616, -3.9156596660614014, -3.9601800441741943, -4.264829635620117, -4.3772687911987305, -4.463534355163574, -4.474489212036133, -4.5156402587890625], [-1.2285792827606201, -1.6577479839324951, -2.3679301738739014, -2.6465232372283936, -2.9170215129852295, -3.007796049118042, -3.218365430831909, -3.2210605144500732, -4.269207954406738, -4.500288963317871], [-1.1613678932189941, -1.6948838233947754, -2.7776684761047363, -2.98740816116333, -3.177929401397705, -3.26456880569458, -3.641390323638916, -3.66685152053833, -3.761824131011963, -3.8575406074523926], [-0.5644032955169678, -1.3324201107025146, -2.2123563289642334, -4.836643218994141, -4.920103073120117, -4.942852020263672, -5.259057998657227, -5.345684051513672, -5.96967887878418, -5.999570846557617], [-1.1008540391921997, -1.3649035692214966, -2.1288976669311523, -2.407965660095215, -3.150139808654785, -3.57071590423584, -3.746657371520996, -4.132142066955566, -4.8261823654174805, -5.146512031555176], [-0.6550359129905701, -2.453000068664551, -2.7393407821655273, -3.2382116317749023, -3.663699150085449, -3.7543516159057617, -4.358124732971191, -4.437529563903809, -4.5435686111450195, -4.566424369812012], [-0.47590482234954834, -1.4417346715927124, -3.2646164894104004, -3.2865891456604004, -4.473898410797119, -4.6549859046936035, -5.436020374298096, -5.677431583404541, -6.119879245758057, -6.226995944976807], [-0.09741981327533722, -4.807788848876953, -5.0341796875, -5.1096343994140625, -5.483171463012695, -5.683038711547852, -5.719146728515625, -5.894802093505859, -5.924582481384277, -5.977922439575195], [-2.084144115447998, -2.0880980491638184, -2.1689677238464355, -2.4609313011169434, -2.8253207206726074, -2.9369425773620605, -2.963479518890381, -2.9947047233581543, -3.153052806854248, -3.7138800621032715], [-0.6718773245811462, -1.4167943000793457, -3.5867199897766113, -3.6662049293518066, -3.8429112434387207, -4.2016282081604, -4.403578281402588, -4.526743412017822, -4.592731952667236, -4.661001682281494], [-0.44033896923065186, -2.679781913757324, -2.7372560501098633, -2.8551759719848633, -3.4560155868530273, -3.613429069519043, -4.591231346130371, -5.089444160461426, -5.098851203918457, -5.265048027038574], [-1.5251798629760742, -1.6255559921264648, -1.8529233932495117, -2.9215269088745117, -2.9635000228881836, -3.06790828704834, -3.159022331237793, -3.433518409729004, -3.457573890686035, -4.020110130310059], [-1.4995787143707275, -2.2044870853424072, -2.783874750137329, -3.1632273197174072, -3.1664068698883057, -3.2560064792633057, -3.5318644046783447, -3.5901434421539307, -3.6712019443511963, -3.6939337253570557], [-0.1282704919576645, -2.3280746936798096, -5.365756511688232, -5.588225841522217, -5.675343990325928, -6.465550899505615, -7.120730876922607, -7.175229549407959, -7.456592082977295, -7.55026388168335], [-0.037312883883714676, -5.778802394866943, -5.945353984832764, -6.109157085418701, -6.419095516204834, -6.543952465057373, -6.959134578704834, -7.060349941253662, -7.260275363922119, -7.295469760894775], [-1.7065759897232056, -2.264465808868408, -2.2648072242736816, -2.331587314605713, -2.6120495796203613, -2.7491135597229004, -3.2253193855285645, -3.2529587745666504, -3.3263535499572754, -3.690718173980713], [-1.1132094860076904, -1.9566333293914795, -2.0513522624969482, -2.350559949874878, -2.8183085918426514, -3.1809165477752686, -3.3500449657440186, -3.4840457439422607, -3.5392959117889404, -4.176163673400879], [-1.017628788948059, -1.059498906135559, -3.6293954849243164, -3.774165153503418, -3.9233694076538086, -4.159289360046387, -4.1617326736450195, -4.182616233825684, -4.1980791091918945, -4.3923540115356445], [-0.5232197046279907, -1.7246795892715454, -2.0910887718200684, -3.7634711265563965, -4.8679118156433105, -4.898148059844971, -5.075856685638428, -5.267436504364014, -5.421945095062256, -5.422755718231201]], \"topKTokens\": [[\"\\n\\n\", \"\\n\", \"<|endoftext|>\", \" \", \" once\", \".\", \" The\", \"?\", \"e\", \" was\"], [\"ill\", \":\", \"aney\", \"od\", \"enny\", \"em\", \"enna\", \"anny\", \"ody\", \"ina\"], [\" and\", \" was\", \".\", \",\", \"y\", \" felt\", \" wanted\", \" went\", \" liked\", \" loved\"], [\" her\", \" the\", \" a\", \" some\", \" to\", \" one\", \" and\", \" down\", \" it\", \" at\"], [\" ball\", \" bird\", \" toy\", \" balloon\", \" car\", \" dog\", \" rope\", \" big\", \" red\", \" scissors\"], [\" and\", \".\", \" in\", \" at\", \" to\", \" with\", \",\", \" on\", \" over\", \" back\"], [\" the\", \" her\", \" Sam\", \" Tom\", \" it\", \" see\", \" Lily\", \" Jill\", \" Ben\", \" a\"], [\".\", \"'s\", \" and\", \",\", \" who\", \"\\ufffd\", \"'\", \" in\", \" with\", \" to\"], [\" He\", \" Jack\", \" She\", \" It\", \" \\\"\", \" They\", \" Jill\", \" The\", \" Tim\", \" Sam\"], [\" was\", \" had\", \" wanted\", \" liked\", \" and\", \" loved\", \"'s\", \" threw\", \" looked\", \" saw\"], [\" the\", \" it\", \" a\", \" his\", \" one\", \" some\", \" and\", \" Lily\", \" her\", \" all\"], [\" ball\", \" hoop\", \" red\", \" toy\", \" stick\", \" big\", \" pieces\", \" rope\", \" balls\", \" string\"], [\" and\", \" to\", \" at\", \" too\", \" on\", \" in\", \" with\", \" very\", \" over\", \" around\"], [\" Jill\", \" Jack\", \" Sam\", \" him\", \" the\", \" his\", \" her\", \" Lily\", \" Ben\", \" Jenny\"], [\".\", \"!\", \",\", \" and\", \"'s\", \" to\", \"s\", \" it\", \" in\", \"y\"], [\" \\\"\", \" He\", \" It\", \" The\", \" Lily\", \" Jill\", \" Jack\", \"\\n\", \" \", \" But\"], [\" was\", \" it\", \" could\", \" else\", \" laughed\", \" did\", \" had\", \" ever\", \" in\", \" wanted\"], [\" it\", \" the\", \" one\", \" this\", \" a\", \" his\", \" on\", \" Lily\", \" and\", \" some\"], [\" ball\", \" red\", \" hoop\", \" big\", \" toy\", \" balls\", \" round\", \" stick\", \" rope\", \" string\"], [\" to\", \" back\", \" at\", \" too\", \" over\", \" down\", \" with\", \" on\", \" in\", \" and\"], [\" and\", \".\", \" to\", \",\", \" in\", \" down\", \" on\", \" at\", \" together\", \" up\"], [\" Jill\", \" Jack\", \" Jane\", \" the\", \" him\", \" Mia\", \" show\", \" Lily\", \" Ben\", \" Sam\"], [\".\", \"'s\", \" and\", \",\", \"!\", \" to\", \" in\", \" who\", \" at\", \"?\"]], \"correctTokenRank\": [722, 0, 204, 1, 0, 4, 10, 0, 1, 7, 0, 0, 1, 419, 0, 35, 84, 1, 0, 1, 2, 0, 0], \"correctTokenLogProb\": [-15.411867141723633, -0.292296439409256, -10.88743782043457, -1.440123200416565, -2.036240339279175, -2.9170215129852295, -4.022715091705322, -0.5644032955169678, -1.3649035692214966, -4.437529563903809, -0.47590482234954834, -0.09741981327533722, -2.0880980491638184, -12.182851791381836, -0.44033896923065186, -6.421948432922363, -6.87144660949707, -2.3280746936798096, -0.037312883883714676, -2.264465808868408, -2.0513522624969482, -1.017628788948059, -0.5232197046279907]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x309f1ff80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This essentially lets us see the confidence {and alternatives} of the tokens\n",
    "import circuitsvis as cv\n",
    "\n",
    "# Let's make a longer prompt and see the log probabilities of the tokens\n",
    "# note: log_softmax converts logits to log probabilities\n",
    "#\n",
    "example_prompt = 'Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to Jill.'\n",
    "logits, cache = model.run_with_cache(example_prompt)\n",
    "\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=model.to_tokens(example_prompt),\n",
    "    log_probs=model(example_prompt)[0].log_softmax(dim=-1),\n",
    "    to_string=model.to_string,\n",
    ")\n",
    "# hover on the output to see the result.\n",
    "#\n",
    "# ex: model's very confident that the thing Jack is about to throw is the ball\n",
    "# ex: not sure whether Will is going to throw it to Jill or Jack (neither am I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: jbloom advice is to make a run_comparer here: https://docs.wandb.ai/guides/app/features/panels/run-comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35d3d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_prompt_as_tokens.shape=torch.Size([1, 24])\n",
      "result_batch.shape=torch.Size([1, 24, 50257])\n",
      "result_logits.shape=torch.Size([24, 50257])\n",
      "result_log_probs.shape=torch.Size([24, 50257])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-aa8cc02e-8a70\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-aa8cc02e-8a70\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<|endoftext|>\", \"J\", \"ill\", \" threw\", \" the\", \" ball\", \" to\", \" Jack\", \".\", \" Jack\", \" threw\", \" the\", \" ball\", \" to\", \" Will\", \".\", \" Will\", \" threw\", \" the\", \" ball\", \" back\", \" to\", \" Jill\", \".\"], \"topKLogProbs\": [[-0.5934329032897949, -0.8225436210632324, -6.80027437210083, -7.126723766326904, -7.372302532196045, -7.41232442855835, -7.713424205780029, -7.727107524871826, -8.219684600830078, -8.278583526611328], [-0.292296439409256, -2.9978532791137695, -3.3918943405151367, -3.6969423294067383, -4.66084098815918, -4.706521034240723, -5.42949104309082, -5.514911651611328, -5.642189979553223, -5.654000282287598], [-0.22012566030025482, -2.480093479156494, -4.011721134185791, -4.168293476104736, -4.817903995513916, -4.927759647369385, -5.2866387367248535, -5.303089618682861, -5.361727237701416, -5.654440402984619], [-1.1415125131607056, -1.440123200416565, -1.6939531564712524, -3.8923211097717285, -4.098717212677002, -4.111515522003174, -4.180269718170166, -4.364828586578369, -4.393873691558838, -4.5630364418029785], [-2.036240339279175, -3.4959027767181396, -3.865129232406616, -3.9156596660614014, -3.9601800441741943, -4.264829635620117, -4.3772687911987305, -4.463534355163574, -4.474489212036133, -4.5156402587890625], [-1.2285792827606201, -1.6577479839324951, -2.3679301738739014, -2.6465232372283936, -2.9170215129852295, -3.007796049118042, -3.218365430831909, -3.2210605144500732, -4.269207954406738, -4.500288963317871], [-1.1613678932189941, -1.6948838233947754, -2.7776684761047363, -2.98740816116333, -3.177929401397705, -3.26456880569458, -3.641390323638916, -3.66685152053833, -3.761824131011963, -3.8575406074523926], [-0.5644032955169678, -1.3324201107025146, -2.2123563289642334, -4.836643218994141, -4.920103073120117, -4.942852020263672, -5.259057998657227, -5.345684051513672, -5.96967887878418, -5.999570846557617], [-1.1008540391921997, -1.3649035692214966, -2.1288976669311523, -2.407965660095215, -3.150139808654785, -3.57071590423584, -3.746657371520996, -4.132142066955566, -4.8261823654174805, -5.146512031555176], [-0.6550359129905701, -2.453000068664551, -2.7393407821655273, -3.2382116317749023, -3.663699150085449, -3.7543516159057617, -4.358124732971191, -4.437529563903809, -4.5435686111450195, -4.566424369812012], [-0.47590482234954834, -1.4417346715927124, -3.2646164894104004, -3.2865891456604004, -4.473898410797119, -4.6549859046936035, -5.436020374298096, -5.677431583404541, -6.119879245758057, -6.226995944976807], [-0.09741981327533722, -4.807788848876953, -5.0341796875, -5.1096343994140625, -5.483171463012695, -5.683038711547852, -5.719146728515625, -5.894802093505859, -5.924582481384277, -5.977922439575195], [-2.084144115447998, -2.0880980491638184, -2.1689677238464355, -2.4609313011169434, -2.8253207206726074, -2.9369425773620605, -2.963479518890381, -2.9947047233581543, -3.153052806854248, -3.7138800621032715], [-0.6718773245811462, -1.4167943000793457, -3.5867199897766113, -3.6662049293518066, -3.8429112434387207, -4.2016282081604, -4.403578281402588, -4.526743412017822, -4.592731952667236, -4.661001682281494], [-0.44033896923065186, -2.679781913757324, -2.7372560501098633, -2.8551759719848633, -3.4560155868530273, -3.613429069519043, -4.591231346130371, -5.089444160461426, -5.098851203918457, -5.265048027038574], [-1.5251798629760742, -1.6255559921264648, -1.8529233932495117, -2.9215269088745117, -2.9635000228881836, -3.06790828704834, -3.159022331237793, -3.433518409729004, -3.457573890686035, -4.020110130310059], [-1.4995787143707275, -2.2044870853424072, -2.783874750137329, -3.1632273197174072, -3.1664068698883057, -3.2560064792633057, -3.5318644046783447, -3.5901434421539307, -3.6712019443511963, -3.6939337253570557], [-0.1282704919576645, -2.3280746936798096, -5.365756511688232, -5.588225841522217, -5.675343990325928, -6.465550899505615, -7.120730876922607, -7.175229549407959, -7.456592082977295, -7.55026388168335], [-0.037312883883714676, -5.778802394866943, -5.945353984832764, -6.109157085418701, -6.419095516204834, -6.543952465057373, -6.959134578704834, -7.060349941253662, -7.260275363922119, -7.295469760894775], [-1.7065759897232056, -2.264465808868408, -2.2648072242736816, -2.331587314605713, -2.6120495796203613, -2.7491135597229004, -3.2253193855285645, -3.2529587745666504, -3.3263535499572754, -3.690718173980713], [-1.1132094860076904, -1.9566333293914795, -2.0513522624969482, -2.350559949874878, -2.8183085918426514, -3.1809165477752686, -3.3500449657440186, -3.4840457439422607, -3.5392959117889404, -4.176163673400879], [-1.017628788948059, -1.059498906135559, -3.6293954849243164, -3.774165153503418, -3.9233694076538086, -4.159289360046387, -4.1617326736450195, -4.182616233825684, -4.1980791091918945, -4.3923540115356445], [-0.5232197046279907, -1.7246795892715454, -2.0910887718200684, -3.7634711265563965, -4.8679118156433105, -4.898148059844971, -5.075856685638428, -5.267436504364014, -5.421945095062256, -5.422755718231201]], \"topKTokens\": [[\"\\n\\n\", \"\\n\", \"<|endoftext|>\", \" \", \" once\", \".\", \" The\", \"?\", \"e\", \" was\"], [\"ill\", \":\", \"aney\", \"od\", \"enny\", \"em\", \"enna\", \"anny\", \"ody\", \"ina\"], [\" and\", \" was\", \".\", \",\", \"y\", \" felt\", \" wanted\", \" went\", \" liked\", \" loved\"], [\" her\", \" the\", \" a\", \" some\", \" to\", \" one\", \" and\", \" down\", \" it\", \" at\"], [\" ball\", \" bird\", \" toy\", \" balloon\", \" car\", \" dog\", \" rope\", \" big\", \" red\", \" scissors\"], [\" and\", \".\", \" in\", \" at\", \" to\", \" with\", \",\", \" on\", \" over\", \" back\"], [\" the\", \" her\", \" Sam\", \" Tom\", \" it\", \" see\", \" Lily\", \" Jill\", \" Ben\", \" a\"], [\".\", \"'s\", \" and\", \",\", \" who\", \"\\ufffd\", \"'\", \" in\", \" with\", \" to\"], [\" He\", \" Jack\", \" She\", \" It\", \" \\\"\", \" They\", \" Jill\", \" The\", \" Tim\", \" Sam\"], [\" was\", \" had\", \" wanted\", \" liked\", \" and\", \" loved\", \"'s\", \" threw\", \" looked\", \" saw\"], [\" the\", \" it\", \" a\", \" his\", \" one\", \" some\", \" and\", \" Lily\", \" her\", \" all\"], [\" ball\", \" hoop\", \" red\", \" toy\", \" stick\", \" big\", \" pieces\", \" rope\", \" balls\", \" string\"], [\" and\", \" to\", \" at\", \" too\", \" on\", \" in\", \" with\", \" very\", \" over\", \" around\"], [\" Jill\", \" Jack\", \" Sam\", \" him\", \" the\", \" his\", \" her\", \" Lily\", \" Ben\", \" Jenny\"], [\".\", \"!\", \",\", \" and\", \"'s\", \" to\", \"s\", \" it\", \" in\", \"y\"], [\" \\\"\", \" He\", \" It\", \" The\", \" Lily\", \" Jill\", \" Jack\", \"\\n\", \" \", \" But\"], [\" was\", \" it\", \" could\", \" else\", \" laughed\", \" did\", \" had\", \" ever\", \" in\", \" wanted\"], [\" it\", \" the\", \" one\", \" this\", \" a\", \" his\", \" on\", \" Lily\", \" and\", \" some\"], [\" ball\", \" red\", \" hoop\", \" big\", \" toy\", \" balls\", \" round\", \" stick\", \" rope\", \" string\"], [\" to\", \" back\", \" at\", \" too\", \" over\", \" down\", \" with\", \" on\", \" in\", \" and\"], [\" and\", \".\", \" to\", \",\", \" in\", \" down\", \" on\", \" at\", \" together\", \" up\"], [\" Jill\", \" Jack\", \" Jane\", \" the\", \" him\", \" Mia\", \" show\", \" Lily\", \" Ben\", \" Sam\"], [\".\", \"'s\", \" and\", \",\", \"!\", \" to\", \" in\", \" who\", \" at\", \"?\"]], \"correctTokenRank\": [722, 0, 204, 1, 0, 4, 10, 0, 1, 7, 0, 0, 1, 419, 0, 35, 84, 1, 0, 1, 2, 0, 0], \"correctTokenLogProb\": [-15.411867141723633, -0.292296439409256, -10.88743782043457, -1.440123200416565, -2.036240339279175, -2.9170215129852295, -4.022715091705322, -0.5644032955169678, -1.3649035692214966, -4.437529563903809, -0.47590482234954834, -0.09741981327533722, -2.0880980491638184, -12.182851791381836, -0.44033896923065186, -6.421948432922363, -6.87144660949707, -2.3280746936798096, -0.037312883883714676, -2.264465808868408, -2.0513522624969482, -1.017628788948059, -0.5232197046279907]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x309e9fb90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's break that down\n",
    "#\n",
    "example_prompt = 'Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to Jill.'\n",
    "\n",
    "# tokenize prompt\n",
    "example_prompt_as_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "print(f\"{example_prompt_as_tokens.shape=}\")\n",
    "\n",
    "# get the logits for each NEXT token in the prompt\n",
    "# note: is 1 just the batch size?\n",
    "result_batch: Float32[torch.Tensor, '1 num_input_tokens vocab_size'] = model(example_prompt)\n",
    "\n",
    "print(f\"{result_batch.shape=}\")\n",
    "\n",
    "result_logits: Float32[torch.Tensor, 'num_input_tokens vocab_size'] = result_batch[0]\n",
    "\n",
    "print(f\"{result_logits.shape=}\")\n",
    "\n",
    "result_log_probs = result_logits.log_softmax(dim=-1)\n",
    "\n",
    "print(f\"{result_log_probs.shape=}\")\n",
    "\n",
    "# finally we can visualize\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=example_prompt_as_tokens,\n",
    "    log_probs=result_log_probs,\n",
    "    to_string=model.to_string,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e78a8",
   "metadata": {},
   "source": [
    "## Loading A Pretrained Sparse Autoencoder\n",
    "\n",
    "In practice, SAEs can be of varying usefulness for general use cases. To start with, we recommend the following:\n",
    "\n",
    "* Joseph's Open Source GPT2 Small Residual (gpt2-small-res-jb)\n",
    "* Joseph's Feature Splitting (gpt2-small-res-jb-feature-splitting)\n",
    "* Gemma SAEs (gemma-2b-res-jb) (0,6) <- on Neuronpedia and good. (12 / 17 aren't very good currently).\n",
    "\n",
    "Other SAEs have various issues--e.g., too dense or not dense enough, or designed for special use cases, or initial drafts of what we hope will be better versions later. Decode Research / Neuronpedia are working on making all SAEs on Neuronpedia loadable in SAE Lens and vice versa, as well as providing public benchmarking stats to help people choose which SAEs to work with.\n",
    "\n",
    "To see all the SAEs contained in a specific release (named after the part of the model they apply to), simply run the below. Each hook point corresponds to a layer or module of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69a8e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sae_lens.toolkit\n",
    "import sae_lens.toolkit.pretrained_saes_directory\n",
    "import sae_lens.toolkit.pretrained_sae_loaders\n",
    "\n",
    "from sae_lens.toolkit.pretrained_saes_directory import PretrainedSAELookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "707dbd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained SAE loaders:\n",
      " - sae_lens\n",
      " - connor_rob_hook_z\n",
      " - gemma_2\n"
     ]
    }
   ],
   "source": [
    "print('Pretrained SAE loaders:')\n",
    "for name in sae_lens.toolkit.pretrained_sae_loaders.NAMED_PRETRAINED_SAE_LOADERS.keys():\n",
    "    print(f' - {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8330fa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 pretrained SAEs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>release</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JoshEngels/Mistral-7B-Residual-Stream-SAEs</td>\n",
       "      <td>mistral-7b-res-wg</td>\n",
       "      <td>mistral-7b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ckkissane/attn-saes-gpt2-small-all-layers</td>\n",
       "      <td>gpt2-small-hook-z-kk</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ctigges/pythia-70m-deduped__att-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-att-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ctigges/pythia-70m-deduped__mlp-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-mlp-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ctigges/pythia-70m-deduped__res-sm_processed</td>\n",
       "      <td>pythia-70m-deduped-res-sm</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>google/gemma-scope-27b-pt-res</td>\n",
       "      <td>gemma-scope-27b-pt-res-canonical</td>\n",
       "      <td>gemma-2-27b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>gemma-2-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>google/gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>google/gemma-scope-9b-it-res</td>\n",
       "      <td>gemma-scope-9b-it-res-canonical</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>google/gemma-scope-9b-pt-att</td>\n",
       "      <td>gemma-scope-9b-pt-att-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>google/gemma-scope-9b-pt-mlp</td>\n",
       "      <td>gemma-scope-9b-pt-mlp-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>google/gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>google/gemma-scope-9b-pt-res</td>\n",
       "      <td>gemma-scope-9b-pt-res-canonical</td>\n",
       "      <td>gemma-2-9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jbloom/GPT2-Small-Feature-Splitting-Experiment...</td>\n",
       "      <td>gpt2-small-res-jb-feature-splitting</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small-attn-out-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small-mlp-out-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small-resid-mid-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small-resid-post-v5-128k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs</td>\n",
       "      <td>gpt2-small-attn-out-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs</td>\n",
       "      <td>gpt2-small-mlp-out-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs</td>\n",
       "      <td>gpt2-small-resid-mid-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs</td>\n",
       "      <td>gpt2-small-resid-post-v5-32k</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jbloom/GPT2-Small-SAEs-Reformatted</td>\n",
       "      <td>gpt2-small-res-jb</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jbloom/Gemma-2b-IT-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-it-res-jb</td>\n",
       "      <td>gemma-2b-it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jbloom/Gemma-2b-Residual-Stream-SAEs</td>\n",
       "      <td>gemma-2b-res-jb</td>\n",
       "      <td>gemma-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sce-ajt</td>\n",
       "      <td>gpt2-small-res_sce-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>neuronpedia/gpt2-small__res_scefr-ajt</td>\n",
       "      <td>gpt2-small-res_scefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>neuronpedia/gpt2-small__res_scl-ajt</td>\n",
       "      <td>gpt2-small-res_scl-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sle-ajt</td>\n",
       "      <td>gpt2-small-res_sle-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>neuronpedia/gpt2-small__res_slefr-ajt</td>\n",
       "      <td>gpt2-small-res_slefr-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>neuronpedia/gpt2-small__res_sll-ajt</td>\n",
       "      <td>gpt2-small-res_sll-ajt</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tommmcgrath/gpt2-small-mlp-out-saes</td>\n",
       "      <td>gpt2-small-mlp-tm</td>\n",
       "      <td>gpt2-small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              repo_id  \\\n",
       "8          JoshEngels/Mistral-7B-Residual-Stream-SAEs   \n",
       "1           ckkissane/attn-saes-gpt2-small-all-layers   \n",
       "33       ctigges/pythia-70m-deduped__att-sm_processed   \n",
       "32       ctigges/pythia-70m-deduped__mlp-sm_processed   \n",
       "31       ctigges/pythia-70m-deduped__res-sm_processed   \n",
       "29                      google/gemma-scope-27b-pt-res   \n",
       "30                      google/gemma-scope-27b-pt-res   \n",
       "19                       google/gemma-scope-2b-pt-att   \n",
       "20                       google/gemma-scope-2b-pt-att   \n",
       "17                       google/gemma-scope-2b-pt-mlp   \n",
       "18                       google/gemma-scope-2b-pt-mlp   \n",
       "15                       google/gemma-scope-2b-pt-res   \n",
       "16                       google/gemma-scope-2b-pt-res   \n",
       "27                       google/gemma-scope-9b-it-res   \n",
       "28                       google/gemma-scope-9b-it-res   \n",
       "23                       google/gemma-scope-9b-pt-att   \n",
       "24                       google/gemma-scope-9b-pt-att   \n",
       "25                       google/gemma-scope-9b-pt-mlp   \n",
       "26                       google/gemma-scope-9b-pt-mlp   \n",
       "21                       google/gemma-scope-9b-pt-res   \n",
       "22                       google/gemma-scope-9b-pt-res   \n",
       "3   jbloom/GPT2-Small-Feature-Splitting-Experiment...   \n",
       "14        jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs   \n",
       "12         jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs   \n",
       "10       jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs   \n",
       "5       jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs   \n",
       "13         jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs   \n",
       "11          jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs   \n",
       "9         jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs   \n",
       "4        jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs   \n",
       "0                  jbloom/GPT2-Small-SAEs-Reformatted   \n",
       "7             jbloom/Gemma-2b-IT-Residual-Stream-SAEs   \n",
       "6                jbloom/Gemma-2b-Residual-Stream-SAEs   \n",
       "38                neuronpedia/gpt2-small__res_sce-ajt   \n",
       "39              neuronpedia/gpt2-small__res_scefr-ajt   \n",
       "36                neuronpedia/gpt2-small__res_scl-ajt   \n",
       "37                neuronpedia/gpt2-small__res_sle-ajt   \n",
       "35              neuronpedia/gpt2-small__res_slefr-ajt   \n",
       "34                neuronpedia/gpt2-small__res_sll-ajt   \n",
       "2                 tommmcgrath/gpt2-small-mlp-out-saes   \n",
       "\n",
       "                                release               model  \n",
       "8                     mistral-7b-res-wg          mistral-7b  \n",
       "1                  gpt2-small-hook-z-kk          gpt2-small  \n",
       "33            pythia-70m-deduped-att-sm  pythia-70m-deduped  \n",
       "32            pythia-70m-deduped-mlp-sm  pythia-70m-deduped  \n",
       "31            pythia-70m-deduped-res-sm  pythia-70m-deduped  \n",
       "29               gemma-scope-27b-pt-res         gemma-2-27b  \n",
       "30     gemma-scope-27b-pt-res-canonical         gemma-2-27b  \n",
       "19                gemma-scope-2b-pt-att          gemma-2-2b  \n",
       "20      gemma-scope-2b-pt-att-canonical          gemma-2-2b  \n",
       "17                gemma-scope-2b-pt-mlp          gemma-2-2b  \n",
       "18      gemma-scope-2b-pt-mlp-canonical          gemma-2-2b  \n",
       "15                gemma-scope-2b-pt-res          gemma-2-2b  \n",
       "16      gemma-scope-2b-pt-res-canonical          gemma-2-2b  \n",
       "27                gemma-scope-9b-it-res          gemma-2-9b  \n",
       "28      gemma-scope-9b-it-res-canonical       gemma-2-9b-it  \n",
       "23                gemma-scope-9b-pt-att          gemma-2-9b  \n",
       "24      gemma-scope-9b-pt-att-canonical          gemma-2-9b  \n",
       "25                gemma-scope-9b-pt-mlp          gemma-2-9b  \n",
       "26      gemma-scope-9b-pt-mlp-canonical          gemma-2-9b  \n",
       "21                gemma-scope-9b-pt-res          gemma-2-9b  \n",
       "22      gemma-scope-9b-pt-res-canonical          gemma-2-9b  \n",
       "3   gpt2-small-res-jb-feature-splitting          gpt2-small  \n",
       "14          gpt2-small-attn-out-v5-128k          gpt2-small  \n",
       "12           gpt2-small-mlp-out-v5-128k          gpt2-small  \n",
       "10         gpt2-small-resid-mid-v5-128k          gpt2-small  \n",
       "5         gpt2-small-resid-post-v5-128k          gpt2-small  \n",
       "13           gpt2-small-attn-out-v5-32k          gpt2-small  \n",
       "11            gpt2-small-mlp-out-v5-32k          gpt2-small  \n",
       "9           gpt2-small-resid-mid-v5-32k          gpt2-small  \n",
       "4          gpt2-small-resid-post-v5-32k          gpt2-small  \n",
       "0                     gpt2-small-res-jb          gpt2-small  \n",
       "7                    gemma-2b-it-res-jb         gemma-2b-it  \n",
       "6                       gemma-2b-res-jb            gemma-2b  \n",
       "38               gpt2-small-res_sce-ajt          gpt2-small  \n",
       "39             gpt2-small-res_scefr-ajt          gpt2-small  \n",
       "36               gpt2-small-res_scl-ajt          gpt2-small  \n",
       "37               gpt2-small-res_sle-ajt          gpt2-small  \n",
       "35             gpt2-small-res_slefr-ajt          gpt2-small  \n",
       "34               gpt2-small-res_sll-ajt          gpt2-small  \n",
       "2                     gpt2-small-mlp-tm          gpt2-small  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads from `pretrained_saes.yaml`\n",
    "pretrained_saes_dir: dict[str, PretrainedSAELookup] = sae_lens.toolkit.pretrained_saes_directory.get_pretrained_saes_directory()\n",
    "\n",
    "print(f'Found {len(pretrained_saes_dir)} pretrained SAEs')\n",
    "df = pd.DataFrame([dataclasses.asdict(x) for x in pretrained_saes_dir.values()])\n",
    "\n",
    "\n",
    "df = df[['repo_id', 'release', 'model']]\n",
    "\n",
    "df.sort_values(by=df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fd4ffa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release</th>\n",
       "      <th>repo_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gemma-scope-2b-pt-att</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gemma-scope-2b-pt-att-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-att</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gemma-scope-2b-pt-mlp</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gemma-scope-2b-pt-mlp-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-mlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemma-scope-2b-pt-res</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gemma-scope-2b-pt-res-canonical</td>\n",
       "      <td>google/gemma-scope-2b-pt-res</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            release                       repo_id\n",
       "19            gemma-scope-2b-pt-att  google/gemma-scope-2b-pt-att\n",
       "20  gemma-scope-2b-pt-att-canonical  google/gemma-scope-2b-pt-att\n",
       "17            gemma-scope-2b-pt-mlp  google/gemma-scope-2b-pt-mlp\n",
       "18  gemma-scope-2b-pt-mlp-canonical  google/gemma-scope-2b-pt-mlp\n",
       "15            gemma-scope-2b-pt-res  google/gemma-scope-2b-pt-res\n",
       "16  gemma-scope-2b-pt-res-canonical  google/gemma-scope-2b-pt-res"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at which ones are there for gemma-2b\n",
    "df[df['model'] == 'gemma-2-2b'][['release', 'repo_id']].sort_values(by=['release', 'repo_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f194c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this one, since it's what's used in the GemmaScope tutorial\n",
    "pretrained_sae_name = 'gemma-scope-2b-pt-res'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6206b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"release\": \"gemma-scope-2b-pt-res\",\n",
      "  \"repo_id\": \"google/gemma-scope-2b-pt-res\",\n",
      "  \"model\": \"gemma-2-2b\",\n",
      "  \"conversion_func\": \"gemma_2\",\n",
      "  \"saes_map\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": \"layer_0/width_16k/average_l0_105\",\n",
      "    \"layer_0/width_16k/average_l0_13\": \"layer_0/width_16k/average_l0_13\",\n",
      "    \"layer_0/width_16k/average_l0_226\": \"layer_0/width_16k/average_l0_226\",\n",
      "    \"layer_0/width_16k/average_l0_25\": \"layer_0/width_16k/average_l0_25\",\n",
      "    \"layer_0/width_16k/average_l0_46\": \"layer_0/width_16k/average_l0_46\",\n",
      "    \"layer_1/width_16k/average_l0_10\": \"layer_1/width_16k/average_l0_10\",\n",
      "    \"layer_1/width_16k/average_l0_102\": \"layer_1/width_16k/average_l0_102\",\n",
      "    \"layer_1/width_16k/average_l0_20\": \"layer_1/width_16k/average_l0_20\",\n",
      "    \"layer_1/width_16k/average_l0_250\": \"layer_1/width_16k/average_l0_250\",\n",
      "    \"layer_1/width_16k/average_l0_40\": \"layer_1/width_16k/average_l0_40\",\n",
      "    \"layer_2/width_16k/average_l0_13\": \"layer_2/width_16k/average_l0_13\",\n",
      "    \"layer_2/width_16k/average_l0_141\": \"layer_2/width_16k/average_l0_141\",\n",
      "    \"layer_2/width_16k/average_l0_142\": \"layer_2/width_16k/average_l0_142\",\n",
      "    \"layer_2/width_16k/average_l0_24\": \"layer_2/width_16k/average_l0_24\",\n",
      "    \"layer_2/width_16k/average_l0_304\": \"layer_2/width_16k/average_l0_304\",\n",
      "    \"layer_2/width_16k/average_l0_53\": \"layer_2/width_16k/average_l0_53\",\n",
      "    \"layer_3/width_16k/average_l0_14\": \"layer_3/width_16k/average_l0_14\",\n",
      "    \"layer_3/width_16k/average_l0_142\": \"layer_3/width_16k/average_l0_142\",\n",
      "    \"layer_3/width_16k/average_l0_28\": \"layer_3/width_16k/average_l0_28\",\n",
      "    \"layer_3/width_16k/average_l0_315\": \"layer_3/width_16k/average_l0_315\",\n",
      "    \"layer_3/width_16k/average_l0_59\": \"layer_3/width_16k/average_l0_59\",\n",
      "    \"layer_4/width_16k/average_l0_124\": \"layer_4/width_16k/average_l0_124\",\n",
      "    \"layer_4/width_16k/average_l0_125\": \"layer_4/width_16k/average_l0_125\",\n",
      "    \"layer_4/width_16k/average_l0_17\": \"layer_4/width_16k/average_l0_17\",\n",
      "    \"layer_4/width_16k/average_l0_281\": \"layer_4/width_16k/average_l0_281\",\n",
      "    \"layer_4/width_16k/average_l0_31\": \"layer_4/width_16k/average_l0_31\",\n",
      "    \"layer_4/width_16k/average_l0_60\": \"layer_4/width_16k/average_l0_60\",\n",
      "    \"layer_5/width_16k/average_l0_143\": \"layer_5/width_16k/average_l0_143\",\n",
      "    \"layer_5/width_16k/average_l0_18\": \"layer_5/width_16k/average_l0_18\",\n",
      "    \"layer_5/width_16k/average_l0_309\": \"layer_5/width_16k/average_l0_309\",\n",
      "    \"layer_5/width_16k/average_l0_34\": \"layer_5/width_16k/average_l0_34\",\n",
      "    \"layer_5/width_16k/average_l0_68\": \"layer_5/width_16k/average_l0_68\",\n",
      "    \"layer_6/width_16k/average_l0_144\": \"layer_6/width_16k/average_l0_144\",\n",
      "    \"layer_6/width_16k/average_l0_19\": \"layer_6/width_16k/average_l0_19\",\n",
      "    \"layer_6/width_16k/average_l0_301\": \"layer_6/width_16k/average_l0_301\",\n",
      "    \"layer_6/width_16k/average_l0_36\": \"layer_6/width_16k/average_l0_36\",\n",
      "    \"layer_6/width_16k/average_l0_70\": \"layer_6/width_16k/average_l0_70\",\n",
      "    \"layer_7/width_16k/average_l0_137\": \"layer_7/width_16k/average_l0_137\",\n",
      "    \"layer_7/width_16k/average_l0_20\": \"layer_7/width_16k/average_l0_20\",\n",
      "    \"layer_7/width_16k/average_l0_285\": \"layer_7/width_16k/average_l0_285\",\n",
      "    \"layer_7/width_16k/average_l0_36\": \"layer_7/width_16k/average_l0_36\",\n",
      "    \"layer_7/width_16k/average_l0_69\": \"layer_7/width_16k/average_l0_69\",\n",
      "    \"layer_8/width_16k/average_l0_142\": \"layer_8/width_16k/average_l0_142\",\n",
      "    \"layer_8/width_16k/average_l0_20\": \"layer_8/width_16k/average_l0_20\",\n",
      "    \"layer_8/width_16k/average_l0_301\": \"layer_8/width_16k/average_l0_301\",\n",
      "    \"layer_8/width_16k/average_l0_37\": \"layer_8/width_16k/average_l0_37\",\n",
      "    \"layer_8/width_16k/average_l0_71\": \"layer_8/width_16k/average_l0_71\",\n",
      "    \"layer_9/width_16k/average_l0_151\": \"layer_9/width_16k/average_l0_151\",\n",
      "    \"layer_9/width_16k/average_l0_21\": \"layer_9/width_16k/average_l0_21\",\n",
      "    \"layer_9/width_16k/average_l0_340\": \"layer_9/width_16k/average_l0_340\",\n",
      "    \"layer_9/width_16k/average_l0_37\": \"layer_9/width_16k/average_l0_37\",\n",
      "    \"layer_9/width_16k/average_l0_73\": \"layer_9/width_16k/average_l0_73\",\n",
      "    \"layer_10/width_16k/average_l0_166\": \"layer_10/width_16k/average_l0_166\",\n",
      "    \"layer_10/width_16k/average_l0_21\": \"layer_10/width_16k/average_l0_21\",\n",
      "    \"layer_10/width_16k/average_l0_39\": \"layer_10/width_16k/average_l0_39\",\n",
      "    \"layer_10/width_16k/average_l0_395\": \"layer_10/width_16k/average_l0_395\",\n",
      "    \"layer_10/width_16k/average_l0_77\": \"layer_10/width_16k/average_l0_77\",\n",
      "    \"layer_11/width_16k/average_l0_168\": \"layer_11/width_16k/average_l0_168\",\n",
      "    \"layer_11/width_16k/average_l0_22\": \"layer_11/width_16k/average_l0_22\",\n",
      "    \"layer_11/width_16k/average_l0_393\": \"layer_11/width_16k/average_l0_393\",\n",
      "    \"layer_11/width_16k/average_l0_41\": \"layer_11/width_16k/average_l0_41\",\n",
      "    \"layer_11/width_16k/average_l0_79\": \"layer_11/width_16k/average_l0_79\",\n",
      "    \"layer_11/width_16k/average_l0_80\": \"layer_11/width_16k/average_l0_80\",\n",
      "    \"layer_12/width_16k/average_l0_176\": \"layer_12/width_16k/average_l0_176\",\n",
      "    \"layer_12/width_16k/average_l0_22\": \"layer_12/width_16k/average_l0_22\",\n",
      "    \"layer_12/width_16k/average_l0_41\": \"layer_12/width_16k/average_l0_41\",\n",
      "    \"layer_12/width_16k/average_l0_445\": \"layer_12/width_16k/average_l0_445\",\n",
      "    \"layer_12/width_16k/average_l0_82\": \"layer_12/width_16k/average_l0_82\",\n",
      "    \"layer_13/width_16k/average_l0_173\": \"layer_13/width_16k/average_l0_173\",\n",
      "    \"layer_13/width_16k/average_l0_23\": \"layer_13/width_16k/average_l0_23\",\n",
      "    \"layer_13/width_16k/average_l0_403\": \"layer_13/width_16k/average_l0_403\",\n",
      "    \"layer_13/width_16k/average_l0_43\": \"layer_13/width_16k/average_l0_43\",\n",
      "    \"layer_13/width_16k/average_l0_83\": \"layer_13/width_16k/average_l0_83\",\n",
      "    \"layer_13/width_16k/average_l0_84\": \"layer_13/width_16k/average_l0_84\",\n",
      "    \"layer_14/width_16k/average_l0_173\": \"layer_14/width_16k/average_l0_173\",\n",
      "    \"layer_14/width_16k/average_l0_23\": \"layer_14/width_16k/average_l0_23\",\n",
      "    \"layer_14/width_16k/average_l0_388\": \"layer_14/width_16k/average_l0_388\",\n",
      "    \"layer_14/width_16k/average_l0_43\": \"layer_14/width_16k/average_l0_43\",\n",
      "    \"layer_14/width_16k/average_l0_83\": \"layer_14/width_16k/average_l0_83\",\n",
      "    \"layer_14/width_16k/average_l0_84\": \"layer_14/width_16k/average_l0_84\",\n",
      "    \"layer_15/width_16k/average_l0_150\": \"layer_15/width_16k/average_l0_150\",\n",
      "    \"layer_15/width_16k/average_l0_23\": \"layer_15/width_16k/average_l0_23\",\n",
      "    \"layer_15/width_16k/average_l0_308\": \"layer_15/width_16k/average_l0_308\",\n",
      "    \"layer_15/width_16k/average_l0_41\": \"layer_15/width_16k/average_l0_41\",\n",
      "    \"layer_15/width_16k/average_l0_78\": \"layer_15/width_16k/average_l0_78\",\n",
      "    \"layer_16/width_16k/average_l0_154\": \"layer_16/width_16k/average_l0_154\",\n",
      "    \"layer_16/width_16k/average_l0_23\": \"layer_16/width_16k/average_l0_23\",\n",
      "    \"layer_16/width_16k/average_l0_335\": \"layer_16/width_16k/average_l0_335\",\n",
      "    \"layer_16/width_16k/average_l0_42\": \"layer_16/width_16k/average_l0_42\",\n",
      "    \"layer_16/width_16k/average_l0_78\": \"layer_16/width_16k/average_l0_78\",\n",
      "    \"layer_17/width_16k/average_l0_150\": \"layer_17/width_16k/average_l0_150\",\n",
      "    \"layer_17/width_16k/average_l0_23\": \"layer_17/width_16k/average_l0_23\",\n",
      "    \"layer_17/width_16k/average_l0_304\": \"layer_17/width_16k/average_l0_304\",\n",
      "    \"layer_17/width_16k/average_l0_42\": \"layer_17/width_16k/average_l0_42\",\n",
      "    \"layer_17/width_16k/average_l0_77\": \"layer_17/width_16k/average_l0_77\",\n",
      "    \"layer_18/width_16k/average_l0_138\": \"layer_18/width_16k/average_l0_138\",\n",
      "    \"layer_18/width_16k/average_l0_23\": \"layer_18/width_16k/average_l0_23\",\n",
      "    \"layer_18/width_16k/average_l0_280\": \"layer_18/width_16k/average_l0_280\",\n",
      "    \"layer_18/width_16k/average_l0_40\": \"layer_18/width_16k/average_l0_40\",\n",
      "    \"layer_18/width_16k/average_l0_74\": \"layer_18/width_16k/average_l0_74\",\n",
      "    \"layer_19/width_16k/average_l0_137\": \"layer_19/width_16k/average_l0_137\",\n",
      "    \"layer_19/width_16k/average_l0_23\": \"layer_19/width_16k/average_l0_23\",\n",
      "    \"layer_19/width_16k/average_l0_279\": \"layer_19/width_16k/average_l0_279\",\n",
      "    \"layer_19/width_16k/average_l0_40\": \"layer_19/width_16k/average_l0_40\",\n",
      "    \"layer_19/width_16k/average_l0_73\": \"layer_19/width_16k/average_l0_73\",\n",
      "    \"layer_20/width_16k/average_l0_139\": \"layer_20/width_16k/average_l0_139\",\n",
      "    \"layer_20/width_16k/average_l0_22\": \"layer_20/width_16k/average_l0_22\",\n",
      "    \"layer_20/width_16k/average_l0_294\": \"layer_20/width_16k/average_l0_294\",\n",
      "    \"layer_20/width_16k/average_l0_38\": \"layer_20/width_16k/average_l0_38\",\n",
      "    \"layer_20/width_16k/average_l0_71\": \"layer_20/width_16k/average_l0_71\",\n",
      "    \"layer_21/width_16k/average_l0_139\": \"layer_21/width_16k/average_l0_139\",\n",
      "    \"layer_21/width_16k/average_l0_22\": \"layer_21/width_16k/average_l0_22\",\n",
      "    \"layer_21/width_16k/average_l0_301\": \"layer_21/width_16k/average_l0_301\",\n",
      "    \"layer_21/width_16k/average_l0_38\": \"layer_21/width_16k/average_l0_38\",\n",
      "    \"layer_21/width_16k/average_l0_70\": \"layer_21/width_16k/average_l0_70\",\n",
      "    \"layer_22/width_16k/average_l0_147\": \"layer_22/width_16k/average_l0_147\",\n",
      "    \"layer_22/width_16k/average_l0_21\": \"layer_22/width_16k/average_l0_21\",\n",
      "    \"layer_22/width_16k/average_l0_349\": \"layer_22/width_16k/average_l0_349\",\n",
      "    \"layer_22/width_16k/average_l0_38\": \"layer_22/width_16k/average_l0_38\",\n",
      "    \"layer_22/width_16k/average_l0_72\": \"layer_22/width_16k/average_l0_72\",\n",
      "    \"layer_23/width_16k/average_l0_157\": \"layer_23/width_16k/average_l0_157\",\n",
      "    \"layer_23/width_16k/average_l0_21\": \"layer_23/width_16k/average_l0_21\",\n",
      "    \"layer_23/width_16k/average_l0_38\": \"layer_23/width_16k/average_l0_38\",\n",
      "    \"layer_23/width_16k/average_l0_404\": \"layer_23/width_16k/average_l0_404\",\n",
      "    \"layer_23/width_16k/average_l0_74\": \"layer_23/width_16k/average_l0_74\",\n",
      "    \"layer_23/width_16k/average_l0_75\": \"layer_23/width_16k/average_l0_75\",\n",
      "    \"layer_24/width_16k/average_l0_158\": \"layer_24/width_16k/average_l0_158\",\n",
      "    \"layer_24/width_16k/average_l0_20\": \"layer_24/width_16k/average_l0_20\",\n",
      "    \"layer_24/width_16k/average_l0_38\": \"layer_24/width_16k/average_l0_38\",\n",
      "    \"layer_24/width_16k/average_l0_457\": \"layer_24/width_16k/average_l0_457\",\n",
      "    \"layer_24/width_16k/average_l0_73\": \"layer_24/width_16k/average_l0_73\",\n",
      "    \"layer_25/width_16k/average_l0_116\": \"layer_25/width_16k/average_l0_116\",\n",
      "    \"layer_25/width_16k/average_l0_16\": \"layer_25/width_16k/average_l0_16\",\n",
      "    \"layer_25/width_16k/average_l0_28\": \"layer_25/width_16k/average_l0_28\",\n",
      "    \"layer_25/width_16k/average_l0_285\": \"layer_25/width_16k/average_l0_285\",\n",
      "    \"layer_25/width_16k/average_l0_55\": \"layer_25/width_16k/average_l0_55\",\n",
      "    \"layer_5/width_1m/average_l0_114\": \"layer_5/width_1m/average_l0_114\",\n",
      "    \"layer_5/width_1m/average_l0_13\": \"layer_5/width_1m/average_l0_13\",\n",
      "    \"layer_5/width_1m/average_l0_21\": \"layer_5/width_1m/average_l0_21\",\n",
      "    \"layer_5/width_1m/average_l0_36\": \"layer_5/width_1m/average_l0_36\",\n",
      "    \"layer_5/width_1m/average_l0_63\": \"layer_5/width_1m/average_l0_63\",\n",
      "    \"layer_5/width_1m/average_l0_9\": \"layer_5/width_1m/average_l0_9\",\n",
      "    \"layer_12/width_1m/average_l0_107\": \"layer_12/width_1m/average_l0_107\",\n",
      "    \"layer_12/width_1m/average_l0_19\": \"layer_12/width_1m/average_l0_19\",\n",
      "    \"layer_12/width_1m/average_l0_207\": \"layer_12/width_1m/average_l0_207\",\n",
      "    \"layer_12/width_1m/average_l0_26\": \"layer_12/width_1m/average_l0_26\",\n",
      "    \"layer_12/width_1m/average_l0_58\": \"layer_12/width_1m/average_l0_58\",\n",
      "    \"layer_12/width_1m/average_l0_73\": \"layer_12/width_1m/average_l0_73\",\n",
      "    \"layer_19/width_1m/average_l0_157\": \"layer_19/width_1m/average_l0_157\",\n",
      "    \"layer_19/width_1m/average_l0_16\": \"layer_19/width_1m/average_l0_16\",\n",
      "    \"layer_19/width_1m/average_l0_18\": \"layer_19/width_1m/average_l0_18\",\n",
      "    \"layer_19/width_1m/average_l0_29\": \"layer_19/width_1m/average_l0_29\",\n",
      "    \"layer_19/width_1m/average_l0_50\": \"layer_19/width_1m/average_l0_50\",\n",
      "    \"layer_19/width_1m/average_l0_88\": \"layer_19/width_1m/average_l0_88\",\n",
      "    \"layer_12/width_262k/average_l0_11\": \"layer_12/width_262k/average_l0_11\",\n",
      "    \"layer_12/width_262k/average_l0_121\": \"layer_12/width_262k/average_l0_121\",\n",
      "    \"layer_12/width_262k/average_l0_21\": \"layer_12/width_262k/average_l0_21\",\n",
      "    \"layer_12/width_262k/average_l0_243\": \"layer_12/width_262k/average_l0_243\",\n",
      "    \"layer_12/width_262k/average_l0_36\": \"layer_12/width_262k/average_l0_36\",\n",
      "    \"layer_12/width_262k/average_l0_67\": \"layer_12/width_262k/average_l0_67\",\n",
      "    \"layer_12/width_32k/average_l0_12\": \"layer_12/width_32k/average_l0_12\",\n",
      "    \"layer_12/width_32k/average_l0_155\": \"layer_12/width_32k/average_l0_155\",\n",
      "    \"layer_12/width_32k/average_l0_22\": \"layer_12/width_32k/average_l0_22\",\n",
      "    \"layer_12/width_32k/average_l0_360\": \"layer_12/width_32k/average_l0_360\",\n",
      "    \"layer_12/width_32k/average_l0_40\": \"layer_12/width_32k/average_l0_40\",\n",
      "    \"layer_12/width_32k/average_l0_76\": \"layer_12/width_32k/average_l0_76\",\n",
      "    \"layer_12/width_524k/average_l0_115\": \"layer_12/width_524k/average_l0_115\",\n",
      "    \"layer_12/width_524k/average_l0_22\": \"layer_12/width_524k/average_l0_22\",\n",
      "    \"layer_12/width_524k/average_l0_227\": \"layer_12/width_524k/average_l0_227\",\n",
      "    \"layer_12/width_524k/average_l0_29\": \"layer_12/width_524k/average_l0_29\",\n",
      "    \"layer_12/width_524k/average_l0_46\": \"layer_12/width_524k/average_l0_46\",\n",
      "    \"layer_12/width_524k/average_l0_65\": \"layer_12/width_524k/average_l0_65\",\n",
      "    \"layer_0/width_65k/average_l0_11\": \"layer_0/width_65k/average_l0_11\",\n",
      "    \"layer_0/width_65k/average_l0_17\": \"layer_0/width_65k/average_l0_17\",\n",
      "    \"layer_0/width_65k/average_l0_27\": \"layer_0/width_65k/average_l0_27\",\n",
      "    \"layer_0/width_65k/average_l0_43\": \"layer_0/width_65k/average_l0_43\",\n",
      "    \"layer_0/width_65k/average_l0_73\": \"layer_0/width_65k/average_l0_73\",\n",
      "    \"layer_1/width_65k/average_l0_121\": \"layer_1/width_65k/average_l0_121\",\n",
      "    \"layer_1/width_65k/average_l0_16\": \"layer_1/width_65k/average_l0_16\",\n",
      "    \"layer_1/width_65k/average_l0_30\": \"layer_1/width_65k/average_l0_30\",\n",
      "    \"layer_1/width_65k/average_l0_54\": \"layer_1/width_65k/average_l0_54\",\n",
      "    \"layer_1/width_65k/average_l0_9\": \"layer_1/width_65k/average_l0_9\",\n",
      "    \"layer_2/width_65k/average_l0_11\": \"layer_2/width_65k/average_l0_11\",\n",
      "    \"layer_2/width_65k/average_l0_169\": \"layer_2/width_65k/average_l0_169\",\n",
      "    \"layer_2/width_65k/average_l0_20\": \"layer_2/width_65k/average_l0_20\",\n",
      "    \"layer_2/width_65k/average_l0_37\": \"layer_2/width_65k/average_l0_37\",\n",
      "    \"layer_2/width_65k/average_l0_77\": \"layer_2/width_65k/average_l0_77\",\n",
      "    \"layer_3/width_65k/average_l0_13\": \"layer_3/width_65k/average_l0_13\",\n",
      "    \"layer_3/width_65k/average_l0_193\": \"layer_3/width_65k/average_l0_193\",\n",
      "    \"layer_3/width_65k/average_l0_23\": \"layer_3/width_65k/average_l0_23\",\n",
      "    \"layer_3/width_65k/average_l0_42\": \"layer_3/width_65k/average_l0_42\",\n",
      "    \"layer_3/width_65k/average_l0_89\": \"layer_3/width_65k/average_l0_89\",\n",
      "    \"layer_4/width_65k/average_l0_14\": \"layer_4/width_65k/average_l0_14\",\n",
      "    \"layer_4/width_65k/average_l0_177\": \"layer_4/width_65k/average_l0_177\",\n",
      "    \"layer_4/width_65k/average_l0_25\": \"layer_4/width_65k/average_l0_25\",\n",
      "    \"layer_4/width_65k/average_l0_46\": \"layer_4/width_65k/average_l0_46\",\n",
      "    \"layer_4/width_65k/average_l0_89\": \"layer_4/width_65k/average_l0_89\",\n",
      "    \"layer_5/width_65k/average_l0_105\": \"layer_5/width_65k/average_l0_105\",\n",
      "    \"layer_5/width_65k/average_l0_17\": \"layer_5/width_65k/average_l0_17\",\n",
      "    \"layer_5/width_65k/average_l0_211\": \"layer_5/width_65k/average_l0_211\",\n",
      "    \"layer_5/width_65k/average_l0_29\": \"layer_5/width_65k/average_l0_29\",\n",
      "    \"layer_5/width_65k/average_l0_53\": \"layer_5/width_65k/average_l0_53\",\n",
      "    \"layer_6/width_65k/average_l0_107\": \"layer_6/width_65k/average_l0_107\",\n",
      "    \"layer_6/width_65k/average_l0_17\": \"layer_6/width_65k/average_l0_17\",\n",
      "    \"layer_6/width_65k/average_l0_208\": \"layer_6/width_65k/average_l0_208\",\n",
      "    \"layer_6/width_65k/average_l0_30\": \"layer_6/width_65k/average_l0_30\",\n",
      "    \"layer_6/width_65k/average_l0_56\": \"layer_6/width_65k/average_l0_56\",\n",
      "    \"layer_7/width_65k/average_l0_107\": \"layer_7/width_65k/average_l0_107\",\n",
      "    \"layer_7/width_65k/average_l0_18\": \"layer_7/width_65k/average_l0_18\",\n",
      "    \"layer_7/width_65k/average_l0_203\": \"layer_7/width_65k/average_l0_203\",\n",
      "    \"layer_7/width_65k/average_l0_31\": \"layer_7/width_65k/average_l0_31\",\n",
      "    \"layer_7/width_65k/average_l0_57\": \"layer_7/width_65k/average_l0_57\",\n",
      "    \"layer_8/width_65k/average_l0_111\": \"layer_8/width_65k/average_l0_111\",\n",
      "    \"layer_8/width_65k/average_l0_19\": \"layer_8/width_65k/average_l0_19\",\n",
      "    \"layer_8/width_65k/average_l0_213\": \"layer_8/width_65k/average_l0_213\",\n",
      "    \"layer_8/width_65k/average_l0_33\": \"layer_8/width_65k/average_l0_33\",\n",
      "    \"layer_8/width_65k/average_l0_59\": \"layer_8/width_65k/average_l0_59\",\n",
      "    \"layer_9/width_65k/average_l0_118\": \"layer_9/width_65k/average_l0_118\",\n",
      "    \"layer_9/width_65k/average_l0_19\": \"layer_9/width_65k/average_l0_19\",\n",
      "    \"layer_9/width_65k/average_l0_240\": \"layer_9/width_65k/average_l0_240\",\n",
      "    \"layer_9/width_65k/average_l0_34\": \"layer_9/width_65k/average_l0_34\",\n",
      "    \"layer_9/width_65k/average_l0_61\": \"layer_9/width_65k/average_l0_61\",\n",
      "    \"layer_10/width_65k/average_l0_128\": \"layer_10/width_65k/average_l0_128\",\n",
      "    \"layer_10/width_65k/average_l0_20\": \"layer_10/width_65k/average_l0_20\",\n",
      "    \"layer_10/width_65k/average_l0_265\": \"layer_10/width_65k/average_l0_265\",\n",
      "    \"layer_10/width_65k/average_l0_36\": \"layer_10/width_65k/average_l0_36\",\n",
      "    \"layer_10/width_65k/average_l0_66\": \"layer_10/width_65k/average_l0_66\",\n",
      "    \"layer_11/width_65k/average_l0_134\": \"layer_11/width_65k/average_l0_134\",\n",
      "    \"layer_11/width_65k/average_l0_21\": \"layer_11/width_65k/average_l0_21\",\n",
      "    \"layer_11/width_65k/average_l0_273\": \"layer_11/width_65k/average_l0_273\",\n",
      "    \"layer_11/width_65k/average_l0_37\": \"layer_11/width_65k/average_l0_37\",\n",
      "    \"layer_11/width_65k/average_l0_70\": \"layer_11/width_65k/average_l0_70\",\n",
      "    \"layer_12/width_65k/average_l0_141\": \"layer_12/width_65k/average_l0_141\",\n",
      "    \"layer_12/width_65k/average_l0_21\": \"layer_12/width_65k/average_l0_21\",\n",
      "    \"layer_12/width_65k/average_l0_297\": \"layer_12/width_65k/average_l0_297\",\n",
      "    \"layer_12/width_65k/average_l0_38\": \"layer_12/width_65k/average_l0_38\",\n",
      "    \"layer_12/width_65k/average_l0_72\": \"layer_12/width_65k/average_l0_72\",\n",
      "    \"layer_13/width_65k/average_l0_142\": \"layer_13/width_65k/average_l0_142\",\n",
      "    \"layer_13/width_65k/average_l0_22\": \"layer_13/width_65k/average_l0_22\",\n",
      "    \"layer_13/width_65k/average_l0_288\": \"layer_13/width_65k/average_l0_288\",\n",
      "    \"layer_13/width_65k/average_l0_40\": \"layer_13/width_65k/average_l0_40\",\n",
      "    \"layer_13/width_65k/average_l0_74\": \"layer_13/width_65k/average_l0_74\",\n",
      "    \"layer_13/width_65k/average_l0_75\": \"layer_13/width_65k/average_l0_75\",\n",
      "    \"layer_14/width_65k/average_l0_144\": \"layer_14/width_65k/average_l0_144\",\n",
      "    \"layer_14/width_65k/average_l0_21\": \"layer_14/width_65k/average_l0_21\",\n",
      "    \"layer_14/width_65k/average_l0_284\": \"layer_14/width_65k/average_l0_284\",\n",
      "    \"layer_14/width_65k/average_l0_40\": \"layer_14/width_65k/average_l0_40\",\n",
      "    \"layer_14/width_65k/average_l0_73\": \"layer_14/width_65k/average_l0_73\",\n",
      "    \"layer_15/width_65k/average_l0_127\": \"layer_15/width_65k/average_l0_127\",\n",
      "    \"layer_15/width_65k/average_l0_21\": \"layer_15/width_65k/average_l0_21\",\n",
      "    \"layer_15/width_65k/average_l0_240\": \"layer_15/width_65k/average_l0_240\",\n",
      "    \"layer_15/width_65k/average_l0_38\": \"layer_15/width_65k/average_l0_38\",\n",
      "    \"layer_15/width_65k/average_l0_68\": \"layer_15/width_65k/average_l0_68\",\n",
      "    \"layer_16/width_65k/average_l0_128\": \"layer_16/width_65k/average_l0_128\",\n",
      "    \"layer_16/width_65k/average_l0_21\": \"layer_16/width_65k/average_l0_21\",\n",
      "    \"layer_16/width_65k/average_l0_244\": \"layer_16/width_65k/average_l0_244\",\n",
      "    \"layer_16/width_65k/average_l0_38\": \"layer_16/width_65k/average_l0_38\",\n",
      "    \"layer_16/width_65k/average_l0_69\": \"layer_16/width_65k/average_l0_69\",\n",
      "    \"layer_17/width_65k/average_l0_125\": \"layer_17/width_65k/average_l0_125\",\n",
      "    \"layer_17/width_65k/average_l0_21\": \"layer_17/width_65k/average_l0_21\",\n",
      "    \"layer_17/width_65k/average_l0_233\": \"layer_17/width_65k/average_l0_233\",\n",
      "    \"layer_17/width_65k/average_l0_38\": \"layer_17/width_65k/average_l0_38\",\n",
      "    \"layer_17/width_65k/average_l0_68\": \"layer_17/width_65k/average_l0_68\",\n",
      "    \"layer_18/width_65k/average_l0_116\": \"layer_18/width_65k/average_l0_116\",\n",
      "    \"layer_18/width_65k/average_l0_117\": \"layer_18/width_65k/average_l0_117\",\n",
      "    \"layer_18/width_65k/average_l0_21\": \"layer_18/width_65k/average_l0_21\",\n",
      "    \"layer_18/width_65k/average_l0_216\": \"layer_18/width_65k/average_l0_216\",\n",
      "    \"layer_18/width_65k/average_l0_36\": \"layer_18/width_65k/average_l0_36\",\n",
      "    \"layer_18/width_65k/average_l0_64\": \"layer_18/width_65k/average_l0_64\",\n",
      "    \"layer_19/width_65k/average_l0_115\": \"layer_19/width_65k/average_l0_115\",\n",
      "    \"layer_19/width_65k/average_l0_21\": \"layer_19/width_65k/average_l0_21\",\n",
      "    \"layer_19/width_65k/average_l0_216\": \"layer_19/width_65k/average_l0_216\",\n",
      "    \"layer_19/width_65k/average_l0_35\": \"layer_19/width_65k/average_l0_35\",\n",
      "    \"layer_19/width_65k/average_l0_63\": \"layer_19/width_65k/average_l0_63\",\n",
      "    \"layer_20/width_65k/average_l0_114\": \"layer_20/width_65k/average_l0_114\",\n",
      "    \"layer_20/width_65k/average_l0_20\": \"layer_20/width_65k/average_l0_20\",\n",
      "    \"layer_20/width_65k/average_l0_221\": \"layer_20/width_65k/average_l0_221\",\n",
      "    \"layer_20/width_65k/average_l0_34\": \"layer_20/width_65k/average_l0_34\",\n",
      "    \"layer_20/width_65k/average_l0_61\": \"layer_20/width_65k/average_l0_61\",\n",
      "    \"layer_21/width_65k/average_l0_111\": \"layer_21/width_65k/average_l0_111\",\n",
      "    \"layer_21/width_65k/average_l0_112\": \"layer_21/width_65k/average_l0_112\",\n",
      "    \"layer_21/width_65k/average_l0_20\": \"layer_21/width_65k/average_l0_20\",\n",
      "    \"layer_21/width_65k/average_l0_225\": \"layer_21/width_65k/average_l0_225\",\n",
      "    \"layer_21/width_65k/average_l0_33\": \"layer_21/width_65k/average_l0_33\",\n",
      "    \"layer_21/width_65k/average_l0_61\": \"layer_21/width_65k/average_l0_61\",\n",
      "    \"layer_22/width_65k/average_l0_116\": \"layer_22/width_65k/average_l0_116\",\n",
      "    \"layer_22/width_65k/average_l0_117\": \"layer_22/width_65k/average_l0_117\",\n",
      "    \"layer_22/width_65k/average_l0_20\": \"layer_22/width_65k/average_l0_20\",\n",
      "    \"layer_22/width_65k/average_l0_248\": \"layer_22/width_65k/average_l0_248\",\n",
      "    \"layer_22/width_65k/average_l0_33\": \"layer_22/width_65k/average_l0_33\",\n",
      "    \"layer_22/width_65k/average_l0_62\": \"layer_22/width_65k/average_l0_62\",\n",
      "    \"layer_23/width_65k/average_l0_123\": \"layer_23/width_65k/average_l0_123\",\n",
      "    \"layer_23/width_65k/average_l0_124\": \"layer_23/width_65k/average_l0_124\",\n",
      "    \"layer_23/width_65k/average_l0_20\": \"layer_23/width_65k/average_l0_20\",\n",
      "    \"layer_23/width_65k/average_l0_272\": \"layer_23/width_65k/average_l0_272\",\n",
      "    \"layer_23/width_65k/average_l0_35\": \"layer_23/width_65k/average_l0_35\",\n",
      "    \"layer_23/width_65k/average_l0_64\": \"layer_23/width_65k/average_l0_64\",\n",
      "    \"layer_24/width_65k/average_l0_124\": \"layer_24/width_65k/average_l0_124\",\n",
      "    \"layer_24/width_65k/average_l0_19\": \"layer_24/width_65k/average_l0_19\",\n",
      "    \"layer_24/width_65k/average_l0_273\": \"layer_24/width_65k/average_l0_273\",\n",
      "    \"layer_24/width_65k/average_l0_34\": \"layer_24/width_65k/average_l0_34\",\n",
      "    \"layer_24/width_65k/average_l0_63\": \"layer_24/width_65k/average_l0_63\",\n",
      "    \"layer_25/width_65k/average_l0_15\": \"layer_25/width_65k/average_l0_15\",\n",
      "    \"layer_25/width_65k/average_l0_197\": \"layer_25/width_65k/average_l0_197\",\n",
      "    \"layer_25/width_65k/average_l0_26\": \"layer_25/width_65k/average_l0_26\",\n",
      "    \"layer_25/width_65k/average_l0_48\": \"layer_25/width_65k/average_l0_48\",\n",
      "    \"layer_25/width_65k/average_l0_93\": \"layer_25/width_65k/average_l0_93\"\n",
      "  },\n",
      "  \"expected_var_explained\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_13\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_226\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_25\": 1.0,\n",
      "    \"layer_0/width_16k/average_l0_46\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_10\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_102\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_250\": 1.0,\n",
      "    \"layer_1/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_13\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_141\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_24\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_304\": 1.0,\n",
      "    \"layer_2/width_16k/average_l0_53\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_14\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_28\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_315\": 1.0,\n",
      "    \"layer_3/width_16k/average_l0_59\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_124\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_125\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_17\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_281\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_31\": 1.0,\n",
      "    \"layer_4/width_16k/average_l0_60\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_143\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_18\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_309\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_34\": 1.0,\n",
      "    \"layer_5/width_16k/average_l0_68\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_144\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_19\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_36\": 1.0,\n",
      "    \"layer_6/width_16k/average_l0_70\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_137\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_285\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_36\": 1.0,\n",
      "    \"layer_7/width_16k/average_l0_69\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_142\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_37\": 1.0,\n",
      "    \"layer_8/width_16k/average_l0_71\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_151\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_340\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_37\": 1.0,\n",
      "    \"layer_9/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_166\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_39\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_395\": 1.0,\n",
      "    \"layer_10/width_16k/average_l0_77\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_168\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_393\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_79\": 1.0,\n",
      "    \"layer_11/width_16k/average_l0_80\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_176\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_445\": 1.0,\n",
      "    \"layer_12/width_16k/average_l0_82\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_173\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_403\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_43\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_83\": 1.0,\n",
      "    \"layer_13/width_16k/average_l0_84\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_173\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_388\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_43\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_83\": 1.0,\n",
      "    \"layer_14/width_16k/average_l0_84\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_150\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_308\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_41\": 1.0,\n",
      "    \"layer_15/width_16k/average_l0_78\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_154\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_335\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_42\": 1.0,\n",
      "    \"layer_16/width_16k/average_l0_78\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_150\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_304\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_42\": 1.0,\n",
      "    \"layer_17/width_16k/average_l0_77\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_138\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_280\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_18/width_16k/average_l0_74\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_137\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_23\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_279\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_40\": 1.0,\n",
      "    \"layer_19/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_139\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_294\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_20/width_16k/average_l0_71\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_139\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_22\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_301\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_21/width_16k/average_l0_70\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_147\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_349\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_22/width_16k/average_l0_72\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_157\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_21\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_404\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_74\": 1.0,\n",
      "    \"layer_23/width_16k/average_l0_75\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_158\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_20\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_38\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_457\": 1.0,\n",
      "    \"layer_24/width_16k/average_l0_73\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_116\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_16\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_28\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_285\": 1.0,\n",
      "    \"layer_25/width_16k/average_l0_55\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_114\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_13\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_21\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_36\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_63\": 1.0,\n",
      "    \"layer_5/width_1m/average_l0_9\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_107\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_19\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_207\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_26\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_58\": 1.0,\n",
      "    \"layer_12/width_1m/average_l0_73\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_157\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_16\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_18\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_29\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_50\": 1.0,\n",
      "    \"layer_19/width_1m/average_l0_88\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_11\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_121\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_21\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_243\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_36\": 1.0,\n",
      "    \"layer_12/width_262k/average_l0_67\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_12\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_155\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_360\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_40\": 1.0,\n",
      "    \"layer_12/width_32k/average_l0_76\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_115\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_22\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_227\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_29\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_46\": 1.0,\n",
      "    \"layer_12/width_524k/average_l0_65\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_11\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_27\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_43\": 1.0,\n",
      "    \"layer_0/width_65k/average_l0_73\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_121\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_16\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_30\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_54\": 1.0,\n",
      "    \"layer_1/width_65k/average_l0_9\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_11\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_169\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_37\": 1.0,\n",
      "    \"layer_2/width_65k/average_l0_77\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_13\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_193\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_23\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_42\": 1.0,\n",
      "    \"layer_3/width_65k/average_l0_89\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_14\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_177\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_25\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_46\": 1.0,\n",
      "    \"layer_4/width_65k/average_l0_89\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_105\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_211\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_29\": 1.0,\n",
      "    \"layer_5/width_65k/average_l0_53\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_107\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_17\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_208\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_30\": 1.0,\n",
      "    \"layer_6/width_65k/average_l0_56\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_107\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_18\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_203\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_31\": 1.0,\n",
      "    \"layer_7/width_65k/average_l0_57\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_111\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_213\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_8/width_65k/average_l0_59\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_118\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_240\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_9/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_128\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_265\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_36\": 1.0,\n",
      "    \"layer_10/width_65k/average_l0_66\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_134\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_273\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_37\": 1.0,\n",
      "    \"layer_11/width_65k/average_l0_70\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_141\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_297\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_12/width_65k/average_l0_72\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_142\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_22\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_288\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_40\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_74\": 1.0,\n",
      "    \"layer_13/width_65k/average_l0_75\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_144\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_284\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_40\": 1.0,\n",
      "    \"layer_14/width_65k/average_l0_73\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_127\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_240\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_15/width_65k/average_l0_68\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_128\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_244\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_16/width_65k/average_l0_69\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_125\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_233\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_38\": 1.0,\n",
      "    \"layer_17/width_65k/average_l0_68\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_116\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_117\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_216\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_36\": 1.0,\n",
      "    \"layer_18/width_65k/average_l0_64\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_115\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_21\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_216\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_35\": 1.0,\n",
      "    \"layer_19/width_65k/average_l0_63\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_114\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_221\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_20/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_111\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_112\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_225\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_21/width_65k/average_l0_61\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_116\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_117\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_248\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_33\": 1.0,\n",
      "    \"layer_22/width_65k/average_l0_62\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_123\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_124\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_20\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_272\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_35\": 1.0,\n",
      "    \"layer_23/width_65k/average_l0_64\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_124\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_19\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_273\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_34\": 1.0,\n",
      "    \"layer_24/width_65k/average_l0_63\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_15\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_197\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_26\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_48\": 1.0,\n",
      "    \"layer_25/width_65k/average_l0_93\": 1.0\n",
      "  },\n",
      "  \"expected_l0\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": 105,\n",
      "    \"layer_0/width_16k/average_l0_13\": 13,\n",
      "    \"layer_0/width_16k/average_l0_226\": 226,\n",
      "    \"layer_0/width_16k/average_l0_25\": 25,\n",
      "    \"layer_0/width_16k/average_l0_46\": 46,\n",
      "    \"layer_1/width_16k/average_l0_10\": 10,\n",
      "    \"layer_1/width_16k/average_l0_102\": 102,\n",
      "    \"layer_1/width_16k/average_l0_20\": 20,\n",
      "    \"layer_1/width_16k/average_l0_250\": 250,\n",
      "    \"layer_1/width_16k/average_l0_40\": 40,\n",
      "    \"layer_2/width_16k/average_l0_13\": 13,\n",
      "    \"layer_2/width_16k/average_l0_141\": 141,\n",
      "    \"layer_2/width_16k/average_l0_142\": 142,\n",
      "    \"layer_2/width_16k/average_l0_24\": 24,\n",
      "    \"layer_2/width_16k/average_l0_304\": 304,\n",
      "    \"layer_2/width_16k/average_l0_53\": 53,\n",
      "    \"layer_3/width_16k/average_l0_14\": 14,\n",
      "    \"layer_3/width_16k/average_l0_142\": 142,\n",
      "    \"layer_3/width_16k/average_l0_28\": 28,\n",
      "    \"layer_3/width_16k/average_l0_315\": 315,\n",
      "    \"layer_3/width_16k/average_l0_59\": 59,\n",
      "    \"layer_4/width_16k/average_l0_124\": 124,\n",
      "    \"layer_4/width_16k/average_l0_125\": 125,\n",
      "    \"layer_4/width_16k/average_l0_17\": 17,\n",
      "    \"layer_4/width_16k/average_l0_281\": 281,\n",
      "    \"layer_4/width_16k/average_l0_31\": 31,\n",
      "    \"layer_4/width_16k/average_l0_60\": 60,\n",
      "    \"layer_5/width_16k/average_l0_143\": 143,\n",
      "    \"layer_5/width_16k/average_l0_18\": 18,\n",
      "    \"layer_5/width_16k/average_l0_309\": 309,\n",
      "    \"layer_5/width_16k/average_l0_34\": 34,\n",
      "    \"layer_5/width_16k/average_l0_68\": 68,\n",
      "    \"layer_6/width_16k/average_l0_144\": 144,\n",
      "    \"layer_6/width_16k/average_l0_19\": 19,\n",
      "    \"layer_6/width_16k/average_l0_301\": 301,\n",
      "    \"layer_6/width_16k/average_l0_36\": 36,\n",
      "    \"layer_6/width_16k/average_l0_70\": 70,\n",
      "    \"layer_7/width_16k/average_l0_137\": 137,\n",
      "    \"layer_7/width_16k/average_l0_20\": 20,\n",
      "    \"layer_7/width_16k/average_l0_285\": 285,\n",
      "    \"layer_7/width_16k/average_l0_36\": 36,\n",
      "    \"layer_7/width_16k/average_l0_69\": 69,\n",
      "    \"layer_8/width_16k/average_l0_142\": 142,\n",
      "    \"layer_8/width_16k/average_l0_20\": 20,\n",
      "    \"layer_8/width_16k/average_l0_301\": 301,\n",
      "    \"layer_8/width_16k/average_l0_37\": 37,\n",
      "    \"layer_8/width_16k/average_l0_71\": 71,\n",
      "    \"layer_9/width_16k/average_l0_151\": 151,\n",
      "    \"layer_9/width_16k/average_l0_21\": 21,\n",
      "    \"layer_9/width_16k/average_l0_340\": 340,\n",
      "    \"layer_9/width_16k/average_l0_37\": 37,\n",
      "    \"layer_9/width_16k/average_l0_73\": 73,\n",
      "    \"layer_10/width_16k/average_l0_166\": 166,\n",
      "    \"layer_10/width_16k/average_l0_21\": 21,\n",
      "    \"layer_10/width_16k/average_l0_39\": 39,\n",
      "    \"layer_10/width_16k/average_l0_395\": 395,\n",
      "    \"layer_10/width_16k/average_l0_77\": 77,\n",
      "    \"layer_11/width_16k/average_l0_168\": 168,\n",
      "    \"layer_11/width_16k/average_l0_22\": 22,\n",
      "    \"layer_11/width_16k/average_l0_393\": 393,\n",
      "    \"layer_11/width_16k/average_l0_41\": 41,\n",
      "    \"layer_11/width_16k/average_l0_79\": 79,\n",
      "    \"layer_11/width_16k/average_l0_80\": 80,\n",
      "    \"layer_12/width_16k/average_l0_176\": 176,\n",
      "    \"layer_12/width_16k/average_l0_22\": 22,\n",
      "    \"layer_12/width_16k/average_l0_41\": 41,\n",
      "    \"layer_12/width_16k/average_l0_445\": 445,\n",
      "    \"layer_12/width_16k/average_l0_82\": 82,\n",
      "    \"layer_13/width_16k/average_l0_173\": 173,\n",
      "    \"layer_13/width_16k/average_l0_23\": 23,\n",
      "    \"layer_13/width_16k/average_l0_403\": 403,\n",
      "    \"layer_13/width_16k/average_l0_43\": 43,\n",
      "    \"layer_13/width_16k/average_l0_83\": 83,\n",
      "    \"layer_13/width_16k/average_l0_84\": 84,\n",
      "    \"layer_14/width_16k/average_l0_173\": 173,\n",
      "    \"layer_14/width_16k/average_l0_23\": 23,\n",
      "    \"layer_14/width_16k/average_l0_388\": 388,\n",
      "    \"layer_14/width_16k/average_l0_43\": 43,\n",
      "    \"layer_14/width_16k/average_l0_83\": 83,\n",
      "    \"layer_14/width_16k/average_l0_84\": 84,\n",
      "    \"layer_15/width_16k/average_l0_150\": 150,\n",
      "    \"layer_15/width_16k/average_l0_23\": 23,\n",
      "    \"layer_15/width_16k/average_l0_308\": 308,\n",
      "    \"layer_15/width_16k/average_l0_41\": 41,\n",
      "    \"layer_15/width_16k/average_l0_78\": 78,\n",
      "    \"layer_16/width_16k/average_l0_154\": 154,\n",
      "    \"layer_16/width_16k/average_l0_23\": 23,\n",
      "    \"layer_16/width_16k/average_l0_335\": 335,\n",
      "    \"layer_16/width_16k/average_l0_42\": 42,\n",
      "    \"layer_16/width_16k/average_l0_78\": 78,\n",
      "    \"layer_17/width_16k/average_l0_150\": 150,\n",
      "    \"layer_17/width_16k/average_l0_23\": 23,\n",
      "    \"layer_17/width_16k/average_l0_304\": 304,\n",
      "    \"layer_17/width_16k/average_l0_42\": 42,\n",
      "    \"layer_17/width_16k/average_l0_77\": 77,\n",
      "    \"layer_18/width_16k/average_l0_138\": 138,\n",
      "    \"layer_18/width_16k/average_l0_23\": 23,\n",
      "    \"layer_18/width_16k/average_l0_280\": 280,\n",
      "    \"layer_18/width_16k/average_l0_40\": 40,\n",
      "    \"layer_18/width_16k/average_l0_74\": 74,\n",
      "    \"layer_19/width_16k/average_l0_137\": 137,\n",
      "    \"layer_19/width_16k/average_l0_23\": 23,\n",
      "    \"layer_19/width_16k/average_l0_279\": 279,\n",
      "    \"layer_19/width_16k/average_l0_40\": 40,\n",
      "    \"layer_19/width_16k/average_l0_73\": 73,\n",
      "    \"layer_20/width_16k/average_l0_139\": 139,\n",
      "    \"layer_20/width_16k/average_l0_22\": 22,\n",
      "    \"layer_20/width_16k/average_l0_294\": 294,\n",
      "    \"layer_20/width_16k/average_l0_38\": 38,\n",
      "    \"layer_20/width_16k/average_l0_71\": 71,\n",
      "    \"layer_21/width_16k/average_l0_139\": 139,\n",
      "    \"layer_21/width_16k/average_l0_22\": 22,\n",
      "    \"layer_21/width_16k/average_l0_301\": 301,\n",
      "    \"layer_21/width_16k/average_l0_38\": 38,\n",
      "    \"layer_21/width_16k/average_l0_70\": 70,\n",
      "    \"layer_22/width_16k/average_l0_147\": 147,\n",
      "    \"layer_22/width_16k/average_l0_21\": 21,\n",
      "    \"layer_22/width_16k/average_l0_349\": 349,\n",
      "    \"layer_22/width_16k/average_l0_38\": 38,\n",
      "    \"layer_22/width_16k/average_l0_72\": 72,\n",
      "    \"layer_23/width_16k/average_l0_157\": 157,\n",
      "    \"layer_23/width_16k/average_l0_21\": 21,\n",
      "    \"layer_23/width_16k/average_l0_38\": 38,\n",
      "    \"layer_23/width_16k/average_l0_404\": 404,\n",
      "    \"layer_23/width_16k/average_l0_74\": 74,\n",
      "    \"layer_23/width_16k/average_l0_75\": 75,\n",
      "    \"layer_24/width_16k/average_l0_158\": 158,\n",
      "    \"layer_24/width_16k/average_l0_20\": 20,\n",
      "    \"layer_24/width_16k/average_l0_38\": 38,\n",
      "    \"layer_24/width_16k/average_l0_457\": 457,\n",
      "    \"layer_24/width_16k/average_l0_73\": 73,\n",
      "    \"layer_25/width_16k/average_l0_116\": 116,\n",
      "    \"layer_25/width_16k/average_l0_16\": 16,\n",
      "    \"layer_25/width_16k/average_l0_28\": 28,\n",
      "    \"layer_25/width_16k/average_l0_285\": 285,\n",
      "    \"layer_25/width_16k/average_l0_55\": 55,\n",
      "    \"layer_5/width_1m/average_l0_114\": 114,\n",
      "    \"layer_5/width_1m/average_l0_13\": 13,\n",
      "    \"layer_5/width_1m/average_l0_21\": 21,\n",
      "    \"layer_5/width_1m/average_l0_36\": 36,\n",
      "    \"layer_5/width_1m/average_l0_63\": 63,\n",
      "    \"layer_5/width_1m/average_l0_9\": 9,\n",
      "    \"layer_12/width_1m/average_l0_107\": 107,\n",
      "    \"layer_12/width_1m/average_l0_19\": 19,\n",
      "    \"layer_12/width_1m/average_l0_207\": 207,\n",
      "    \"layer_12/width_1m/average_l0_26\": 26,\n",
      "    \"layer_12/width_1m/average_l0_58\": 58,\n",
      "    \"layer_12/width_1m/average_l0_73\": 73,\n",
      "    \"layer_19/width_1m/average_l0_157\": 157,\n",
      "    \"layer_19/width_1m/average_l0_16\": 16,\n",
      "    \"layer_19/width_1m/average_l0_18\": 18,\n",
      "    \"layer_19/width_1m/average_l0_29\": 29,\n",
      "    \"layer_19/width_1m/average_l0_50\": 50,\n",
      "    \"layer_19/width_1m/average_l0_88\": 88,\n",
      "    \"layer_12/width_262k/average_l0_11\": 11,\n",
      "    \"layer_12/width_262k/average_l0_121\": 121,\n",
      "    \"layer_12/width_262k/average_l0_21\": 21,\n",
      "    \"layer_12/width_262k/average_l0_243\": 243,\n",
      "    \"layer_12/width_262k/average_l0_36\": 36,\n",
      "    \"layer_12/width_262k/average_l0_67\": 67,\n",
      "    \"layer_12/width_32k/average_l0_12\": 12,\n",
      "    \"layer_12/width_32k/average_l0_155\": 155,\n",
      "    \"layer_12/width_32k/average_l0_22\": 22,\n",
      "    \"layer_12/width_32k/average_l0_360\": 360,\n",
      "    \"layer_12/width_32k/average_l0_40\": 40,\n",
      "    \"layer_12/width_32k/average_l0_76\": 76,\n",
      "    \"layer_12/width_524k/average_l0_115\": 115,\n",
      "    \"layer_12/width_524k/average_l0_22\": 22,\n",
      "    \"layer_12/width_524k/average_l0_227\": 227,\n",
      "    \"layer_12/width_524k/average_l0_29\": 29,\n",
      "    \"layer_12/width_524k/average_l0_46\": 46,\n",
      "    \"layer_12/width_524k/average_l0_65\": 65,\n",
      "    \"layer_0/width_65k/average_l0_11\": 11,\n",
      "    \"layer_0/width_65k/average_l0_17\": 17,\n",
      "    \"layer_0/width_65k/average_l0_27\": 27,\n",
      "    \"layer_0/width_65k/average_l0_43\": 43,\n",
      "    \"layer_0/width_65k/average_l0_73\": 73,\n",
      "    \"layer_1/width_65k/average_l0_121\": 121,\n",
      "    \"layer_1/width_65k/average_l0_16\": 16,\n",
      "    \"layer_1/width_65k/average_l0_30\": 30,\n",
      "    \"layer_1/width_65k/average_l0_54\": 54,\n",
      "    \"layer_1/width_65k/average_l0_9\": 9,\n",
      "    \"layer_2/width_65k/average_l0_11\": 11,\n",
      "    \"layer_2/width_65k/average_l0_169\": 169,\n",
      "    \"layer_2/width_65k/average_l0_20\": 20,\n",
      "    \"layer_2/width_65k/average_l0_37\": 37,\n",
      "    \"layer_2/width_65k/average_l0_77\": 77,\n",
      "    \"layer_3/width_65k/average_l0_13\": 13,\n",
      "    \"layer_3/width_65k/average_l0_193\": 193,\n",
      "    \"layer_3/width_65k/average_l0_23\": 23,\n",
      "    \"layer_3/width_65k/average_l0_42\": 42,\n",
      "    \"layer_3/width_65k/average_l0_89\": 89,\n",
      "    \"layer_4/width_65k/average_l0_14\": 14,\n",
      "    \"layer_4/width_65k/average_l0_177\": 177,\n",
      "    \"layer_4/width_65k/average_l0_25\": 25,\n",
      "    \"layer_4/width_65k/average_l0_46\": 46,\n",
      "    \"layer_4/width_65k/average_l0_89\": 89,\n",
      "    \"layer_5/width_65k/average_l0_105\": 105,\n",
      "    \"layer_5/width_65k/average_l0_17\": 17,\n",
      "    \"layer_5/width_65k/average_l0_211\": 211,\n",
      "    \"layer_5/width_65k/average_l0_29\": 29,\n",
      "    \"layer_5/width_65k/average_l0_53\": 53,\n",
      "    \"layer_6/width_65k/average_l0_107\": 107,\n",
      "    \"layer_6/width_65k/average_l0_17\": 17,\n",
      "    \"layer_6/width_65k/average_l0_208\": 208,\n",
      "    \"layer_6/width_65k/average_l0_30\": 30,\n",
      "    \"layer_6/width_65k/average_l0_56\": 56,\n",
      "    \"layer_7/width_65k/average_l0_107\": 107,\n",
      "    \"layer_7/width_65k/average_l0_18\": 18,\n",
      "    \"layer_7/width_65k/average_l0_203\": 203,\n",
      "    \"layer_7/width_65k/average_l0_31\": 31,\n",
      "    \"layer_7/width_65k/average_l0_57\": 57,\n",
      "    \"layer_8/width_65k/average_l0_111\": 111,\n",
      "    \"layer_8/width_65k/average_l0_19\": 19,\n",
      "    \"layer_8/width_65k/average_l0_213\": 213,\n",
      "    \"layer_8/width_65k/average_l0_33\": 33,\n",
      "    \"layer_8/width_65k/average_l0_59\": 59,\n",
      "    \"layer_9/width_65k/average_l0_118\": 118,\n",
      "    \"layer_9/width_65k/average_l0_19\": 19,\n",
      "    \"layer_9/width_65k/average_l0_240\": 240,\n",
      "    \"layer_9/width_65k/average_l0_34\": 34,\n",
      "    \"layer_9/width_65k/average_l0_61\": 61,\n",
      "    \"layer_10/width_65k/average_l0_128\": 128,\n",
      "    \"layer_10/width_65k/average_l0_20\": 20,\n",
      "    \"layer_10/width_65k/average_l0_265\": 265,\n",
      "    \"layer_10/width_65k/average_l0_36\": 36,\n",
      "    \"layer_10/width_65k/average_l0_66\": 66,\n",
      "    \"layer_11/width_65k/average_l0_134\": 134,\n",
      "    \"layer_11/width_65k/average_l0_21\": 21,\n",
      "    \"layer_11/width_65k/average_l0_273\": 273,\n",
      "    \"layer_11/width_65k/average_l0_37\": 37,\n",
      "    \"layer_11/width_65k/average_l0_70\": 70,\n",
      "    \"layer_12/width_65k/average_l0_141\": 141,\n",
      "    \"layer_12/width_65k/average_l0_21\": 21,\n",
      "    \"layer_12/width_65k/average_l0_297\": 297,\n",
      "    \"layer_12/width_65k/average_l0_38\": 38,\n",
      "    \"layer_12/width_65k/average_l0_72\": 72,\n",
      "    \"layer_13/width_65k/average_l0_142\": 142,\n",
      "    \"layer_13/width_65k/average_l0_22\": 22,\n",
      "    \"layer_13/width_65k/average_l0_288\": 288,\n",
      "    \"layer_13/width_65k/average_l0_40\": 40,\n",
      "    \"layer_13/width_65k/average_l0_74\": 74,\n",
      "    \"layer_13/width_65k/average_l0_75\": 75,\n",
      "    \"layer_14/width_65k/average_l0_144\": 144,\n",
      "    \"layer_14/width_65k/average_l0_21\": 21,\n",
      "    \"layer_14/width_65k/average_l0_284\": 284,\n",
      "    \"layer_14/width_65k/average_l0_40\": 40,\n",
      "    \"layer_14/width_65k/average_l0_73\": 73,\n",
      "    \"layer_15/width_65k/average_l0_127\": 127,\n",
      "    \"layer_15/width_65k/average_l0_21\": 21,\n",
      "    \"layer_15/width_65k/average_l0_240\": 240,\n",
      "    \"layer_15/width_65k/average_l0_38\": 38,\n",
      "    \"layer_15/width_65k/average_l0_68\": 68,\n",
      "    \"layer_16/width_65k/average_l0_128\": 128,\n",
      "    \"layer_16/width_65k/average_l0_21\": 21,\n",
      "    \"layer_16/width_65k/average_l0_244\": 244,\n",
      "    \"layer_16/width_65k/average_l0_38\": 38,\n",
      "    \"layer_16/width_65k/average_l0_69\": 69,\n",
      "    \"layer_17/width_65k/average_l0_125\": 125,\n",
      "    \"layer_17/width_65k/average_l0_21\": 21,\n",
      "    \"layer_17/width_65k/average_l0_233\": 233,\n",
      "    \"layer_17/width_65k/average_l0_38\": 38,\n",
      "    \"layer_17/width_65k/average_l0_68\": 68,\n",
      "    \"layer_18/width_65k/average_l0_116\": 116,\n",
      "    \"layer_18/width_65k/average_l0_117\": 117,\n",
      "    \"layer_18/width_65k/average_l0_21\": 21,\n",
      "    \"layer_18/width_65k/average_l0_216\": 216,\n",
      "    \"layer_18/width_65k/average_l0_36\": 36,\n",
      "    \"layer_18/width_65k/average_l0_64\": 64,\n",
      "    \"layer_19/width_65k/average_l0_115\": 115,\n",
      "    \"layer_19/width_65k/average_l0_21\": 21,\n",
      "    \"layer_19/width_65k/average_l0_216\": 216,\n",
      "    \"layer_19/width_65k/average_l0_35\": 35,\n",
      "    \"layer_19/width_65k/average_l0_63\": 63,\n",
      "    \"layer_20/width_65k/average_l0_114\": 114,\n",
      "    \"layer_20/width_65k/average_l0_20\": 20,\n",
      "    \"layer_20/width_65k/average_l0_221\": 221,\n",
      "    \"layer_20/width_65k/average_l0_34\": 34,\n",
      "    \"layer_20/width_65k/average_l0_61\": 61,\n",
      "    \"layer_21/width_65k/average_l0_111\": 111,\n",
      "    \"layer_21/width_65k/average_l0_112\": 112,\n",
      "    \"layer_21/width_65k/average_l0_20\": 20,\n",
      "    \"layer_21/width_65k/average_l0_225\": 225,\n",
      "    \"layer_21/width_65k/average_l0_33\": 33,\n",
      "    \"layer_21/width_65k/average_l0_61\": 61,\n",
      "    \"layer_22/width_65k/average_l0_116\": 116,\n",
      "    \"layer_22/width_65k/average_l0_117\": 117,\n",
      "    \"layer_22/width_65k/average_l0_20\": 20,\n",
      "    \"layer_22/width_65k/average_l0_248\": 248,\n",
      "    \"layer_22/width_65k/average_l0_33\": 33,\n",
      "    \"layer_22/width_65k/average_l0_62\": 62,\n",
      "    \"layer_23/width_65k/average_l0_123\": 123,\n",
      "    \"layer_23/width_65k/average_l0_124\": 124,\n",
      "    \"layer_23/width_65k/average_l0_20\": 20,\n",
      "    \"layer_23/width_65k/average_l0_272\": 272,\n",
      "    \"layer_23/width_65k/average_l0_35\": 35,\n",
      "    \"layer_23/width_65k/average_l0_64\": 64,\n",
      "    \"layer_24/width_65k/average_l0_124\": 124,\n",
      "    \"layer_24/width_65k/average_l0_19\": 19,\n",
      "    \"layer_24/width_65k/average_l0_273\": 273,\n",
      "    \"layer_24/width_65k/average_l0_34\": 34,\n",
      "    \"layer_24/width_65k/average_l0_63\": 63,\n",
      "    \"layer_25/width_65k/average_l0_15\": 15,\n",
      "    \"layer_25/width_65k/average_l0_197\": 197,\n",
      "    \"layer_25/width_65k/average_l0_26\": 26,\n",
      "    \"layer_25/width_65k/average_l0_48\": 48,\n",
      "    \"layer_25/width_65k/average_l0_93\": 93\n",
      "  },\n",
      "  \"neuronpedia_id\": {\n",
      "    \"layer_0/width_16k/average_l0_105\": null,\n",
      "    \"layer_0/width_16k/average_l0_13\": null,\n",
      "    \"layer_0/width_16k/average_l0_226\": null,\n",
      "    \"layer_0/width_16k/average_l0_25\": null,\n",
      "    \"layer_0/width_16k/average_l0_46\": null,\n",
      "    \"layer_1/width_16k/average_l0_10\": null,\n",
      "    \"layer_1/width_16k/average_l0_102\": null,\n",
      "    \"layer_1/width_16k/average_l0_20\": null,\n",
      "    \"layer_1/width_16k/average_l0_250\": null,\n",
      "    \"layer_1/width_16k/average_l0_40\": null,\n",
      "    \"layer_2/width_16k/average_l0_13\": null,\n",
      "    \"layer_2/width_16k/average_l0_141\": null,\n",
      "    \"layer_2/width_16k/average_l0_142\": null,\n",
      "    \"layer_2/width_16k/average_l0_24\": null,\n",
      "    \"layer_2/width_16k/average_l0_304\": null,\n",
      "    \"layer_2/width_16k/average_l0_53\": null,\n",
      "    \"layer_3/width_16k/average_l0_14\": null,\n",
      "    \"layer_3/width_16k/average_l0_142\": null,\n",
      "    \"layer_3/width_16k/average_l0_28\": null,\n",
      "    \"layer_3/width_16k/average_l0_315\": null,\n",
      "    \"layer_3/width_16k/average_l0_59\": null,\n",
      "    \"layer_4/width_16k/average_l0_124\": null,\n",
      "    \"layer_4/width_16k/average_l0_125\": null,\n",
      "    \"layer_4/width_16k/average_l0_17\": null,\n",
      "    \"layer_4/width_16k/average_l0_281\": null,\n",
      "    \"layer_4/width_16k/average_l0_31\": null,\n",
      "    \"layer_4/width_16k/average_l0_60\": null,\n",
      "    \"layer_5/width_16k/average_l0_143\": null,\n",
      "    \"layer_5/width_16k/average_l0_18\": null,\n",
      "    \"layer_5/width_16k/average_l0_309\": null,\n",
      "    \"layer_5/width_16k/average_l0_34\": null,\n",
      "    \"layer_5/width_16k/average_l0_68\": null,\n",
      "    \"layer_6/width_16k/average_l0_144\": null,\n",
      "    \"layer_6/width_16k/average_l0_19\": null,\n",
      "    \"layer_6/width_16k/average_l0_301\": null,\n",
      "    \"layer_6/width_16k/average_l0_36\": null,\n",
      "    \"layer_6/width_16k/average_l0_70\": null,\n",
      "    \"layer_7/width_16k/average_l0_137\": null,\n",
      "    \"layer_7/width_16k/average_l0_20\": null,\n",
      "    \"layer_7/width_16k/average_l0_285\": null,\n",
      "    \"layer_7/width_16k/average_l0_36\": null,\n",
      "    \"layer_7/width_16k/average_l0_69\": null,\n",
      "    \"layer_8/width_16k/average_l0_142\": null,\n",
      "    \"layer_8/width_16k/average_l0_20\": null,\n",
      "    \"layer_8/width_16k/average_l0_301\": null,\n",
      "    \"layer_8/width_16k/average_l0_37\": null,\n",
      "    \"layer_8/width_16k/average_l0_71\": null,\n",
      "    \"layer_9/width_16k/average_l0_151\": null,\n",
      "    \"layer_9/width_16k/average_l0_21\": null,\n",
      "    \"layer_9/width_16k/average_l0_340\": null,\n",
      "    \"layer_9/width_16k/average_l0_37\": null,\n",
      "    \"layer_9/width_16k/average_l0_73\": null,\n",
      "    \"layer_10/width_16k/average_l0_166\": null,\n",
      "    \"layer_10/width_16k/average_l0_21\": null,\n",
      "    \"layer_10/width_16k/average_l0_39\": null,\n",
      "    \"layer_10/width_16k/average_l0_395\": null,\n",
      "    \"layer_10/width_16k/average_l0_77\": null,\n",
      "    \"layer_11/width_16k/average_l0_168\": null,\n",
      "    \"layer_11/width_16k/average_l0_22\": null,\n",
      "    \"layer_11/width_16k/average_l0_393\": null,\n",
      "    \"layer_11/width_16k/average_l0_41\": null,\n",
      "    \"layer_11/width_16k/average_l0_79\": null,\n",
      "    \"layer_11/width_16k/average_l0_80\": null,\n",
      "    \"layer_12/width_16k/average_l0_176\": null,\n",
      "    \"layer_12/width_16k/average_l0_22\": null,\n",
      "    \"layer_12/width_16k/average_l0_41\": null,\n",
      "    \"layer_12/width_16k/average_l0_445\": null,\n",
      "    \"layer_12/width_16k/average_l0_82\": null,\n",
      "    \"layer_13/width_16k/average_l0_173\": null,\n",
      "    \"layer_13/width_16k/average_l0_23\": null,\n",
      "    \"layer_13/width_16k/average_l0_403\": null,\n",
      "    \"layer_13/width_16k/average_l0_43\": null,\n",
      "    \"layer_13/width_16k/average_l0_83\": null,\n",
      "    \"layer_13/width_16k/average_l0_84\": null,\n",
      "    \"layer_14/width_16k/average_l0_173\": null,\n",
      "    \"layer_14/width_16k/average_l0_23\": null,\n",
      "    \"layer_14/width_16k/average_l0_388\": null,\n",
      "    \"layer_14/width_16k/average_l0_43\": null,\n",
      "    \"layer_14/width_16k/average_l0_83\": null,\n",
      "    \"layer_14/width_16k/average_l0_84\": null,\n",
      "    \"layer_15/width_16k/average_l0_150\": null,\n",
      "    \"layer_15/width_16k/average_l0_23\": null,\n",
      "    \"layer_15/width_16k/average_l0_308\": null,\n",
      "    \"layer_15/width_16k/average_l0_41\": null,\n",
      "    \"layer_15/width_16k/average_l0_78\": null,\n",
      "    \"layer_16/width_16k/average_l0_154\": null,\n",
      "    \"layer_16/width_16k/average_l0_23\": null,\n",
      "    \"layer_16/width_16k/average_l0_335\": null,\n",
      "    \"layer_16/width_16k/average_l0_42\": null,\n",
      "    \"layer_16/width_16k/average_l0_78\": null,\n",
      "    \"layer_17/width_16k/average_l0_150\": null,\n",
      "    \"layer_17/width_16k/average_l0_23\": null,\n",
      "    \"layer_17/width_16k/average_l0_304\": null,\n",
      "    \"layer_17/width_16k/average_l0_42\": null,\n",
      "    \"layer_17/width_16k/average_l0_77\": null,\n",
      "    \"layer_18/width_16k/average_l0_138\": null,\n",
      "    \"layer_18/width_16k/average_l0_23\": null,\n",
      "    \"layer_18/width_16k/average_l0_280\": null,\n",
      "    \"layer_18/width_16k/average_l0_40\": null,\n",
      "    \"layer_18/width_16k/average_l0_74\": null,\n",
      "    \"layer_19/width_16k/average_l0_137\": null,\n",
      "    \"layer_19/width_16k/average_l0_23\": null,\n",
      "    \"layer_19/width_16k/average_l0_279\": null,\n",
      "    \"layer_19/width_16k/average_l0_40\": null,\n",
      "    \"layer_19/width_16k/average_l0_73\": null,\n",
      "    \"layer_20/width_16k/average_l0_139\": null,\n",
      "    \"layer_20/width_16k/average_l0_22\": null,\n",
      "    \"layer_20/width_16k/average_l0_294\": null,\n",
      "    \"layer_20/width_16k/average_l0_38\": null,\n",
      "    \"layer_20/width_16k/average_l0_71\": null,\n",
      "    \"layer_21/width_16k/average_l0_139\": null,\n",
      "    \"layer_21/width_16k/average_l0_22\": null,\n",
      "    \"layer_21/width_16k/average_l0_301\": null,\n",
      "    \"layer_21/width_16k/average_l0_38\": null,\n",
      "    \"layer_21/width_16k/average_l0_70\": null,\n",
      "    \"layer_22/width_16k/average_l0_147\": null,\n",
      "    \"layer_22/width_16k/average_l0_21\": null,\n",
      "    \"layer_22/width_16k/average_l0_349\": null,\n",
      "    \"layer_22/width_16k/average_l0_38\": null,\n",
      "    \"layer_22/width_16k/average_l0_72\": null,\n",
      "    \"layer_23/width_16k/average_l0_157\": null,\n",
      "    \"layer_23/width_16k/average_l0_21\": null,\n",
      "    \"layer_23/width_16k/average_l0_38\": null,\n",
      "    \"layer_23/width_16k/average_l0_404\": null,\n",
      "    \"layer_23/width_16k/average_l0_74\": null,\n",
      "    \"layer_23/width_16k/average_l0_75\": null,\n",
      "    \"layer_24/width_16k/average_l0_158\": null,\n",
      "    \"layer_24/width_16k/average_l0_20\": null,\n",
      "    \"layer_24/width_16k/average_l0_38\": null,\n",
      "    \"layer_24/width_16k/average_l0_457\": null,\n",
      "    \"layer_24/width_16k/average_l0_73\": null,\n",
      "    \"layer_25/width_16k/average_l0_116\": null,\n",
      "    \"layer_25/width_16k/average_l0_16\": null,\n",
      "    \"layer_25/width_16k/average_l0_28\": null,\n",
      "    \"layer_25/width_16k/average_l0_285\": null,\n",
      "    \"layer_25/width_16k/average_l0_55\": null,\n",
      "    \"layer_5/width_1m/average_l0_114\": null,\n",
      "    \"layer_5/width_1m/average_l0_13\": null,\n",
      "    \"layer_5/width_1m/average_l0_21\": null,\n",
      "    \"layer_5/width_1m/average_l0_36\": null,\n",
      "    \"layer_5/width_1m/average_l0_63\": null,\n",
      "    \"layer_5/width_1m/average_l0_9\": null,\n",
      "    \"layer_12/width_1m/average_l0_107\": null,\n",
      "    \"layer_12/width_1m/average_l0_19\": null,\n",
      "    \"layer_12/width_1m/average_l0_207\": null,\n",
      "    \"layer_12/width_1m/average_l0_26\": null,\n",
      "    \"layer_12/width_1m/average_l0_58\": null,\n",
      "    \"layer_12/width_1m/average_l0_73\": null,\n",
      "    \"layer_19/width_1m/average_l0_157\": null,\n",
      "    \"layer_19/width_1m/average_l0_16\": null,\n",
      "    \"layer_19/width_1m/average_l0_18\": null,\n",
      "    \"layer_19/width_1m/average_l0_29\": null,\n",
      "    \"layer_19/width_1m/average_l0_50\": null,\n",
      "    \"layer_19/width_1m/average_l0_88\": null,\n",
      "    \"layer_12/width_262k/average_l0_11\": null,\n",
      "    \"layer_12/width_262k/average_l0_121\": null,\n",
      "    \"layer_12/width_262k/average_l0_21\": null,\n",
      "    \"layer_12/width_262k/average_l0_243\": null,\n",
      "    \"layer_12/width_262k/average_l0_36\": null,\n",
      "    \"layer_12/width_262k/average_l0_67\": null,\n",
      "    \"layer_12/width_32k/average_l0_12\": null,\n",
      "    \"layer_12/width_32k/average_l0_155\": null,\n",
      "    \"layer_12/width_32k/average_l0_22\": null,\n",
      "    \"layer_12/width_32k/average_l0_360\": null,\n",
      "    \"layer_12/width_32k/average_l0_40\": null,\n",
      "    \"layer_12/width_32k/average_l0_76\": null,\n",
      "    \"layer_12/width_524k/average_l0_115\": null,\n",
      "    \"layer_12/width_524k/average_l0_22\": null,\n",
      "    \"layer_12/width_524k/average_l0_227\": null,\n",
      "    \"layer_12/width_524k/average_l0_29\": null,\n",
      "    \"layer_12/width_524k/average_l0_46\": null,\n",
      "    \"layer_12/width_524k/average_l0_65\": null,\n",
      "    \"layer_0/width_65k/average_l0_11\": null,\n",
      "    \"layer_0/width_65k/average_l0_17\": null,\n",
      "    \"layer_0/width_65k/average_l0_27\": null,\n",
      "    \"layer_0/width_65k/average_l0_43\": null,\n",
      "    \"layer_0/width_65k/average_l0_73\": null,\n",
      "    \"layer_1/width_65k/average_l0_121\": null,\n",
      "    \"layer_1/width_65k/average_l0_16\": null,\n",
      "    \"layer_1/width_65k/average_l0_30\": null,\n",
      "    \"layer_1/width_65k/average_l0_54\": null,\n",
      "    \"layer_1/width_65k/average_l0_9\": null,\n",
      "    \"layer_2/width_65k/average_l0_11\": null,\n",
      "    \"layer_2/width_65k/average_l0_169\": null,\n",
      "    \"layer_2/width_65k/average_l0_20\": null,\n",
      "    \"layer_2/width_65k/average_l0_37\": null,\n",
      "    \"layer_2/width_65k/average_l0_77\": null,\n",
      "    \"layer_3/width_65k/average_l0_13\": null,\n",
      "    \"layer_3/width_65k/average_l0_193\": null,\n",
      "    \"layer_3/width_65k/average_l0_23\": null,\n",
      "    \"layer_3/width_65k/average_l0_42\": null,\n",
      "    \"layer_3/width_65k/average_l0_89\": null,\n",
      "    \"layer_4/width_65k/average_l0_14\": null,\n",
      "    \"layer_4/width_65k/average_l0_177\": null,\n",
      "    \"layer_4/width_65k/average_l0_25\": null,\n",
      "    \"layer_4/width_65k/average_l0_46\": null,\n",
      "    \"layer_4/width_65k/average_l0_89\": null,\n",
      "    \"layer_5/width_65k/average_l0_105\": null,\n",
      "    \"layer_5/width_65k/average_l0_17\": null,\n",
      "    \"layer_5/width_65k/average_l0_211\": null,\n",
      "    \"layer_5/width_65k/average_l0_29\": null,\n",
      "    \"layer_5/width_65k/average_l0_53\": null,\n",
      "    \"layer_6/width_65k/average_l0_107\": null,\n",
      "    \"layer_6/width_65k/average_l0_17\": null,\n",
      "    \"layer_6/width_65k/average_l0_208\": null,\n",
      "    \"layer_6/width_65k/average_l0_30\": null,\n",
      "    \"layer_6/width_65k/average_l0_56\": null,\n",
      "    \"layer_7/width_65k/average_l0_107\": null,\n",
      "    \"layer_7/width_65k/average_l0_18\": null,\n",
      "    \"layer_7/width_65k/average_l0_203\": null,\n",
      "    \"layer_7/width_65k/average_l0_31\": null,\n",
      "    \"layer_7/width_65k/average_l0_57\": null,\n",
      "    \"layer_8/width_65k/average_l0_111\": null,\n",
      "    \"layer_8/width_65k/average_l0_19\": null,\n",
      "    \"layer_8/width_65k/average_l0_213\": null,\n",
      "    \"layer_8/width_65k/average_l0_33\": null,\n",
      "    \"layer_8/width_65k/average_l0_59\": null,\n",
      "    \"layer_9/width_65k/average_l0_118\": null,\n",
      "    \"layer_9/width_65k/average_l0_19\": null,\n",
      "    \"layer_9/width_65k/average_l0_240\": null,\n",
      "    \"layer_9/width_65k/average_l0_34\": null,\n",
      "    \"layer_9/width_65k/average_l0_61\": null,\n",
      "    \"layer_10/width_65k/average_l0_128\": null,\n",
      "    \"layer_10/width_65k/average_l0_20\": null,\n",
      "    \"layer_10/width_65k/average_l0_265\": null,\n",
      "    \"layer_10/width_65k/average_l0_36\": null,\n",
      "    \"layer_10/width_65k/average_l0_66\": null,\n",
      "    \"layer_11/width_65k/average_l0_134\": null,\n",
      "    \"layer_11/width_65k/average_l0_21\": null,\n",
      "    \"layer_11/width_65k/average_l0_273\": null,\n",
      "    \"layer_11/width_65k/average_l0_37\": null,\n",
      "    \"layer_11/width_65k/average_l0_70\": null,\n",
      "    \"layer_12/width_65k/average_l0_141\": null,\n",
      "    \"layer_12/width_65k/average_l0_21\": null,\n",
      "    \"layer_12/width_65k/average_l0_297\": null,\n",
      "    \"layer_12/width_65k/average_l0_38\": null,\n",
      "    \"layer_12/width_65k/average_l0_72\": null,\n",
      "    \"layer_13/width_65k/average_l0_142\": null,\n",
      "    \"layer_13/width_65k/average_l0_22\": null,\n",
      "    \"layer_13/width_65k/average_l0_288\": null,\n",
      "    \"layer_13/width_65k/average_l0_40\": null,\n",
      "    \"layer_13/width_65k/average_l0_74\": null,\n",
      "    \"layer_13/width_65k/average_l0_75\": null,\n",
      "    \"layer_14/width_65k/average_l0_144\": null,\n",
      "    \"layer_14/width_65k/average_l0_21\": null,\n",
      "    \"layer_14/width_65k/average_l0_284\": null,\n",
      "    \"layer_14/width_65k/average_l0_40\": null,\n",
      "    \"layer_14/width_65k/average_l0_73\": null,\n",
      "    \"layer_15/width_65k/average_l0_127\": null,\n",
      "    \"layer_15/width_65k/average_l0_21\": null,\n",
      "    \"layer_15/width_65k/average_l0_240\": null,\n",
      "    \"layer_15/width_65k/average_l0_38\": null,\n",
      "    \"layer_15/width_65k/average_l0_68\": null,\n",
      "    \"layer_16/width_65k/average_l0_128\": null,\n",
      "    \"layer_16/width_65k/average_l0_21\": null,\n",
      "    \"layer_16/width_65k/average_l0_244\": null,\n",
      "    \"layer_16/width_65k/average_l0_38\": null,\n",
      "    \"layer_16/width_65k/average_l0_69\": null,\n",
      "    \"layer_17/width_65k/average_l0_125\": null,\n",
      "    \"layer_17/width_65k/average_l0_21\": null,\n",
      "    \"layer_17/width_65k/average_l0_233\": null,\n",
      "    \"layer_17/width_65k/average_l0_38\": null,\n",
      "    \"layer_17/width_65k/average_l0_68\": null,\n",
      "    \"layer_18/width_65k/average_l0_116\": null,\n",
      "    \"layer_18/width_65k/average_l0_117\": null,\n",
      "    \"layer_18/width_65k/average_l0_21\": null,\n",
      "    \"layer_18/width_65k/average_l0_216\": null,\n",
      "    \"layer_18/width_65k/average_l0_36\": null,\n",
      "    \"layer_18/width_65k/average_l0_64\": null,\n",
      "    \"layer_19/width_65k/average_l0_115\": null,\n",
      "    \"layer_19/width_65k/average_l0_21\": null,\n",
      "    \"layer_19/width_65k/average_l0_216\": null,\n",
      "    \"layer_19/width_65k/average_l0_35\": null,\n",
      "    \"layer_19/width_65k/average_l0_63\": null,\n",
      "    \"layer_20/width_65k/average_l0_114\": null,\n",
      "    \"layer_20/width_65k/average_l0_20\": null,\n",
      "    \"layer_20/width_65k/average_l0_221\": null,\n",
      "    \"layer_20/width_65k/average_l0_34\": null,\n",
      "    \"layer_20/width_65k/average_l0_61\": null,\n",
      "    \"layer_21/width_65k/average_l0_111\": null,\n",
      "    \"layer_21/width_65k/average_l0_112\": null,\n",
      "    \"layer_21/width_65k/average_l0_20\": null,\n",
      "    \"layer_21/width_65k/average_l0_225\": null,\n",
      "    \"layer_21/width_65k/average_l0_33\": null,\n",
      "    \"layer_21/width_65k/average_l0_61\": null,\n",
      "    \"layer_22/width_65k/average_l0_116\": null,\n",
      "    \"layer_22/width_65k/average_l0_117\": null,\n",
      "    \"layer_22/width_65k/average_l0_20\": null,\n",
      "    \"layer_22/width_65k/average_l0_248\": null,\n",
      "    \"layer_22/width_65k/average_l0_33\": null,\n",
      "    \"layer_22/width_65k/average_l0_62\": null,\n",
      "    \"layer_23/width_65k/average_l0_123\": null,\n",
      "    \"layer_23/width_65k/average_l0_124\": null,\n",
      "    \"layer_23/width_65k/average_l0_20\": null,\n",
      "    \"layer_23/width_65k/average_l0_272\": null,\n",
      "    \"layer_23/width_65k/average_l0_35\": null,\n",
      "    \"layer_23/width_65k/average_l0_64\": null,\n",
      "    \"layer_24/width_65k/average_l0_124\": null,\n",
      "    \"layer_24/width_65k/average_l0_19\": null,\n",
      "    \"layer_24/width_65k/average_l0_273\": null,\n",
      "    \"layer_24/width_65k/average_l0_34\": null,\n",
      "    \"layer_24/width_65k/average_l0_63\": null,\n",
      "    \"layer_25/width_65k/average_l0_15\": null,\n",
      "    \"layer_25/width_65k/average_l0_197\": null,\n",
      "    \"layer_25/width_65k/average_l0_26\": null,\n",
      "    \"layer_25/width_65k/average_l0_48\": null,\n",
      "    \"layer_25/width_65k/average_l0_93\": null\n",
      "  },\n",
      "  \"config_overrides\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# note: `\"saes_map\"` maps `<sae-id>: <hook-point>`\n",
    "pretrained_sae_lookup: PretrainedSAELookup = pretrained_saes_dir[pretrained_sae_name]\n",
    "\n",
    "# note: only layers 5, 12, and 19 seem to have the 1m width\n",
    "python_utils.print_json(pretrained_sae_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4695ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose:\n",
    "# - last layer\n",
    "# - largest available width\n",
    "# - lowest l0 sparsity \"on average, how many neurons (features for SAEs) activate\"\n",
    "sae_id = 'layer_25/width_65k/average_l0_15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c11a5820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2edb6b3b024e22b5e173f10a369c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = get_best_available_torch_device()\n",
    "\n",
    "# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)\n",
    "# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict\n",
    "# We also return the feature sparsities which are stored in HF for convenience. \n",
    "sae, cfg_dict, sparsity = sae_lens.SAE.from_pretrained(\n",
    "    release = pretrained_sae_name, # <- Release name \n",
    "    sae_id = sae_id, # <- SAE id (not always a hook point!)\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d74d100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out into types for readability\n",
    "sae: sae_lens.SAE = sae\n",
    "cfg_dict: dict[str, str | int | None | torch.device | bool] = cfg_dict\n",
    "sparsity: dict = sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0486a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show config (since usually small)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'architecture': 'jumprelu',\n",
       " 'd_in': 2304,\n",
       " 'd_sae': 65536,\n",
       " 'dtype': 'float32',\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'hook_name': 'blocks.25.hook_resid_post',\n",
       " 'hook_layer': 25,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': True,\n",
       " 'dataset_path': 'monology/pile-uncopyrighted',\n",
       " 'context_size': 1024,\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'apply_b_dec_to_input': False,\n",
       " 'normalize_activations': None,\n",
       " 'device': device(type='mps')}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Show config (since usually small)')\n",
    "cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0fc72e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        NoneType\n",
      "\u001b[0;31mString form:\u001b[0m None\n",
      "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
     ]
    }
   ],
   "source": [
    "# note: sparsity is average l0 sparsity? is this because already in the name?\n",
    "sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15494e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d43096c6ce4f1c8e61ba09d4ab6ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba84f1d331d41879705b44623c3ce27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03249d1b53694901990d0188869b7097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345c095a699c447d8ea2009216f75125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55a8fa2981e445dbeaa83bc7998db60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0509b6e58b4c4b18984bbe53c63126c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e0c13ee31a4483980e8fe70dcab426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75fba67476b4de5ad1b6c5b524a8e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4371b72f15455b98e1814511a915db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a039a57ff44f13ae36e84b7d6cda66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5251015c26e4cd09f6c920975b43d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6caa5892e34af994e1f302bc3cfc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# now we'll load the model\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "\n",
    "# WARNING:root:You tried to specify center_unembed=True for a model using logit softcap,\n",
    "#              but this can't be done! Softcapping is not invariant upon adding a\n",
    "#              constantSetting center_unembed=False instead.\n",
    "#\n",
    "# WARNING:root:You are not using LayerNorm, so the writing weights can't be centered!\n",
    "#              Skipping\n",
    "#\n",
    "model = sae_lens.HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb971998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformerConfig:\n",
       "{'act_fn': 'gelu_pytorch_tanh',\n",
       " 'attention_dir': 'causal',\n",
       " 'attn_only': False,\n",
       " 'attn_scale': 16.0,\n",
       " 'attn_scores_soft_cap': 50.0,\n",
       " 'attn_types': ['global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local',\n",
       "                'global',\n",
       "                'local'],\n",
       " 'checkpoint_index': None,\n",
       " 'checkpoint_label_type': None,\n",
       " 'checkpoint_value': None,\n",
       " 'd_head': 256,\n",
       " 'd_mlp': 9216,\n",
       " 'd_model': 2304,\n",
       " 'd_vocab': 256000,\n",
       " 'd_vocab_out': 256000,\n",
       " 'decoder_start_token_id': None,\n",
       " 'default_prepend_bos': True,\n",
       " 'device': device(type='mps'),\n",
       " 'dtype': torch.float32,\n",
       " 'eps': 1e-06,\n",
       " 'experts_per_token': None,\n",
       " 'final_rms': True,\n",
       " 'from_checkpoint': False,\n",
       " 'gated_mlp': True,\n",
       " 'init_mode': 'gpt2',\n",
       " 'init_weights': False,\n",
       " 'initializer_range': 0.02,\n",
       " 'load_in_4bit': False,\n",
       " 'model_name': 'gemma-2-2b',\n",
       " 'n_ctx': 8192,\n",
       " 'n_devices': 1,\n",
       " 'n_heads': 8,\n",
       " 'n_key_value_heads': 4,\n",
       " 'n_layers': 26,\n",
       " 'n_params': 2146959360,\n",
       " 'normalization_type': 'RMSPre',\n",
       " 'num_experts': None,\n",
       " 'original_architecture': 'Gemma2ForCausalLM',\n",
       " 'output_logits_soft_cap': 30.0,\n",
       " 'parallel_attn_mlp': False,\n",
       " 'positional_embedding_type': 'rotary',\n",
       " 'post_embedding_ln': False,\n",
       " 'relative_attention_max_distance': None,\n",
       " 'relative_attention_num_buckets': None,\n",
       " 'rotary_adjacent_pairs': False,\n",
       " 'rotary_base': 10000.0,\n",
       " 'rotary_dim': 256,\n",
       " 'scale_attn_by_inverse_layer_idx': False,\n",
       " 'seed': None,\n",
       " 'tie_word_embeddings': False,\n",
       " 'tokenizer_name': 'google/gemma-2-2b',\n",
       " 'tokenizer_prepends_bos': True,\n",
       " 'trust_remote_code': False,\n",
       " 'use_attn_in': False,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_hook_mlp_in': False,\n",
       " 'use_hook_tokens': False,\n",
       " 'use_local_attn': True,\n",
       " 'use_normalization_before_and_after': True,\n",
       " 'use_split_qkv_input': False,\n",
       " 'window_size': 4096}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can inspect the model config, which is often useful\n",
    "model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4d77c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-31bd0d87-bbd9\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-31bd0d87-bbd9\",\n",
       "      TokenLogProbs,\n",
       "      {\"prompt\": [\"<bos>\", \"Jill\", \" threw\", \" the\", \" ball\", \" to\", \" Jack\", \".\", \" Jack\", \" threw\", \" the\", \" ball\", \" to\", \" Will\", \".\", \" Will\", \" threw\", \" the\", \" ball\", \" back\", \" to\", \" Jill\", \".\"], \"topKLogProbs\": [[-1.9983851909637451, -2.5378756523132324, -2.6550519466400146, -2.796070098876953, -2.9108171463012695, -3.042771339416504, -3.2088470458984375, -3.5410985946655273, -3.602886199951172, -3.6941709518432617], [-2.0806832313537598, -2.4027466773986816, -2.4341683387756348, -2.669386386871338, -2.968100070953369, -3.1400132179260254, -3.3099722862243652, -3.5481114387512207, -3.653533458709717, -3.675690174102783], [-1.1520839929580688, -1.9118629693984985, -2.55922269821167, -2.8072333335876465, -2.940965175628662, -3.1875338554382324, -3.656226634979248, -3.6705069541931152, -3.7006659507751465, -3.881598949432373], [-1.0549979209899902, -2.3014025688171387, -3.4601168632507324, -3.618772029876709, -3.746537685394287, -3.786729335784912, -3.8257346153259277, -3.8872885704040527, -4.0006327629089355, -4.227790355682373], [-2.1943914890289307, -2.4303934574127197, -2.4650347232818604, -2.911722421646118, -3.0292513370513916, -3.0826971530914307, -3.18178391456604, -3.2946970462799072, -3.3922464847564697, -3.3940794467926025], [-1.5264428853988647, -1.6425565481185913, -2.4890685081481934, -3.232694149017334, -4.127151012420654, -4.389451503753662, -4.4775214195251465, -4.560933589935303, -4.580234050750732, -4.672091960906982], [-1.0656713247299194, -1.5716050863265991, -2.164245128631592, -2.9808764457702637, -3.4274935722351074, -3.4864706993103027, -3.9294867515563965, -3.9946742057800293, -4.050033092498779, -4.114367961883545], [-1.4211949110031128, -1.8728264570236206, -2.326951026916504, -2.760073661804199, -2.8048620223999023, -2.8362741470336914, -3.115267753601074, -3.300168037414551, -3.629532814025879, -4.005480766296387], [-0.9676473736763, -1.349698781967163, -3.254342794418335, -3.941556692123413, -4.012365341186523, -4.110891342163086, -4.241743087768555, -4.2966766357421875, -4.43986701965332, -4.62774658203125], [-0.1901981383562088, -1.8397839069366455, -5.802762031555176, -6.122956275939941, -6.337265968322754, -7.037463188171387, -7.100769996643066, -7.542632102966309, -7.9837236404418945, -8.030373573303223], [-0.013450946658849716, -6.348577976226807, -6.831078052520752, -6.949337005615234, -7.091055393218994, -7.332365989685059, -7.984274864196777, -8.152535438537598, -8.416635513305664, -8.495341300964355], [-0.1472037136554718, -2.736938714981079, -4.670980930328369, -5.249786853790283, -5.392664432525635, -5.6255574226379395, -5.771923542022705, -5.7884202003479, -5.8492913246154785, -6.027058124542236], [-0.3572036027908325, -4.371039390563965, -4.586287498474121, -4.687148094177246, -4.694510459899902, -4.745224952697754, -4.822340965270996, -4.840821266174316, -5.134703636169434, -5.140007972717285], [-0.09080391377210617, -3.132103681564331, -4.2052717208862305, -5.08670711517334, -6.0023088455200195, -6.805842399597168, -6.825339317321777, -6.981854438781738, -7.146897315979004, -7.207278251647949], [-0.21325969696044922, -3.6444482803344727, -3.9428491592407227, -4.140473365783691, -4.540219306945801, -4.554068565368652, -4.676327705383301, -4.794241905212402, -4.842171669006348, -4.988997459411621], [-0.27826401591300964, -2.119797706604004, -3.9590253829956055, -4.328673362731934, -4.5388593673706055, -5.059954643249512, -5.373757362365723, -5.402802467346191, -5.444584846496582, -5.651663780212402], [-0.039647217839956284, -3.505864143371582, -6.688549041748047, -6.945247650146484, -7.051596641540527, -7.213376998901367, -7.217453956604004, -7.39909029006958, -7.672064304351807, -8.0425386428833], [-0.013924358412623405, -6.4203901290893555, -6.426197052001953, -6.480836868286133, -7.078489780426025, -7.500594615936279, -7.77639627456665, -7.776846885681152, -7.80850076675415, -7.929489612579346], [-0.10863295942544937, -2.5973072052001953, -5.731220245361328, -6.273527145385742, -6.356388092041016, -6.6020660400390625, -6.602319717407227, -6.690830230712891, -6.7559814453125, -6.828329086303711], [-0.06811370700597763, -3.360802173614502, -5.174564838409424, -5.4539666175842285, -5.59878396987915, -6.573050022125244, -6.768784046173096, -6.779015064239502, -6.863827228546143, -6.873545169830322], [-0.37600722908973694, -1.2825947999954224, -5.659055709838867, -6.286735534667969, -6.449258804321289, -6.515144348144531, -6.87725830078125, -6.878013610839844, -6.989336013793945, -6.9996185302734375], [-0.0834820419549942, -3.708544969558716, -4.16548490524292, -5.408961772918701, -5.822892665863037, -6.168097972869873, -6.33575963973999, -6.420032024383545, -6.469996929168701, -6.474175930023193]], \"topKTokens\": [[\"<h1>\", \"<strong>\", \"The\", \"<h2>\", \"<\", \"package\", \"import\", \"<h3>\", \"A\", \"<b>\"], [\" Biden\", \" Dug\", \" and\", \" is\", \" Scott\", \",\", \" has\", \" D\", \" was\", \" Stein\"], [\" a\", \" her\", \" the\", \" out\", \" herself\", \" me\", \" \", \" an\", \" up\", \" in\"], [\" ball\", \" first\", \" book\", \" dice\", \" football\", \" \", \" last\", \" party\", \" door\", \" best\"], [\" into\", \" up\", \" to\", \" \", \".\", \" and\", \" in\", \" as\", \" so\", \",\"], [\" the\", \" her\", \" me\", \" a\", \" Jack\", \" John\", \" Mike\", \" Jake\", \" Jill\", \" Sam\"], [\".\", \",\", \" and\", \" who\", \" in\", \" at\", \" with\", \" as\", \" so\", \" on\"], [\" Jack\", \"\\n\\n\", \" Jill\", \"\\n\", \" The\", \" He\", \" She\", \" It\", \" Then\", \"  \"], [\" caught\", \" threw\", \" ran\", \" dropped\", \" kicked\", \" tossed\", \" then\", \" passed\", \" was\", \" took\"], [\" the\", \" it\", \" a\", \" to\", \" back\", \"\\n\", \" his\", \" her\", \" Jill\", \" \"], [\" ball\", \" same\", \"\\n\", \" basketball\", \" baseball\", \" football\", \" tennis\", \" soccer\", \" \", \" bat\"], [\" to\", \" back\", \" at\", \" up\", \" into\", \".\", \" down\", \" over\", \" in\", \" higher\"], [\" Jill\", \" Jane\", \" Mary\", \" the\", \" Joe\", \" Jim\", \" Sam\", \" Jack\", \" John\", \" Kate\"], [\".\", \",\", \" and\", \" who\", \";\", \"\\n\\n\", \" in\", \" at\", \" .\", \" as\"], [\" Will\", \" Jill\", \"\\n\\n\", \" Jack\", \"\\n\", \" Who\", \" Then\", \" The\", \" Which\", \" He\"], [\" threw\", \" caught\", \" dropped\", \" then\", \" tossed\", \" ran\", \" passed\", \" did\", \" throws\", \" kicked\"], [\" the\", \" it\", \" a\", \" to\", \"...\", \"\\n\", \" ball\", \" back\", \" his\", \"\\n\\n\"], [\" ball\", \"\\n\", \" basketball\", \" baseball\", \"...\", \" football\", \" ba\", \" bal\", \"\\n\\n\", \" b\"], [\" to\", \" back\", \" into\", \" at\", \"...\", \" in\", \" down\", \" up\", \".\", \"\\n\"], [\" to\", \".\", \" at\", \" and\", \",\", \"...\", \" toward\", \" again\", \" into\", \" in\"], [\" Jill\", \" Jack\", \" the\", \" Jake\", \" J\", \" Joe\", \" him\", \" Jane\", \"...\", \" Alice\"], [\".\", \",\", \" and\", \"\\n\\n\", \" who\", \" again\", \" in\", \"...\", \"....\", \" so\"]], \"correctTokenRank\": [26126, 5467, 2, 0, 2, 4, 0, 0, 1, 0, 0, 0, 50, 0, 0, 0, 0, 0, 1, 0, 0, 0], \"correctTokenLogProb\": [-18.810827255249023, -15.894781112670898, -2.55922269821167, -1.0549979209899902, -2.4650347232818604, -4.127151012420654, -1.0656713247299194, -1.4211949110031128, -1.349698781967163, -0.1901981383562088, -0.013450946658849716, -0.1472037136554718, -6.649674415588379, -0.09080391377210617, -0.21325969696044922, -0.27826401591300964, -0.039647217839956284, -0.013924358412623405, -2.5973072052001953, -0.06811370700597763, -0.37600722908973694, -0.0834820419549942]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x309df54f0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's first test our previous cv visualization to sanity check\n",
    "example_prompt = 'Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to Jill.'\n",
    "logits, cache = model.run_with_cache(example_prompt)\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "cv.logits.token_log_probs(\n",
    "    token_indices=model.to_tokens(example_prompt),\n",
    "    log_probs=log_probs,\n",
    "    to_string=model.to_string,\n",
    ")\n",
    "\n",
    "# note: way higher confidence that we'll throw it back to jill, which is more correct\n",
    "# note: in general pretty high confidence for pretty much everything except for when we introduced a new character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "506fd301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Jill', ' threw', ' the', ' ball', ' to', ' Jack', '.', ' Jack', ' threw', ' the', ' ball', ' to', ' Will', '.', ' Will', ' threw', ' the', ' ball', ' back', ' to']\n",
      "Tokenized answer: [' Jill']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.67</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68.66</span><span style=\"font-weight: bold\">% Token: | Jill|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m27.67\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m68.66\u001b[0m\u001b[1m% Token: | Jill|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 27.67 Prob: 68.66% Token: | Jill|\n",
      "Top 1th token. Logit: 26.76 Prob: 27.73% Token: | Jack|\n",
      "Top 2th token. Logit: 22.38 Prob:  0.35% Token: | the|\n",
      "Top 3th token. Logit: 21.76 Prob:  0.19% Token: | Jake|\n",
      "Top 4th token. Logit: 21.59 Prob:  0.16% Token: | J|\n",
      "Top 5th token. Logit: 21.53 Prob:  0.15% Token: | Joe|\n",
      "Top 6th token. Logit: 21.17 Prob:  0.10% Token: | him|\n",
      "Top 7th token. Logit: 21.17 Prob:  0.10% Token: | Jane|\n",
      "Top 8th token. Logit: 21.05 Prob:  0.09% Token: |...|\n",
      "Top 9th token. Logit: 21.04 Prob:  0.09% Token: | Alice|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Jill'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Jill'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and let's do an example of seeing answer to test\n",
    "transformer_lens.utils.test_prompt(\n",
    "    prompt='Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to',\n",
    "    answer=' Jill',\n",
    "    model=model,\n",
    "    prepend_space_to_answer=True, # default\n",
    "    print_details=True, # default\n",
    "    prepend_bos=None, # default\n",
    "    top_k=10, # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de18606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c8997fb",
   "metadata": {},
   "source": [
    "#### Using the hooked SAE transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5828aa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: we'll assume these SAEs have small error term since they're from GemmaScope\n",
    "\n",
    "# SAEs don't reconstruct activation perfectly, so if you attach an SAE and want the model to stay performant, you need to use the error term.\n",
    "# This is because the SAE will be used to modify the forward pass, and if it doesn't reconstruct the activations well, the outputs may be effected.\n",
    "# Good SAEs have small error terms but it's something to be mindful of.\n",
    "\n",
    "sae.use_error_term # If use error term is set to false, we will modify the forward pass by using the sae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5bc27db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  --------------------------\n",
      "blocks.25.hook_resid_post.hook_sae_input      torch.Size([1, 21, 2304])\n",
      "blocks.25.hook_resid_post.hook_sae_acts_pre   torch.Size([1, 21, 65536])\n",
      "blocks.25.hook_resid_post.hook_sae_acts_post  torch.Size([1, 21, 65536])\n",
      "blocks.25.hook_resid_post.hook_sae_recons     torch.Size([1, 21, 2304])\n",
      "blocks.25.hook_resid_post.hook_sae_output     torch.Size([1, 21, 2304])\n",
      "--------------------------------------------  --------------------------\n"
     ]
    }
   ],
   "source": [
    "# hooked SAE Transformer will enable us to get the feature activations from the SAE\n",
    "example_prompt = 'Jill threw the ball to Jack. Jack threw the ball to Will. Will threw the ball back to'\n",
    "logits, cache = model.run_with_cache_with_saes(\n",
    "    example_prompt,\n",
    "    saes=[sae],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "88e63a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  --------------------------\n",
      "blocks.25.hook_resid_post.hook_sae_input      torch.Size([1, 21, 2304])\n",
      "blocks.25.hook_resid_post.hook_sae_acts_pre   torch.Size([1, 21, 65536])\n",
      "blocks.25.hook_resid_post.hook_sae_acts_post  torch.Size([1, 21, 65536])\n",
      "blocks.25.hook_resid_post.hook_sae_recons     torch.Size([1, 21, 2304])\n",
      "blocks.25.hook_resid_post.hook_sae_output     torch.Size([1, 21, 2304])\n",
      "--------------------------------------------  --------------------------\n",
      "Relevant numbers:\n",
      "- example_prompt_tokens.shape=torch.Size([1, 21])\n",
      "- model.cfg.d_model=2304\n",
      "- sae.cfg.d_sae=65536\n"
     ]
    }
   ],
   "source": [
    "# see what's in the cache related to SAE\n",
    "print(tabulate.tabulate([(k, v.shape) for k,v in cache.items() if \"sae\" in k]))\n",
    "\n",
    "# ex: because this SAE is operating on the residual stream\n",
    "assert sae.cfg.d_in == model.cfg.d_model\n",
    "\n",
    "example_prompt_tokens = model.to_tokens(example_prompt)\n",
    "\n",
    "print(f\"Relevant numbers:\")\n",
    "print(f'- {example_prompt_tokens.shape=}')\n",
    "print(f'- {model.cfg.d_model=}')\n",
    "print(f'- {sae.cfg.d_sae=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01ba4160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------  ---------------------------\n",
      "hook_embed                          torch.Size([1, 21, 2304])\n",
      "blocks.0.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.0.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.0.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.0.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.0.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.0.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.0.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.0.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.0.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.0.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.0.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.0.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.0.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.0.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.0.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.0.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.0.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.0.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.0.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.0.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.0.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.0.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.0.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.0.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.1.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.1.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.1.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.1.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.1.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.1.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.1.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.1.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.1.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.1.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.1.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.1.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.1.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.1.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.1.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.1.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.1.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.1.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.1.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.1.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.1.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.1.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.1.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.1.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.2.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.2.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.2.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.2.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.2.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.2.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.2.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.2.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.2.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.2.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.2.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.2.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.2.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.2.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.2.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.2.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.2.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.2.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.2.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.2.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.2.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.2.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.2.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.2.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.3.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.3.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.3.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.3.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.3.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.3.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.3.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.3.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.3.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.3.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.3.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.3.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.3.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.3.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.3.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.3.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.3.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.3.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.3.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.3.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.3.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.3.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.3.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.3.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.4.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.4.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.4.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.4.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.4.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.4.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.4.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.4.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.4.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.4.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.4.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.4.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.4.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.4.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.4.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.4.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.4.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.4.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.4.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.4.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.4.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.4.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.4.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.4.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.5.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.5.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.5.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.5.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.5.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.5.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.5.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.5.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.5.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.5.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.5.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.5.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.5.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.5.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.5.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.5.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.5.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.5.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.5.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.5.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.5.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.5.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.5.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.5.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.6.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.6.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.6.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.6.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.6.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.6.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.6.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.6.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.6.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.6.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.6.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.6.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.6.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.6.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.6.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.6.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.6.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.6.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.6.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.6.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.6.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.6.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.6.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.6.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.7.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.7.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.7.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.7.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.7.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.7.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.7.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.7.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.7.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.7.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.7.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.7.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.7.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.7.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.7.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.7.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.7.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.7.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.7.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.7.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.7.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.7.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.7.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.7.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.8.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.8.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.8.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.8.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.8.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.8.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.8.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.8.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.8.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.8.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.8.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.8.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.8.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.8.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.8.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.8.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.8.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.8.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.8.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.8.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.8.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.8.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.8.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.8.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.9.hook_resid_pre             torch.Size([1, 21, 2304])\n",
      "blocks.9.ln1.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.9.ln1.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.9.attn.hook_q                torch.Size([1, 21, 8, 256])\n",
      "blocks.9.attn.hook_k                torch.Size([1, 21, 4, 256])\n",
      "blocks.9.attn.hook_v                torch.Size([1, 21, 4, 256])\n",
      "blocks.9.attn.hook_rot_q            torch.Size([1, 21, 8, 256])\n",
      "blocks.9.attn.hook_rot_k            torch.Size([1, 21, 4, 256])\n",
      "blocks.9.attn.hook_attn_scores      torch.Size([1, 8, 21, 21])\n",
      "blocks.9.attn.hook_pattern          torch.Size([1, 8, 21, 21])\n",
      "blocks.9.attn.hook_z                torch.Size([1, 21, 8, 256])\n",
      "blocks.9.ln1_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.9.ln1_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.9.hook_attn_out              torch.Size([1, 21, 2304])\n",
      "blocks.9.hook_resid_mid             torch.Size([1, 21, 2304])\n",
      "blocks.9.ln2.hook_scale             torch.Size([1, 21, 1])\n",
      "blocks.9.ln2.hook_normalized        torch.Size([1, 21, 2304])\n",
      "blocks.9.mlp.hook_pre               torch.Size([1, 21, 9216])\n",
      "blocks.9.mlp.hook_pre_linear        torch.Size([1, 21, 9216])\n",
      "blocks.9.mlp.hook_post              torch.Size([1, 21, 9216])\n",
      "blocks.9.ln2_post.hook_scale        torch.Size([1, 21, 1])\n",
      "blocks.9.ln2_post.hook_normalized   torch.Size([1, 21, 2304])\n",
      "blocks.9.hook_mlp_out               torch.Size([1, 21, 2304])\n",
      "blocks.9.hook_resid_post            torch.Size([1, 21, 2304])\n",
      "blocks.10.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.10.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.10.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.10.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.10.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.10.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.10.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.10.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.10.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.10.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.10.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.10.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.10.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.10.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.10.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.10.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.10.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.10.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.10.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.10.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.10.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.10.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.10.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.10.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.11.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.11.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.11.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.11.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.11.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.11.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.11.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.11.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.11.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.11.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.11.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.11.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.11.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.11.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.11.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.11.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.11.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.11.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.11.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.11.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.11.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.11.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.11.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.11.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.12.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.12.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.12.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.12.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.12.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.12.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.12.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.12.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.12.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.12.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.12.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.12.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.12.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.12.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.12.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.12.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.12.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.12.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.12.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.12.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.12.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.12.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.12.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.12.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.13.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.13.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.13.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.13.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.13.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.13.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.13.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.13.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.13.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.13.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.13.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.13.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.13.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.13.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.13.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.13.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.13.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.13.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.13.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.13.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.13.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.13.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.13.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.13.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.14.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.14.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.14.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.14.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.14.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.14.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.14.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.14.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.14.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.14.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.14.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.14.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.14.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.14.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.14.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.14.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.14.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.14.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.14.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.14.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.14.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.14.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.14.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.14.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.15.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.15.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.15.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.15.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.15.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.15.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.15.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.15.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.15.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.15.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.15.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.15.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.15.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.15.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.15.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.15.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.15.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.15.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.15.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.15.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.15.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.15.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.15.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.15.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.16.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.16.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.16.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.16.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.16.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.16.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.16.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.16.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.16.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.16.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.16.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.16.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.16.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.16.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.16.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.16.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.16.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.16.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.16.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.16.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.16.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.16.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.16.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.16.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.17.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.17.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.17.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.17.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.17.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.17.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.17.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.17.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.17.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.17.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.17.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.17.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.17.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.17.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.17.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.17.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.17.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.17.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.17.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.17.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.17.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.17.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.17.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.17.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.18.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.18.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.18.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.18.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.18.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.18.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.18.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.18.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.18.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.18.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.18.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.18.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.18.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.18.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.18.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.18.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.18.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.18.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.18.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.18.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.18.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.18.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.18.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.18.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.19.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.19.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.19.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.19.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.19.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.19.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.19.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.19.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.19.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.19.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.19.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.19.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.19.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.19.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.19.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.19.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.19.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.19.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.19.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.19.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.19.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.19.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.19.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.19.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.20.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.20.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.20.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.20.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.20.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.20.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.20.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.20.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.20.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.20.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.20.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.20.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.20.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.20.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.20.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.20.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.20.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.20.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.20.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.20.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.20.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.20.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.20.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.20.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.21.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.21.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.21.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.21.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.21.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.21.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.21.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.21.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.21.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.21.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.21.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.21.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.21.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.21.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.21.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.21.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.21.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.21.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.21.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.21.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.21.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.21.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.21.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.21.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.22.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.22.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.22.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.22.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.22.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.22.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.22.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.22.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.22.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.22.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.22.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.22.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.22.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.22.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.22.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.22.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.22.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.22.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.22.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.22.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.22.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.22.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.22.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.22.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.23.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.23.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.23.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.23.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.23.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.23.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.23.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.23.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.23.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.23.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.23.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.23.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.23.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.23.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.23.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.23.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.23.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.23.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.23.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.23.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.23.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.23.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.23.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.23.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.24.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.24.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.24.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.24.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.24.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.24.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.24.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.24.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.24.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.24.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.24.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.24.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.24.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.24.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.24.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.24.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.24.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.24.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.24.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.24.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.24.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.24.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.24.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "blocks.24.hook_resid_post           torch.Size([1, 21, 2304])\n",
      "blocks.25.hook_resid_pre            torch.Size([1, 21, 2304])\n",
      "blocks.25.ln1.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.25.ln1.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.25.attn.hook_q               torch.Size([1, 21, 8, 256])\n",
      "blocks.25.attn.hook_k               torch.Size([1, 21, 4, 256])\n",
      "blocks.25.attn.hook_v               torch.Size([1, 21, 4, 256])\n",
      "blocks.25.attn.hook_rot_q           torch.Size([1, 21, 8, 256])\n",
      "blocks.25.attn.hook_rot_k           torch.Size([1, 21, 4, 256])\n",
      "blocks.25.attn.hook_attn_scores     torch.Size([1, 8, 21, 21])\n",
      "blocks.25.attn.hook_pattern         torch.Size([1, 8, 21, 21])\n",
      "blocks.25.attn.hook_z               torch.Size([1, 21, 8, 256])\n",
      "blocks.25.ln1_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.25.ln1_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.25.hook_attn_out             torch.Size([1, 21, 2304])\n",
      "blocks.25.hook_resid_mid            torch.Size([1, 21, 2304])\n",
      "blocks.25.ln2.hook_scale            torch.Size([1, 21, 1])\n",
      "blocks.25.ln2.hook_normalized       torch.Size([1, 21, 2304])\n",
      "blocks.25.mlp.hook_pre              torch.Size([1, 21, 9216])\n",
      "blocks.25.mlp.hook_pre_linear       torch.Size([1, 21, 9216])\n",
      "blocks.25.mlp.hook_post             torch.Size([1, 21, 9216])\n",
      "blocks.25.ln2_post.hook_scale       torch.Size([1, 21, 1])\n",
      "blocks.25.ln2_post.hook_normalized  torch.Size([1, 21, 2304])\n",
      "blocks.25.hook_mlp_out              torch.Size([1, 21, 2304])\n",
      "ln_final.hook_scale                 torch.Size([1, 21, 1])\n",
      "ln_final.hook_normalized            torch.Size([1, 21, 2304])\n",
      "----------------------------------  ---------------------------\n"
     ]
    }
   ],
   "source": [
    "# show everything not related to SAE (note it's essentially just every operation hooked)\n",
    "print(tabulate.tabulate([(k, v.shape) for k,v in cache.items() if \"sae\" not in k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f000776f",
   "metadata": {},
   "source": [
    "#### What feature explanations do we have for this SAE?\n",
    "\n",
    "* Explanations are generated by GPT-4o-mini looking at activating examples in `ThePile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "29a7d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# ex: https://www.neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/3742\n",
    "#     from url directly\n",
    "\n",
    "# note: not all SAEs in neuronpedia yet, so we get the closest one\n",
    "class NeuronpediaConstants:\n",
    "\n",
    "    MODEL_ID = 'gemma-2-2b'\n",
    "    SAE_ID = '25-gemmascope-res-16k'\n",
    "\n",
    "    EXPORT_URL = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "def get_neuronpedia_dashboard_html_url(\n",
    "    feature_index: int,\n",
    "    model_id: str = NeuronpediaConstants.MODEL_ID,\n",
    "    sae_id: str = NeuronpediaConstants.SAE_ID,\n",
    ") -> str:\n",
    "    \"\"\"Create URL for getting an individual feature's HTML, rendered via IFrame\"\"\"\n",
    "    return (\n",
    "        f\"https://www.neuronpedia.org/{model_id}/{sae_id}/{feature_index}\"\n",
    "        \"?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "    )\n",
    "\n",
    "# API: https://www.neuronpedia.org/api-doc\n",
    "#\n",
    "# note: so neuronpedia is also a store of autointerp explanations\n",
    "def get_neuronpedia_explanations(\n",
    "    model_id: str = NeuronpediaConstants.MODEL_ID,\n",
    "    sae_id: str = NeuronpediaConstants.SAE_ID,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get explanations from neuronpedia for a given model and sae.\"\"\"\n",
    "\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "    payload = {\n",
    "        \"modelId\": model_id,\n",
    "        \"saeId\": sae_id,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # - explanations\n",
    "    # - explanationsCount\n",
    "    response_json = response.json()\n",
    "\n",
    "    num_explanations = response_json['explanationsCount']\n",
    "    print(f\"{num_explanations=}\")\n",
    "\n",
    "    # convert to pandas\n",
    "    explanations_df = pd.DataFrame(response_json[\"explanations\"])\n",
    "\n",
    "    # rename index to \"feature\"\n",
    "    explanations_df = explanations_df.rename(columns={\"index\": \"feature\"})\n",
    "\n",
    "    # explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "    explanations_df[\"description\"] = explanations_df[\"description\"].apply(lambda x: x.lower())\n",
    "\n",
    "    return explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13aceec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_explanations=16601\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "      <th>description</th>\n",
       "      <th>scoreV1</th>\n",
       "      <th>scoreV2</th>\n",
       "      <th>autoInterpModel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>34</td>\n",
       "      <td>references to customer support and service in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>38</td>\n",
       "      <td>phrases related to collaboration and teamwork</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>173</td>\n",
       "      <td>references to conservation and environmental m...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>213</td>\n",
       "      <td>punctuation marks and statistical or structur...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>470</td>\n",
       "      <td>html tags and structural elements in a document</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16596</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>6619</td>\n",
       "      <td>specific chemical or scientific terms related ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16597</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>3734</td>\n",
       "      <td>phrases related to conditional formations and...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16598</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>602</td>\n",
       "      <td>specific terms and phrases related to classifi...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16599</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>9784</td>\n",
       "      <td>the presence of the token \"mk\" repeatedly, ind...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16600</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>25-gemmascope-res-16k</td>\n",
       "      <td>6735</td>\n",
       "      <td>phrases indicating instructions or commands f...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16601 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          modelId                  layer feature  \\\n",
       "0      gemma-2-2b  25-gemmascope-res-16k      34   \n",
       "1      gemma-2-2b  25-gemmascope-res-16k      38   \n",
       "2      gemma-2-2b  25-gemmascope-res-16k     173   \n",
       "3      gemma-2-2b  25-gemmascope-res-16k     213   \n",
       "4      gemma-2-2b  25-gemmascope-res-16k     470   \n",
       "...           ...                    ...     ...   \n",
       "16596  gemma-2-2b  25-gemmascope-res-16k    6619   \n",
       "16597  gemma-2-2b  25-gemmascope-res-16k    3734   \n",
       "16598  gemma-2-2b  25-gemmascope-res-16k     602   \n",
       "16599  gemma-2-2b  25-gemmascope-res-16k    9784   \n",
       "16600  gemma-2-2b  25-gemmascope-res-16k    6735   \n",
       "\n",
       "                                             description  scoreV1 scoreV2  \\\n",
       "0      references to customer support and service in ...        0    None   \n",
       "1          phrases related to collaboration and teamwork        0    None   \n",
       "2      references to conservation and environmental m...        0    None   \n",
       "3       punctuation marks and statistical or structur...        0    None   \n",
       "4        html tags and structural elements in a document        0    None   \n",
       "...                                                  ...      ...     ...   \n",
       "16596  specific chemical or scientific terms related ...        0    None   \n",
       "16597   phrases related to conditional formations and...        0    None   \n",
       "16598  specific terms and phrases related to classifi...        0    None   \n",
       "16599  the presence of the token \"mk\" repeatedly, ind...        0    None   \n",
       "16600   phrases indicating instructions or commands f...        0    None   \n",
       "\n",
       "      autoInterpModel  \n",
       "0         gpt-4o-mini  \n",
       "1         gpt-4o-mini  \n",
       "2         gpt-4o-mini  \n",
       "3         gpt-4o-mini  \n",
       "4         gpt-4o-mini  \n",
       "...               ...  \n",
       "16596     gpt-4o-mini  \n",
       "16597     gpt-4o-mini  \n",
       "16598     gpt-4o-mini  \n",
       "16599     gpt-4o-mini  \n",
       "16600     gpt-4o-mini  \n",
       "\n",
       "[16601 rows x 7 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations_df = get_neuronpedia_explanations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae4bb3e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m explanations_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mis_unique\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO(bschoen): How are features not unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7f454805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelId                                                   gemma-2-2b\n",
       "layer                                          25-gemmascope-res-16k\n",
       "feature                                                           34\n",
       "description        references to customer support and service in ...\n",
       "scoreV1                                                            0\n",
       "scoreV2                                                         None\n",
       "autoInterpModel                                          gpt-4o-mini\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c6fddfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoInterpModel\n",
       "gpt-4o-mini    16601\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay so only gpt-4o-mini for now\n",
    "explanations_df['autoInterpModel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf41819",
   "metadata": {},
   "source": [
    "##### Searching for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2e676cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3186: 'words or phrases related to deception or falsehood',\n",
       " 5298: 'assertions and statements related to deception or dishonesty',\n",
       " 8398: 'variations of the prefix \"mis\" indicating incorrectness or deception',\n",
       " 14919: 'elements related to plots, schemes, or acts of deception and betrayal'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_description = \"decept\"\n",
    "\n",
    "df_target_descriptions = explanations_df.loc[explanations_df.description.str.contains(target_description)]\n",
    "\n",
    "df_target_descriptions['description'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "93fe1089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/14971?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x15bd03e60>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "\n",
    "feature_index = explanations_df['feature'].iloc[14919]\n",
    "\n",
    "html_url = get_neuronpedia_dashboard_html_url(feature_index=feature_index)\n",
    "\n",
    "IPython.display.IFrame(html_url, width=800, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dee17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
