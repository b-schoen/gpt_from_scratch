{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM+AWqf5sAxMR7nUYCFNyuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-schoen/alignment-playground/blob/main/colab/initial_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3ud6qHeQM-O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "vw-H40kXRdpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cNQyY4EORR59"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Iterable, Any\n",
        "import math\n",
        "import dataclasses"
      ],
      "metadata": {
        "id": "x5ARQO9fTcHS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p82mnOCCMIpB",
        "outputId": "0285aa47-891c-428d-f700-f7e89d993bae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jaxtyping\n",
        "import jaxtyping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3pv_6fZURWZ6",
        "outputId": "1ed71b51-85fd-4842-b164-4ea610d61907"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.33)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note: needed for jaxtyping, using jaxtyping idiom\n",
        "import typeguard\n",
        "from typeguard import typechecked as typechecker"
      ],
      "metadata": {
        "id": "PVYClaqTXz1s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn on all warnings, since a lot of torch errors are warnings\n",
        "# import warnings\n",
        "# warnings.simplefilter(\"always\")"
      ],
      "metadata": {
        "id": "UGl7bpIrTjpI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Device"
      ],
      "metadata": {
        "id": "9sqvXYgrR79R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "IZZqwEC4RiU7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device.type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "xWb3e3_mRvVi",
        "outputId": "d4fbcca8-cb8c-4217-9104-3fd64a629af4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Main_Demo.ipynb#scrollTo=VGvdkQwSIi9Q\n",
        "!pip install transformer_lens\n",
        "!pip install circuitsvis"
      ],
      "metadata": {
        "id": "YFvRUkdcRv8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143cfc1a-494e-429a-f959-0f0a586216bc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.32.1)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.20.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.8.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.2.33)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.1.4)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (13.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (2.1.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.42.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (4.12.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens) (0.17.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens) (3.10.1)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (2.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens) (2.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer_lens) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer_lens) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (2.12.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.11)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer_lens) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.1)\n",
            "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.10/dist-packages (1.43.2)\n",
            "Requirement already satisfied: importlib-metadata>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (8.2.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (1.26.4)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (12.1.105)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (2.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from circuitsvis) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->circuitsvis) (12.6.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.1.0->circuitsvis) (3.15.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5.1.0->circuitsvis) (3.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis) (2024.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->circuitsvis) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->circuitsvis) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "\n",
        "pio.renderers.default = \"colab\""
      ],
      "metadata": {
        "id": "xVNiYqT_NTV7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformer_lens\n",
        "import circuitsvis as cv"
      ],
      "metadata": {
        "id": "PYoNMinuNWaW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Type"
      ],
      "metadata": {
        "id": "dSbm1wQ5SMWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(bschoen): Generalize dataset type to just take generator function"
      ],
      "metadata": {
        "id": "8GEq1pYmSPLQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jaxtyping import Float, jaxtyped\n",
        "\n",
        "# type aliases\n",
        "NumSamples = int\n",
        "SeqLength = int\n",
        "\n",
        "# TODO(bschoen): Abstract more than just integer sequence\n",
        "InputTensor = Float[torch.Tensor, \"seq_len *feature_dim\"]\n",
        "OutputTensor = Float[torch.Tensor, \"*output_dim\"]\n",
        "\n",
        "InputGeneratorFn = Callable[[], InputTensor]\n",
        "OutputGeneratorFn = Callable[[InputTensor], OutputTensor]\n",
        "\n",
        "class SequenceDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Generic sequence dataset taking both an input and output generator.\"\"\"\n",
        "\n",
        "    @jaxtyped(typechecker=typechecker)\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_samples: int,\n",
        "        x_generator: InputGeneratorFn,\n",
        "        y_computer: OutputGeneratorFn,\n",
        "    ) -> None:\n",
        "        self.data    = torch.stack([x_generator() for _ in range(num_samples)])\n",
        "        self.targets = torch.stack([y_computer(row) for row in self.data])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "\n",
        "        return len(self.data)\n",
        "\n",
        "    @jaxtyped(typechecker=typechecker)\n",
        "    def __getitem__(self, idx: int) -> tuple[InputTensor, OutputTensor]:\n",
        "\n",
        "        return self.data[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "nIOCr3iSSUHm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define types for our datasets and dataloaders\n",
        "SequenceDataSubset = torch.utils.data.Subset[tuple[Float[torch.Tensor, \"seq_len\"], Float[torch.Tensor, \"\"]]]\n",
        "SequenceDataLoader = torch.utils.data.DataLoader[tuple[Float[torch.Tensor, \"batch seq_len\"], Float[torch.Tensor, \"batch\"]]]\n",
        "\n",
        "# note: uses default values for pytorch random_split\n",
        "def split_dataset(\n",
        "    dataset: SequenceDataset,\n",
        "    train_ratio: float = 0.7,\n",
        "    val_ratio: float = 0.15,\n",
        "    test_ratio: float = 0.15\n",
        ") -> tuple[SequenceDataSubset, SequenceDataSubset, SequenceDataSubset]:\n",
        "\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1\"\n",
        "\n",
        "    print(f'Creating splits for {len(dataset)} samples {train_ratio=}, {val_ratio=}, {test_ratio=}')\n",
        "\n",
        "    total_size: int = len(dataset)\n",
        "\n",
        "    train_size: int = int(train_ratio * total_size)\n",
        "    val_size: int = int(val_ratio * total_size)\n",
        "\n",
        "    # Ensure we use all samples\n",
        "    test_size: int = total_size - train_size - val_size\n",
        "\n",
        "    # For reproducibility\n",
        "    random_generator = torch.Generator().manual_seed(42)\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=random_generator,\n",
        "    )\n",
        "\n",
        "    print(f'Created splits from {total_size} total samples:')\n",
        "    print(f' - train ({len(train_dataset)} samples)')\n",
        "    print(f' - validation ({len(val_dataset)} samples)')\n",
        "    print(f' - test ({len(test_dataset)} samples)')\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "HTdV44c2H2Zb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(\n",
        "    train_dataset: SequenceDataSubset,\n",
        "    val_dataset: SequenceDataSubset,\n",
        "    test_dataset: SequenceDataSubset,\n",
        "    batch_size: int,\n",
        "    num_workers: int,\n",
        ") -> tuple[SequenceDataLoader, SequenceDataLoader, SequenceDataLoader]:\n",
        "\n",
        "    print(f'Creating dataloaders for {batch_size=}, {num_workers=}, pin_memory={torch.cuda.is_available()}')\n",
        "\n",
        "    def make_dataloader(dataset: torch.utils.data.Subset, shuffle: bool) -> SequenceDataLoader:\n",
        "      return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "      )\n",
        "\n",
        "    train_loader = make_dataloader(train_dataset, shuffle=True)\n",
        "    val_loader   = make_dataloader(val_dataset,   shuffle=False)\n",
        "    test_loader  = make_dataloader(test_dataset,  shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "RKWvMlyvJaMY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "8D0kIogobKt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note: Why use an encoder at all?\n",
        "#\n",
        "#       However, it may be less suitable for tasks that require processing the entire input\n",
        "#       before generating any output, which is where encoder-decoder models excel.\n",
        "#\n",
        "#       We'll likely stick to decoder only problems for now, so we don't have to do\n",
        "#       interpretability on the input"
      ],
      "metadata": {
        "id": "FuuHEUEabM1L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding"
      ],
      "metadata": {
        "id": "ItTrduwE-BPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    d_model: int,\n",
        "    max_len: int = 5000,\n",
        "  ) -> None:\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # note: d_model must be even for positional embedding\n",
        "    assert d_model % 2 == 0, f\"{d_model} must be even\"\n",
        "\n",
        "    # Create a long enough `pe` matrix\n",
        "    pe: Float[torch.Tensor, \"max_len d_model\"] = torch.zeros(max_len, d_model)\n",
        "    position: Float[torch.Tensor, \"max_len 1\"] = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term: Float[torch.Tensor, \"d_model//2\"] = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "    # Fill the `pe` matrix\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    # Add new intermediate variables with type annotations\n",
        "    pe_unsqueezed: Float[torch.Tensor, \"1 max_len d_model\"] = pe.unsqueeze(0)\n",
        "    pe_transposed: Float[torch.Tensor, \"max_len 1 d_model\"] = pe_unsqueezed.transpose(0, 1)\n",
        "\n",
        "    self.register_buffer('pe', pe_transposed)\n",
        "\n",
        "  @jaxtyped(typechecker=typechecker)\n",
        "  def forward(\n",
        "      self,\n",
        "      x: Float[torch.Tensor, \"seq_len batch_size d_model\"],\n",
        "  ) -> Float[torch.Tensor, \"seq_len batch_size d_model\"]:\n",
        "\n",
        "    # Create a new intermediate variable for the sliced pe\n",
        "    pe_sliced: Float[torch.Tensor, \"seq_len 1 d_model\"] = self.pe[:x.size(0), :]\n",
        "\n",
        "    # print(f'{x.shape=}, {pe_sliced.shape=}')\n",
        "\n",
        "    return x + pe_sliced"
      ],
      "metadata": {
        "id": "oOuatyxNHIDI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_encoding() -> None:\n",
        "  # test positional encoding\n",
        "  input_dim = 1\n",
        "  seq_len = 10\n",
        "  d_model = 4\n",
        "  batch_size = 2\n",
        "\n",
        "  positional_encoding = PositionalEncoding(d_model=d_model)\n",
        "\n",
        "  # happens before positional encoding\n",
        "  input_projection = torch.nn.Linear(input_dim, d_model)\n",
        "\n",
        "  batch_x = torch.randint(0, 5, (batch_size, seq_len, input_dim)).float()\n",
        "\n",
        "  print(f'Before input projection\\n\\t{batch_x.shape=}\\n\\t{batch_x[0]=}')\n",
        "\n",
        "  batch_x = input_projection.forward(batch_x)\n",
        "\n",
        "  print(f'After input projection:\\n\\t{batch_x.shape=}\\n\\t{batch_x[0]=}')\n",
        "\n",
        "  batch_x = positional_encoding.forward(batch_x)\n",
        "\n",
        "  print(f'After adding positional encoding\\n\\t{batch_x.shape=}\\n\\t{batch_x[0]=}')\n",
        "\n",
        "test_positional_encoding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvAd-dfn_zOk",
        "outputId": "563f45e7-719b-42d0-bd3d-a343c990304c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before input projection\n",
            "\tbatch_x.shape=torch.Size([2, 10, 1])\n",
            "\tbatch_x[0]=tensor([[2.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [2.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "After input projection:\n",
            "\tbatch_x.shape=torch.Size([2, 10, 4])\n",
            "\tbatch_x[0]=tensor([[ 2.2565, -0.9736,  1.9765,  0.6300],\n",
            "        [ 3.1362, -1.2093,  2.7340,  0.5887],\n",
            "        [ 2.2565, -0.9736,  1.9765,  0.6300],\n",
            "        [ 0.4969, -0.5020,  0.4613,  0.7126],\n",
            "        [ 2.2565, -0.9736,  1.9765,  0.6300],\n",
            "        [ 0.4969, -0.5020,  0.4613,  0.7126],\n",
            "        [ 0.4969, -0.5020,  0.4613,  0.7126],\n",
            "        [ 3.1362, -1.2093,  2.7340,  0.5887],\n",
            "        [ 2.2565, -0.9736,  1.9765,  0.6300],\n",
            "        [ 3.1362, -1.2093,  2.7340,  0.5887]], grad_fn=<SelectBackward0>)\n",
            "After adding positional encoding\n",
            "\tbatch_x.shape=torch.Size([2, 10, 4])\n",
            "\tbatch_x[0]=tensor([[ 2.2565,  0.0264,  1.9765,  1.6300],\n",
            "        [ 3.1362, -0.2093,  2.7340,  1.5887],\n",
            "        [ 2.2565,  0.0264,  1.9765,  1.6300],\n",
            "        [ 0.4969,  0.4980,  0.4613,  1.7126],\n",
            "        [ 2.2565,  0.0264,  1.9765,  1.6300],\n",
            "        [ 0.4969,  0.4980,  0.4613,  1.7126],\n",
            "        [ 0.4969,  0.4980,  0.4613,  1.7126],\n",
            "        [ 3.1362, -0.2093,  2.7340,  1.5887],\n",
            "        [ 2.2565,  0.0264,  1.9765,  1.6300],\n",
            "        [ 3.1362, -0.2093,  2.7340,  1.5887]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "kIXV0ULE_U6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz: int) -> Float[torch.Tensor, \"sz sz\"]:\n",
        "  \"\"\"\n",
        "  Generate a square mask for the sequence. The mask ensures that the\n",
        "  prediction for position i can depend only on known outputs at positions less than i.\n",
        "  \"\"\"\n",
        "  # Intuition: This mask implements causal attention, preventing the model from looking at future tokens\n",
        "  mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "  return mask"
      ],
      "metadata": {
        "id": "PeKxJ__K-7eA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: The 4 in 4 * d_model:\n",
        "# This multiplier comes from the original Transformer paper, \"Attention Is All You Need\" by Vaswani et al. It's a hyperparameter that determines the expansion factor for the hidden layer in the feedforward network.\n",
        "#\n",
        "# - Smaller values (e.g., 2) might be used for more efficient, compact models.\n",
        "# - Larger values might be used when more capacity is needed, at the cost of increased computation.\n",
        "# - Some recent models (like GPT-3) use even larger expansion factors.\n",
        "#\n",
        "DEFAULT_MLP_EXPANSION_FACTOR: int = 4\n",
        "\n",
        "class DecoderLayer(torch.nn.Module):\n",
        "  \"\"\"This is essentially a transformer block.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      d_model: int,\n",
        "      num_heads: int,\n",
        "      mlp_expansion_factor: int = DEFAULT_MLP_EXPANSION_FACTOR,\n",
        "    ) -> None:\n",
        "\n",
        "      print(f'Creating decoder layer: {d_model=}, {num_heads=}, {mlp_expansion_factor=}')\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      # Self-attention mechanism\n",
        "      # Intuition: Allow the model to weigh the importance of different parts of the input for each position\n",
        "      self.self_attn = torch.nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "\n",
        "      # Feedforward neural network\n",
        "      # Intuition: Apply non-linear transformations to each position independently\n",
        "      self.feed_forward = torch.nn.Sequential(\n",
        "          torch.nn.Linear(d_model, mlp_expansion_factor * d_model),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(mlp_expansion_factor * d_model, d_model)\n",
        "      )\n",
        "\n",
        "      # Layer normalization\n",
        "      # Intuition: Stabilize the learning process and allow for deeper networks\n",
        "      self.norm1 = torch.nn.LayerNorm(d_model)\n",
        "      self.norm2 = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "  @jaxtyped(typechecker=typechecker)\n",
        "  def forward(\n",
        "      self,\n",
        "      x: Float[torch.Tensor, \"batch seq d_model\"],\n",
        "      mask: Float[torch.Tensor, \"seq seq\"] | None,\n",
        "  ) -> Float[torch.Tensor, \"batch seq d_model\"]:\n",
        "\n",
        "      # Self-attention block\n",
        "      # Intuition: Update each position based on its relationship to all previous positions\n",
        "      attn_output, _ = self.self_attn(x, x, x, attn_mask=mask)\n",
        "\n",
        "      # Residual connection and normalization\n",
        "      x = self.norm1(x + attn_output)\n",
        "\n",
        "      # Feedforward block\n",
        "      # Intuition: Apply position-wise transformations to integrate information\n",
        "      ff_output = self.feed_forward(x)\n",
        "\n",
        "      # Residual connection and normalization\n",
        "      x = self.norm2(x + ff_output)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "Ec56csqSBpyu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: `input_dim` == `context_size`\n",
        "class DecoderOnlyTransformer(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        num_layers: int,\n",
        "      ) -> None:\n",
        "\n",
        "        print(f'Creating decoder only transformer: {input_dim=}, {d_model=}, {num_heads=}, {num_layers=}')\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input projection: maps the input floats to the model dimension\n",
        "        # Intuition: Learn a richer representation of the input numbers\n",
        "        self.input_projection = torch.nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Positional encoding: adds information about token position in the sequence\n",
        "        # Intuition: Since self-attention has no inherent notion of order, this helps the model understand sequence order\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        # Stack of decoder layers: the core of the transformer\n",
        "        # Intuition: Each layer refines the representation, capturing increasingly complex patterns\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "          DecoderLayer(d_model, num_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer: projects to vocabulary size for token prediction\n",
        "        # Intuition: Convert the final representation back to vocabulary space for next-token prediction\n",
        "        # This is the \"unembedding\" / output layer\n",
        "        self.final_layer = torch.nn.Linear(d_model, 1)\n",
        "\n",
        "    @jaxtyped(typechecker=typechecker)\n",
        "    def forward(\n",
        "        self, x: Float[torch.Tensor, \"batch seq input_dim\"],\n",
        "    ) -> Float[torch.Tensor, \"batch\"]:\n",
        "        # x shape: (batch_size, seq_len)\n",
        "\n",
        "        # Project input to model dimension\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        # Generate causal mask\n",
        "        # Intuition: Ensure the model only uses previous elements in the sequence for each prediction\n",
        "        # note: no mask needed yet\n",
        "        # mask = generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
        "\n",
        "        # Apply decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=None)\n",
        "\n",
        "        # Use the last sequence element for prediction\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Project to single float output\n",
        "        return self.final_layer(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "gJ5vT785EC1H"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test transformer\n",
        "def test_transformer() -> None:\n",
        "\n",
        "  input_dim = 1\n",
        "  seq_len = 10\n",
        "  d_model = 6\n",
        "  batch_size = 2\n",
        "  num_heads = 3\n",
        "  num_layers = 5\n",
        "\n",
        "  model = DecoderOnlyTransformer(\n",
        "      input_dim=input_dim,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      num_layers=num_layers,\n",
        "  )\n",
        "\n",
        "  batch_x = torch.randint(0, 5, (batch_size, seq_len, input_dim)).float()\n",
        "\n",
        "  print(f'Before transformer\\n\\t{batch_x.shape=}\\n\\t{batch_x[0]=}')\n",
        "\n",
        "  batch_x = model.forward(batch_x)\n",
        "\n",
        "  print(f'After transformer:\\n\\t{batch_x.shape=}\\n\\t{batch_x[0]=}')\n",
        "\n",
        "test_transformer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSNYqGmBJsy-",
        "outputId": "f74a9617-5f78-445c-d498-43e4c2070c6b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating decoder only transformer: input_dim=1, d_model=6, num_heads=3, num_layers=5\n",
            "Creating decoder layer: d_model=6, num_heads=3, mlp_expansion_factor=4\n",
            "Creating decoder layer: d_model=6, num_heads=3, mlp_expansion_factor=4\n",
            "Creating decoder layer: d_model=6, num_heads=3, mlp_expansion_factor=4\n",
            "Creating decoder layer: d_model=6, num_heads=3, mlp_expansion_factor=4\n",
            "Creating decoder layer: d_model=6, num_heads=3, mlp_expansion_factor=4\n",
            "Before transformer\n",
            "\tbatch_x.shape=torch.Size([2, 10, 1])\n",
            "\tbatch_x[0]=tensor([[4.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [1.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [4.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [4.]])\n",
            "After transformer:\n",
            "\tbatch_x.shape=torch.Size([2])\n",
            "\tbatch_x[0]=tensor(-0.7798, grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjBo32PMJtBG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "Im4Pr2-NLpFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Device"
      ],
      "metadata": {
        "id": "qunQPQeLNoGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device() -> torch.device:\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "U53KNUpPNXDu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Accuracy"
      ],
      "metadata": {
        "id": "gURG8WyLmE9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note: threshold of 0.5 since we're dealing with ints\n",
        "def compute_accuracy(model, dataloader, device, threshold: float = 0.5):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute accuracy within threshold\n",
        "            error = torch.abs(outputs - labels)\n",
        "            correct += torch.sum(error <= threshold).item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "_v4N_Ze_mHIe"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train For One Epoch"
      ],
      "metadata": {
        "id": "2ZjO8wA-NdOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EpochLoss = float\n",
        "\n",
        "def train_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device\n",
        ") -> EpochLoss:\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(batch_X)\n",
        "\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "DXGycUWyMhMv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model Performance"
      ],
      "metadata": {
        "id": "KUS22pMeN4SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(\n",
        "    model: torch.nn.Module,\n",
        "    data_loader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    device: torch.device\n",
        ") -> float:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_X, batch_y in data_loader:\n",
        "\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)"
      ],
      "metadata": {
        "id": "uueV-2ZjMhTO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Training Function"
      ],
      "metadata": {
        "id": "BPcpMj1nOcvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "def train_model(\n",
        "    model: torch.nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        "    num_epochs: int,\n",
        "    learning_rate: float,\n",
        "    epochs_without_improvement_tolerance: int = 5,\n",
        "    eval_every_n_epochs: int = 10,\n",
        ") -> tuple[list[float], list[float], float]:\n",
        "\n",
        "    print(f'Training for {num_epochs=} with {learning_rate=}')\n",
        "\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_epochs // 10, # arbitrarily chosen\n",
        "        num_training_steps=num_epochs,\n",
        "    )\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            device,\n",
        "        )\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # update learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, \")\n",
        "\n",
        "        # only evaluate every `eval_every_n_epochs` epochs\n",
        "        if (epoch % eval_every_n_epochs) != 0:\n",
        "          continue\n",
        "\n",
        "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        val_accuracy = compute_accuracy(model, val_loader, device)\n",
        "\n",
        "        # show learning rate\n",
        "        # TODO(bschoen): Why is this a 1 element list?\n",
        "        last_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Learning Rate: {last_lr:.8f}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, \"\n",
        "              f\"Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "          # TODO(bschoen): save new best model\n",
        "          pass\n",
        "        else:\n",
        "          epochs_without_improvement += 1\n",
        "          if epochs_without_improvement >= epochs_without_improvement_tolerance:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    print('Evaluating final model')\n",
        "    test_loss = evaluate_model(model, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses, test_loss"
      ],
      "metadata": {
        "id": "xr8t0rwtMhZC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna Objective Function"
      ],
      "metadata": {
        "id": "l44I1DqUPKZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(\n",
        "    trial: optuna.Trial,\n",
        "    model_class: type,\n",
        "    input_dim: int,\n",
        "    num_epochs: int,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    test_loader: torch.utils.data.DataLoader\n",
        ") -> float:\n",
        "    # Define hyperparameters to optimize\n",
        "    # done since `d_model` must be even\n",
        "    d_model = trial.suggest_int('d_model', 4, 16, step=2)\n",
        "    num_heads = trial.suggest_int('num_heads', 2, 8)\n",
        "    num_layers = trial.suggest_int('num_layers', 1, 6)\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
        "\n",
        "    print(f'Optimizing across: {input_dim=}, {d_model=}, {num_heads=}, {num_layers=}, {learning_rate=}, {num_epochs=}')\n",
        "\n",
        "    # prune cases with invalid number of heads\n",
        "    if (d_model % num_heads) != 0:\n",
        "      raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    # Create model with suggested hyperparameters\n",
        "    model = model_class(\n",
        "        input_dim=input_dim,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    _, _, test_loss = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        test_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "4VuKKpooPAeP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimize Hyperparameters"
      ],
      "metadata": {
        "id": "ZOg9LQVLQDXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_hyperparameters(\n",
        "    model_class: type,\n",
        "    input_dim: int,\n",
        "    num_epochs: int,\n",
        "    n_trials_per_param: int,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    test_loader: torch.utils.data.DataLoader,\n",
        ") -> tuple[optuna.Study, dict[str, int | float]]:\n",
        "\n",
        "    print(f'Creating a study to optimize hyperparams across {n_trials_per_param}')\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "\n",
        "    study.optimize(\n",
        "        lambda trial: objective(trial, model_class, input_dim, num_epochs, train_loader, val_loader, test_loader),\n",
        "        n_trials=n_trials_per_param,\n",
        "        n_jobs=-1, # note: tells optuna to use all available CPU\n",
        "    )\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"  Value: {trial.value}\")\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "    return study, trial.params"
      ],
      "metadata": {
        "id": "vs14MsZvPAkR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Specific"
      ],
      "metadata": {
        "id": "9QbRq0AEMhmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(bschoen): Generalize this over `TransformerProblem`\n",
        "num_samples = 1000\n",
        "batch_size = 32\n",
        "context_size = 10\n",
        "input_dim = 1\n",
        "\n",
        "# define generator for input\n",
        "def generate_randint_sequence(\n",
        "    min_value: int,\n",
        "    max_value: int,\n",
        "    seq_len: int,\n",
        "    input_dim: int,\n",
        "  ) -> Float[torch.Tensor, \"seq_len input_dim\"]:\n",
        "    \"\"\"Generate arbitrary integer sequence.\"\"\"\n",
        "    return torch.randint(min_value, max_value + 1, (seq_len, input_dim)).float()\n",
        "\n",
        "# define generator for output\n",
        "def compute_min_max_sum(x: InputTensor) -> OutputTensor:\n",
        "  return torch.min(x) + torch.max(x)\n",
        "\n",
        "dataset = SequenceDataset(\n",
        "    num_samples=num_samples,\n",
        "    x_generator=lambda: generate_randint_sequence(0, 100, context_size, input_dim),\n",
        "    y_computer=compute_min_max_sum,\n",
        ")"
      ],
      "metadata": {
        "id": "L7cCIXnYMjH6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Construction"
      ],
      "metadata": {
        "id": "8_o4O3qkM0ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample from the dataset to sanity check\n",
        "print('Showing example dataset member:')\n",
        "x, y = dataset[0]\n",
        "print(f\"x: {x.shape}\\t{x}\")\n",
        "print(f\"y: {y.shape}\\t{y}\")\n",
        "\n",
        "# create splits\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset)\n",
        "\n",
        "# create dataloaders\n",
        "train_dataloader, val_dataloader, test_dataloader = create_dataloaders(\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4, # arbitrarily chosen to be non-zero\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPwLCjPCMj1M",
        "outputId": "c1b12ac2-b818-4ecc-9cdd-1d56b649a99b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing example dataset member:\n",
            "x: torch.Size([10, 1])\ttensor([[65.],\n",
            "        [81.],\n",
            "        [73.],\n",
            "        [80.],\n",
            "        [41.],\n",
            "        [24.],\n",
            "        [19.],\n",
            "        [22.],\n",
            "        [21.],\n",
            "        [19.]])\n",
            "y: torch.Size([])\t100.0\n",
            "Creating splits for 1000 samples train_ratio=0.7, val_ratio=0.15, test_ratio=0.15\n",
            "Created splits from 1000 total samples:\n",
            " - train (700 samples)\n",
            " - validation (150 samples)\n",
            " - test (150 samples)\n",
            "Creating dataloaders for batch_size=32, num_workers=4, pin_memory=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying Run Before Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "kVgz0MMbTIC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trial run with arbitrary parameters to make sure everything works\n",
        "def test_evaluate_model() -> None:\n",
        "  model = DecoderOnlyTransformer(input_dim=1, d_model=20, num_heads=2, num_layers=3)\n",
        "\n",
        "  device = get_device()\n",
        "  model.to(device)\n",
        "\n",
        "  loss = evaluate_model(\n",
        "      model=model,\n",
        "      data_loader=val_dataloader,\n",
        "      criterion=torch.nn.MSELoss(),\n",
        "      device=get_device(),\n",
        "  )\n",
        "\n",
        "  print(f'{loss=:.4f}')\n",
        "\n",
        "test_evaluate_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKGfPpIEQQ3X",
        "outputId": "048f0e32-9656-496b-8797-645b76dd5919"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating decoder only transformer: input_dim=1, d_model=20, num_heads=2, num_layers=3\n",
            "Creating decoder layer: d_model=20, num_heads=2, mlp_expansion_factor=4\n",
            "Creating decoder layer: d_model=20, num_heads=2, mlp_expansion_factor=4\n",
            "Creating decoder layer: d_model=20, num_heads=2, mlp_expansion_factor=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss=10073.4689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine Best Hyperparameters"
      ],
      "metadata": {
        "id": "IQJrvV3rRNzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna.visualization\n",
        "\n",
        "is_hyperparameter_study_needed = False\n",
        "\n",
        "if is_hyperparameter_study_needed:\n",
        "\n",
        "  num_epochs_for_optimizing_hyperparameter_trials = 100\n",
        "  n_trials_per_param = 100\n",
        "\n",
        "  # Assuming DecoderOnlyTransformer is defined elsewhere\n",
        "  study, best_params = optimize_hyperparameters(\n",
        "      model_class=DecoderOnlyTransformer,\n",
        "      input_dim=input_dim,\n",
        "      num_epochs=num_epochs_for_optimizing_hyperparameter_trials,\n",
        "      n_trials_per_param=n_trials_per_param,\n",
        "      train_loader=train_dataloader,\n",
        "      val_loader=val_dataloader,\n",
        "      test_loader=test_dataloader,\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  optuna.visualization.plot_optimization_history(study)\n",
        "  optuna.visualization.plot_contour(study)\n",
        "  optuna.visualization.plot_param_importances(study)\n",
        "  optuna.visualization.plot_parallel_coordinate(study)\n",
        "  optuna.visualization.plot_timeline(study)\n",
        "  optuna.visualization.plot_slice(study)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JwwZ_09vQQ6Q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Optimization Study"
      ],
      "metadata": {
        "id": "kq4ALMcQWtif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Best Model"
      ],
      "metadata": {
        "id": "z_lrK9DwRR1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(bschoen): Save and load study from local db\n",
        "\"\"\"\n",
        "Best trial:\n",
        "  Value: 0.10772023797035217\n",
        "  Params:\n",
        "    d_model: 12\n",
        "    nhead: 6\n",
        "    num_layers: 1\n",
        "    learning_rate: 0.005134454801759864\n",
        "\"\"\"\n",
        "best_params = {\n",
        "    'd_model': 4,\n",
        "    'num_heads': 2,\n",
        "    'num_layers': 2,\n",
        "    'learning_rate': 0.005,\n",
        "}"
      ],
      "metadata": {
        "id": "1G9CzLlJeKYi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "513CSWN_hk3r"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(bschoen): Profile a single run here, because studies are taking forever\n",
        "\n",
        "# note: can manually scale up parameters from here\n",
        "\n",
        "# TODO(bschoen): Give models names\n",
        "\n",
        "# TODO(bschoen): Ablate the MLP, in case that's what's learning it\n",
        "# TODO(bschoen): Does the residual stream representation still hold with\n",
        "#                MLP? It has to for the next layer to access it\n",
        "\n",
        "# note: more epochs than we use in our study\n",
        "num_epochs = 1000\n",
        "\n",
        "# Create the best model using the optimized hyperparameters\n",
        "best_model = DecoderOnlyTransformer(\n",
        "    input_dim=input_dim,\n",
        "    d_model=best_params['d_model'],\n",
        "    num_heads=best_params['num_heads'],\n",
        "    num_layers=best_params['num_layers'],\n",
        ")\n",
        "\n",
        "# Train the best model\n",
        "train_losses, val_losses, final_test_loss = train_model(\n",
        "    best_model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    test_dataloader,\n",
        "    num_epochs=num_epochs,\n",
        "    learning_rate=best_params['learning_rate'],\n",
        ")\n",
        "\n",
        "print(f\"Final test loss with best model: {final_test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItoDQynfRRWJ",
        "outputId": "3282469f-c432-4080-e1fd-c4191c4867f5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating decoder only transformer: input_dim=1, d_model=4, num_heads=2, num_layers=2\n",
            "Creating decoder layer: d_model=4, num_heads=2, mlp_expansion_factor=4\n",
            "Creating decoder layer: d_model=4, num_heads=2, mlp_expansion_factor=4\n",
            "Training for num_epochs=1000 with learning_rate=0.005\n",
            "Epoch [1/1000], Learning Rate: 0.00005000, Train Loss: 10193.9582, Val Loss: 9997.8986, Val Accuracy: 0.0000\n",
            "Epoch [11/1000], Learning Rate: 0.00055000, Train Loss: 10020.2452, Val Loss: 9816.6037, Val Accuracy: 0.0000\n",
            "Epoch [21/1000], Learning Rate: 0.00105000, Train Loss: 9690.4900, Val Loss: 9469.0748, Val Accuracy: 0.0000\n",
            "Epoch [31/1000], Learning Rate: 0.00155000, Train Loss: 8679.9061, Val Loss: 8418.3759, Val Accuracy: 0.0000\n",
            "Epoch [41/1000], Learning Rate: 0.00205000, Train Loss: 6293.2039, Val Loss: 5988.1242, Val Accuracy: 0.0000\n",
            "Epoch [51/1000], Learning Rate: 0.00255000, Train Loss: 3053.8848, Val Loss: 2800.7419, Val Accuracy: 0.0000\n",
            "Epoch [61/1000], Learning Rate: 0.00305000, Train Loss: 797.8245, Val Loss: 696.3068, Val Accuracy: 0.0000\n",
            "Epoch [71/1000], Learning Rate: 0.00355000, Train Loss: 196.4222, Val Loss: 191.0405, Val Accuracy: 0.0400\n",
            "Epoch [81/1000], Learning Rate: 0.00405000, Train Loss: 131.6356, Val Loss: 128.3694, Val Accuracy: 0.0467\n",
            "Epoch [91/1000], Learning Rate: 0.00455000, Train Loss: 99.4019, Val Loss: 95.9423, Val Accuracy: 0.0467\n",
            "Epoch [101/1000], Learning Rate: 0.00499444, Train Loss: 77.0569, Val Loss: 75.4699, Val Accuracy: 0.0800\n",
            "Epoch [111/1000], Learning Rate: 0.00493889, Train Loss: 16.6314, Val Loss: 19.3891, Val Accuracy: 0.1600\n",
            "Epoch [121/1000], Learning Rate: 0.00488333, Train Loss: 8.6129, Val Loss: 11.7697, Val Accuracy: 0.3467\n",
            "Epoch [131/1000], Learning Rate: 0.00482778, Train Loss: 6.0128, Val Loss: 9.7535, Val Accuracy: 0.3000\n",
            "Epoch [141/1000], Learning Rate: 0.00477222, Train Loss: 4.3943, Val Loss: 7.8620, Val Accuracy: 0.3400\n",
            "Epoch [151/1000], Learning Rate: 0.00471667, Train Loss: 3.7689, Val Loss: 6.9646, Val Accuracy: 0.3867\n",
            "Epoch [161/1000], Learning Rate: 0.00466111, Train Loss: 3.4460, Val Loss: 6.0998, Val Accuracy: 0.3733\n",
            "Epoch [171/1000], Learning Rate: 0.00460556, Train Loss: 2.5039, Val Loss: 5.5999, Val Accuracy: 0.3200\n",
            "Epoch [181/1000], Learning Rate: 0.00455000, Train Loss: 2.3239, Val Loss: 4.6678, Val Accuracy: 0.4267\n",
            "Epoch [191/1000], Learning Rate: 0.00449444, Train Loss: 2.2668, Val Loss: 4.4038, Val Accuracy: 0.3933\n",
            "Epoch [201/1000], Learning Rate: 0.00443889, Train Loss: 1.6815, Val Loss: 3.9649, Val Accuracy: 0.3933\n",
            "Epoch [211/1000], Learning Rate: 0.00438333, Train Loss: 1.6064, Val Loss: 3.6490, Val Accuracy: 0.4267\n",
            "Epoch [221/1000], Learning Rate: 0.00432778, Train Loss: 1.4149, Val Loss: 3.3618, Val Accuracy: 0.4200\n",
            "Epoch [231/1000], Learning Rate: 0.00427222, Train Loss: 1.2499, Val Loss: 3.1263, Val Accuracy: 0.3933\n",
            "Epoch [241/1000], Learning Rate: 0.00421667, Train Loss: 1.4986, Val Loss: 2.9539, Val Accuracy: 0.4267\n",
            "Epoch [251/1000], Learning Rate: 0.00416111, Train Loss: 1.3796, Val Loss: 2.2644, Val Accuracy: 0.5600\n",
            "Epoch [261/1000], Learning Rate: 0.00410556, Train Loss: 0.8113, Val Loss: 2.4449, Val Accuracy: 0.3733\n",
            "Epoch [271/1000], Learning Rate: 0.00405000, Train Loss: 0.8132, Val Loss: 1.9300, Val Accuracy: 0.6467\n",
            "Epoch [281/1000], Learning Rate: 0.00399444, Train Loss: 0.4953, Val Loss: 1.7374, Val Accuracy: 0.6333\n",
            "Epoch [291/1000], Learning Rate: 0.00393889, Train Loss: 0.5932, Val Loss: 2.2527, Val Accuracy: 0.2067\n",
            "Epoch [301/1000], Learning Rate: 0.00388333, Train Loss: 0.5773, Val Loss: 1.5327, Val Accuracy: 0.6933\n",
            "Epoch [311/1000], Learning Rate: 0.00382778, Train Loss: 0.3191, Val Loss: 1.2737, Val Accuracy: 0.7467\n",
            "Epoch [321/1000], Learning Rate: 0.00377222, Train Loss: 0.4324, Val Loss: 1.6623, Val Accuracy: 0.3733\n",
            "Epoch [331/1000], Learning Rate: 0.00371667, Train Loss: 0.4154, Val Loss: 1.4267, Val Accuracy: 0.4133\n",
            "Epoch [341/1000], Learning Rate: 0.00366111, Train Loss: 0.5108, Val Loss: 1.1728, Val Accuracy: 0.6533\n",
            "Epoch [351/1000], Learning Rate: 0.00360556, Train Loss: 0.2782, Val Loss: 0.9566, Val Accuracy: 0.7867\n",
            "Epoch [361/1000], Learning Rate: 0.00355000, Train Loss: 0.2556, Val Loss: 0.9615, Val Accuracy: 0.6533\n",
            "Epoch [371/1000], Learning Rate: 0.00349444, Train Loss: 0.2144, Val Loss: 0.7620, Val Accuracy: 0.8667\n",
            "Epoch [381/1000], Learning Rate: 0.00343889, Train Loss: 0.2394, Val Loss: 0.7841, Val Accuracy: 0.7800\n",
            "Epoch [391/1000], Learning Rate: 0.00338333, Train Loss: 0.1527, Val Loss: 0.6424, Val Accuracy: 0.8867\n",
            "Epoch [401/1000], Learning Rate: 0.00332778, Train Loss: 0.1756, Val Loss: 0.6645, Val Accuracy: 0.8067\n",
            "Epoch [411/1000], Learning Rate: 0.00327222, Train Loss: 0.2337, Val Loss: 0.5193, Val Accuracy: 0.9467\n",
            "Epoch [421/1000], Learning Rate: 0.00321667, Train Loss: 0.1710, Val Loss: 0.4915, Val Accuracy: 0.9133\n",
            "Epoch [431/1000], Learning Rate: 0.00316111, Train Loss: 0.1936, Val Loss: 0.5979, Val Accuracy: 0.6667\n",
            "Epoch [441/1000], Learning Rate: 0.00310556, Train Loss: 0.1379, Val Loss: 0.4206, Val Accuracy: 0.9067\n",
            "Epoch [451/1000], Learning Rate: 0.00305000, Train Loss: 0.4620, Val Loss: 0.6542, Val Accuracy: 0.4667\n",
            "Epoch [461/1000], Learning Rate: 0.00299444, Train Loss: 0.1458, Val Loss: 0.3514, Val Accuracy: 0.9400\n",
            "Epoch [471/1000], Learning Rate: 0.00293889, Train Loss: 0.1881, Val Loss: 0.7865, Val Accuracy: 0.1667\n",
            "Epoch [481/1000], Learning Rate: 0.00288333, Train Loss: 0.2238, Val Loss: 0.8327, Val Accuracy: 0.1933\n",
            "Epoch [491/1000], Learning Rate: 0.00282778, Train Loss: 0.0979, Val Loss: 0.3700, Val Accuracy: 0.8000\n",
            "Epoch [501/1000], Learning Rate: 0.00277222, Train Loss: 0.5843, Val Loss: 0.4691, Val Accuracy: 0.6333\n",
            "Epoch [511/1000], Learning Rate: 0.00271667, Train Loss: 0.1436, Val Loss: 0.3867, Val Accuracy: 0.6933\n",
            "Epoch [521/1000], Learning Rate: 0.00266111, Train Loss: 0.2542, Val Loss: 0.3731, Val Accuracy: 0.7667\n",
            "Epoch [531/1000], Learning Rate: 0.00260556, Train Loss: 0.0797, Val Loss: 0.2053, Val Accuracy: 0.9600\n",
            "Epoch [541/1000], Learning Rate: 0.00255000, Train Loss: 0.0600, Val Loss: 0.1978, Val Accuracy: 0.9733\n",
            "Epoch [551/1000], Learning Rate: 0.00249444, Train Loss: 0.1617, Val Loss: 0.2392, Val Accuracy: 0.9067\n",
            "Epoch [561/1000], Learning Rate: 0.00243889, Train Loss: 0.2684, Val Loss: 0.4122, Val Accuracy: 0.5733\n",
            "Epoch [571/1000], Learning Rate: 0.00238333, Train Loss: 0.0757, Val Loss: 0.1733, Val Accuracy: 0.9800\n",
            "Epoch [581/1000], Learning Rate: 0.00232778, Train Loss: 0.0693, Val Loss: 0.1799, Val Accuracy: 0.9667\n",
            "Epoch [591/1000], Learning Rate: 0.00227222, Train Loss: 0.0827, Val Loss: 0.1565, Val Accuracy: 0.9867\n",
            "Epoch [601/1000], Learning Rate: 0.00221667, Train Loss: 0.1212, Val Loss: 0.2138, Val Accuracy: 0.8533\n",
            "Epoch [611/1000], Learning Rate: 0.00216111, Train Loss: 0.2076, Val Loss: 0.2058, Val Accuracy: 0.9067\n",
            "Epoch [621/1000], Learning Rate: 0.00210556, Train Loss: 0.0582, Val Loss: 0.2138, Val Accuracy: 0.8933\n",
            "Epoch [631/1000], Learning Rate: 0.00205000, Train Loss: 0.0653, Val Loss: 0.1273, Val Accuracy: 0.9800\n",
            "Epoch [641/1000], Learning Rate: 0.00199444, Train Loss: 0.0438, Val Loss: 0.1310, Val Accuracy: 0.9800\n",
            "Epoch [651/1000], Learning Rate: 0.00193889, Train Loss: 0.0816, Val Loss: 0.1607, Val Accuracy: 0.9467\n",
            "Epoch [661/1000], Learning Rate: 0.00188333, Train Loss: 0.0755, Val Loss: 0.2715, Val Accuracy: 0.7800\n",
            "Epoch [671/1000], Learning Rate: 0.00182778, Train Loss: 0.0828, Val Loss: 0.1649, Val Accuracy: 0.9333\n",
            "Epoch [681/1000], Learning Rate: 0.00177222, Train Loss: 0.0690, Val Loss: 0.1553, Val Accuracy: 0.9667\n",
            "Epoch [691/1000], Learning Rate: 0.00171667, Train Loss: 0.0364, Val Loss: 0.1062, Val Accuracy: 0.9800\n",
            "Epoch [701/1000], Learning Rate: 0.00166111, Train Loss: 0.0578, Val Loss: 0.1343, Val Accuracy: 0.9600\n",
            "Epoch [711/1000], Learning Rate: 0.00160556, Train Loss: 0.0769, Val Loss: 0.1203, Val Accuracy: 0.9733\n",
            "Epoch [721/1000], Learning Rate: 0.00155000, Train Loss: 0.0468, Val Loss: 0.1129, Val Accuracy: 0.9800\n",
            "Epoch [731/1000], Learning Rate: 0.00149444, Train Loss: 0.0820, Val Loss: 0.1100, Val Accuracy: 0.9867\n",
            "Epoch [741/1000], Learning Rate: 0.00143889, Train Loss: 0.0416, Val Loss: 0.1177, Val Accuracy: 0.9733\n",
            "Epoch [751/1000], Learning Rate: 0.00138333, Train Loss: 0.0736, Val Loss: 0.1153, Val Accuracy: 0.9667\n",
            "Epoch [761/1000], Learning Rate: 0.00132778, Train Loss: 0.0456, Val Loss: 0.1549, Val Accuracy: 0.9400\n",
            "Epoch [771/1000], Learning Rate: 0.00127222, Train Loss: 0.0378, Val Loss: 0.1035, Val Accuracy: 0.9733\n",
            "Epoch [781/1000], Learning Rate: 0.00121667, Train Loss: 0.0666, Val Loss: 0.1188, Val Accuracy: 0.9800\n",
            "Epoch [791/1000], Learning Rate: 0.00116111, Train Loss: 0.0637, Val Loss: 0.0934, Val Accuracy: 0.9933\n",
            "Epoch [801/1000], Learning Rate: 0.00110556, Train Loss: 0.0685, Val Loss: 0.1274, Val Accuracy: 0.9333\n",
            "Epoch [811/1000], Learning Rate: 0.00105000, Train Loss: 0.0445, Val Loss: 0.1086, Val Accuracy: 0.9400\n",
            "Epoch [821/1000], Learning Rate: 0.00099444, Train Loss: 0.0549, Val Loss: 0.0938, Val Accuracy: 0.9867\n",
            "Epoch [831/1000], Learning Rate: 0.00093889, Train Loss: 0.0434, Val Loss: 0.1524, Val Accuracy: 0.9133\n",
            "Epoch [841/1000], Learning Rate: 0.00088333, Train Loss: 0.0242, Val Loss: 0.0941, Val Accuracy: 0.9733\n",
            "Epoch [851/1000], Learning Rate: 0.00082778, Train Loss: 0.0289, Val Loss: 0.0803, Val Accuracy: 0.9800\n",
            "Epoch [861/1000], Learning Rate: 0.00077222, Train Loss: 0.0520, Val Loss: 0.0878, Val Accuracy: 0.9867\n",
            "Epoch [871/1000], Learning Rate: 0.00071667, Train Loss: 0.0282, Val Loss: 0.0812, Val Accuracy: 0.9933\n",
            "Epoch [881/1000], Learning Rate: 0.00066111, Train Loss: 0.0226, Val Loss: 0.0802, Val Accuracy: 0.9867\n",
            "Epoch [891/1000], Learning Rate: 0.00060556, Train Loss: 0.0239, Val Loss: 0.0768, Val Accuracy: 0.9867\n",
            "Epoch [901/1000], Learning Rate: 0.00055000, Train Loss: 0.0375, Val Loss: 0.0820, Val Accuracy: 0.9867\n",
            "Epoch [911/1000], Learning Rate: 0.00049444, Train Loss: 0.0307, Val Loss: 0.0855, Val Accuracy: 0.9867\n",
            "Epoch [921/1000], Learning Rate: 0.00043889, Train Loss: 0.0228, Val Loss: 0.0751, Val Accuracy: 0.9867\n",
            "Epoch [931/1000], Learning Rate: 0.00038333, Train Loss: 0.0245, Val Loss: 0.0758, Val Accuracy: 0.9933\n",
            "Epoch [941/1000], Learning Rate: 0.00032778, Train Loss: 0.0211, Val Loss: 0.0769, Val Accuracy: 0.9933\n",
            "Epoch [951/1000], Learning Rate: 0.00027222, Train Loss: 0.0222, Val Loss: 0.0820, Val Accuracy: 0.9800\n",
            "Epoch [961/1000], Learning Rate: 0.00021667, Train Loss: 0.0224, Val Loss: 0.0777, Val Accuracy: 0.9867\n",
            "Epoch [971/1000], Learning Rate: 0.00016111, Train Loss: 0.0216, Val Loss: 0.0731, Val Accuracy: 0.9867\n",
            "Epoch [981/1000], Learning Rate: 0.00010556, Train Loss: 0.0196, Val Loss: 0.0747, Val Accuracy: 0.9933\n",
            "Epoch [991/1000], Learning Rate: 0.00005000, Train Loss: 0.0202, Val Loss: 0.0750, Val Accuracy: 0.9867\n",
            "Evaluating final model\n",
            "Test Loss: 0.0300\n",
            "Final test loss with best model: 0.0300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cpQ7VCEGdVF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_device()\n",
        "\n",
        "for name, dataloader in {\n",
        "    'train': train_dataloader,\n",
        "    'val': val_dataloader,\n",
        "    'test': test_dataloader,\n",
        "}.items():\n",
        "\n",
        "  accuracy = compute_accuracy(best_model, dataloader, device)\n",
        "\n",
        "  print(f'{name} accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "YweYSxdhgFXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97509cb-27a1-4d7c-81c0-5014f726f86c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy: 0.9971\n",
            "val accuracy: 0.9933\n",
            "test accuracy: 0.9933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample some outputs to sanity check\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_X, batch_y in test_dataloader:\n",
        "\n",
        "    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "    predicted_y = best_model(batch_X)\n",
        "\n",
        "    for i in range(5):\n",
        "      print('---')\n",
        "      print(f'x_i:\\t{[x.item() for x in batch_X[i]]}')\n",
        "      print(f'y_pred:\\t{predicted_y[i]}')\n",
        "      print(f'y_i:\\t{batch_y[i]}')\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "-GpeewdXk_yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02428137-60ff-4a9b-88e8-4ac8fc433b47"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "x_i:\t[89.0, 43.0, 96.0, 10.0, 21.0, 55.0, 5.0, 3.0, 34.0, 66.0]\n",
            "y_pred:\t99.1990737915039\n",
            "y_i:\t99.0\n",
            "---\n",
            "x_i:\t[93.0, 93.0, 67.0, 72.0, 91.0, 16.0, 65.0, 18.0, 89.0, 10.0]\n",
            "y_pred:\t103.17760467529297\n",
            "y_i:\t103.0\n",
            "---\n",
            "x_i:\t[76.0, 71.0, 37.0, 20.0, 21.0, 83.0, 88.0, 44.0, 53.0, 93.0]\n",
            "y_pred:\t113.03359985351562\n",
            "y_i:\t113.0\n",
            "---\n",
            "x_i:\t[53.0, 94.0, 27.0, 48.0, 59.0, 51.0, 80.0, 98.0, 68.0, 97.0]\n",
            "y_pred:\t125.07417297363281\n",
            "y_i:\t125.0\n",
            "---\n",
            "x_i:\t[5.0, 95.0, 74.0, 92.0, 78.0, 57.0, 48.0, 96.0, 91.0, 16.0]\n",
            "y_pred:\t100.9544906616211\n",
            "y_i:\t101.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pc5jEPJ9lk1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Helper Functions"
      ],
      "metadata": {
        "id": "aI2Kq1R1QCIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "\n",
        "    px.imshow(\n",
        "        transformer_lens.utils.to_numpy(tensor),\n",
        "        color_continuous_midpoint=0.0,\n",
        "        color_continuous_scale=\"RdBu\",\n",
        "        labels={\"x\":xaxis, \"y\":yaxis},\n",
        "        **kwargs,\n",
        "    ).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "\n",
        "    px.line(\n",
        "        transformer_lens.utils.to_numpy(tensor),\n",
        "        labels={\"x\":xaxis, \"y\":yaxis},\n",
        "        **kwargs,\n",
        "    ).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "\n",
        "    x = transformer_lens.utils.to_numpy(x)\n",
        "    y = transformer_lens.utils.to_numpy(y)\n",
        "\n",
        "    px.scatter(\n",
        "        y=y,\n",
        "        x=x,\n",
        "        labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis},\n",
        "        **kwargs,\n",
        "    ).show(renderer)"
      ],
      "metadata": {
        "id": "GJYprSfcQEoi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}