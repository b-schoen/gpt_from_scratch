{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNDuW9ZQDnbMa9HSCZlTtYI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-schoen/gpt_from_scratch/blob/main/colab/gpt2_tinystories_custom_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOq7bHwLIPc7",
        "outputId": "c5908f16-09ad-434f-9ab4-dbc882de9b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt_from_scratch'...\n",
            "remote: Enumerating objects: 324, done.\u001b[K\n",
            "remote: Counting objects: 100% (324/324), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 324 (delta 174), reused 215 (delta 72), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (324/324), 4.85 MiB | 19.24 MiB/s, done.\n",
            "Resolving deltas: 100% (174/174), done.\n"
          ]
        }
      ],
      "source": [
        "# clone repo\n",
        "!rm -rf gpt_from_scratch\n",
        "!git clone https://github.com/b-schoen/gpt_from_scratch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into the repo directory\n",
        "import os\n",
        "\n",
        "os.chdir('gpt_from_scratch')\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2heAkYZIp0g",
        "outputId": "c553ca22-8cbb-442e-daf1-c71791cc3df0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content/gpt_from_scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can operate as if this was a local notebook"
      ],
      "metadata": {
        "id": "tUYSCNYTIyWy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "DfSXTCO_I15z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset locally"
      ],
      "metadata": {
        "id": "AGmpQSn2gx-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load tinystories for comparison\n",
        "from gpt_from_scratch.dataset_loaders import tinystories_loader\n",
        "\n",
        "tinystories_version = tinystories_loader.TinyStoriesVersion.V2\n",
        "\n",
        "tinystories_filepaths = tinystories_loader.download_tinystories(tinystories_version)\n",
        "\n",
        "# read train as input text\n",
        "input_text = tinystories_filepaths.train.read_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28TU5lPCJx16",
        "outputId": "bd9c6bfa-5ef5-4340-cc87-f2ac652ed22b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TinyStoriesV2-GPT4-train.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded TinyStoriesV2-GPT4-train.txt to /root/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-train.txt\n",
            "Downloading TinyStoriesV2-GPT4-valid.txt...\n",
            "Downloaded TinyStoriesV2-GPT4-valid.txt to /root/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-valid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note: if this gets annoying can do an actual pip install requirements\n",
        "!pip install tiktoken\n",
        "!pip install jaxtyping\n",
        "!pip install colored"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_83OHQPJ52z",
        "outputId": "dd5e6242-ae7a-4b44-8e3c-2c89ad0e605d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.33)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n",
            "Requirement already satisfied: colored in /usr/local/lib/python3.10/dist-packages (2.2.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from gpt_from_scratch.gpt2_from_scratch import data_loader\n",
        "from gpt_from_scratch.gpt2_from_scratch.train_gpt2 import (\n",
        "    GPT,\n",
        "    GPTConfig,\n",
        "    get_best_available_torch_device,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "bQ9dtV0EJ0Ha"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "C6v7v2kHgIv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample some outputs to get an idea of where we are\n",
        "\n",
        "from typing import TYPE_CHECKING\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "  from gpt_from_scratch import tokenizer_utils\n",
        "\n",
        "def sample_model(\n",
        "    prompt: str,\n",
        "    num_samples: int,\n",
        "    max_tokens: int,\n",
        "    model: nn.Module,\n",
        "    tokenizer: 'tokenizer_utils.Tokenizer',\n",
        "    device: torch.device,\n",
        ") -> None:\n",
        "\n",
        "    # tokenize\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "    tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (5, 8)\n",
        "\n",
        "    # tokens in this case is just the prompt, and is small enough to fit on GPU\n",
        "    x = tokens.to(device)\n",
        "\n",
        "    while x.size(1) < max_tokens:\n",
        "\n",
        "        # forward the model to get the logits\n",
        "        with torch.no_grad():\n",
        "\n",
        "            logits, loss = model(x) # (B, T, vocab_size)\n",
        "\n",
        "            # take the logits at the last position\n",
        "            # throw away all the logits from things other than the last position\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "\n",
        "            # get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # do top-k sampling of 50 (huggingface pipeline default)\n",
        "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "            #\n",
        "            # \"anything lower than the 50th, we clamp to 0 and never sample it\"\n",
        "            #\n",
        "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "            # select a token from the top-k probabilities\n",
        "            # note: multinomial does not demand the input to sum to 1\n",
        "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "\n",
        "            # gather the corresponding indices\n",
        "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "\n",
        "            # append to the sequence\n",
        "            x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "    # print the generated text\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        tokens = x[i, :max_tokens].tolist()\n",
        "\n",
        "        decoded = tokenizer.decode(tokens)\n",
        "\n",
        "        print(f\"\\n [{i}] >\", decoded)"
      ],
      "metadata": {
        "id": "vQnXakW0gH2I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "n0sUP9PagFcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "from jaxtyping import Float32\n",
        "\n",
        "class DataLoaderLiteBasedOnPytorch(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        B: int,\n",
        "        T: int,\n",
        "        tokens: Float32[torch.Tensor, \"num_samples\"],\n",
        "    ) -> None:\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.tokens = tokens\n",
        "        self.num_samples = len(tokens) - T\n",
        "\n",
        "        print(f\"loaded {len(self.tokens)} tokens\")\n",
        "        print(\n",
        "            f\"1 epoch = {self.num_samples // B} \"\n",
        "            \"batches (steps to make one pass through data)\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.tokens[idx:idx+self.T]\n",
        "        y = self.tokens[idx+1:idx+self.T+1]\n",
        "        return x, y\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        x = torch.stack([item[0] for item in batch])\n",
        "        y = torch.stack([item[1] for item in batch])\n",
        "        return x, y\n",
        "\n",
        "    def get_dataloader(self, num_workers=4, pin_memory=True, shuffle=True):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self,\n",
        "            batch_size=self.B,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "            collate_fn=self.collate_fn,\n",
        "            prefetch_factor=2\n",
        "        )"
      ],
      "metadata": {
        "id": "wNioyUtpbWKs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n--- First 1000 characters: ---\\n')\n",
        "print(input_text[:1000])\n",
        "\n",
        "# print('\\n--- Last 1000 characters: ---\\n')\n",
        "# print(input_text[:-1000])"
      ],
      "metadata": {
        "id": "GfNoQtFAh-FU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9174c807-069d-4290-bc75-a351b1b1ea8e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- First 1000 characters: ---\n",
            "\n",
            "\n",
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
            "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
            "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
            "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
            "And that's how Ben found an amazing vase in the store!\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
            "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his fri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_from_scratch import python_utils\n",
        "\n",
        "# 2,717,700 stories\n",
        "num_samples = len(input_text.split('<|endoftext|>'))\n",
        "\n",
        "print(f'{num_samples=}')\n",
        "\n",
        "# arbitrarily choosing 1/10 as scale factor\n",
        "num_samples = num_samples // 10\n",
        "\n",
        "print(f'{num_samples=} after scaling')\n",
        "\n",
        "num_samples = python_utils.closest_power_of_two(num_samples)\n",
        "\n",
        "print(f'{num_samples=} after choosing closest power of 2')"
      ],
      "metadata": {
        "id": "Bj5jzFIFko-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086b904b-2ae8-45d2-bf0c-97ea55c8f9f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_samples=2717700\n",
            "num_samples=271770 after scaling\n",
            "num_samples=262144 after choosing closest power of 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clip the input text at number of samples\n",
        "input_text = python_utils.get_first_n_examples(input_text, n=num_samples, delimiter='<|endoftext|>')"
      ],
      "metadata": {
        "id": "cEbqmb1OluSE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll trim down the dataset to something that loads quickly"
      ],
      "metadata": {
        "id": "aV-7fZA_jm6t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenizer\n",
        "from gpt_from_scratch import (\n",
        "    byte_pair_encoding_tokenizer,\n",
        "    file_utils,\n",
        "    tokenizer_utils,\n",
        ")\n",
        "\n",
        "# load pretrained tokenizer\n",
        "tokenizer_filepath = 'tokenizer_bin/tokenizer__vocab_2048_samples_100000_dataset_tinystories.pkl'\n",
        "tokenizer = file_utils.deserialize_dataclass_from_pickle_file(\n",
        "    cls=byte_pair_encoding_tokenizer.BytePairEncodingWordTokenizer,\n",
        "    file_path=tokenizer_filepath,\n",
        ")\n",
        "print(f\"Loaded tokenizer from {tokenizer_filepath}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL1jIemKRXiI",
        "outputId": "d47b9f31-330e-4b3e-ac8f-474e0781e42b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded tokenizer from tokenizer_bin/tokenizer__vocab_2048_samples_100000_dataset_tinystories.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure we can tokenize some example text\n",
        "tokenizer_utils.show_token_mapping(tokenizer, \"Jack and Jill were doing mechinterp research\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3oz5hTrRcg2",
        "outputId": "8940d0a2-15f8-478d-dbcb-b14169616f78"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting text into words via regex...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding words as tokens: 100%|██████████| 13/13 [00:00<00:00, 67902.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\t\tJack and Jill were doing mechinterp research\n",
            "Splitting text into words via regex...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding words as tokens: 100%|██████████| 13/13 [00:00<00:00, 76153.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized:\tJack and Jill were doing mechinterp research\n",
            "Token ID | Token Bytes | Token String\n",
            "---------+-------------+--------------\n",
            "     957 | 4A 61 63 6B | 'Jack'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+004A LATIN CAPITAL LETTER J (1 bytes: 4A)\n",
            "          U+0061 LATIN SMALL LETTER A (1 bytes: 61)\n",
            "          U+0063 LATIN SMALL LETTER C (1 bytes: 63)\n",
            "          U+006B LATIN SMALL LETTER K (1 bytes: 6B)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "     263 | 61 6E 64    | 'and'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0061 LATIN SMALL LETTER A (1 bytes: 61)\n",
            "          U+006E LATIN SMALL LETTER N (1 bytes: 6E)\n",
            "          U+0064 LATIN SMALL LETTER D (1 bytes: 64)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "    1890 | 4A 69 6C 6C | 'Jill'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+004A LATIN CAPITAL LETTER J (1 bytes: 4A)\n",
            "          U+0069 LATIN SMALL LETTER I (1 bytes: 69)\n",
            "          U+006C LATIN SMALL LETTER L (1 bytes: 6C)\n",
            "          U+006C LATIN SMALL LETTER L (1 bytes: 6C)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "     377 | 77 65 72 65 | 'were'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0077 LATIN SMALL LETTER W (1 bytes: 77)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "          U+0072 LATIN SMALL LETTER R (1 bytes: 72)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "    1279 | 64 6F 69 6E 67 | 'doing'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0064 LATIN SMALL LETTER D (1 bytes: 64)\n",
            "          U+006F LATIN SMALL LETTER O (1 bytes: 6F)\n",
            "          U+0069 LATIN SMALL LETTER I (1 bytes: 69)\n",
            "          U+006E LATIN SMALL LETTER N (1 bytes: 6E)\n",
            "          U+0067 LATIN SMALL LETTER G (1 bytes: 67)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "     421 | 6D 65       | 'me'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+006D LATIN SMALL LETTER M (1 bytes: 6D)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "     350 | 63 68       | 'ch'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0063 LATIN SMALL LETTER C (1 bytes: 63)\n",
            "          U+0068 LATIN SMALL LETTER H (1 bytes: 68)\n",
            "     264 | 69 6E       | 'in'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0069 LATIN SMALL LETTER I (1 bytes: 69)\n",
            "          U+006E LATIN SMALL LETTER N (1 bytes: 6E)\n",
            "     402 | 74 65 72    | 'ter'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0074 LATIN SMALL LETTER T (1 bytes: 74)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "          U+0072 LATIN SMALL LETTER R (1 bytes: 72)\n",
            "     112 | 70          | 'p'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0070 LATIN SMALL LETTER P (1 bytes: 70)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "     265 | 72 65       | 're'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0072 LATIN SMALL LETTER R (1 bytes: 72)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "    1708 | 73 65 61 72 | 'sear'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0073 LATIN SMALL LETTER S (1 bytes: 73)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "          U+0061 LATIN SMALL LETTER A (1 bytes: 61)\n",
            "          U+0072 LATIN SMALL LETTER R (1 bytes: 72)\n",
            "     350 | 63 68       | 'ch'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0063 LATIN SMALL LETTER C (1 bytes: 63)\n",
            "          U+0068 LATIN SMALL LETTER H (1 bytes: 68)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize input text\n",
        "# note: tiktoken is using their implementation of lib.rs in rust, so much faster\n",
        "print(f'Tokenizing input text of length: {len(input_text)}')\n",
        "tokens = tokenizer.encode(input_text)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "print('Finished tokenizing input text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TdxcriXRaFE",
        "outputId": "436301d7-0e64-44bf-f448-911d502bfb75"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing input text of length: 214634470\n",
            "Splitting text into words via regex...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding words as tokens: 100%|██████████| 93299457/93299457 [06:11<00:00, 250814.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished tokenizing input text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gx2Wgn2VGMO",
        "outputId": "7422c439-be44-4bb0-a29a-34d6e7b020c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.merges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAaNWM_qWePA",
        "outputId": "bf757b4f-bcc1-4676-dc3d-2518a5563a20"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move to GPU, since we can fit it for this dataset\n",
        "device = get_best_available_torch_device()\n",
        "\n",
        "tokens = tokens.to(device)"
      ],
      "metadata": {
        "id": "B9OnivLhd9DW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(bschoen): Shrink other parameters from `config` to match TinyStories paper\n",
        "\n",
        "# vocab_size = 50304 # note: nice number after ~52,000 initially used by GPT-2\n",
        "\n",
        "# TODO(bschoen): It's really 2048 + 1 special tokens lmao\n",
        "vocab_size = 4096\n",
        "\n",
        "# load text via dataloader\n",
        "# TODO(bschoen): Why do we pick this?\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "\n",
        "# let's pick number of tokens closest power of two (above so we get all tokens)\n",
        "# total_batch_size = python_utils.next_power_of_two(len(tokens))\n",
        "\n",
        "B = 64 # micro batch size\n",
        "T = 1024 # sequence length (from GPT-2)\n",
        "# T = 512 # sequence length (matches tinystories paper)\n",
        "\n",
        "assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
        "\n",
        "# compute what our gradient accumulation should be\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "\n",
        "print(f'total length of input text (in characters): {len(input_text)}')\n",
        "print(f'total number of tokens: {len(tokens)}')\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "# create a train loader that will continually give us new batches\n",
        "train_loader = data_loader.DataLoaderLite(B=B, T=T, tokens=tokens)\n",
        "# train_loader = DataLoaderLiteBasedOnPytorch(B=B, T=T, tokens=tokens)\n",
        "# pytorch_train_data_loader = train_loader.get_dataloader()\n",
        "\n",
        "# note: these are computed based on data loading\n",
        "\n",
        "# want to make it through all of our tokens\n",
        "\n",
        "# this seems way too low @ 100, thus the override\n",
        "max_steps = 1000\n",
        "# max_steps = len(tokens) // total_batch_size\n",
        "\n",
        "# chosen fairly arbitrarily\n",
        "# TODO(bschoen): GPT-2 seems to do this as a faction of tokens (proportional)\n",
        "warmup_steps = int(max_steps * 0.1)\n",
        "\n",
        "# learning rate\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "\n",
        "print(f'| {max_steps=} | {warmup_steps=} | {max_lr=:.6f} | {min_lr=:.6f} |')"
      ],
      "metadata": {
        "id": "Piwp_wFzgEPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ea0c0b-310a-4c59-9e9b-c7fa6e8314a8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total length of input text (in characters): 214634470\n",
            "total number of tokens: 98282078\n",
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 8\n",
            "loaded 98282078 tokens\n",
            "1 epoch = 1499 batches (steps to make one pass through data)\n",
            "| max_steps=1000 | warmup_steps=100 | max_lr=0.000600 | min_lr=0.000060 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial layer dominates pretty much everything\n",
        "#\n",
        "# Decrease your batch size until things fit\n",
        "# By default you want to max it out with nice numbers\n",
        "#\n",
        "# ... + switching over to tinystories\n",
        "#\n",
        "#   | step   49 | loss: 4.6334 | lr 6.0832e-05 | norm: 0.3634 | dt: 3104.65ms | tok/sec: 168872.06 |\n",
        "#\n",
        "#   * interestingly the same tokens per second\n",
        "#\n",
        "# ... + (B=16) (since was running out of GPU space)\n",
        "#\n",
        "#   | step   49 | loss: 4.2973 | lr 3.0000e-04 | norm: 1.3571 | dt: 3232.42ms | tok/sec: 162196.82 |\n",
        "#   ...\n",
        "#   | step  999 | loss: 1.1815 | lr 6.0002e-05 | norm: 0.3806 | dt: 3234.65ms | tok/sec: 162084.83 |\n",
        "#\n",
        "# ... + custom tokenizer\n",
        "#\n",
        "#   | step   49 | loss: 3.2605 | lr 6.0658e-05 | norm: 0.1714 | dt: 2323.44ms | tok/sec: 225651.47 |\n",
        "#\n",
        "# ... + custom data loader\n",
        "#\n",
        "#   | step   49 | loss: 3.2779 | lr 6.0658e-05 | norm: 0.1305 | dt: 4052.50ms | tok/sec: 129373.97 |\n",
        "#\n",
        "#   * literally slower, reverting in favor of just putting everything on the GPU since can fit it for this dataset\n",
        "#\n",
        "# ... + moving everything in input dataset to GPU first\n",
        "#\n",
        "#   | step   49 | loss: 3.1756 | lr 3.0000e-04 | norm: 2.3773 | dt: 2296.51ms | tok/sec: 228297.49 |\n",
        "#\n",
        "# ... + increasing microbatch size to 64\n",
        "#\n",
        "#   | step   49 | loss: 3.1903 | lr 3.0000e-04 | norm: 1.8769 | dt: 2132.62ms | tok/sec: 245841.70 |\n",
        "#   ...\n",
        "#   | step  999 | loss: 0.8000 | lr 6.0002e-05 | norm: 0.2571 | dt: 2136.10ms | tok/sec: 245441.52 |"
      ],
      "metadata": {
        "id": "OAWujXuwLwwZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_learning_rate(\n",
        "    step: int,\n",
        "    warmup_steps: int,\n",
        "    max_steps: int,\n",
        "    min_lr: float,\n",
        "    max_lr: float,\n",
        "  ) -> float:\n",
        "\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if step < warmup_steps:\n",
        "        # the +1 is because for the 1st iteration no reason to multiply by 0\n",
        "        return max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if step > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "\n",
        "    # coeff starts at 1 and goes to 0\n",
        "    # TODO(bschoen): Is this cos weight decay?\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "OyEwX6_hXYsF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# reset unused CUDA memory\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# {use F32 multiplication}\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# now we'll try multiple batches\n",
        "device = get_best_available_torch_device()\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "print(\"Creating model...\")\n",
        "config = GPTConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=T,\n",
        ")\n",
        "\n",
        "model = GPT(config)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9JiXn7tXLNy",
        "outputId": "958fbd8e-0d80-4fc8-aa9f-8d9641cbef76"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Creating model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(4096, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=4096, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Compiling model...\")\n",
        "model = torch.compile(model)\n",
        "print(\"Done compiling model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfDa8_SFXnvZ",
        "outputId": "a7ded508-ff65-4dc1-a300-066b91a8265b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling model...\n",
            "Done compiling model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Karpathy: \"AdamW is basically a bugfix of Adam\"\n",
        "#\n",
        "# note: pretty good default learning rate for early experimentation\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=max_lr,\n",
        "    device=device.type,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNhEUZuXYT3I",
        "outputId": "a7a9b504-7a94-4ca4-dcd3-18af98287cd2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 88,866,816 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(max_steps):\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # gradient accumulation\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "\n",
        "        # print(f' - {micro_step=}')\n",
        "        x, y = train_loader.next_batch()\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # automatic mixed precision\n",
        "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "\n",
        "          logits, loss = model(x, y)\n",
        "\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        #\n",
        "        # \"accumulation in the gradients is equivalent to the sum in the loss\"\n",
        "        #\n",
        "        # used small self contained version of just this chunk to debug\n",
        "        # since the loss objects etc can be used in isolation\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_learning_rate(\n",
        "        step=i,\n",
        "        warmup_steps=warmup_steps,\n",
        "        max_steps=max_steps,\n",
        "        min_lr=min_lr,\n",
        "        max_lr=max_lr,\n",
        "    )\n",
        "\n",
        "    # update optimizer\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "    print(f\"| step {i:4d} | loss: {loss_accum:.4f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f} |\")"
      ],
      "metadata": {
        "id": "9Tn19WOAJ39u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03312828-f37f-49a4-f3b4-3bc088b4a7f5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| step    0 | loss: 8.7066 | lr 6.0000e-06 | norm: 96.2999 | dt: 31338.22ms | tok/sec: 16729.98 |\n",
            "| step    1 | loss: 6.8595 | lr 1.2000e-05 | norm: 70.2158 | dt: 2099.20ms | tok/sec: 249756.66 |\n",
            "| step    2 | loss: 5.5509 | lr 1.8000e-05 | norm: 14.6152 | dt: 2100.64ms | tok/sec: 249585.05 |\n",
            "| step    3 | loss: 5.2141 | lr 2.4000e-05 | norm: 14.9169 | dt: 2104.17ms | tok/sec: 249166.51 |\n",
            "| step    4 | loss: 5.4854 | lr 3.0000e-05 | norm: 41.5063 | dt: 2102.36ms | tok/sec: 249380.50 |\n",
            "| step    5 | loss: 4.9793 | lr 3.6000e-05 | norm: 21.1398 | dt: 2105.05ms | tok/sec: 249061.67 |\n",
            "| step    6 | loss: 4.9115 | lr 4.2000e-05 | norm: 19.5750 | dt: 2109.05ms | tok/sec: 248589.73 |\n",
            "| step    7 | loss: 4.7201 | lr 4.8000e-05 | norm: 11.9020 | dt: 2110.67ms | tok/sec: 248399.06 |\n",
            "| step    8 | loss: 4.6659 | lr 5.4000e-05 | norm: 14.2885 | dt: 2109.68ms | tok/sec: 248515.84 |\n",
            "| step    9 | loss: 4.5131 | lr 6.0000e-05 | norm: 11.8298 | dt: 2110.94ms | tok/sec: 248367.45 |\n",
            "| step   10 | loss: 4.3607 | lr 6.6000e-05 | norm: 5.5784 | dt: 2116.26ms | tok/sec: 247742.99 |\n",
            "| step   11 | loss: 4.2401 | lr 7.2000e-05 | norm: 3.1138 | dt: 2116.42ms | tok/sec: 247724.23 |\n",
            "| step   12 | loss: 4.4395 | lr 7.8000e-05 | norm: 15.4337 | dt: 2116.26ms | tok/sec: 247743.18 |\n",
            "| step   13 | loss: 4.3026 | lr 8.4000e-05 | norm: 10.2171 | dt: 2116.51ms | tok/sec: 247713.49 |\n",
            "| step   14 | loss: 4.1048 | lr 9.0000e-05 | norm: 2.5609 | dt: 2118.69ms | tok/sec: 247458.07 |\n",
            "| step   15 | loss: 4.1137 | lr 9.6000e-05 | norm: 3.4852 | dt: 2119.29ms | tok/sec: 247388.03 |\n",
            "| step   16 | loss: 4.0849 | lr 1.0200e-04 | norm: 3.5721 | dt: 2120.13ms | tok/sec: 247290.18 |\n",
            "| step   17 | loss: 3.9929 | lr 1.0800e-04 | norm: 1.7273 | dt: 2118.41ms | tok/sec: 247490.93 |\n",
            "| step   18 | loss: 3.9858 | lr 1.1400e-04 | norm: 4.7422 | dt: 2121.12ms | tok/sec: 247175.61 |\n",
            "| step   19 | loss: 3.9837 | lr 1.2000e-04 | norm: 6.1911 | dt: 2121.09ms | tok/sec: 247178.22 |\n",
            "| step   20 | loss: 3.8972 | lr 1.2600e-04 | norm: 3.7274 | dt: 2121.96ms | tok/sec: 247077.68 |\n",
            "| step   21 | loss: 3.8135 | lr 1.3200e-04 | norm: 1.2250 | dt: 2123.07ms | tok/sec: 246947.52 |\n",
            "| step   22 | loss: 3.7854 | lr 1.3800e-04 | norm: 2.0261 | dt: 2125.98ms | tok/sec: 246609.82 |\n",
            "| step   23 | loss: 3.7305 | lr 1.4400e-04 | norm: 1.2252 | dt: 2126.30ms | tok/sec: 246573.46 |\n",
            "| step   24 | loss: 3.6835 | lr 1.5000e-04 | norm: 1.4428 | dt: 2126.29ms | tok/sec: 246574.37 |\n",
            "| step   25 | loss: 3.6460 | lr 1.5600e-04 | norm: 1.4253 | dt: 2127.12ms | tok/sec: 246478.36 |\n",
            "| step   26 | loss: 3.5901 | lr 1.6200e-04 | norm: 1.0919 | dt: 2128.59ms | tok/sec: 246307.47 |\n",
            "| step   27 | loss: 3.5462 | lr 1.6800e-04 | norm: 1.3099 | dt: 2128.17ms | tok/sec: 246355.87 |\n",
            "| step   28 | loss: 3.5098 | lr 1.7400e-04 | norm: 1.2945 | dt: 2128.43ms | tok/sec: 246326.40 |\n",
            "| step   29 | loss: 3.4896 | lr 1.8000e-04 | norm: 5.5122 | dt: 2128.66ms | tok/sec: 246299.72 |\n",
            "| step   30 | loss: 3.4441 | lr 1.8600e-04 | norm: 1.4767 | dt: 2128.50ms | tok/sec: 246317.62 |\n",
            "| step   31 | loss: 3.3948 | lr 1.9200e-04 | norm: 1.0777 | dt: 2128.74ms | tok/sec: 246290.40 |\n",
            "| step   32 | loss: 3.5438 | lr 1.9800e-04 | norm: 10.8385 | dt: 2129.10ms | tok/sec: 246248.31 |\n",
            "| step   33 | loss: 3.4825 | lr 2.0400e-04 | norm: 7.2756 | dt: 2129.80ms | tok/sec: 246167.43 |\n",
            "| step   34 | loss: 3.3897 | lr 2.1000e-04 | norm: 4.1447 | dt: 2130.84ms | tok/sec: 246047.95 |\n",
            "| step   35 | loss: 3.3494 | lr 2.1600e-04 | norm: 2.3864 | dt: 2129.67ms | tok/sec: 246183.03 |\n",
            "| step   36 | loss: 3.3349 | lr 2.2200e-04 | norm: 1.6354 | dt: 2129.48ms | tok/sec: 246204.89 |\n",
            "| step   37 | loss: 3.3077 | lr 2.2800e-04 | norm: 0.8035 | dt: 2130.19ms | tok/sec: 246122.99 |\n",
            "| step   38 | loss: 3.3245 | lr 2.3400e-04 | norm: 2.4987 | dt: 2130.80ms | tok/sec: 246051.66 |\n",
            "| step   39 | loss: 3.2961 | lr 2.4000e-04 | norm: 1.3491 | dt: 2131.44ms | tok/sec: 245977.76 |\n",
            "| step   40 | loss: 3.2588 | lr 2.4600e-04 | norm: 1.0861 | dt: 2130.74ms | tok/sec: 246059.56 |\n",
            "| step   41 | loss: 3.2679 | lr 2.5200e-04 | norm: 1.6663 | dt: 2131.59ms | tok/sec: 245961.48 |\n",
            "| step   42 | loss: 3.2612 | lr 2.5800e-04 | norm: 1.1781 | dt: 2132.02ms | tok/sec: 245911.61 |\n",
            "| step   43 | loss: 3.2425 | lr 2.6400e-04 | norm: 0.8234 | dt: 2132.01ms | tok/sec: 245912.10 |\n",
            "| step   44 | loss: 3.2277 | lr 2.7000e-04 | norm: 0.6307 | dt: 2131.11ms | tok/sec: 246016.54 |\n",
            "| step   45 | loss: 3.2228 | lr 2.7600e-04 | norm: 0.8240 | dt: 2132.16ms | tok/sec: 245895.47 |\n",
            "| step   46 | loss: 3.2174 | lr 2.8200e-04 | norm: 1.4121 | dt: 2132.92ms | tok/sec: 245807.21 |\n",
            "| step   47 | loss: 3.2157 | lr 2.8800e-04 | norm: 2.2075 | dt: 2132.91ms | tok/sec: 245809.35 |\n",
            "| step   48 | loss: 3.2070 | lr 2.9400e-04 | norm: 0.5292 | dt: 2133.08ms | tok/sec: 245788.64 |\n",
            "| step   49 | loss: 3.1903 | lr 3.0000e-04 | norm: 1.8769 | dt: 2132.62ms | tok/sec: 245841.70 |\n",
            "| step   50 | loss: 3.1817 | lr 3.0600e-04 | norm: 0.6105 | dt: 2131.31ms | tok/sec: 245993.26 |\n",
            "| step   51 | loss: 3.1731 | lr 3.1200e-04 | norm: 0.9886 | dt: 2133.38ms | tok/sec: 245754.58 |\n",
            "| step   52 | loss: 3.1954 | lr 3.1800e-04 | norm: 1.9920 | dt: 2133.34ms | tok/sec: 245759.14 |\n",
            "| step   53 | loss: 3.1583 | lr 3.2400e-04 | norm: 0.5686 | dt: 2134.56ms | tok/sec: 245618.32 |\n",
            "| step   54 | loss: 3.1916 | lr 3.3000e-04 | norm: 2.5649 | dt: 2133.31ms | tok/sec: 245763.15 |\n",
            "| step   55 | loss: 3.1646 | lr 3.3600e-04 | norm: 0.9811 | dt: 2131.95ms | tok/sec: 245919.39 |\n",
            "| step   56 | loss: 3.1704 | lr 3.4200e-04 | norm: 2.1428 | dt: 2133.62ms | tok/sec: 245727.55 |\n",
            "| step   57 | loss: 3.1619 | lr 3.4800e-04 | norm: 0.7671 | dt: 2132.07ms | tok/sec: 245905.78 |\n",
            "| step   58 | loss: 3.2048 | lr 3.5400e-04 | norm: 2.5912 | dt: 2133.59ms | tok/sec: 245730.96 |\n",
            "| step   59 | loss: 3.1386 | lr 3.6000e-04 | norm: 0.8746 | dt: 2132.99ms | tok/sec: 245799.41 |\n",
            "| step   60 | loss: 3.1946 | lr 3.6600e-04 | norm: 2.4624 | dt: 2132.78ms | tok/sec: 245823.59 |\n",
            "| step   61 | loss: 3.2067 | lr 3.7200e-04 | norm: 1.9949 | dt: 2134.01ms | tok/sec: 245682.50 |\n",
            "| step   62 | loss: 3.1627 | lr 3.7800e-04 | norm: 1.5231 | dt: 2133.34ms | tok/sec: 245759.30 |\n",
            "| step   63 | loss: 3.1450 | lr 3.8400e-04 | norm: 2.0903 | dt: 2134.03ms | tok/sec: 245679.40 |\n",
            "| step   64 | loss: 3.1329 | lr 3.9000e-04 | norm: 1.0058 | dt: 2132.93ms | tok/sec: 245805.97 |\n",
            "| step   65 | loss: 3.1621 | lr 3.9600e-04 | norm: 1.9993 | dt: 2133.62ms | tok/sec: 245726.98 |\n",
            "| step   66 | loss: 3.1509 | lr 4.0200e-04 | norm: 1.3742 | dt: 2134.76ms | tok/sec: 245595.41 |\n",
            "| step   67 | loss: 3.1379 | lr 4.0800e-04 | norm: 1.4615 | dt: 2133.15ms | tok/sec: 245781.03 |\n",
            "| step   68 | loss: 3.1307 | lr 4.1400e-04 | norm: 0.9381 | dt: 2134.25ms | tok/sec: 245654.73 |\n",
            "| step   69 | loss: 3.1253 | lr 4.2000e-04 | norm: 0.9759 | dt: 2135.43ms | tok/sec: 245518.17 |\n",
            "| step   70 | loss: 3.1268 | lr 4.2600e-04 | norm: 1.0210 | dt: 2133.23ms | tok/sec: 245772.32 |\n",
            "| step   71 | loss: 3.1072 | lr 4.3200e-04 | norm: 0.5257 | dt: 2133.85ms | tok/sec: 245700.04 |\n",
            "| step   72 | loss: 3.1148 | lr 4.3800e-04 | norm: 0.7075 | dt: 2135.42ms | tok/sec: 245520.33 |\n",
            "| step   73 | loss: 3.1104 | lr 4.4400e-04 | norm: 0.6332 | dt: 2134.46ms | tok/sec: 245629.95 |\n",
            "| step   74 | loss: 3.0980 | lr 4.5000e-04 | norm: 0.4909 | dt: 2135.16ms | tok/sec: 245549.29 |\n",
            "| step   75 | loss: 3.0968 | lr 4.5600e-04 | norm: 0.6480 | dt: 2134.49ms | tok/sec: 245626.33 |\n",
            "| step   76 | loss: 3.0887 | lr 4.6200e-04 | norm: 0.4706 | dt: 2134.54ms | tok/sec: 245620.79 |\n",
            "| step   77 | loss: 3.0975 | lr 4.6800e-04 | norm: 0.5452 | dt: 2134.11ms | tok/sec: 245670.89 |\n",
            "| step   78 | loss: 3.0889 | lr 4.7400e-04 | norm: 0.3324 | dt: 2135.18ms | tok/sec: 245546.93 |\n",
            "| step   79 | loss: 3.0943 | lr 4.8000e-04 | norm: 0.3459 | dt: 2134.91ms | tok/sec: 245578.24 |\n",
            "| step   80 | loss: 3.0857 | lr 4.8600e-04 | norm: 0.3558 | dt: 2135.04ms | tok/sec: 245563.30 |\n",
            "| step   81 | loss: 3.0868 | lr 4.9200e-04 | norm: 0.3361 | dt: 2133.92ms | tok/sec: 245692.91 |\n",
            "| step   82 | loss: 3.0779 | lr 4.9800e-04 | norm: 0.4537 | dt: 2135.46ms | tok/sec: 245514.80 |\n",
            "| step   83 | loss: 3.0739 | lr 5.0400e-04 | norm: 0.7087 | dt: 2134.65ms | tok/sec: 245608.06 |\n",
            "| step   84 | loss: 3.0915 | lr 5.1000e-04 | norm: 1.5035 | dt: 2133.79ms | tok/sec: 245706.85 |\n",
            "| step   85 | loss: 3.0852 | lr 5.1600e-04 | norm: 0.8852 | dt: 2134.62ms | tok/sec: 245611.73 |\n",
            "| step   86 | loss: 3.0795 | lr 5.2200e-04 | norm: 0.3929 | dt: 2134.38ms | tok/sec: 245639.77 |\n",
            "| step   87 | loss: 3.0617 | lr 5.2800e-04 | norm: 0.3204 | dt: 2134.22ms | tok/sec: 245658.27 |\n",
            "| step   88 | loss: 3.0639 | lr 5.3400e-04 | norm: 0.6520 | dt: 2135.07ms | tok/sec: 245559.98 |\n",
            "| step   89 | loss: 3.0654 | lr 5.4000e-04 | norm: 1.1873 | dt: 2133.35ms | tok/sec: 245758.06 |\n",
            "| step   90 | loss: 3.0817 | lr 5.4600e-04 | norm: 1.0461 | dt: 2134.98ms | tok/sec: 245570.92 |\n",
            "| step   91 | loss: 3.0585 | lr 5.5200e-04 | norm: 0.6668 | dt: 2135.23ms | tok/sec: 245541.94 |\n",
            "| step   92 | loss: 3.0593 | lr 5.5800e-04 | norm: 0.4378 | dt: 2134.44ms | tok/sec: 245632.61 |\n",
            "| step   93 | loss: 3.0574 | lr 5.6400e-04 | norm: 0.3893 | dt: 2135.00ms | tok/sec: 245567.93 |\n",
            "| step   94 | loss: 3.0647 | lr 5.7000e-04 | norm: 0.7292 | dt: 2135.64ms | tok/sec: 245494.46 |\n",
            "| step   95 | loss: 3.0559 | lr 5.7600e-04 | norm: 1.0512 | dt: 2135.72ms | tok/sec: 245485.33 |\n",
            "| step   96 | loss: 3.0754 | lr 5.8200e-04 | norm: 0.9017 | dt: 2134.84ms | tok/sec: 245586.22 |\n",
            "| step   97 | loss: 3.0643 | lr 5.8800e-04 | norm: 0.7404 | dt: 2136.05ms | tok/sec: 245447.22 |\n",
            "| step   98 | loss: 3.0701 | lr 5.9400e-04 | norm: 0.5267 | dt: 2136.21ms | tok/sec: 245429.03 |\n",
            "| step   99 | loss: 3.0682 | lr 6.0000e-04 | norm: 0.7275 | dt: 2134.92ms | tok/sec: 245577.23 |\n",
            "| step  100 | loss: 3.0745 | lr 6.0000e-04 | norm: 0.8898 | dt: 2136.46ms | tok/sec: 245400.30 |\n",
            "| step  101 | loss: 3.0684 | lr 6.0000e-04 | norm: 1.2869 | dt: 2135.25ms | tok/sec: 245538.84 |\n",
            "| step  102 | loss: 3.0603 | lr 5.9999e-04 | norm: 0.6270 | dt: 2134.23ms | tok/sec: 245656.62 |\n",
            "| step  103 | loss: 3.0466 | lr 5.9999e-04 | norm: 0.4651 | dt: 2134.82ms | tok/sec: 245588.47 |\n",
            "| step  104 | loss: 3.0515 | lr 5.9997e-04 | norm: 0.9400 | dt: 2137.10ms | tok/sec: 245327.37 |\n",
            "| step  105 | loss: 3.0443 | lr 5.9996e-04 | norm: 1.1464 | dt: 2135.31ms | tok/sec: 245532.81 |\n",
            "| step  106 | loss: 3.0325 | lr 5.9994e-04 | norm: 0.4609 | dt: 2134.52ms | tok/sec: 245623.56 |\n",
            "| step  107 | loss: 3.0414 | lr 5.9992e-04 | norm: 0.6874 | dt: 2133.73ms | tok/sec: 245714.29 |\n",
            "| step  108 | loss: 3.0371 | lr 5.9989e-04 | norm: 0.6156 | dt: 2137.60ms | tok/sec: 245269.88 |\n",
            "| step  109 | loss: 3.0397 | lr 5.9987e-04 | norm: 0.5319 | dt: 2136.03ms | tok/sec: 245449.60 |\n",
            "| step  110 | loss: 3.0288 | lr 5.9984e-04 | norm: 0.4873 | dt: 2134.22ms | tok/sec: 245658.35 |\n",
            "| step  111 | loss: 3.0338 | lr 5.9980e-04 | norm: 0.6255 | dt: 2135.36ms | tok/sec: 245526.91 |\n",
            "| step  112 | loss: 3.0250 | lr 5.9976e-04 | norm: 0.5820 | dt: 2135.72ms | tok/sec: 245485.01 |\n",
            "| step  113 | loss: 3.0223 | lr 5.9972e-04 | norm: 0.8152 | dt: 2135.13ms | tok/sec: 245552.82 |\n",
            "| step  114 | loss: 3.0380 | lr 5.9968e-04 | norm: 1.2629 | dt: 2135.49ms | tok/sec: 245511.65 |\n",
            "| step  115 | loss: 3.0300 | lr 5.9963e-04 | norm: 0.6280 | dt: 2136.78ms | tok/sec: 245363.53 |\n",
            "| step  116 | loss: 3.0290 | lr 5.9958e-04 | norm: 0.5119 | dt: 2136.93ms | tok/sec: 245346.06 |\n",
            "| step  117 | loss: 3.0187 | lr 5.9952e-04 | norm: 0.7768 | dt: 2135.22ms | tok/sec: 245542.57 |\n",
            "| step  118 | loss: 3.0151 | lr 5.9947e-04 | norm: 0.9797 | dt: 2136.01ms | tok/sec: 245452.29 |\n",
            "| step  119 | loss: 3.0039 | lr 5.9941e-04 | norm: 0.8729 | dt: 2136.80ms | tok/sec: 245361.42 |\n",
            "| step  120 | loss: 2.9748 | lr 5.9934e-04 | norm: 0.4111 | dt: 2136.52ms | tok/sec: 245393.48 |\n",
            "| step  121 | loss: 2.9939 | lr 5.9927e-04 | norm: 0.6531 | dt: 2137.71ms | tok/sec: 245257.02 |\n",
            "| step  122 | loss: 2.9906 | lr 5.9920e-04 | norm: 0.7820 | dt: 2136.16ms | tok/sec: 245434.54 |\n",
            "| step  123 | loss: 2.9787 | lr 5.9913e-04 | norm: 0.7098 | dt: 2136.99ms | tok/sec: 245339.60 |\n",
            "| step  124 | loss: 3.0052 | lr 5.9905e-04 | norm: 1.5000 | dt: 2137.56ms | tok/sec: 245273.49 |\n",
            "| step  125 | loss: 3.0071 | lr 5.9897e-04 | norm: 0.6682 | dt: 2137.09ms | tok/sec: 245327.75 |\n",
            "| step  126 | loss: 2.9887 | lr 5.9889e-04 | norm: 0.7038 | dt: 2136.28ms | tok/sec: 245420.51 |\n",
            "| step  127 | loss: 2.9933 | lr 5.9880e-04 | norm: 0.6363 | dt: 2136.19ms | tok/sec: 245430.78 |\n",
            "| step  128 | loss: 3.0033 | lr 5.9871e-04 | norm: 1.0476 | dt: 2135.90ms | tok/sec: 245464.26 |\n",
            "| step  129 | loss: 3.0101 | lr 5.9862e-04 | norm: 0.8201 | dt: 2135.40ms | tok/sec: 245522.39 |\n",
            "| step  130 | loss: 2.9941 | lr 5.9852e-04 | norm: 1.0006 | dt: 2136.36ms | tok/sec: 245411.64 |\n",
            "| step  131 | loss: 2.9813 | lr 5.9842e-04 | norm: 0.8761 | dt: 2136.50ms | tok/sec: 245395.75 |\n",
            "| step  132 | loss: 2.9792 | lr 5.9832e-04 | norm: 0.5848 | dt: 2136.28ms | tok/sec: 245421.39 |\n",
            "| step  133 | loss: 2.9814 | lr 5.9821e-04 | norm: 1.8848 | dt: 2136.34ms | tok/sec: 245414.24 |\n",
            "| step  134 | loss: 3.0059 | lr 5.9810e-04 | norm: 1.0961 | dt: 2135.48ms | tok/sec: 245512.60 |\n",
            "| step  135 | loss: 2.9749 | lr 5.9799e-04 | norm: 0.8939 | dt: 2135.78ms | tok/sec: 245478.57 |\n",
            "| step  136 | loss: 2.9818 | lr 5.9787e-04 | norm: 0.9108 | dt: 2136.42ms | tok/sec: 245404.93 |\n",
            "| step  137 | loss: 2.9785 | lr 5.9775e-04 | norm: 0.8869 | dt: 2135.52ms | tok/sec: 245508.00 |\n",
            "| step  138 | loss: 2.9837 | lr 5.9763e-04 | norm: 1.2822 | dt: 2135.11ms | tok/sec: 245555.02 |\n",
            "| step  139 | loss: 2.9502 | lr 5.9750e-04 | norm: 0.5057 | dt: 2137.14ms | tok/sec: 245322.47 |\n",
            "| step  140 | loss: 2.9565 | lr 5.9737e-04 | norm: 0.5899 | dt: 2135.31ms | tok/sec: 245532.70 |\n",
            "| step  141 | loss: 2.9474 | lr 5.9724e-04 | norm: 0.8542 | dt: 2137.14ms | tok/sec: 245322.06 |\n",
            "| step  142 | loss: 2.9495 | lr 5.9710e-04 | norm: 1.0395 | dt: 2135.79ms | tok/sec: 245476.84 |\n",
            "| step  143 | loss: 2.9427 | lr 5.9696e-04 | norm: 0.9447 | dt: 2135.85ms | tok/sec: 245470.56 |\n",
            "| step  144 | loss: 2.9112 | lr 5.9682e-04 | norm: 0.9390 | dt: 2136.53ms | tok/sec: 245392.36 |\n",
            "| step  145 | loss: 2.9470 | lr 5.9668e-04 | norm: 0.8822 | dt: 2136.02ms | tok/sec: 245450.59 |\n",
            "| step  146 | loss: 2.9185 | lr 5.9653e-04 | norm: 0.9146 | dt: 2135.94ms | tok/sec: 245459.74 |\n",
            "| step  147 | loss: 2.8990 | lr 5.9637e-04 | norm: 0.7775 | dt: 2136.89ms | tok/sec: 245351.37 |\n",
            "| step  148 | loss: 2.8837 | lr 5.9622e-04 | norm: 0.7734 | dt: 2136.10ms | tok/sec: 245441.25 |\n",
            "| step  149 | loss: 2.8949 | lr 5.9606e-04 | norm: 0.7567 | dt: 2136.12ms | tok/sec: 245439.30 |\n",
            "| step  150 | loss: 2.8774 | lr 5.9590e-04 | norm: 1.0136 | dt: 2139.24ms | tok/sec: 245081.15 |\n",
            "| step  151 | loss: 2.8762 | lr 5.9573e-04 | norm: 0.9963 | dt: 2135.35ms | tok/sec: 245527.87 |\n",
            "| step  152 | loss: 2.8527 | lr 5.9556e-04 | norm: 0.7543 | dt: 2136.73ms | tok/sec: 245369.22 |\n",
            "| step  153 | loss: 2.8577 | lr 5.9539e-04 | norm: 1.2348 | dt: 2136.54ms | tok/sec: 245391.13 |\n",
            "| step  154 | loss: 2.8455 | lr 5.9522e-04 | norm: 0.6500 | dt: 2135.41ms | tok/sec: 245521.21 |\n",
            "| step  155 | loss: 2.8288 | lr 5.9504e-04 | norm: 0.6658 | dt: 2136.44ms | tok/sec: 245402.96 |\n",
            "| step  156 | loss: 2.8187 | lr 5.9486e-04 | norm: 0.8269 | dt: 2135.33ms | tok/sec: 245530.48 |\n",
            "| step  157 | loss: 2.8335 | lr 5.9467e-04 | norm: 2.4768 | dt: 2138.17ms | tok/sec: 245203.53 |\n",
            "| step  158 | loss: 2.8116 | lr 5.9449e-04 | norm: 1.3464 | dt: 2135.90ms | tok/sec: 245464.78 |\n",
            "| step  159 | loss: 2.7818 | lr 5.9429e-04 | norm: 0.9124 | dt: 2136.86ms | tok/sec: 245354.49 |\n",
            "| step  160 | loss: 2.7872 | lr 5.9410e-04 | norm: 1.1035 | dt: 2136.31ms | tok/sec: 245418.10 |\n",
            "| step  161 | loss: 2.7805 | lr 5.9390e-04 | norm: 1.5741 | dt: 2137.34ms | tok/sec: 245299.65 |\n",
            "| step  162 | loss: 2.7636 | lr 5.9370e-04 | norm: 0.7601 | dt: 2136.17ms | tok/sec: 245433.96 |\n",
            "| step  163 | loss: 2.7825 | lr 5.9350e-04 | norm: 1.2405 | dt: 2136.24ms | tok/sec: 245425.63 |\n",
            "| step  164 | loss: 2.7481 | lr 5.9329e-04 | norm: 0.9845 | dt: 2137.26ms | tok/sec: 245307.99 |\n",
            "| step  165 | loss: 2.7255 | lr 5.9308e-04 | norm: 0.5327 | dt: 2137.09ms | tok/sec: 245328.46 |\n",
            "| step  166 | loss: 2.7316 | lr 5.9287e-04 | norm: 0.6697 | dt: 2137.36ms | tok/sec: 245297.29 |\n",
            "| step  167 | loss: 2.7189 | lr 5.9265e-04 | norm: 1.1335 | dt: 2137.94ms | tok/sec: 245230.71 |\n",
            "| step  168 | loss: 2.7127 | lr 5.9243e-04 | norm: 1.2910 | dt: 2136.41ms | tok/sec: 245405.75 |\n",
            "| step  169 | loss: 2.6879 | lr 5.9221e-04 | norm: 0.7659 | dt: 2137.38ms | tok/sec: 245294.80 |\n",
            "| step  170 | loss: 2.6689 | lr 5.9198e-04 | norm: 0.6826 | dt: 2137.37ms | tok/sec: 245296.03 |\n",
            "| step  171 | loss: 2.6566 | lr 5.9175e-04 | norm: 1.8259 | dt: 2137.90ms | tok/sec: 245234.78 |\n",
            "| step  172 | loss: 2.6853 | lr 5.9152e-04 | norm: 2.5864 | dt: 2136.76ms | tok/sec: 245365.36 |\n",
            "| step  173 | loss: 2.6466 | lr 5.9128e-04 | norm: 0.7497 | dt: 2139.80ms | tok/sec: 245017.67 |\n",
            "| step  174 | loss: 2.6608 | lr 5.9104e-04 | norm: 1.7882 | dt: 2137.11ms | tok/sec: 245326.14 |\n",
            "| step  175 | loss: 2.6461 | lr 5.9080e-04 | norm: 1.3892 | dt: 2138.29ms | tok/sec: 245190.87 |\n",
            "| step  176 | loss: 2.6548 | lr 5.9055e-04 | norm: 3.0153 | dt: 2137.45ms | tok/sec: 245286.81 |\n",
            "| step  177 | loss: 2.6392 | lr 5.9031e-04 | norm: 1.1170 | dt: 2136.85ms | tok/sec: 245355.20 |\n",
            "| step  178 | loss: 2.6377 | lr 5.9005e-04 | norm: 2.7474 | dt: 2137.02ms | tok/sec: 245335.61 |\n",
            "| step  179 | loss: 2.6542 | lr 5.8980e-04 | norm: 1.3858 | dt: 2137.43ms | tok/sec: 245288.92 |\n",
            "| step  180 | loss: 2.6010 | lr 5.8954e-04 | norm: 0.7693 | dt: 2137.21ms | tok/sec: 245314.20 |\n",
            "| step  181 | loss: 2.5869 | lr 5.8928e-04 | norm: 0.7610 | dt: 2137.28ms | tok/sec: 245306.19 |\n",
            "| step  182 | loss: 2.5938 | lr 5.8901e-04 | norm: 2.8299 | dt: 2136.80ms | tok/sec: 245361.31 |\n",
            "| step  183 | loss: 2.6013 | lr 5.8875e-04 | norm: 0.9609 | dt: 2137.88ms | tok/sec: 245237.60 |\n",
            "| step  184 | loss: 2.6156 | lr 5.8848e-04 | norm: 4.0365 | dt: 2137.54ms | tok/sec: 245276.06 |\n",
            "| step  185 | loss: 2.6028 | lr 5.8820e-04 | norm: 1.7475 | dt: 2137.46ms | tok/sec: 245285.39 |\n",
            "| step  186 | loss: 2.5999 | lr 5.8793e-04 | norm: 1.9887 | dt: 2136.60ms | tok/sec: 245384.17 |\n",
            "| step  187 | loss: 2.5638 | lr 5.8764e-04 | norm: 1.0540 | dt: 2137.79ms | tok/sec: 245247.58 |\n",
            "| step  188 | loss: 2.5684 | lr 5.8736e-04 | norm: 1.6352 | dt: 2137.16ms | tok/sec: 245319.65 |\n",
            "| step  189 | loss: 2.5319 | lr 5.8707e-04 | norm: 1.0757 | dt: 2136.21ms | tok/sec: 245429.06 |\n",
            "| step  190 | loss: 2.5130 | lr 5.8679e-04 | norm: 1.2028 | dt: 2138.07ms | tok/sec: 245215.75 |\n",
            "| step  191 | loss: 2.5393 | lr 5.8649e-04 | norm: 2.0097 | dt: 2136.84ms | tok/sec: 245356.93 |\n",
            "| step  192 | loss: 2.5225 | lr 5.8620e-04 | norm: 1.0848 | dt: 2137.42ms | tok/sec: 245290.45 |\n",
            "| step  193 | loss: 2.4792 | lr 5.8590e-04 | norm: 1.2624 | dt: 2137.68ms | tok/sec: 245260.69 |\n",
            "| step  194 | loss: 2.4724 | lr 5.8560e-04 | norm: 1.6155 | dt: 2136.54ms | tok/sec: 245391.45 |\n",
            "| step  195 | loss: 2.4412 | lr 5.8529e-04 | norm: 0.9759 | dt: 2137.80ms | tok/sec: 245246.38 |\n",
            "| step  196 | loss: 2.4319 | lr 5.8498e-04 | norm: 1.1936 | dt: 2138.49ms | tok/sec: 245167.28 |\n",
            "| step  197 | loss: 2.4083 | lr 5.8467e-04 | norm: 1.0167 | dt: 2136.93ms | tok/sec: 245346.80 |\n",
            "| step  198 | loss: 2.4213 | lr 5.8436e-04 | norm: 1.9094 | dt: 2137.76ms | tok/sec: 245250.87 |\n",
            "| step  199 | loss: 2.3787 | lr 5.8404e-04 | norm: 0.8371 | dt: 2136.50ms | tok/sec: 245396.00 |\n",
            "| step  200 | loss: 2.3928 | lr 5.8372e-04 | norm: 1.6116 | dt: 2137.85ms | tok/sec: 245241.05 |\n",
            "| step  201 | loss: 2.3687 | lr 5.8339e-04 | norm: 0.9847 | dt: 2137.83ms | tok/sec: 245242.74 |\n",
            "| step  202 | loss: 2.3739 | lr 5.8307e-04 | norm: 1.7718 | dt: 2136.46ms | tok/sec: 245400.00 |\n",
            "| step  203 | loss: 2.3501 | lr 5.8274e-04 | norm: 1.1320 | dt: 2137.73ms | tok/sec: 245254.83 |\n",
            "| step  204 | loss: 2.3632 | lr 5.8240e-04 | norm: 1.9535 | dt: 2137.60ms | tok/sec: 245269.39 |\n",
            "| step  205 | loss: 2.3405 | lr 5.8207e-04 | norm: 1.2824 | dt: 2137.55ms | tok/sec: 245275.40 |\n",
            "| step  206 | loss: 2.3235 | lr 5.8173e-04 | norm: 1.1836 | dt: 2136.34ms | tok/sec: 245414.08 |\n",
            "| step  207 | loss: 2.3147 | lr 5.8139e-04 | norm: 1.8418 | dt: 2137.75ms | tok/sec: 245252.18 |\n",
            "| step  208 | loss: 2.2924 | lr 5.8104e-04 | norm: 0.9289 | dt: 2138.07ms | tok/sec: 245215.89 |\n",
            "| step  209 | loss: 2.3003 | lr 5.8069e-04 | norm: 1.5114 | dt: 2137.70ms | tok/sec: 245258.53 |\n",
            "| step  210 | loss: 2.3021 | lr 5.8034e-04 | norm: 1.8209 | dt: 2137.59ms | tok/sec: 245271.03 |\n",
            "| step  211 | loss: 2.2628 | lr 5.7999e-04 | norm: 0.8744 | dt: 2137.04ms | tok/sec: 245333.74 |\n",
            "| step  212 | loss: 2.2335 | lr 5.7963e-04 | norm: 0.7307 | dt: 2139.24ms | tok/sec: 245081.56 |\n",
            "| step  213 | loss: 2.2475 | lr 5.7927e-04 | norm: 1.1467 | dt: 2138.06ms | tok/sec: 245216.90 |\n",
            "| step  214 | loss: 2.2261 | lr 5.7890e-04 | norm: 1.1233 | dt: 2136.64ms | tok/sec: 245379.95 |\n",
            "| step  215 | loss: 2.2178 | lr 5.7854e-04 | norm: 0.9487 | dt: 2138.38ms | tok/sec: 245179.72 |\n",
            "| step  216 | loss: 2.2079 | lr 5.7817e-04 | norm: 1.3471 | dt: 2138.02ms | tok/sec: 245221.52 |\n",
            "| step  217 | loss: 2.1986 | lr 5.7779e-04 | norm: 0.7697 | dt: 2138.37ms | tok/sec: 245181.28 |\n",
            "| step  218 | loss: 2.1792 | lr 5.7742e-04 | norm: 0.7241 | dt: 2138.33ms | tok/sec: 245186.20 |\n",
            "| step  219 | loss: 2.1515 | lr 5.7704e-04 | norm: 0.7411 | dt: 2137.57ms | tok/sec: 245272.56 |\n",
            "| step  220 | loss: 2.1667 | lr 5.7666e-04 | norm: 0.7953 | dt: 2139.22ms | tok/sec: 245084.21 |\n",
            "| step  221 | loss: 2.1436 | lr 5.7627e-04 | norm: 0.7479 | dt: 2139.65ms | tok/sec: 245034.16 |\n",
            "| step  222 | loss: 2.1437 | lr 5.7588e-04 | norm: 0.9633 | dt: 2137.15ms | tok/sec: 245321.51 |\n",
            "| step  223 | loss: 2.1218 | lr 5.7549e-04 | norm: 0.7979 | dt: 2137.61ms | tok/sec: 245268.87 |\n",
            "| step  224 | loss: 2.1016 | lr 5.7510e-04 | norm: 0.7604 | dt: 2136.43ms | tok/sec: 245404.27 |\n",
            "| step  225 | loss: 2.1129 | lr 5.7470e-04 | norm: 0.6041 | dt: 2137.48ms | tok/sec: 245283.15 |\n",
            "| step  226 | loss: 2.1054 | lr 5.7430e-04 | norm: 0.8569 | dt: 2138.45ms | tok/sec: 245172.39 |\n",
            "| step  227 | loss: 2.0981 | lr 5.7390e-04 | norm: 1.2667 | dt: 2138.24ms | tok/sec: 245195.90 |\n",
            "| step  228 | loss: 2.0736 | lr 5.7349e-04 | norm: 0.6878 | dt: 2137.48ms | tok/sec: 245282.98 |\n",
            "| step  229 | loss: 2.0681 | lr 5.7309e-04 | norm: 0.8769 | dt: 2138.01ms | tok/sec: 245223.00 |\n",
            "| step  230 | loss: 2.0753 | lr 5.7267e-04 | norm: 0.7746 | dt: 2137.74ms | tok/sec: 245253.90 |\n",
            "| step  231 | loss: 2.0459 | lr 5.7226e-04 | norm: 0.4716 | dt: 2137.32ms | tok/sec: 245302.05 |\n",
            "| step  232 | loss: 2.0345 | lr 5.7184e-04 | norm: 0.5648 | dt: 2139.00ms | tok/sec: 245109.35 |\n",
            "| step  233 | loss: 2.0391 | lr 5.7142e-04 | norm: 0.5780 | dt: 2136.89ms | tok/sec: 245351.13 |\n",
            "| step  234 | loss: 2.0197 | lr 5.7100e-04 | norm: 0.6203 | dt: 2137.82ms | tok/sec: 245243.92 |\n",
            "| step  235 | loss: 2.0361 | lr 5.7057e-04 | norm: 0.8672 | dt: 2138.03ms | tok/sec: 245220.15 |\n",
            "| step  236 | loss: 2.0281 | lr 5.7014e-04 | norm: 1.4366 | dt: 2138.31ms | tok/sec: 245187.84 |\n",
            "| step  237 | loss: 2.0117 | lr 5.6971e-04 | norm: 0.8464 | dt: 2137.10ms | tok/sec: 245326.35 |\n",
            "| step  238 | loss: 2.0037 | lr 5.6927e-04 | norm: 0.6959 | dt: 2138.19ms | tok/sec: 245202.13 |\n",
            "| step  239 | loss: 1.9885 | lr 5.6884e-04 | norm: 0.5694 | dt: 2137.91ms | tok/sec: 245234.18 |\n",
            "| step  240 | loss: 1.9775 | lr 5.6840e-04 | norm: 0.7370 | dt: 2137.48ms | tok/sec: 245283.28 |\n",
            "| step  241 | loss: 1.9916 | lr 5.6795e-04 | norm: 1.0053 | dt: 2138.24ms | tok/sec: 245195.90 |\n",
            "| step  242 | loss: 1.9800 | lr 5.6751e-04 | norm: 1.1507 | dt: 2136.74ms | tok/sec: 245368.21 |\n",
            "| step  243 | loss: 1.9611 | lr 5.6706e-04 | norm: 0.9421 | dt: 2137.98ms | tok/sec: 245226.25 |\n",
            "| step  244 | loss: 1.9804 | lr 5.6660e-04 | norm: 1.4367 | dt: 2138.19ms | tok/sec: 245201.34 |\n",
            "| step  245 | loss: 1.9597 | lr 5.6615e-04 | norm: 0.7277 | dt: 2138.03ms | tok/sec: 245219.63 |\n",
            "| step  246 | loss: 1.9499 | lr 5.6569e-04 | norm: 1.1981 | dt: 2138.58ms | tok/sec: 245156.59 |\n",
            "| step  247 | loss: 1.9509 | lr 5.6523e-04 | norm: 1.4957 | dt: 2138.09ms | tok/sec: 245213.32 |\n",
            "| step  248 | loss: 1.9340 | lr 5.6476e-04 | norm: 0.5871 | dt: 2138.74ms | tok/sec: 245139.27 |\n",
            "| step  249 | loss: 1.9186 | lr 5.6430e-04 | norm: 0.8807 | dt: 2137.57ms | tok/sec: 245273.43 |\n",
            "| step  250 | loss: 1.9212 | lr 5.6383e-04 | norm: 0.8535 | dt: 2137.88ms | tok/sec: 245237.33 |\n",
            "| step  251 | loss: 1.9138 | lr 5.6335e-04 | norm: 0.9630 | dt: 2138.00ms | tok/sec: 245223.13 |\n",
            "| step  252 | loss: 1.9257 | lr 5.6288e-04 | norm: 1.0657 | dt: 2138.43ms | tok/sec: 245174.11 |\n",
            "| step  253 | loss: 1.8946 | lr 5.6240e-04 | norm: 0.8839 | dt: 2137.57ms | tok/sec: 245273.11 |\n",
            "| step  254 | loss: 1.8918 | lr 5.6192e-04 | norm: 0.6665 | dt: 2137.32ms | tok/sec: 245301.97 |\n",
            "| step  255 | loss: 1.8834 | lr 5.6144e-04 | norm: 0.5061 | dt: 2136.82ms | tok/sec: 245359.20 |\n",
            "| step  256 | loss: 1.8731 | lr 5.6095e-04 | norm: 0.6549 | dt: 2137.78ms | tok/sec: 245249.09 |\n",
            "| step  257 | loss: 1.8574 | lr 5.6046e-04 | norm: 0.6271 | dt: 2138.17ms | tok/sec: 245203.67 |\n",
            "| step  258 | loss: 1.8622 | lr 5.5997e-04 | norm: 0.7112 | dt: 2136.44ms | tok/sec: 245402.93 |\n",
            "| step  259 | loss: 1.8714 | lr 5.5947e-04 | norm: 1.0377 | dt: 2137.77ms | tok/sec: 245250.21 |\n",
            "| step  260 | loss: 1.8764 | lr 5.5897e-04 | norm: 1.0535 | dt: 2137.49ms | tok/sec: 245282.52 |\n",
            "| step  261 | loss: 1.8543 | lr 5.5847e-04 | norm: 0.8835 | dt: 2137.69ms | tok/sec: 245259.43 |\n",
            "| step  262 | loss: 1.8545 | lr 5.5797e-04 | norm: 0.8862 | dt: 2137.79ms | tok/sec: 245247.42 |\n",
            "| step  263 | loss: 1.8340 | lr 5.5746e-04 | norm: 0.7940 | dt: 2136.95ms | tok/sec: 245343.52 |\n",
            "| step  264 | loss: 1.8506 | lr 5.5695e-04 | norm: 1.1242 | dt: 2137.73ms | tok/sec: 245254.91 |\n",
            "| step  265 | loss: 1.8272 | lr 5.5644e-04 | norm: 0.6978 | dt: 2138.91ms | tok/sec: 245118.91 |\n",
            "| step  266 | loss: 1.8304 | lr 5.5593e-04 | norm: 0.8052 | dt: 2137.55ms | tok/sec: 245275.21 |\n",
            "| step  267 | loss: 1.8286 | lr 5.5541e-04 | norm: 1.2440 | dt: 2137.86ms | tok/sec: 245239.98 |\n",
            "| step  268 | loss: 1.8139 | lr 5.5489e-04 | norm: 0.9882 | dt: 2138.41ms | tok/sec: 245177.01 |\n",
            "| step  269 | loss: 1.8144 | lr 5.5437e-04 | norm: 0.8050 | dt: 2138.93ms | tok/sec: 245117.49 |\n",
            "| step  270 | loss: 1.7906 | lr 5.5384e-04 | norm: 0.7469 | dt: 2136.77ms | tok/sec: 245364.68 |\n",
            "| step  271 | loss: 1.7901 | lr 5.5331e-04 | norm: 0.7632 | dt: 2136.38ms | tok/sec: 245409.64 |\n",
            "| step  272 | loss: 1.7894 | lr 5.5278e-04 | norm: 0.8137 | dt: 2137.61ms | tok/sec: 245268.15 |\n",
            "| step  273 | loss: 1.7784 | lr 5.5225e-04 | norm: 0.5733 | dt: 2137.13ms | tok/sec: 245322.93 |\n",
            "| step  274 | loss: 1.7693 | lr 5.5171e-04 | norm: 0.6968 | dt: 2137.85ms | tok/sec: 245241.13 |\n",
            "| step  275 | loss: 1.7530 | lr 5.5117e-04 | norm: 0.5710 | dt: 2136.49ms | tok/sec: 245396.44 |\n",
            "| step  276 | loss: 1.7345 | lr 5.5063e-04 | norm: 0.5551 | dt: 2137.67ms | tok/sec: 245261.75 |\n",
            "| step  277 | loss: 1.7561 | lr 5.5008e-04 | norm: 0.5305 | dt: 2137.74ms | tok/sec: 245253.79 |\n",
            "| step  278 | loss: 1.7319 | lr 5.4954e-04 | norm: 0.7761 | dt: 2138.70ms | tok/sec: 245143.28 |\n",
            "| step  279 | loss: 1.7327 | lr 5.4899e-04 | norm: 0.9244 | dt: 2137.83ms | tok/sec: 245242.52 |\n",
            "| step  280 | loss: 1.7576 | lr 5.4843e-04 | norm: 1.0737 | dt: 2136.73ms | tok/sec: 245369.82 |\n",
            "| step  281 | loss: 1.7669 | lr 5.4788e-04 | norm: 1.1639 | dt: 2137.74ms | tok/sec: 245253.66 |\n",
            "| step  282 | loss: 1.7491 | lr 5.4732e-04 | norm: 1.2446 | dt: 2136.86ms | tok/sec: 245354.96 |\n",
            "| step  283 | loss: 1.7355 | lr 5.4676e-04 | norm: 0.8206 | dt: 2136.29ms | tok/sec: 245419.36 |\n",
            "| step  284 | loss: 1.7261 | lr 5.4620e-04 | norm: 0.8712 | dt: 2136.89ms | tok/sec: 245351.29 |\n",
            "| step  285 | loss: 1.7333 | lr 5.4563e-04 | norm: 1.0158 | dt: 2137.80ms | tok/sec: 245246.90 |\n",
            "| step  286 | loss: 1.7425 | lr 5.4506e-04 | norm: 1.3489 | dt: 2138.04ms | tok/sec: 245218.73 |\n",
            "| step  287 | loss: 1.7184 | lr 5.4449e-04 | norm: 0.6167 | dt: 2139.05ms | tok/sec: 245102.95 |\n",
            "| step  288 | loss: 1.7189 | lr 5.4392e-04 | norm: 0.6939 | dt: 2138.64ms | tok/sec: 245150.06 |\n",
            "| step  289 | loss: 1.7077 | lr 5.4334e-04 | norm: 0.6617 | dt: 2136.72ms | tok/sec: 245370.07 |\n",
            "| step  290 | loss: 1.6914 | lr 5.4276e-04 | norm: 0.6388 | dt: 2136.96ms | tok/sec: 245342.37 |\n",
            "| step  291 | loss: 1.6868 | lr 5.4218e-04 | norm: 0.6896 | dt: 2137.28ms | tok/sec: 245306.54 |\n",
            "| step  292 | loss: 1.6689 | lr 5.4160e-04 | norm: 0.6835 | dt: 2136.37ms | tok/sec: 245410.95 |\n",
            "| step  293 | loss: 1.6768 | lr 5.4101e-04 | norm: 0.6798 | dt: 2136.54ms | tok/sec: 245390.96 |\n",
            "| step  294 | loss: 1.6718 | lr 5.4042e-04 | norm: 0.6369 | dt: 2137.55ms | tok/sec: 245274.94 |\n",
            "| step  295 | loss: 1.6608 | lr 5.3983e-04 | norm: 0.7986 | dt: 2137.02ms | tok/sec: 245336.04 |\n",
            "| step  296 | loss: 1.6657 | lr 5.3924e-04 | norm: 0.7191 | dt: 2137.90ms | tok/sec: 245234.87 |\n",
            "| step  297 | loss: 1.6570 | lr 5.3864e-04 | norm: 0.7183 | dt: 2138.46ms | tok/sec: 245170.91 |\n",
            "| step  298 | loss: 1.6567 | lr 5.3804e-04 | norm: 0.6741 | dt: 2136.80ms | tok/sec: 245361.23 |\n",
            "| step  299 | loss: 1.6412 | lr 5.3744e-04 | norm: 0.6548 | dt: 2137.73ms | tok/sec: 245254.53 |\n",
            "| step  300 | loss: 1.6355 | lr 5.3683e-04 | norm: 0.6089 | dt: 2137.33ms | tok/sec: 245300.06 |\n",
            "| step  301 | loss: 1.6411 | lr 5.3622e-04 | norm: 0.5745 | dt: 2137.02ms | tok/sec: 245336.59 |\n",
            "| step  302 | loss: 1.6430 | lr 5.3562e-04 | norm: 0.5316 | dt: 2136.90ms | tok/sec: 245349.32 |\n",
            "| step  303 | loss: 1.6376 | lr 5.3500e-04 | norm: 0.6659 | dt: 2137.58ms | tok/sec: 245271.82 |\n",
            "| step  304 | loss: 1.6320 | lr 5.3439e-04 | norm: 0.8574 | dt: 2137.19ms | tok/sec: 245315.98 |\n",
            "| step  305 | loss: 1.6308 | lr 5.3377e-04 | norm: 0.9965 | dt: 2138.29ms | tok/sec: 245190.32 |\n",
            "| step  306 | loss: 1.6341 | lr 5.3315e-04 | norm: 1.1330 | dt: 2137.13ms | tok/sec: 245323.45 |\n",
            "| step  307 | loss: 1.6155 | lr 5.3253e-04 | norm: 0.9003 | dt: 2136.87ms | tok/sec: 245352.96 |\n",
            "| step  308 | loss: 1.6078 | lr 5.3191e-04 | norm: 0.8788 | dt: 2138.29ms | tok/sec: 245190.02 |\n",
            "| step  309 | loss: 1.6147 | lr 5.3128e-04 | norm: 0.8211 | dt: 2136.95ms | tok/sec: 245343.76 |\n",
            "| step  310 | loss: 1.5996 | lr 5.3065e-04 | norm: 0.6085 | dt: 2137.24ms | tok/sec: 245310.84 |\n",
            "| step  311 | loss: 1.6016 | lr 5.3002e-04 | norm: 0.6278 | dt: 2137.79ms | tok/sec: 245247.78 |\n",
            "| step  312 | loss: 1.5971 | lr 5.2938e-04 | norm: 0.6483 | dt: 2137.15ms | tok/sec: 245321.10 |\n",
            "| step  313 | loss: 1.5810 | lr 5.2875e-04 | norm: 0.5906 | dt: 2137.63ms | tok/sec: 245266.35 |\n",
            "| step  314 | loss: 1.5703 | lr 5.2811e-04 | norm: 0.5107 | dt: 2137.09ms | tok/sec: 245327.45 |\n",
            "| step  315 | loss: 1.5806 | lr 5.2747e-04 | norm: 0.4761 | dt: 2136.94ms | tok/sec: 245345.16 |\n",
            "| step  316 | loss: 1.5652 | lr 5.2682e-04 | norm: 0.5058 | dt: 2137.61ms | tok/sec: 245268.67 |\n",
            "| step  317 | loss: 1.5589 | lr 5.2618e-04 | norm: 0.5111 | dt: 2136.46ms | tok/sec: 245400.66 |\n",
            "| step  318 | loss: 1.5477 | lr 5.2553e-04 | norm: 0.4908 | dt: 2137.13ms | tok/sec: 245323.59 |\n",
            "| step  319 | loss: 1.5452 | lr 5.2488e-04 | norm: 0.5001 | dt: 2137.02ms | tok/sec: 245335.63 |\n",
            "| step  320 | loss: 1.5517 | lr 5.2422e-04 | norm: 0.6629 | dt: 2138.05ms | tok/sec: 245217.56 |\n",
            "| step  321 | loss: 1.5607 | lr 5.2357e-04 | norm: 0.8794 | dt: 2136.99ms | tok/sec: 245339.44 |\n",
            "| step  322 | loss: 1.5674 | lr 5.2291e-04 | norm: 0.9587 | dt: 2138.39ms | tok/sec: 245178.30 |\n",
            "| step  323 | loss: 1.5528 | lr 5.2225e-04 | norm: 0.7160 | dt: 2137.51ms | tok/sec: 245279.86 |\n",
            "| step  324 | loss: 1.5409 | lr 5.2158e-04 | norm: 0.6149 | dt: 2137.32ms | tok/sec: 245301.83 |\n",
            "| step  325 | loss: 1.5272 | lr 5.2092e-04 | norm: 0.6252 | dt: 2137.94ms | tok/sec: 245230.96 |\n",
            "| step  326 | loss: 1.5449 | lr 5.2025e-04 | norm: 0.7854 | dt: 2136.89ms | tok/sec: 245351.48 |\n",
            "| step  327 | loss: 1.5271 | lr 5.1958e-04 | norm: 0.6655 | dt: 2136.51ms | tok/sec: 245395.01 |\n",
            "| step  328 | loss: 1.5301 | lr 5.1891e-04 | norm: 0.6679 | dt: 2137.97ms | tok/sec: 245226.94 |\n",
            "| step  329 | loss: 1.5256 | lr 5.1823e-04 | norm: 0.6098 | dt: 2137.36ms | tok/sec: 245296.94 |\n",
            "| step  330 | loss: 1.5266 | lr 5.1756e-04 | norm: 0.6460 | dt: 2136.54ms | tok/sec: 245391.56 |\n",
            "| step  331 | loss: 1.4936 | lr 5.1688e-04 | norm: 0.5606 | dt: 2137.76ms | tok/sec: 245251.03 |\n",
            "| step  332 | loss: 1.4975 | lr 5.1620e-04 | norm: 0.5887 | dt: 2138.15ms | tok/sec: 245206.48 |\n",
            "| step  333 | loss: 1.4957 | lr 5.1551e-04 | norm: 0.6948 | dt: 2137.87ms | tok/sec: 245238.61 |\n",
            "| step  334 | loss: 1.4983 | lr 5.1483e-04 | norm: 0.9586 | dt: 2136.02ms | tok/sec: 245450.51 |\n",
            "| step  335 | loss: 1.4828 | lr 5.1414e-04 | norm: 0.8802 | dt: 2136.46ms | tok/sec: 245400.35 |\n",
            "| step  336 | loss: 1.4894 | lr 5.1345e-04 | norm: 0.6352 | dt: 2138.20ms | tok/sec: 245201.04 |\n",
            "| step  337 | loss: 1.4883 | lr 5.1276e-04 | norm: 0.5353 | dt: 2137.18ms | tok/sec: 245318.14 |\n",
            "| step  338 | loss: 1.4892 | lr 5.1206e-04 | norm: 0.5780 | dt: 2137.63ms | tok/sec: 245266.08 |\n",
            "| step  339 | loss: 1.4711 | lr 5.1136e-04 | norm: 0.6221 | dt: 2138.14ms | tok/sec: 245207.11 |\n",
            "| step  340 | loss: 1.4756 | lr 5.1067e-04 | norm: 0.5216 | dt: 2136.64ms | tok/sec: 245379.19 |\n",
            "| step  341 | loss: 1.4705 | lr 5.0996e-04 | norm: 0.5579 | dt: 2136.81ms | tok/sec: 245359.64 |\n",
            "| step  342 | loss: 1.4675 | lr 5.0926e-04 | norm: 0.5850 | dt: 2136.67ms | tok/sec: 245375.77 |\n",
            "| step  343 | loss: 1.4592 | lr 5.0855e-04 | norm: 0.5971 | dt: 2137.63ms | tok/sec: 245265.69 |\n",
            "| step  344 | loss: 1.4566 | lr 5.0785e-04 | norm: 0.6320 | dt: 2136.55ms | tok/sec: 245390.14 |\n",
            "| step  345 | loss: 1.4630 | lr 5.0714e-04 | norm: 0.6595 | dt: 2136.62ms | tok/sec: 245381.41 |\n",
            "| step  346 | loss: 1.4696 | lr 5.0642e-04 | norm: 0.7835 | dt: 2137.10ms | tok/sec: 245327.39 |\n",
            "| step  347 | loss: 1.4594 | lr 5.0571e-04 | norm: 0.6929 | dt: 2135.70ms | tok/sec: 245487.94 |\n",
            "| step  348 | loss: 1.4555 | lr 5.0499e-04 | norm: 0.6459 | dt: 2138.16ms | tok/sec: 245205.22 |\n",
            "| step  349 | loss: 1.4399 | lr 5.0427e-04 | norm: 0.6132 | dt: 2137.83ms | tok/sec: 245243.65 |\n",
            "| step  350 | loss: 1.4397 | lr 5.0355e-04 | norm: 0.5968 | dt: 2138.20ms | tok/sec: 245200.55 |\n",
            "| step  351 | loss: 1.4323 | lr 5.0283e-04 | norm: 0.5537 | dt: 2137.33ms | tok/sec: 245300.99 |\n",
            "| step  352 | loss: 1.4189 | lr 5.0210e-04 | norm: 0.5170 | dt: 2137.03ms | tok/sec: 245335.19 |\n",
            "| step  353 | loss: 1.4129 | lr 5.0138e-04 | norm: 0.5033 | dt: 2136.18ms | tok/sec: 245432.15 |\n",
            "| step  354 | loss: 1.4120 | lr 5.0065e-04 | norm: 0.5339 | dt: 2136.87ms | tok/sec: 245353.26 |\n",
            "| step  355 | loss: 1.4222 | lr 4.9992e-04 | norm: 0.5674 | dt: 2137.97ms | tok/sec: 245226.99 |\n",
            "| step  356 | loss: 1.4073 | lr 4.9918e-04 | norm: 0.6103 | dt: 2137.46ms | tok/sec: 245285.25 |\n",
            "| step  357 | loss: 1.3980 | lr 4.9845e-04 | norm: 0.5650 | dt: 2137.19ms | tok/sec: 245316.42 |\n",
            "| step  358 | loss: 1.3950 | lr 4.9771e-04 | norm: 0.5534 | dt: 2138.69ms | tok/sec: 245144.21 |\n",
            "| step  359 | loss: 1.3980 | lr 4.9697e-04 | norm: 0.6470 | dt: 2137.11ms | tok/sec: 245325.78 |\n",
            "| step  360 | loss: 1.4171 | lr 4.9623e-04 | norm: 0.8123 | dt: 2136.92ms | tok/sec: 245347.02 |\n",
            "| step  361 | loss: 1.4186 | lr 4.9548e-04 | norm: 1.0206 | dt: 2136.88ms | tok/sec: 245351.56 |\n",
            "| step  362 | loss: 1.4109 | lr 4.9474e-04 | norm: 0.9009 | dt: 2136.81ms | tok/sec: 245360.49 |\n",
            "| step  363 | loss: 1.4022 | lr 4.9399e-04 | norm: 0.7984 | dt: 2137.80ms | tok/sec: 245246.08 |\n",
            "| step  364 | loss: 1.3950 | lr 4.9324e-04 | norm: 0.6510 | dt: 2138.46ms | tok/sec: 245170.83 |\n",
            "| step  365 | loss: 1.3874 | lr 4.9249e-04 | norm: 0.5290 | dt: 2136.94ms | tok/sec: 245345.65 |\n",
            "| step  366 | loss: 1.3797 | lr 4.9174e-04 | norm: 0.4726 | dt: 2137.11ms | tok/sec: 245325.15 |\n",
            "| step  367 | loss: 1.3804 | lr 4.9098e-04 | norm: 0.5244 | dt: 2137.37ms | tok/sec: 245295.90 |\n",
            "| step  368 | loss: 1.3611 | lr 4.9022e-04 | norm: 0.4551 | dt: 2137.82ms | tok/sec: 245244.17 |\n",
            "| step  369 | loss: 1.3657 | lr 4.8946e-04 | norm: 0.4777 | dt: 2135.70ms | tok/sec: 245487.86 |\n",
            "| step  370 | loss: 1.3696 | lr 4.8870e-04 | norm: 0.5433 | dt: 2135.86ms | tok/sec: 245469.00 |\n",
            "| step  371 | loss: 1.3489 | lr 4.8794e-04 | norm: 0.3960 | dt: 2137.58ms | tok/sec: 245272.09 |\n",
            "| step  372 | loss: 1.3335 | lr 4.8717e-04 | norm: 0.3875 | dt: 2136.28ms | tok/sec: 245421.28 |\n",
            "| step  373 | loss: 1.3410 | lr 4.8641e-04 | norm: 0.4423 | dt: 2137.87ms | tok/sec: 245238.70 |\n",
            "| step  374 | loss: 1.3489 | lr 4.8564e-04 | norm: 0.4893 | dt: 2135.87ms | tok/sec: 245468.45 |\n",
            "| step  375 | loss: 1.3549 | lr 4.8487e-04 | norm: 0.6596 | dt: 2136.94ms | tok/sec: 245345.51 |\n",
            "| step  376 | loss: 1.3528 | lr 4.8409e-04 | norm: 0.7217 | dt: 2137.74ms | tok/sec: 245253.85 |\n",
            "| step  377 | loss: 1.3508 | lr 4.8332e-04 | norm: 0.6650 | dt: 2137.85ms | tok/sec: 245240.66 |\n",
            "| step  378 | loss: 1.3538 | lr 4.8254e-04 | norm: 0.6583 | dt: 2136.84ms | tok/sec: 245356.38 |\n",
            "| step  379 | loss: 1.3532 | lr 4.8176e-04 | norm: 0.6766 | dt: 2136.05ms | tok/sec: 245447.63 |\n",
            "| step  380 | loss: 1.3552 | lr 4.8098e-04 | norm: 0.8285 | dt: 2136.67ms | tok/sec: 245376.59 |\n",
            "| step  381 | loss: 1.3396 | lr 4.8020e-04 | norm: 0.7705 | dt: 2136.79ms | tok/sec: 245362.21 |\n",
            "| step  382 | loss: 1.3227 | lr 4.7942e-04 | norm: 0.5971 | dt: 2137.45ms | tok/sec: 245286.35 |\n",
            "| step  383 | loss: 1.3320 | lr 4.7863e-04 | norm: 0.7670 | dt: 2137.88ms | tok/sec: 245237.68 |\n",
            "| step  384 | loss: 1.3394 | lr 4.7784e-04 | norm: 0.7936 | dt: 2137.16ms | tok/sec: 245319.65 |\n",
            "| step  385 | loss: 1.3274 | lr 4.7705e-04 | norm: 0.6502 | dt: 2137.06ms | tok/sec: 245331.86 |\n",
            "| step  386 | loss: 1.3142 | lr 4.7626e-04 | norm: 0.4476 | dt: 2136.56ms | tok/sec: 245388.66 |\n",
            "| step  387 | loss: 1.3054 | lr 4.7547e-04 | norm: 0.5508 | dt: 2137.72ms | tok/sec: 245255.76 |\n",
            "| step  388 | loss: 1.3118 | lr 4.7467e-04 | norm: 0.5618 | dt: 2136.95ms | tok/sec: 245343.98 |\n",
            "| step  389 | loss: 1.3032 | lr 4.7388e-04 | norm: 0.5806 | dt: 2137.11ms | tok/sec: 245325.29 |\n",
            "| step  390 | loss: 1.3059 | lr 4.7308e-04 | norm: 0.4843 | dt: 2136.30ms | tok/sec: 245418.21 |\n",
            "| step  391 | loss: 1.3030 | lr 4.7228e-04 | norm: 0.4081 | dt: 2137.65ms | tok/sec: 245264.08 |\n",
            "| step  392 | loss: 1.2891 | lr 4.7148e-04 | norm: 0.4660 | dt: 2136.44ms | tok/sec: 245402.90 |\n",
            "| step  393 | loss: 1.2992 | lr 4.7067e-04 | norm: 0.4814 | dt: 2137.46ms | tok/sec: 245285.14 |\n",
            "| step  394 | loss: 1.2935 | lr 4.6987e-04 | norm: 0.4717 | dt: 2137.60ms | tok/sec: 245269.11 |\n",
            "| step  395 | loss: 1.2878 | lr 4.6906e-04 | norm: 0.5229 | dt: 2137.15ms | tok/sec: 245320.72 |\n",
            "| step  396 | loss: 1.2857 | lr 4.6825e-04 | norm: 0.6020 | dt: 2138.65ms | tok/sec: 245148.97 |\n",
            "| step  397 | loss: 1.2860 | lr 4.6744e-04 | norm: 0.7044 | dt: 2137.81ms | tok/sec: 245244.99 |\n",
            "| step  398 | loss: 1.2862 | lr 4.6663e-04 | norm: 0.7109 | dt: 2136.75ms | tok/sec: 245366.57 |\n",
            "| step  399 | loss: 1.2850 | lr 4.6582e-04 | norm: 0.6120 | dt: 2138.11ms | tok/sec: 245211.13 |\n",
            "| step  400 | loss: 1.2803 | lr 4.6500e-04 | norm: 0.5456 | dt: 2136.36ms | tok/sec: 245412.30 |\n",
            "| step  401 | loss: 1.2796 | lr 4.6418e-04 | norm: 0.6003 | dt: 2135.95ms | tok/sec: 245459.44 |\n",
            "| step  402 | loss: 1.2749 | lr 4.6336e-04 | norm: 0.5895 | dt: 2138.64ms | tok/sec: 245150.06 |\n",
            "| step  403 | loss: 1.2799 | lr 4.6254e-04 | norm: 0.5094 | dt: 2136.85ms | tok/sec: 245355.31 |\n",
            "| step  404 | loss: 1.2763 | lr 4.6172e-04 | norm: 0.4962 | dt: 2136.38ms | tok/sec: 245409.69 |\n",
            "| step  405 | loss: 1.2582 | lr 4.6090e-04 | norm: 0.4786 | dt: 2136.81ms | tok/sec: 245360.46 |\n",
            "| step  406 | loss: 1.2468 | lr 4.6007e-04 | norm: 0.4495 | dt: 2136.88ms | tok/sec: 245352.08 |\n",
            "| step  407 | loss: 1.2520 | lr 4.5925e-04 | norm: 0.4382 | dt: 2136.05ms | tok/sec: 245447.25 |\n",
            "| step  408 | loss: 1.2613 | lr 4.5842e-04 | norm: 0.4575 | dt: 2136.25ms | tok/sec: 245423.94 |\n",
            "| step  409 | loss: 1.2550 | lr 4.5759e-04 | norm: 0.4897 | dt: 2136.80ms | tok/sec: 245361.47 |\n",
            "| step  410 | loss: 1.2645 | lr 4.5676e-04 | norm: 0.5799 | dt: 2136.23ms | tok/sec: 245427.25 |\n",
            "| step  411 | loss: 1.2546 | lr 4.5592e-04 | norm: 0.6096 | dt: 2138.06ms | tok/sec: 245216.90 |\n",
            "| step  412 | loss: 1.2552 | lr 4.5509e-04 | norm: 0.5973 | dt: 2138.10ms | tok/sec: 245212.55 |\n",
            "| step  413 | loss: 1.2693 | lr 4.5425e-04 | norm: 0.6737 | dt: 2136.65ms | tok/sec: 245378.28 |\n",
            "| step  414 | loss: 1.2488 | lr 4.5342e-04 | norm: 0.6016 | dt: 2137.56ms | tok/sec: 245274.47 |\n",
            "| step  415 | loss: 1.2382 | lr 4.5258e-04 | norm: 0.5617 | dt: 2136.93ms | tok/sec: 245346.86 |\n",
            "| step  416 | loss: 1.2303 | lr 4.5174e-04 | norm: 0.5445 | dt: 2137.54ms | tok/sec: 245275.84 |\n",
            "| step  417 | loss: 1.2358 | lr 4.5089e-04 | norm: 0.5658 | dt: 2137.24ms | tok/sec: 245311.11 |\n",
            "| step  418 | loss: 1.2309 | lr 4.5005e-04 | norm: 0.5616 | dt: 2137.00ms | tok/sec: 245337.80 |\n",
            "| step  419 | loss: 1.2329 | lr 4.4921e-04 | norm: 0.5304 | dt: 2136.26ms | tok/sec: 245423.83 |\n",
            "| step  420 | loss: 1.2323 | lr 4.4836e-04 | norm: 0.5689 | dt: 2137.12ms | tok/sec: 245324.82 |\n",
            "| step  421 | loss: 1.2305 | lr 4.4751e-04 | norm: 0.5216 | dt: 2136.32ms | tok/sec: 245416.10 |\n",
            "| step  422 | loss: 1.2194 | lr 4.4666e-04 | norm: 0.4883 | dt: 2136.99ms | tok/sec: 245340.01 |\n",
            "| step  423 | loss: 1.2364 | lr 4.4581e-04 | norm: 0.6347 | dt: 2137.03ms | tok/sec: 245334.37 |\n",
            "| step  424 | loss: 1.2247 | lr 4.4496e-04 | norm: 0.6061 | dt: 2137.56ms | tok/sec: 245274.01 |\n",
            "| step  425 | loss: 1.2281 | lr 4.4411e-04 | norm: 0.4817 | dt: 2138.69ms | tok/sec: 245144.05 |\n",
            "| step  426 | loss: 1.2104 | lr 4.4325e-04 | norm: 0.5568 | dt: 2136.25ms | tok/sec: 245424.73 |\n",
            "| step  427 | loss: 1.2161 | lr 4.4240e-04 | norm: 0.5104 | dt: 2136.09ms | tok/sec: 245442.54 |\n",
            "| step  428 | loss: 1.2179 | lr 4.4154e-04 | norm: 0.5198 | dt: 2137.75ms | tok/sec: 245251.88 |\n",
            "| step  429 | loss: 1.2090 | lr 4.4068e-04 | norm: 0.4672 | dt: 2137.01ms | tok/sec: 245336.65 |\n",
            "| step  430 | loss: 1.2184 | lr 4.3982e-04 | norm: 0.4622 | dt: 2135.50ms | tok/sec: 245510.74 |\n",
            "| step  431 | loss: 1.1998 | lr 4.3896e-04 | norm: 0.4595 | dt: 2138.48ms | tok/sec: 245168.56 |\n",
            "| step  432 | loss: 1.1962 | lr 4.3809e-04 | norm: 0.5059 | dt: 2138.54ms | tok/sec: 245161.54 |\n",
            "| step  433 | loss: 1.2122 | lr 4.3723e-04 | norm: 0.5054 | dt: 2136.82ms | tok/sec: 245358.98 |\n",
            "| step  434 | loss: 1.1845 | lr 4.3636e-04 | norm: 0.4346 | dt: 2136.97ms | tok/sec: 245342.28 |\n",
            "| step  435 | loss: 1.1857 | lr 4.3550e-04 | norm: 0.4864 | dt: 2137.29ms | tok/sec: 245305.42 |\n",
            "| step  436 | loss: 1.1896 | lr 4.3463e-04 | norm: 0.5603 | dt: 2137.18ms | tok/sec: 245317.57 |\n",
            "| step  437 | loss: 1.1882 | lr 4.3376e-04 | norm: 0.5128 | dt: 2136.08ms | tok/sec: 245444.26 |\n",
            "| step  438 | loss: 1.1857 | lr 4.3289e-04 | norm: 0.5004 | dt: 2137.06ms | tok/sec: 245331.17 |\n",
            "| step  439 | loss: 1.1945 | lr 4.3202e-04 | norm: 0.5206 | dt: 2137.03ms | tok/sec: 245335.36 |\n",
            "| step  440 | loss: 1.1722 | lr 4.3114e-04 | norm: 0.5136 | dt: 2137.00ms | tok/sec: 245337.96 |\n",
            "| step  441 | loss: 1.1804 | lr 4.3027e-04 | norm: 0.4153 | dt: 2137.73ms | tok/sec: 245254.15 |\n",
            "| step  442 | loss: 1.1734 | lr 4.2939e-04 | norm: 0.4785 | dt: 2136.79ms | tok/sec: 245362.65 |\n",
            "| step  443 | loss: 1.1726 | lr 4.2852e-04 | norm: 0.5157 | dt: 2136.46ms | tok/sec: 245400.03 |\n",
            "| step  444 | loss: 1.1744 | lr 4.2764e-04 | norm: 0.4786 | dt: 2136.86ms | tok/sec: 245354.85 |\n",
            "| step  445 | loss: 1.1748 | lr 4.2676e-04 | norm: 0.4287 | dt: 2135.65ms | tok/sec: 245493.80 |\n",
            "| step  446 | loss: 1.1558 | lr 4.2588e-04 | norm: 0.5019 | dt: 2136.68ms | tok/sec: 245375.46 |\n",
            "| step  447 | loss: 1.1835 | lr 4.2500e-04 | norm: 0.5763 | dt: 2137.30ms | tok/sec: 245303.53 |\n",
            "| step  448 | loss: 1.1715 | lr 4.2411e-04 | norm: 0.6473 | dt: 2136.67ms | tok/sec: 245375.68 |\n",
            "| step  449 | loss: 1.1642 | lr 4.2323e-04 | norm: 0.5298 | dt: 2137.37ms | tok/sec: 245295.98 |\n",
            "| step  450 | loss: 1.1673 | lr 4.2235e-04 | norm: 0.4407 | dt: 2137.36ms | tok/sec: 245296.58 |\n",
            "| step  451 | loss: 1.1507 | lr 4.2146e-04 | norm: 0.3806 | dt: 2135.91ms | tok/sec: 245463.30 |\n",
            "| step  452 | loss: 1.1558 | lr 4.2057e-04 | norm: 0.3936 | dt: 2137.37ms | tok/sec: 245295.43 |\n",
            "| step  453 | loss: 1.1428 | lr 4.1968e-04 | norm: 0.3526 | dt: 2136.04ms | tok/sec: 245448.32 |\n",
            "| step  454 | loss: 1.1507 | lr 4.1879e-04 | norm: 0.4013 | dt: 2137.43ms | tok/sec: 245288.76 |\n",
            "| step  455 | loss: 1.1466 | lr 4.1790e-04 | norm: 0.4619 | dt: 2137.22ms | tok/sec: 245312.78 |\n",
            "| step  456 | loss: 1.1621 | lr 4.1701e-04 | norm: 0.5269 | dt: 2150.36ms | tok/sec: 243814.59 |\n",
            "| step  457 | loss: 1.1395 | lr 4.1612e-04 | norm: 0.5128 | dt: 2135.41ms | tok/sec: 245521.40 |\n",
            "| step  458 | loss: 1.1449 | lr 4.1523e-04 | norm: 0.5618 | dt: 2137.67ms | tok/sec: 245261.75 |\n",
            "| step  459 | loss: 1.1493 | lr 4.1433e-04 | norm: 0.6441 | dt: 2137.28ms | tok/sec: 245306.02 |\n",
            "| step  460 | loss: 1.1508 | lr 4.1343e-04 | norm: 0.6084 | dt: 2137.15ms | tok/sec: 245320.80 |\n",
            "| step  461 | loss: 1.1500 | lr 4.1254e-04 | norm: 0.4911 | dt: 2135.88ms | tok/sec: 245467.52 |\n",
            "| step  462 | loss: 1.1290 | lr 4.1164e-04 | norm: 0.4188 | dt: 2137.24ms | tok/sec: 245311.11 |\n",
            "| step  463 | loss: 1.1306 | lr 4.1074e-04 | norm: 0.4437 | dt: 2138.01ms | tok/sec: 245222.29 |\n",
            "| step  464 | loss: 1.1265 | lr 4.0984e-04 | norm: 0.4483 | dt: 2137.70ms | tok/sec: 245258.22 |\n",
            "| step  465 | loss: 1.1341 | lr 4.0894e-04 | norm: 0.4338 | dt: 2137.73ms | tok/sec: 245254.70 |\n",
            "| step  466 | loss: 1.1210 | lr 4.0804e-04 | norm: 0.4134 | dt: 2136.14ms | tok/sec: 245436.56 |\n",
            "| step  467 | loss: 1.1285 | lr 4.0714e-04 | norm: 0.3595 | dt: 2135.52ms | tok/sec: 245508.85 |\n",
            "| step  468 | loss: 1.1307 | lr 4.0623e-04 | norm: 0.3959 | dt: 2136.98ms | tok/sec: 245340.37 |\n",
            "| step  469 | loss: 1.1282 | lr 4.0533e-04 | norm: 0.5006 | dt: 2136.89ms | tok/sec: 245350.82 |\n",
            "| step  470 | loss: 1.1263 | lr 4.0442e-04 | norm: 0.6440 | dt: 2135.95ms | tok/sec: 245458.95 |\n",
            "| step  471 | loss: 1.1281 | lr 4.0352e-04 | norm: 0.6296 | dt: 2136.57ms | tok/sec: 245387.87 |\n",
            "| step  472 | loss: 1.1198 | lr 4.0261e-04 | norm: 0.5833 | dt: 2137.28ms | tok/sec: 245305.64 |\n",
            "| step  473 | loss: 1.1399 | lr 4.0170e-04 | norm: 0.5816 | dt: 2135.78ms | tok/sec: 245478.43 |\n",
            "| step  474 | loss: 1.1345 | lr 4.0079e-04 | norm: 0.5013 | dt: 2137.01ms | tok/sec: 245336.67 |\n",
            "| step  475 | loss: 1.1208 | lr 3.9988e-04 | norm: 0.4410 | dt: 2136.10ms | tok/sec: 245441.55 |\n",
            "| step  476 | loss: 1.1177 | lr 3.9897e-04 | norm: 0.3566 | dt: 2137.34ms | tok/sec: 245299.67 |\n",
            "| step  477 | loss: 1.1163 | lr 3.9806e-04 | norm: 0.3789 | dt: 2137.97ms | tok/sec: 245226.99 |\n",
            "| step  478 | loss: 1.1070 | lr 3.9715e-04 | norm: 0.4144 | dt: 2136.11ms | tok/sec: 245440.97 |\n",
            "| step  479 | loss: 1.1002 | lr 3.9623e-04 | norm: 0.4021 | dt: 2136.72ms | tok/sec: 245370.34 |\n",
            "| step  480 | loss: 1.1009 | lr 3.9532e-04 | norm: 0.4073 | dt: 2136.36ms | tok/sec: 245412.13 |\n",
            "| step  481 | loss: 1.0946 | lr 3.9440e-04 | norm: 0.4238 | dt: 2136.71ms | tok/sec: 245371.30 |\n",
            "| step  482 | loss: 1.1009 | lr 3.9349e-04 | norm: 0.4574 | dt: 2137.53ms | tok/sec: 245277.07 |\n",
            "| step  483 | loss: 1.1006 | lr 3.9257e-04 | norm: 0.4471 | dt: 2136.89ms | tok/sec: 245351.13 |\n",
            "| step  484 | loss: 1.1122 | lr 3.9165e-04 | norm: 0.4715 | dt: 2136.14ms | tok/sec: 245437.33 |\n",
            "| step  485 | loss: 1.0942 | lr 3.9074e-04 | norm: 0.4908 | dt: 2136.55ms | tok/sec: 245390.14 |\n",
            "| step  486 | loss: 1.1025 | lr 3.8982e-04 | norm: 0.4844 | dt: 2137.45ms | tok/sec: 245287.11 |\n",
            "| step  487 | loss: 1.0914 | lr 3.8890e-04 | norm: 0.4637 | dt: 2137.30ms | tok/sec: 245304.24 |\n",
            "| step  488 | loss: 1.1011 | lr 3.8798e-04 | norm: 0.3890 | dt: 2137.25ms | tok/sec: 245309.58 |\n",
            "| step  489 | loss: 1.0988 | lr 3.8706e-04 | norm: 0.3963 | dt: 2137.80ms | tok/sec: 245246.00 |\n",
            "| step  490 | loss: 1.1172 | lr 3.8614e-04 | norm: 0.5034 | dt: 2135.13ms | tok/sec: 245553.70 |\n",
            "| step  491 | loss: 1.0963 | lr 3.8521e-04 | norm: 0.5466 | dt: 2136.65ms | tok/sec: 245378.15 |\n",
            "| step  492 | loss: 1.1035 | lr 3.8429e-04 | norm: 0.5047 | dt: 2137.11ms | tok/sec: 245325.92 |\n",
            "| step  493 | loss: 1.0904 | lr 3.8337e-04 | norm: 0.4601 | dt: 2137.67ms | tok/sec: 245261.01 |\n",
            "| step  494 | loss: 1.0847 | lr 3.8244e-04 | norm: 0.4375 | dt: 2136.18ms | tok/sec: 245432.04 |\n",
            "| step  495 | loss: 1.0596 | lr 3.8152e-04 | norm: 0.4468 | dt: 2138.09ms | tok/sec: 245212.88 |\n",
            "| step  496 | loss: 1.0843 | lr 3.8059e-04 | norm: 0.4337 | dt: 2136.72ms | tok/sec: 245370.73 |\n",
            "| step  497 | loss: 1.0848 | lr 3.7967e-04 | norm: 0.3795 | dt: 2135.40ms | tok/sec: 245522.14 |\n",
            "| step  498 | loss: 1.0752 | lr 3.7874e-04 | norm: 0.3749 | dt: 2136.78ms | tok/sec: 245363.66 |\n",
            "| step  499 | loss: 1.0856 | lr 3.7781e-04 | norm: 0.4140 | dt: 2137.28ms | tok/sec: 245306.62 |\n",
            "| step  500 | loss: 1.0723 | lr 3.7689e-04 | norm: 0.4233 | dt: 2135.76ms | tok/sec: 245480.76 |\n",
            "| step  501 | loss: 1.0740 | lr 3.7596e-04 | norm: 0.3591 | dt: 2137.68ms | tok/sec: 245260.74 |\n",
            "| step  502 | loss: 1.0811 | lr 3.7503e-04 | norm: 0.3483 | dt: 2135.85ms | tok/sec: 245469.93 |\n",
            "| step  503 | loss: 1.0703 | lr 3.7410e-04 | norm: 0.3800 | dt: 2135.86ms | tok/sec: 245469.69 |\n",
            "| step  504 | loss: 1.0725 | lr 3.7317e-04 | norm: 0.3638 | dt: 2136.00ms | tok/sec: 245452.97 |\n",
            "| step  505 | loss: 1.0578 | lr 3.7224e-04 | norm: 0.4283 | dt: 2137.16ms | tok/sec: 245319.98 |\n",
            "| step  506 | loss: 1.0757 | lr 3.7131e-04 | norm: 0.4757 | dt: 2136.28ms | tok/sec: 245421.50 |\n",
            "| step  507 | loss: 1.0655 | lr 3.7037e-04 | norm: 0.4219 | dt: 2135.03ms | tok/sec: 245564.34 |\n",
            "| step  508 | loss: 1.0598 | lr 3.6944e-04 | norm: 0.3777 | dt: 2135.84ms | tok/sec: 245471.85 |\n",
            "| step  509 | loss: 1.0669 | lr 3.6851e-04 | norm: 0.3758 | dt: 2137.41ms | tok/sec: 245291.79 |\n",
            "| step  510 | loss: 1.0603 | lr 3.6758e-04 | norm: 0.3806 | dt: 2136.25ms | tok/sec: 245424.13 |\n",
            "| step  511 | loss: 1.0587 | lr 3.6664e-04 | norm: 0.3634 | dt: 2136.53ms | tok/sec: 245391.98 |\n",
            "| step  512 | loss: 1.0532 | lr 3.6571e-04 | norm: 0.4457 | dt: 2137.25ms | tok/sec: 245309.25 |\n",
            "| step  513 | loss: 1.0719 | lr 3.6477e-04 | norm: 0.4601 | dt: 2136.63ms | tok/sec: 245381.27 |\n",
            "| step  514 | loss: 1.0606 | lr 3.6384e-04 | norm: 0.4598 | dt: 2136.70ms | tok/sec: 245372.81 |\n",
            "| step  515 | loss: 1.0599 | lr 3.6290e-04 | norm: 0.4671 | dt: 2137.12ms | tok/sec: 245325.01 |\n",
            "| step  516 | loss: 1.0615 | lr 3.6197e-04 | norm: 0.4746 | dt: 2135.65ms | tok/sec: 245493.36 |\n",
            "| step  517 | loss: 1.0670 | lr 3.6103e-04 | norm: 0.3840 | dt: 2135.53ms | tok/sec: 245507.70 |\n",
            "| step  518 | loss: 1.0528 | lr 3.6010e-04 | norm: 0.3942 | dt: 2137.17ms | tok/sec: 245319.35 |\n",
            "| step  519 | loss: 1.0449 | lr 3.5916e-04 | norm: 0.4640 | dt: 2136.76ms | tok/sec: 245365.94 |\n",
            "| step  520 | loss: 1.0486 | lr 3.5822e-04 | norm: 0.4656 | dt: 2137.56ms | tok/sec: 245274.56 |\n",
            "| step  521 | loss: 1.0329 | lr 3.5729e-04 | norm: 0.3857 | dt: 2135.78ms | tok/sec: 245478.02 |\n",
            "| step  522 | loss: 1.0457 | lr 3.5635e-04 | norm: 0.4054 | dt: 2136.78ms | tok/sec: 245363.99 |\n",
            "| step  523 | loss: 1.0354 | lr 3.5541e-04 | norm: 0.4661 | dt: 2136.13ms | tok/sec: 245438.45 |\n",
            "| step  524 | loss: 1.0590 | lr 3.5447e-04 | norm: 0.5259 | dt: 2137.12ms | tok/sec: 245324.08 |\n",
            "| step  525 | loss: 1.0637 | lr 3.5353e-04 | norm: 0.5600 | dt: 2136.52ms | tok/sec: 245393.56 |\n",
            "| step  526 | loss: 1.0433 | lr 3.5259e-04 | norm: 0.5045 | dt: 2137.65ms | tok/sec: 245263.48 |\n",
            "| step  527 | loss: 1.0438 | lr 3.5165e-04 | norm: 0.4642 | dt: 2136.00ms | tok/sec: 245453.58 |\n",
            "| step  528 | loss: 1.0554 | lr 3.5071e-04 | norm: 0.4021 | dt: 2139.14ms | tok/sec: 245092.71 |\n",
            "| step  529 | loss: 1.0487 | lr 3.4977e-04 | norm: 0.3935 | dt: 2137.98ms | tok/sec: 245226.01 |\n",
            "| step  530 | loss: 1.0406 | lr 3.4883e-04 | norm: 0.3538 | dt: 2137.64ms | tok/sec: 245264.93 |\n",
            "| step  531 | loss: 1.0396 | lr 3.4789e-04 | norm: 0.3680 | dt: 2136.23ms | tok/sec: 245426.92 |\n",
            "| step  532 | loss: 1.0392 | lr 3.4695e-04 | norm: 0.3565 | dt: 2136.08ms | tok/sec: 245443.55 |\n",
            "| step  533 | loss: 1.0490 | lr 3.4601e-04 | norm: 0.3227 | dt: 2137.39ms | tok/sec: 245293.38 |\n",
            "| step  534 | loss: 1.0303 | lr 3.4507e-04 | norm: 0.3021 | dt: 2136.66ms | tok/sec: 245377.87 |\n",
            "| step  535 | loss: 1.0284 | lr 3.4413e-04 | norm: 0.2834 | dt: 2136.82ms | tok/sec: 245358.90 |\n",
            "| step  536 | loss: 1.0317 | lr 3.4319e-04 | norm: 0.2977 | dt: 2137.27ms | tok/sec: 245307.58 |\n",
            "| step  537 | loss: 1.0202 | lr 3.4225e-04 | norm: 0.3097 | dt: 2135.86ms | tok/sec: 245469.08 |\n",
            "| step  538 | loss: 1.0226 | lr 3.4131e-04 | norm: 0.3598 | dt: 2136.37ms | tok/sec: 245410.82 |\n",
            "| step  539 | loss: 1.0230 | lr 3.4036e-04 | norm: 0.4134 | dt: 2137.09ms | tok/sec: 245328.02 |\n",
            "| step  540 | loss: 1.0147 | lr 3.3942e-04 | norm: 0.4427 | dt: 2136.78ms | tok/sec: 245364.07 |\n",
            "| step  541 | loss: 1.0296 | lr 3.3848e-04 | norm: 0.4235 | dt: 2137.20ms | tok/sec: 245314.89 |\n",
            "| step  542 | loss: 1.0309 | lr 3.3754e-04 | norm: 0.3543 | dt: 2136.68ms | tok/sec: 245374.94 |\n",
            "| step  543 | loss: 1.0221 | lr 3.3660e-04 | norm: 0.3022 | dt: 2137.58ms | tok/sec: 245271.93 |\n",
            "| step  544 | loss: 1.0114 | lr 3.3565e-04 | norm: 0.3181 | dt: 2137.93ms | tok/sec: 245231.20 |\n",
            "| step  545 | loss: 1.0156 | lr 3.3471e-04 | norm: 0.3222 | dt: 2136.71ms | tok/sec: 245372.07 |\n",
            "| step  546 | loss: 1.0088 | lr 3.3377e-04 | norm: 0.3803 | dt: 2136.86ms | tok/sec: 245354.44 |\n",
            "| step  547 | loss: 1.0287 | lr 3.3283e-04 | norm: 0.4963 | dt: 2136.08ms | tok/sec: 245443.71 |\n",
            "| step  548 | loss: 1.0176 | lr 3.3188e-04 | norm: 0.6141 | dt: 2137.22ms | tok/sec: 245312.86 |\n",
            "| step  549 | loss: 1.0294 | lr 3.3094e-04 | norm: 0.5827 | dt: 2136.36ms | tok/sec: 245411.75 |\n",
            "| step  550 | loss: 1.0164 | lr 3.3000e-04 | norm: 0.4452 | dt: 2139.47ms | tok/sec: 245055.18 |\n",
            "| step  551 | loss: 1.0181 | lr 3.2906e-04 | norm: 0.4378 | dt: 2136.93ms | tok/sec: 245346.75 |\n",
            "| step  552 | loss: 1.0217 | lr 3.2812e-04 | norm: 0.4149 | dt: 2136.95ms | tok/sec: 245344.34 |\n",
            "| step  553 | loss: 1.0076 | lr 3.2717e-04 | norm: 0.3740 | dt: 2137.27ms | tok/sec: 245306.84 |\n",
            "| step  554 | loss: 1.0150 | lr 3.2623e-04 | norm: 0.3922 | dt: 2135.65ms | tok/sec: 245492.95 |\n",
            "| step  555 | loss: 1.0150 | lr 3.2529e-04 | norm: 0.3567 | dt: 2136.43ms | tok/sec: 245404.22 |\n",
            "| step  556 | loss: 0.9994 | lr 3.2435e-04 | norm: 0.3572 | dt: 2137.02ms | tok/sec: 245335.88 |\n",
            "| step  557 | loss: 1.0072 | lr 3.2340e-04 | norm: 0.3407 | dt: 2137.17ms | tok/sec: 245319.27 |\n",
            "| step  558 | loss: 1.0104 | lr 3.2246e-04 | norm: 0.3087 | dt: 2135.96ms | tok/sec: 245458.04 |\n",
            "| step  559 | loss: 0.9940 | lr 3.2152e-04 | norm: 0.3075 | dt: 2135.08ms | tok/sec: 245559.02 |\n",
            "| step  560 | loss: 0.9925 | lr 3.2058e-04 | norm: 0.2805 | dt: 2136.62ms | tok/sec: 245381.84 |\n",
            "| step  561 | loss: 0.9966 | lr 3.1964e-04 | norm: 0.3248 | dt: 2137.02ms | tok/sec: 245335.58 |\n",
            "| step  562 | loss: 1.0009 | lr 3.1869e-04 | norm: 0.3135 | dt: 2137.54ms | tok/sec: 245276.85 |\n",
            "| step  563 | loss: 1.0106 | lr 3.1775e-04 | norm: 0.3447 | dt: 2137.05ms | tok/sec: 245333.14 |\n",
            "| step  564 | loss: 1.0047 | lr 3.1681e-04 | norm: 0.3703 | dt: 2135.77ms | tok/sec: 245479.99 |\n",
            "| step  565 | loss: 1.0048 | lr 3.1587e-04 | norm: 0.3528 | dt: 2135.58ms | tok/sec: 245501.15 |\n",
            "| step  566 | loss: 1.0024 | lr 3.1493e-04 | norm: 0.3277 | dt: 2135.06ms | tok/sec: 245561.02 |\n",
            "| step  567 | loss: 0.9999 | lr 3.1399e-04 | norm: 0.3424 | dt: 2136.34ms | tok/sec: 245414.65 |\n",
            "| step  568 | loss: 0.9922 | lr 3.1305e-04 | norm: 0.3914 | dt: 2136.43ms | tok/sec: 245404.27 |\n",
            "| step  569 | loss: 0.9954 | lr 3.1211e-04 | norm: 0.4298 | dt: 2135.56ms | tok/sec: 245503.23 |\n",
            "| step  570 | loss: 0.9878 | lr 3.1117e-04 | norm: 0.4634 | dt: 2136.50ms | tok/sec: 245395.59 |\n",
            "| step  571 | loss: 0.9967 | lr 3.1023e-04 | norm: 0.4445 | dt: 2136.36ms | tok/sec: 245411.50 |\n",
            "| step  572 | loss: 1.0000 | lr 3.0929e-04 | norm: 0.3640 | dt: 2136.43ms | tok/sec: 245403.75 |\n",
            "| step  573 | loss: 0.9994 | lr 3.0835e-04 | norm: 0.4097 | dt: 2136.48ms | tok/sec: 245398.52 |\n",
            "| step  574 | loss: 0.9848 | lr 3.0741e-04 | norm: 0.4291 | dt: 2136.48ms | tok/sec: 245398.60 |\n",
            "| step  575 | loss: 0.9902 | lr 3.0647e-04 | norm: 0.3990 | dt: 2137.39ms | tok/sec: 245293.30 |\n",
            "| step  576 | loss: 0.9884 | lr 3.0553e-04 | norm: 0.3620 | dt: 2137.45ms | tok/sec: 245286.73 |\n",
            "| step  577 | loss: 0.9958 | lr 3.0459e-04 | norm: 0.3407 | dt: 2137.62ms | tok/sec: 245267.58 |\n",
            "| step  578 | loss: 0.9795 | lr 3.0365e-04 | norm: 0.3066 | dt: 2138.18ms | tok/sec: 245203.42 |\n",
            "| step  579 | loss: 0.9946 | lr 3.0271e-04 | norm: 0.3120 | dt: 2136.67ms | tok/sec: 245375.74 |\n",
            "| step  580 | loss: 0.9908 | lr 3.0178e-04 | norm: 0.3146 | dt: 2136.78ms | tok/sec: 245364.13 |\n",
            "| step  581 | loss: 0.9867 | lr 3.0084e-04 | norm: 0.3224 | dt: 2138.48ms | tok/sec: 245168.32 |\n",
            "| step  582 | loss: 0.9808 | lr 2.9990e-04 | norm: 0.3232 | dt: 2136.25ms | tok/sec: 245423.99 |\n",
            "| step  583 | loss: 0.9774 | lr 2.9897e-04 | norm: 0.3304 | dt: 2136.21ms | tok/sec: 245428.78 |\n",
            "| step  584 | loss: 0.9774 | lr 2.9803e-04 | norm: 0.3684 | dt: 2136.70ms | tok/sec: 245373.14 |\n",
            "| step  585 | loss: 0.9800 | lr 2.9710e-04 | norm: 0.3789 | dt: 2136.77ms | tok/sec: 245364.70 |\n",
            "| step  586 | loss: 0.9762 | lr 2.9616e-04 | norm: 0.3737 | dt: 2136.36ms | tok/sec: 245412.27 |\n",
            "| step  587 | loss: 0.9748 | lr 2.9523e-04 | norm: 0.3487 | dt: 2136.25ms | tok/sec: 245424.40 |\n",
            "| step  588 | loss: 0.9751 | lr 2.9429e-04 | norm: 0.3463 | dt: 2136.58ms | tok/sec: 245386.66 |\n",
            "| step  589 | loss: 0.9778 | lr 2.9336e-04 | norm: 0.3328 | dt: 2136.57ms | tok/sec: 245387.84 |\n",
            "| step  590 | loss: 0.9820 | lr 2.9242e-04 | norm: 0.3424 | dt: 2135.99ms | tok/sec: 245453.88 |\n",
            "| step  591 | loss: 0.9799 | lr 2.9149e-04 | norm: 0.3460 | dt: 2134.97ms | tok/sec: 245571.58 |\n",
            "| step  592 | loss: 0.9767 | lr 2.9056e-04 | norm: 0.3495 | dt: 2135.96ms | tok/sec: 245458.34 |\n",
            "| step  593 | loss: 0.9633 | lr 2.8963e-04 | norm: 0.3216 | dt: 2136.91ms | tok/sec: 245348.88 |\n",
            "| step  594 | loss: 0.9697 | lr 2.8869e-04 | norm: 0.3331 | dt: 2137.33ms | tok/sec: 245300.14 |\n",
            "| step  595 | loss: 0.9806 | lr 2.8776e-04 | norm: 0.3509 | dt: 2137.46ms | tok/sec: 245285.58 |\n",
            "| step  596 | loss: 0.9724 | lr 2.8683e-04 | norm: 0.3467 | dt: 2136.84ms | tok/sec: 245357.07 |\n",
            "| step  597 | loss: 0.9759 | lr 2.8590e-04 | norm: 0.3371 | dt: 2135.50ms | tok/sec: 245510.58 |\n",
            "| step  598 | loss: 0.9781 | lr 2.8497e-04 | norm: 0.3419 | dt: 2136.72ms | tok/sec: 245369.91 |\n",
            "| step  599 | loss: 0.9600 | lr 2.8404e-04 | norm: 0.3223 | dt: 2136.03ms | tok/sec: 245449.58 |\n",
            "| step  600 | loss: 0.9841 | lr 2.8311e-04 | norm: 0.3146 | dt: 2136.18ms | tok/sec: 245431.93 |\n",
            "| step  601 | loss: 0.9785 | lr 2.8219e-04 | norm: 0.3723 | dt: 2136.65ms | tok/sec: 245379.08 |\n",
            "| step  602 | loss: 0.9585 | lr 2.8126e-04 | norm: 0.3976 | dt: 2135.71ms | tok/sec: 245486.02 |\n",
            "| step  603 | loss: 0.9617 | lr 2.8033e-04 | norm: 0.4099 | dt: 2137.59ms | tok/sec: 245270.59 |\n",
            "| step  604 | loss: 0.9681 | lr 2.7941e-04 | norm: 0.4324 | dt: 2137.33ms | tok/sec: 245300.68 |\n",
            "| step  605 | loss: 0.9671 | lr 2.7848e-04 | norm: 0.3512 | dt: 2135.83ms | tok/sec: 245472.67 |\n",
            "| step  606 | loss: 0.9598 | lr 2.7756e-04 | norm: 0.3201 | dt: 2136.69ms | tok/sec: 245374.23 |\n",
            "| step  607 | loss: 0.9642 | lr 2.7663e-04 | norm: 0.3854 | dt: 2135.94ms | tok/sec: 245459.91 |\n",
            "| step  608 | loss: 0.9718 | lr 2.7571e-04 | norm: 0.3690 | dt: 2136.32ms | tok/sec: 245416.95 |\n",
            "| step  609 | loss: 0.9573 | lr 2.7479e-04 | norm: 0.3271 | dt: 2137.10ms | tok/sec: 245327.34 |\n",
            "| step  610 | loss: 0.9708 | lr 2.7386e-04 | norm: 0.2833 | dt: 2136.86ms | tok/sec: 245354.90 |\n",
            "| step  611 | loss: 0.9507 | lr 2.7294e-04 | norm: 0.3001 | dt: 2136.96ms | tok/sec: 245342.56 |\n",
            "| step  612 | loss: 0.9677 | lr 2.7202e-04 | norm: 0.2970 | dt: 2136.39ms | tok/sec: 245408.60 |\n",
            "| step  613 | loss: 0.9588 | lr 2.7110e-04 | norm: 0.3076 | dt: 2136.19ms | tok/sec: 245430.89 |\n",
            "| step  614 | loss: 0.9640 | lr 2.7018e-04 | norm: 0.3157 | dt: 2136.42ms | tok/sec: 245404.46 |\n",
            "| step  615 | loss: 0.9532 | lr 2.6926e-04 | norm: 0.3214 | dt: 2136.37ms | tok/sec: 245411.04 |\n",
            "| step  616 | loss: 0.9622 | lr 2.6835e-04 | norm: 0.3331 | dt: 2137.33ms | tok/sec: 245300.66 |\n",
            "| step  617 | loss: 0.9639 | lr 2.6743e-04 | norm: 0.3226 | dt: 2137.43ms | tok/sec: 245288.97 |\n",
            "| step  618 | loss: 0.9523 | lr 2.6651e-04 | norm: 0.3200 | dt: 2137.27ms | tok/sec: 245307.12 |\n",
            "| step  619 | loss: 0.9448 | lr 2.6560e-04 | norm: 0.3160 | dt: 2135.85ms | tok/sec: 245470.43 |\n",
            "| step  620 | loss: 0.9570 | lr 2.6468e-04 | norm: 0.3226 | dt: 2137.35ms | tok/sec: 245297.98 |\n",
            "| step  621 | loss: 0.9535 | lr 2.6377e-04 | norm: 0.2924 | dt: 2136.06ms | tok/sec: 245446.15 |\n",
            "| step  622 | loss: 0.9442 | lr 2.6285e-04 | norm: 0.3271 | dt: 2136.45ms | tok/sec: 245402.05 |\n",
            "| step  623 | loss: 0.9526 | lr 2.6194e-04 | norm: 0.3799 | dt: 2136.72ms | tok/sec: 245370.81 |\n",
            "| step  624 | loss: 0.9480 | lr 2.6103e-04 | norm: 0.4159 | dt: 2137.48ms | tok/sec: 245282.93 |\n",
            "| step  625 | loss: 0.9547 | lr 2.6012e-04 | norm: 0.4016 | dt: 2136.61ms | tok/sec: 245383.40 |\n",
            "| step  626 | loss: 0.9450 | lr 2.5921e-04 | norm: 0.3682 | dt: 2137.22ms | tok/sec: 245313.11 |\n",
            "| step  627 | loss: 0.9506 | lr 2.5830e-04 | norm: 0.3416 | dt: 2136.78ms | tok/sec: 245363.72 |\n",
            "| step  628 | loss: 0.9506 | lr 2.5739e-04 | norm: 0.3310 | dt: 2137.25ms | tok/sec: 245309.44 |\n",
            "| step  629 | loss: 0.9320 | lr 2.5648e-04 | norm: 0.3111 | dt: 2136.65ms | tok/sec: 245378.89 |\n",
            "| step  630 | loss: 0.9511 | lr 2.5558e-04 | norm: 0.3084 | dt: 2136.48ms | tok/sec: 245398.16 |\n",
            "| step  631 | loss: 0.9484 | lr 2.5467e-04 | norm: 0.2855 | dt: 2136.02ms | tok/sec: 245450.51 |\n",
            "| step  632 | loss: 0.9355 | lr 2.5377e-04 | norm: 0.2808 | dt: 2136.52ms | tok/sec: 245393.70 |\n",
            "| step  633 | loss: 0.9329 | lr 2.5286e-04 | norm: 0.2817 | dt: 2136.68ms | tok/sec: 245375.41 |\n",
            "| step  634 | loss: 0.9513 | lr 2.5196e-04 | norm: 0.2915 | dt: 2136.54ms | tok/sec: 245391.54 |\n",
            "| step  635 | loss: 0.9454 | lr 2.5106e-04 | norm: 0.3111 | dt: 2137.87ms | tok/sec: 245238.61 |\n",
            "| step  636 | loss: 0.9294 | lr 2.5016e-04 | norm: 0.3290 | dt: 2137.14ms | tok/sec: 245322.41 |\n",
            "| step  637 | loss: 0.9447 | lr 2.4926e-04 | norm: 0.3357 | dt: 2136.01ms | tok/sec: 245452.43 |\n",
            "| step  638 | loss: 0.9358 | lr 2.4836e-04 | norm: 0.3509 | dt: 2136.67ms | tok/sec: 245375.66 |\n",
            "| step  639 | loss: 0.9367 | lr 2.4746e-04 | norm: 0.3488 | dt: 2136.71ms | tok/sec: 245371.52 |\n",
            "| step  640 | loss: 0.9327 | lr 2.4657e-04 | norm: 0.3108 | dt: 2136.12ms | tok/sec: 245439.74 |\n",
            "| step  641 | loss: 0.9334 | lr 2.4567e-04 | norm: 0.3040 | dt: 2136.89ms | tok/sec: 245351.18 |\n",
            "| step  642 | loss: 0.9339 | lr 2.4477e-04 | norm: 0.2851 | dt: 2136.17ms | tok/sec: 245433.61 |\n",
            "| step  643 | loss: 0.9364 | lr 2.4388e-04 | norm: 0.2732 | dt: 2137.84ms | tok/sec: 245241.92 |\n",
            "| step  644 | loss: 0.9299 | lr 2.4299e-04 | norm: 0.2840 | dt: 2136.43ms | tok/sec: 245404.00 |\n",
            "| step  645 | loss: 0.9283 | lr 2.4210e-04 | norm: 0.2786 | dt: 2136.91ms | tok/sec: 245348.17 |\n",
            "| step  646 | loss: 0.9316 | lr 2.4121e-04 | norm: 0.2693 | dt: 2136.44ms | tok/sec: 245402.38 |\n",
            "| step  647 | loss: 0.9351 | lr 2.4032e-04 | norm: 0.2667 | dt: 2136.02ms | tok/sec: 245450.56 |\n",
            "| step  648 | loss: 0.9342 | lr 2.3943e-04 | norm: 0.2668 | dt: 2137.84ms | tok/sec: 245242.25 |\n",
            "| step  649 | loss: 0.9268 | lr 2.3854e-04 | norm: 0.2923 | dt: 2136.94ms | tok/sec: 245345.08 |\n",
            "| step  650 | loss: 0.9229 | lr 2.3765e-04 | norm: 0.3054 | dt: 2136.29ms | tok/sec: 245420.02 |\n",
            "| step  651 | loss: 0.9217 | lr 2.3677e-04 | norm: 0.3392 | dt: 2135.92ms | tok/sec: 245462.84 |\n",
            "| step  652 | loss: 0.9280 | lr 2.3589e-04 | norm: 0.3668 | dt: 2137.49ms | tok/sec: 245282.52 |\n",
            "| step  653 | loss: 0.9337 | lr 2.3500e-04 | norm: 0.3757 | dt: 2136.31ms | tok/sec: 245417.25 |\n",
            "| step  654 | loss: 0.9185 | lr 2.3412e-04 | norm: 0.3578 | dt: 2137.68ms | tok/sec: 245260.03 |\n",
            "| step  655 | loss: 0.9369 | lr 2.3324e-04 | norm: 0.3514 | dt: 2137.11ms | tok/sec: 245325.31 |\n",
            "| step  656 | loss: 0.9345 | lr 2.3236e-04 | norm: 0.3741 | dt: 2135.77ms | tok/sec: 245479.17 |\n",
            "| step  657 | loss: 0.9235 | lr 2.3148e-04 | norm: 0.3163 | dt: 2137.05ms | tok/sec: 245332.10 |\n",
            "| step  658 | loss: 0.9219 | lr 2.3061e-04 | norm: 0.2793 | dt: 2138.60ms | tok/sec: 245154.24 |\n",
            "| step  659 | loss: 0.9219 | lr 2.2973e-04 | norm: 0.3083 | dt: 2135.29ms | tok/sec: 245535.28 |\n",
            "| step  660 | loss: 0.9231 | lr 2.2886e-04 | norm: 0.3146 | dt: 2136.11ms | tok/sec: 245440.04 |\n",
            "| step  661 | loss: 0.9372 | lr 2.2798e-04 | norm: 0.2925 | dt: 2138.56ms | tok/sec: 245159.79 |\n",
            "| step  662 | loss: 0.9294 | lr 2.2711e-04 | norm: 0.2992 | dt: 2136.65ms | tok/sec: 245378.42 |\n",
            "| step  663 | loss: 0.9272 | lr 2.2624e-04 | norm: 0.2995 | dt: 2135.78ms | tok/sec: 245478.92 |\n",
            "| step  664 | loss: 0.9231 | lr 2.2537e-04 | norm: 0.3124 | dt: 2136.77ms | tok/sec: 245364.51 |\n",
            "| step  665 | loss: 0.9134 | lr 2.2450e-04 | norm: 0.3368 | dt: 2136.43ms | tok/sec: 245403.70 |\n",
            "| step  666 | loss: 0.9169 | lr 2.2364e-04 | norm: 0.3324 | dt: 2137.00ms | tok/sec: 245338.40 |\n",
            "| step  667 | loss: 0.9066 | lr 2.2277e-04 | norm: 0.3319 | dt: 2137.04ms | tok/sec: 245334.13 |\n",
            "| step  668 | loss: 0.9120 | lr 2.2191e-04 | norm: 0.3676 | dt: 2137.31ms | tok/sec: 245303.26 |\n",
            "| step  669 | loss: 0.9157 | lr 2.2104e-04 | norm: 0.3367 | dt: 2135.55ms | tok/sec: 245504.77 |\n",
            "| step  670 | loss: 0.9021 | lr 2.2018e-04 | norm: 0.2949 | dt: 2138.03ms | tok/sec: 245219.69 |\n",
            "| step  671 | loss: 0.9164 | lr 2.1932e-04 | norm: 0.2750 | dt: 2136.09ms | tok/sec: 245442.34 |\n",
            "| step  672 | loss: 0.9144 | lr 2.1846e-04 | norm: 0.2677 | dt: 2137.13ms | tok/sec: 245323.51 |\n",
            "| step  673 | loss: 0.9108 | lr 2.1760e-04 | norm: 0.2938 | dt: 2136.81ms | tok/sec: 245360.21 |\n",
            "| step  674 | loss: 0.9054 | lr 2.1675e-04 | norm: 0.2867 | dt: 2137.26ms | tok/sec: 245308.84 |\n",
            "| step  675 | loss: 0.9137 | lr 2.1589e-04 | norm: 0.2943 | dt: 2136.90ms | tok/sec: 245350.14 |\n",
            "| step  676 | loss: 0.9161 | lr 2.1504e-04 | norm: 0.2887 | dt: 2136.15ms | tok/sec: 245436.48 |\n",
            "| step  677 | loss: 0.9319 | lr 2.1419e-04 | norm: 0.2748 | dt: 2136.37ms | tok/sec: 245410.43 |\n",
            "| step  678 | loss: 0.9186 | lr 2.1334e-04 | norm: 0.2640 | dt: 2137.22ms | tok/sec: 245312.70 |\n",
            "| step  679 | loss: 0.9141 | lr 2.1249e-04 | norm: 0.2637 | dt: 2136.57ms | tok/sec: 245387.90 |\n",
            "| step  680 | loss: 0.9153 | lr 2.1164e-04 | norm: 0.2648 | dt: 2137.13ms | tok/sec: 245323.32 |\n",
            "| step  681 | loss: 0.9030 | lr 2.1079e-04 | norm: 0.2673 | dt: 2136.98ms | tok/sec: 245341.00 |\n",
            "| step  682 | loss: 0.8867 | lr 2.0995e-04 | norm: 0.2724 | dt: 2136.85ms | tok/sec: 245355.78 |\n",
            "| step  683 | loss: 0.9061 | lr 2.0911e-04 | norm: 0.2556 | dt: 2136.68ms | tok/sec: 245375.49 |\n",
            "| step  684 | loss: 0.9112 | lr 2.0826e-04 | norm: 0.3011 | dt: 2137.80ms | tok/sec: 245246.76 |\n",
            "| step  685 | loss: 0.8951 | lr 2.0742e-04 | norm: 0.2758 | dt: 2135.27ms | tok/sec: 245536.67 |\n",
            "| step  686 | loss: 0.9103 | lr 2.0658e-04 | norm: 0.2672 | dt: 2154.25ms | tok/sec: 243374.16 |\n",
            "| step  687 | loss: 0.9091 | lr 2.0575e-04 | norm: 0.2825 | dt: 2136.22ms | tok/sec: 245428.37 |\n",
            "| step  688 | loss: 0.9070 | lr 2.0491e-04 | norm: 0.3077 | dt: 2137.51ms | tok/sec: 245279.51 |\n",
            "| step  689 | loss: 0.9055 | lr 2.0408e-04 | norm: 0.3170 | dt: 2136.35ms | tok/sec: 245412.73 |\n",
            "| step  690 | loss: 0.9108 | lr 2.0324e-04 | norm: 0.3400 | dt: 2135.62ms | tok/sec: 245496.30 |\n",
            "| step  691 | loss: 0.9026 | lr 2.0241e-04 | norm: 0.3312 | dt: 2137.30ms | tok/sec: 245304.00 |\n",
            "| step  692 | loss: 0.8996 | lr 2.0158e-04 | norm: 0.2982 | dt: 2136.83ms | tok/sec: 245358.35 |\n",
            "| step  693 | loss: 0.8976 | lr 2.0075e-04 | norm: 0.2778 | dt: 2135.99ms | tok/sec: 245454.26 |\n",
            "| step  694 | loss: 0.9049 | lr 1.9993e-04 | norm: 0.2914 | dt: 2136.22ms | tok/sec: 245428.13 |\n",
            "| step  695 | loss: 0.8996 | lr 1.9910e-04 | norm: 0.3013 | dt: 2136.91ms | tok/sec: 245348.80 |\n",
            "| step  696 | loss: 0.8967 | lr 1.9828e-04 | norm: 0.2581 | dt: 2137.57ms | tok/sec: 245272.86 |\n",
            "| step  697 | loss: 0.8985 | lr 1.9746e-04 | norm: 0.2741 | dt: 2136.14ms | tok/sec: 245436.62 |\n",
            "| step  698 | loss: 0.8962 | lr 1.9664e-04 | norm: 0.2796 | dt: 2136.71ms | tok/sec: 245371.66 |\n",
            "| step  699 | loss: 0.9008 | lr 1.9582e-04 | norm: 0.2850 | dt: 2136.36ms | tok/sec: 245411.94 |\n",
            "| step  700 | loss: 0.8969 | lr 1.9500e-04 | norm: 0.3082 | dt: 2136.26ms | tok/sec: 245423.58 |\n",
            "| step  701 | loss: 0.9028 | lr 1.9418e-04 | norm: 0.2845 | dt: 2137.24ms | tok/sec: 245310.67 |\n",
            "| step  702 | loss: 0.8926 | lr 1.9337e-04 | norm: 0.2759 | dt: 2138.06ms | tok/sec: 245216.30 |\n",
            "| step  703 | loss: 0.9035 | lr 1.9256e-04 | norm: 0.2887 | dt: 2136.34ms | tok/sec: 245413.72 |\n",
            "| step  704 | loss: 0.9063 | lr 1.9175e-04 | norm: 0.2753 | dt: 2137.21ms | tok/sec: 245313.68 |\n",
            "| step  705 | loss: 0.9005 | lr 1.9094e-04 | norm: 0.2792 | dt: 2137.65ms | tok/sec: 245263.20 |\n",
            "| step  706 | loss: 0.8822 | lr 1.9013e-04 | norm: 0.2914 | dt: 2136.94ms | tok/sec: 245344.99 |\n",
            "| step  707 | loss: 0.8940 | lr 1.8933e-04 | norm: 0.3187 | dt: 2136.30ms | tok/sec: 245418.32 |\n",
            "| step  708 | loss: 0.8879 | lr 1.8852e-04 | norm: 0.3033 | dt: 2136.54ms | tok/sec: 245391.10 |\n",
            "| step  709 | loss: 0.8898 | lr 1.8772e-04 | norm: 0.2853 | dt: 2135.95ms | tok/sec: 245458.81 |\n",
            "| step  710 | loss: 0.8800 | lr 1.8692e-04 | norm: 0.2801 | dt: 2137.26ms | tok/sec: 245308.29 |\n",
            "| step  711 | loss: 0.8996 | lr 1.8612e-04 | norm: 0.2872 | dt: 2135.56ms | tok/sec: 245503.64 |\n",
            "| step  712 | loss: 0.8978 | lr 1.8533e-04 | norm: 0.2808 | dt: 2137.09ms | tok/sec: 245327.45 |\n",
            "| step  713 | loss: 0.8985 | lr 1.8453e-04 | norm: 0.2771 | dt: 2135.72ms | tok/sec: 245484.84 |\n",
            "| step  714 | loss: 0.8826 | lr 1.8374e-04 | norm: 0.2923 | dt: 2135.71ms | tok/sec: 245486.92 |\n",
            "| step  715 | loss: 0.9002 | lr 1.8295e-04 | norm: 0.3163 | dt: 2135.60ms | tok/sec: 245499.48 |\n",
            "| step  716 | loss: 0.8966 | lr 1.8216e-04 | norm: 0.3159 | dt: 2136.78ms | tok/sec: 245363.06 |\n",
            "| step  717 | loss: 0.8998 | lr 1.8137e-04 | norm: 0.3054 | dt: 2136.62ms | tok/sec: 245382.31 |\n",
            "| step  718 | loss: 0.8880 | lr 1.8058e-04 | norm: 0.2850 | dt: 2137.11ms | tok/sec: 245325.72 |\n",
            "| step  719 | loss: 0.8911 | lr 1.7980e-04 | norm: 0.2710 | dt: 2136.69ms | tok/sec: 245373.71 |\n",
            "| step  720 | loss: 0.8944 | lr 1.7902e-04 | norm: 0.2906 | dt: 2136.61ms | tok/sec: 245383.16 |\n",
            "| step  721 | loss: 0.8905 | lr 1.7824e-04 | norm: 0.2805 | dt: 2136.83ms | tok/sec: 245358.19 |\n",
            "| step  722 | loss: 0.8930 | lr 1.7746e-04 | norm: 0.2538 | dt: 2136.73ms | tok/sec: 245369.55 |\n",
            "| step  723 | loss: 0.8832 | lr 1.7668e-04 | norm: 0.2586 | dt: 2136.29ms | tok/sec: 245419.42 |\n",
            "| step  724 | loss: 0.8875 | lr 1.7591e-04 | norm: 0.2661 | dt: 2136.41ms | tok/sec: 245406.52 |\n",
            "| step  725 | loss: 0.8792 | lr 1.7513e-04 | norm: 0.2649 | dt: 2137.04ms | tok/sec: 245333.88 |\n",
            "| step  726 | loss: 0.8761 | lr 1.7436e-04 | norm: 0.2674 | dt: 2135.39ms | tok/sec: 245523.87 |\n",
            "| step  727 | loss: 0.8753 | lr 1.7359e-04 | norm: 0.2909 | dt: 2135.66ms | tok/sec: 245491.75 |\n",
            "| step  728 | loss: 0.8822 | lr 1.7283e-04 | norm: 0.3169 | dt: 2136.71ms | tok/sec: 245371.77 |\n",
            "| step  729 | loss: 0.8849 | lr 1.7206e-04 | norm: 0.3507 | dt: 2135.99ms | tok/sec: 245453.96 |\n",
            "| step  730 | loss: 0.8835 | lr 1.7130e-04 | norm: 0.3775 | dt: 2135.80ms | tok/sec: 245476.51 |\n",
            "| step  731 | loss: 0.8833 | lr 1.7054e-04 | norm: 0.3527 | dt: 2136.77ms | tok/sec: 245364.62 |\n",
            "| step  732 | loss: 0.8779 | lr 1.6978e-04 | norm: 0.2952 | dt: 2136.15ms | tok/sec: 245435.99 |\n",
            "| step  733 | loss: 0.8703 | lr 1.6902e-04 | norm: 0.2528 | dt: 2136.57ms | tok/sec: 245388.20 |\n",
            "| step  734 | loss: 0.8900 | lr 1.6826e-04 | norm: 0.3158 | dt: 2136.39ms | tok/sec: 245408.90 |\n",
            "| step  735 | loss: 0.8783 | lr 1.6751e-04 | norm: 0.2681 | dt: 2137.65ms | tok/sec: 245264.19 |\n",
            "| step  736 | loss: 0.8843 | lr 1.6676e-04 | norm: 0.2731 | dt: 2136.00ms | tok/sec: 245453.08 |\n",
            "| step  737 | loss: 0.8771 | lr 1.6601e-04 | norm: 0.2956 | dt: 2136.92ms | tok/sec: 245347.29 |\n",
            "| step  738 | loss: 0.8723 | lr 1.6526e-04 | norm: 0.2581 | dt: 2136.75ms | tok/sec: 245366.76 |\n",
            "| step  739 | loss: 0.8815 | lr 1.6452e-04 | norm: 0.2687 | dt: 2136.90ms | tok/sec: 245349.54 |\n",
            "| step  740 | loss: 0.8715 | lr 1.6377e-04 | norm: 0.2866 | dt: 2136.65ms | tok/sec: 245378.37 |\n",
            "| step  741 | loss: 0.8832 | lr 1.6303e-04 | norm: 0.2598 | dt: 2137.70ms | tok/sec: 245258.14 |\n",
            "| step  742 | loss: 0.8731 | lr 1.6229e-04 | norm: 0.2745 | dt: 2135.63ms | tok/sec: 245495.89 |\n",
            "| step  743 | loss: 0.8747 | lr 1.6155e-04 | norm: 0.2616 | dt: 2137.38ms | tok/sec: 245294.50 |\n",
            "| step  744 | loss: 0.8725 | lr 1.6082e-04 | norm: 0.2896 | dt: 2135.37ms | tok/sec: 245525.19 |\n",
            "| step  745 | loss: 0.8809 | lr 1.6008e-04 | norm: 0.2652 | dt: 2136.47ms | tok/sec: 245398.71 |\n",
            "| step  746 | loss: 0.8697 | lr 1.5935e-04 | norm: 0.2736 | dt: 2137.20ms | tok/sec: 245315.08 |\n",
            "| step  747 | loss: 0.8621 | lr 1.5862e-04 | norm: 0.2731 | dt: 2136.49ms | tok/sec: 245396.55 |\n",
            "| step  748 | loss: 0.8691 | lr 1.5790e-04 | norm: 0.2842 | dt: 2137.14ms | tok/sec: 245321.78 |\n",
            "| step  749 | loss: 0.8739 | lr 1.5717e-04 | norm: 0.2660 | dt: 2136.89ms | tok/sec: 245350.80 |\n",
            "| step  750 | loss: 0.8845 | lr 1.5645e-04 | norm: 0.2890 | dt: 2136.77ms | tok/sec: 245364.70 |\n",
            "| step  751 | loss: 0.8705 | lr 1.5573e-04 | norm: 0.2753 | dt: 2137.15ms | tok/sec: 245321.54 |\n",
            "| step  752 | loss: 0.8771 | lr 1.5501e-04 | norm: 0.2750 | dt: 2135.63ms | tok/sec: 245495.53 |\n",
            "| step  753 | loss: 0.8819 | lr 1.5429e-04 | norm: 0.2803 | dt: 2136.78ms | tok/sec: 245363.72 |\n",
            "| step  754 | loss: 0.8771 | lr 1.5358e-04 | norm: 0.2667 | dt: 2134.96ms | tok/sec: 245572.57 |\n",
            "| step  755 | loss: 0.8695 | lr 1.5286e-04 | norm: 0.2629 | dt: 2136.15ms | tok/sec: 245436.13 |\n",
            "| step  756 | loss: 0.8633 | lr 1.5215e-04 | norm: 0.2893 | dt: 2136.01ms | tok/sec: 245451.82 |\n",
            "| step  757 | loss: 0.8673 | lr 1.5145e-04 | norm: 0.2799 | dt: 2136.31ms | tok/sec: 245417.25 |\n",
            "| step  758 | loss: 0.8688 | lr 1.5074e-04 | norm: 0.2779 | dt: 2134.77ms | tok/sec: 245594.45 |\n",
            "| step  759 | loss: 0.8725 | lr 1.5004e-04 | norm: 0.2758 | dt: 2137.10ms | tok/sec: 245326.90 |\n",
            "| step  760 | loss: 0.8779 | lr 1.4933e-04 | norm: 0.2802 | dt: 2136.04ms | tok/sec: 245448.78 |\n",
            "| step  761 | loss: 0.8623 | lr 1.4864e-04 | norm: 0.2709 | dt: 2135.83ms | tok/sec: 245473.22 |\n",
            "| step  762 | loss: 0.8617 | lr 1.4794e-04 | norm: 0.2804 | dt: 2136.31ms | tok/sec: 245417.66 |\n",
            "| step  763 | loss: 0.8686 | lr 1.4724e-04 | norm: 0.2641 | dt: 2136.19ms | tok/sec: 245431.41 |\n",
            "| step  764 | loss: 0.8651 | lr 1.4655e-04 | norm: 0.2554 | dt: 2136.28ms | tok/sec: 245421.55 |\n",
            "| step  765 | loss: 0.8665 | lr 1.4586e-04 | norm: 0.2671 | dt: 2136.67ms | tok/sec: 245376.12 |\n",
            "| step  766 | loss: 0.8727 | lr 1.4517e-04 | norm: 0.2769 | dt: 2136.19ms | tok/sec: 245431.82 |\n",
            "| step  767 | loss: 0.8676 | lr 1.4449e-04 | norm: 0.2519 | dt: 2137.03ms | tok/sec: 245335.03 |\n",
            "| step  768 | loss: 0.8726 | lr 1.4380e-04 | norm: 0.2694 | dt: 2137.30ms | tok/sec: 245303.94 |\n",
            "| step  769 | loss: 0.8638 | lr 1.4312e-04 | norm: 0.2543 | dt: 2136.29ms | tok/sec: 245420.38 |\n",
            "| step  770 | loss: 0.8618 | lr 1.4244e-04 | norm: 0.2659 | dt: 2136.81ms | tok/sec: 245360.49 |\n",
            "| step  771 | loss: 0.8608 | lr 1.4177e-04 | norm: 0.2686 | dt: 2136.76ms | tok/sec: 245365.99 |\n",
            "| step  772 | loss: 0.8601 | lr 1.4109e-04 | norm: 0.2536 | dt: 2135.38ms | tok/sec: 245524.83 |\n",
            "| step  773 | loss: 0.8602 | lr 1.4042e-04 | norm: 0.2869 | dt: 2136.99ms | tok/sec: 245339.44 |\n",
            "| step  774 | loss: 0.8577 | lr 1.3975e-04 | norm: 0.2660 | dt: 2137.48ms | tok/sec: 245283.72 |\n",
            "| step  775 | loss: 0.8607 | lr 1.3908e-04 | norm: 0.2638 | dt: 2136.32ms | tok/sec: 245416.95 |\n",
            "| step  776 | loss: 0.8611 | lr 1.3842e-04 | norm: 0.2530 | dt: 2137.43ms | tok/sec: 245288.84 |\n",
            "| step  777 | loss: 0.8640 | lr 1.3775e-04 | norm: 0.2823 | dt: 2136.63ms | tok/sec: 245380.97 |\n",
            "| step  778 | loss: 0.8603 | lr 1.3709e-04 | norm: 0.2766 | dt: 2135.41ms | tok/sec: 245520.50 |\n",
            "| step  779 | loss: 0.8653 | lr 1.3643e-04 | norm: 0.2652 | dt: 2136.37ms | tok/sec: 245411.06 |\n",
            "| step  780 | loss: 0.8599 | lr 1.3578e-04 | norm: 0.2794 | dt: 2137.15ms | tok/sec: 245320.74 |\n",
            "| step  781 | loss: 0.8515 | lr 1.3512e-04 | norm: 0.2646 | dt: 2136.37ms | tok/sec: 245410.68 |\n",
            "| step  782 | loss: 0.8668 | lr 1.3447e-04 | norm: 0.2682 | dt: 2136.20ms | tok/sec: 245430.40 |\n",
            "| step  783 | loss: 0.8619 | lr 1.3382e-04 | norm: 0.2733 | dt: 2136.16ms | tok/sec: 245434.89 |\n",
            "| step  784 | loss: 0.8595 | lr 1.3318e-04 | norm: 0.2670 | dt: 2135.46ms | tok/sec: 245515.13 |\n",
            "| step  785 | loss: 0.8677 | lr 1.3253e-04 | norm: 0.2765 | dt: 2138.38ms | tok/sec: 245180.32 |\n",
            "| step  786 | loss: 0.8517 | lr 1.3189e-04 | norm: 0.2556 | dt: 2136.55ms | tok/sec: 245389.46 |\n",
            "| step  787 | loss: 0.8692 | lr 1.3125e-04 | norm: 0.2742 | dt: 2135.91ms | tok/sec: 245463.63 |\n",
            "| step  788 | loss: 0.8650 | lr 1.3062e-04 | norm: 0.2873 | dt: 2136.10ms | tok/sec: 245441.52 |\n",
            "| step  789 | loss: 0.8526 | lr 1.2998e-04 | norm: 0.2561 | dt: 2135.93ms | tok/sec: 245460.75 |\n",
            "| step  790 | loss: 0.8560 | lr 1.2935e-04 | norm: 0.2589 | dt: 2137.64ms | tok/sec: 245264.52 |\n",
            "| step  791 | loss: 0.8508 | lr 1.2872e-04 | norm: 0.2578 | dt: 2136.36ms | tok/sec: 245411.99 |\n",
            "| step  792 | loss: 0.8585 | lr 1.2809e-04 | norm: 0.2399 | dt: 2138.14ms | tok/sec: 245207.38 |\n",
            "| step  793 | loss: 0.8563 | lr 1.2747e-04 | norm: 0.2524 | dt: 2138.41ms | tok/sec: 245176.71 |\n",
            "| step  794 | loss: 0.8549 | lr 1.2685e-04 | norm: 0.2502 | dt: 2135.79ms | tok/sec: 245477.14 |\n",
            "| step  795 | loss: 0.8560 | lr 1.2623e-04 | norm: 0.2541 | dt: 2137.12ms | tok/sec: 245324.22 |\n",
            "| step  796 | loss: 0.8554 | lr 1.2561e-04 | norm: 0.2544 | dt: 2136.47ms | tok/sec: 245398.99 |\n",
            "| step  797 | loss: 0.8658 | lr 1.2500e-04 | norm: 0.2669 | dt: 2137.19ms | tok/sec: 245316.47 |\n",
            "| step  798 | loss: 0.8522 | lr 1.2438e-04 | norm: 0.2700 | dt: 2136.87ms | tok/sec: 245353.59 |\n",
            "| step  799 | loss: 0.8556 | lr 1.2378e-04 | norm: 0.2623 | dt: 2136.90ms | tok/sec: 245350.28 |\n",
            "| step  800 | loss: 0.8666 | lr 1.2317e-04 | norm: 0.2821 | dt: 2136.53ms | tok/sec: 245392.25 |\n",
            "| step  801 | loss: 0.8537 | lr 1.2256e-04 | norm: 0.2957 | dt: 2142.31ms | tok/sec: 244730.01 |\n",
            "| step  802 | loss: 0.8523 | lr 1.2196e-04 | norm: 0.2849 | dt: 2136.29ms | tok/sec: 245419.75 |\n",
            "| step  803 | loss: 0.8569 | lr 1.2136e-04 | norm: 0.2577 | dt: 2135.36ms | tok/sec: 245527.05 |\n",
            "| step  804 | loss: 0.8605 | lr 1.2076e-04 | norm: 0.3151 | dt: 2135.91ms | tok/sec: 245463.14 |\n",
            "| step  805 | loss: 0.8531 | lr 1.2017e-04 | norm: 0.3238 | dt: 2136.80ms | tok/sec: 245361.47 |\n",
            "| step  806 | loss: 0.8517 | lr 1.1958e-04 | norm: 0.2607 | dt: 2136.90ms | tok/sec: 245350.17 |\n",
            "| step  807 | loss: 0.8534 | lr 1.1899e-04 | norm: 0.2791 | dt: 2136.71ms | tok/sec: 245371.66 |\n",
            "| step  808 | loss: 0.8549 | lr 1.1840e-04 | norm: 0.2677 | dt: 2135.40ms | tok/sec: 245521.60 |\n",
            "| step  809 | loss: 0.8449 | lr 1.1782e-04 | norm: 0.2584 | dt: 2136.33ms | tok/sec: 245414.92 |\n",
            "| step  810 | loss: 0.8493 | lr 1.1724e-04 | norm: 0.2665 | dt: 2136.60ms | tok/sec: 245384.14 |\n",
            "| step  811 | loss: 0.8451 | lr 1.1666e-04 | norm: 0.2894 | dt: 2135.70ms | tok/sec: 245487.47 |\n",
            "| step  812 | loss: 0.8519 | lr 1.1608e-04 | norm: 0.2502 | dt: 2136.74ms | tok/sec: 245368.02 |\n",
            "| step  813 | loss: 0.8439 | lr 1.1551e-04 | norm: 0.2717 | dt: 2136.57ms | tok/sec: 245388.11 |\n",
            "| step  814 | loss: 0.8567 | lr 1.1494e-04 | norm: 0.2580 | dt: 2136.07ms | tok/sec: 245444.59 |\n",
            "| step  815 | loss: 0.8384 | lr 1.1437e-04 | norm: 0.2786 | dt: 2135.43ms | tok/sec: 245518.96 |\n",
            "| step  816 | loss: 0.8539 | lr 1.1380e-04 | norm: 0.2614 | dt: 2135.85ms | tok/sec: 245470.81 |\n",
            "| step  817 | loss: 0.8493 | lr 1.1324e-04 | norm: 0.2722 | dt: 2137.10ms | tok/sec: 245327.15 |\n",
            "| step  818 | loss: 0.8436 | lr 1.1268e-04 | norm: 0.2554 | dt: 2137.02ms | tok/sec: 245335.85 |\n",
            "| step  819 | loss: 0.8440 | lr 1.1212e-04 | norm: 0.2645 | dt: 2137.42ms | tok/sec: 245290.62 |\n",
            "| step  820 | loss: 0.8483 | lr 1.1157e-04 | norm: 0.2744 | dt: 2136.65ms | tok/sec: 245377.98 |\n",
            "| step  821 | loss: 0.8471 | lr 1.1101e-04 | norm: 0.2916 | dt: 2135.72ms | tok/sec: 245484.81 |\n",
            "| step  822 | loss: 0.8519 | lr 1.1046e-04 | norm: 0.2673 | dt: 2136.65ms | tok/sec: 245378.56 |\n",
            "| step  823 | loss: 0.8384 | lr 1.0992e-04 | norm: 0.2536 | dt: 2136.23ms | tok/sec: 245426.35 |\n",
            "| step  824 | loss: 0.8458 | lr 1.0937e-04 | norm: 0.2628 | dt: 2136.50ms | tok/sec: 245395.18 |\n",
            "| step  825 | loss: 0.8398 | lr 1.0883e-04 | norm: 0.2517 | dt: 2138.26ms | tok/sec: 245193.55 |\n",
            "| step  826 | loss: 0.8427 | lr 1.0829e-04 | norm: 0.2524 | dt: 2136.36ms | tok/sec: 245411.91 |\n",
            "| step  827 | loss: 0.8437 | lr 1.0775e-04 | norm: 0.2563 | dt: 2136.26ms | tok/sec: 245423.80 |\n",
            "| step  828 | loss: 0.8366 | lr 1.0722e-04 | norm: 0.2468 | dt: 2136.07ms | tok/sec: 245444.84 |\n",
            "| step  829 | loss: 0.8407 | lr 1.0669e-04 | norm: 0.2444 | dt: 2135.44ms | tok/sec: 245516.99 |\n",
            "| step  830 | loss: 0.8427 | lr 1.0616e-04 | norm: 0.2447 | dt: 2137.07ms | tok/sec: 245330.38 |\n",
            "| step  831 | loss: 0.8411 | lr 1.0563e-04 | norm: 0.2514 | dt: 2136.66ms | tok/sec: 245377.49 |\n",
            "| step  832 | loss: 0.8421 | lr 1.0511e-04 | norm: 0.2551 | dt: 2137.24ms | tok/sec: 245311.19 |\n",
            "| step  833 | loss: 0.8363 | lr 1.0459e-04 | norm: 0.2626 | dt: 2136.84ms | tok/sec: 245356.57 |\n",
            "| step  834 | loss: 0.8402 | lr 1.0407e-04 | norm: 0.2563 | dt: 2136.23ms | tok/sec: 245427.06 |\n",
            "| step  835 | loss: 0.8464 | lr 1.0356e-04 | norm: 0.2532 | dt: 2136.71ms | tok/sec: 245371.52 |\n",
            "| step  836 | loss: 0.8399 | lr 1.0305e-04 | norm: 0.2701 | dt: 2137.51ms | tok/sec: 245280.36 |\n",
            "| step  837 | loss: 0.8349 | lr 1.0254e-04 | norm: 0.2421 | dt: 2137.67ms | tok/sec: 245261.40 |\n",
            "| step  838 | loss: 0.8324 | lr 1.0203e-04 | norm: 0.2425 | dt: 2136.97ms | tok/sec: 245341.85 |\n",
            "| step  839 | loss: 0.8395 | lr 1.0153e-04 | norm: 0.2539 | dt: 2136.89ms | tok/sec: 245350.55 |\n",
            "| step  840 | loss: 0.8405 | lr 1.0103e-04 | norm: 0.2512 | dt: 2136.82ms | tok/sec: 245358.87 |\n",
            "| step  841 | loss: 0.8273 | lr 1.0053e-04 | norm: 0.2540 | dt: 2136.22ms | tok/sec: 245427.88 |\n",
            "| step  842 | loss: 0.8426 | lr 1.0003e-04 | norm: 0.2449 | dt: 2136.24ms | tok/sec: 245425.96 |\n",
            "| step  843 | loss: 0.8473 | lr 9.9541e-05 | norm: 0.2656 | dt: 2136.68ms | tok/sec: 245375.27 |\n",
            "| step  844 | loss: 0.8371 | lr 9.9052e-05 | norm: 0.2581 | dt: 2136.34ms | tok/sec: 245413.77 |\n",
            "| step  845 | loss: 0.8411 | lr 9.8565e-05 | norm: 0.2711 | dt: 2137.52ms | tok/sec: 245278.96 |\n",
            "| step  846 | loss: 0.8363 | lr 9.8081e-05 | norm: 0.2561 | dt: 2136.35ms | tok/sec: 245412.46 |\n",
            "| step  847 | loss: 0.8331 | lr 9.7600e-05 | norm: 0.2671 | dt: 2136.63ms | tok/sec: 245380.72 |\n",
            "| step  848 | loss: 0.8525 | lr 9.7121e-05 | norm: 0.2735 | dt: 2136.68ms | tok/sec: 245374.64 |\n",
            "| step  849 | loss: 0.8396 | lr 9.6646e-05 | norm: 0.2623 | dt: 2136.24ms | tok/sec: 245425.17 |\n",
            "| step  850 | loss: 0.8340 | lr 9.6173e-05 | norm: 0.2810 | dt: 2135.97ms | tok/sec: 245456.29 |\n",
            "| step  851 | loss: 0.8466 | lr 9.5703e-05 | norm: 0.2588 | dt: 2135.19ms | tok/sec: 245546.24 |\n",
            "| step  852 | loss: 0.8329 | lr 9.5236e-05 | norm: 0.2790 | dt: 2136.43ms | tok/sec: 245404.30 |\n",
            "| step  853 | loss: 0.8304 | lr 9.4772e-05 | norm: 0.2722 | dt: 2136.54ms | tok/sec: 245391.18 |\n",
            "| step  854 | loss: 0.8273 | lr 9.4311e-05 | norm: 0.2692 | dt: 2136.05ms | tok/sec: 245447.63 |\n",
            "| step  855 | loss: 0.8272 | lr 9.3853e-05 | norm: 0.2722 | dt: 2137.51ms | tok/sec: 245279.73 |\n",
            "| step  856 | loss: 0.8281 | lr 9.3397e-05 | norm: 0.2811 | dt: 2136.87ms | tok/sec: 245353.53 |\n",
            "| step  857 | loss: 0.8166 | lr 9.2945e-05 | norm: 0.2668 | dt: 2136.29ms | tok/sec: 245420.38 |\n",
            "| step  858 | loss: 0.8365 | lr 9.2495e-05 | norm: 0.2781 | dt: 2137.55ms | tok/sec: 245275.13 |\n",
            "| step  859 | loss: 0.8362 | lr 9.2048e-05 | norm: 0.2646 | dt: 2135.74ms | tok/sec: 245482.59 |\n",
            "| step  860 | loss: 0.8263 | lr 9.1604e-05 | norm: 0.2521 | dt: 2136.63ms | tok/sec: 245380.69 |\n",
            "| step  861 | loss: 0.8316 | lr 9.1163e-05 | norm: 0.2596 | dt: 2136.14ms | tok/sec: 245437.25 |\n",
            "| step  862 | loss: 0.8234 | lr 9.0725e-05 | norm: 0.2419 | dt: 2135.98ms | tok/sec: 245455.11 |\n",
            "| step  863 | loss: 0.8364 | lr 9.0290e-05 | norm: 0.2571 | dt: 2137.12ms | tok/sec: 245324.25 |\n",
            "| step  864 | loss: 0.8417 | lr 8.9858e-05 | norm: 0.2515 | dt: 2137.88ms | tok/sec: 245237.36 |\n",
            "| step  865 | loss: 0.8522 | lr 8.9428e-05 | norm: 0.2569 | dt: 2136.86ms | tok/sec: 245354.55 |\n",
            "| step  866 | loss: 0.8329 | lr 8.9002e-05 | norm: 0.2489 | dt: 2136.51ms | tok/sec: 245394.63 |\n",
            "| step  867 | loss: 0.8304 | lr 8.8578e-05 | norm: 0.2610 | dt: 2137.65ms | tok/sec: 245263.86 |\n",
            "| step  868 | loss: 0.8273 | lr 8.8158e-05 | norm: 0.2459 | dt: 2135.35ms | tok/sec: 245528.04 |\n",
            "| step  869 | loss: 0.8205 | lr 8.7740e-05 | norm: 0.2521 | dt: 2136.25ms | tok/sec: 245424.18 |\n",
            "| step  870 | loss: 0.8185 | lr 8.7326e-05 | norm: 0.2467 | dt: 2136.28ms | tok/sec: 245421.53 |\n",
            "| step  871 | loss: 0.8293 | lr 8.6914e-05 | norm: 0.2434 | dt: 2136.41ms | tok/sec: 245406.35 |\n",
            "| step  872 | loss: 0.8245 | lr 8.6505e-05 | norm: 0.2361 | dt: 2137.40ms | tok/sec: 245292.83 |\n",
            "| step  873 | loss: 0.8274 | lr 8.6099e-05 | norm: 0.2449 | dt: 2135.61ms | tok/sec: 245498.43 |\n",
            "| step  874 | loss: 0.8355 | lr 8.5697e-05 | norm: 0.2317 | dt: 2136.18ms | tok/sec: 245432.76 |\n",
            "| step  875 | loss: 0.8210 | lr 8.5297e-05 | norm: 0.2415 | dt: 2136.19ms | tok/sec: 245431.19 |\n",
            "| step  876 | loss: 0.8251 | lr 8.4900e-05 | norm: 0.2471 | dt: 2136.59ms | tok/sec: 245385.43 |\n",
            "| step  877 | loss: 0.8386 | lr 8.4506e-05 | norm: 0.2373 | dt: 2138.24ms | tok/sec: 245196.17 |\n",
            "| step  878 | loss: 0.8257 | lr 8.4115e-05 | norm: 0.2564 | dt: 2136.01ms | tok/sec: 245451.52 |\n",
            "| step  879 | loss: 0.8236 | lr 8.3728e-05 | norm: 0.2502 | dt: 2135.89ms | tok/sec: 245465.82 |\n",
            "| step  880 | loss: 0.8216 | lr 8.3343e-05 | norm: 0.2473 | dt: 2136.36ms | tok/sec: 245411.56 |\n",
            "| step  881 | loss: 0.8232 | lr 8.2961e-05 | norm: 0.2591 | dt: 2136.66ms | tok/sec: 245377.33 |\n",
            "| step  882 | loss: 0.8294 | lr 8.2582e-05 | norm: 0.2497 | dt: 2137.46ms | tok/sec: 245285.42 |\n",
            "| step  883 | loss: 0.8228 | lr 8.2206e-05 | norm: 0.2564 | dt: 2135.61ms | tok/sec: 245497.97 |\n",
            "| step  884 | loss: 0.8265 | lr 8.1833e-05 | norm: 0.2616 | dt: 2137.81ms | tok/sec: 245245.45 |\n",
            "| step  885 | loss: 0.8256 | lr 8.1464e-05 | norm: 0.2553 | dt: 2136.74ms | tok/sec: 245368.73 |\n",
            "| step  886 | loss: 0.8170 | lr 8.1097e-05 | norm: 0.2601 | dt: 2136.96ms | tok/sec: 245343.43 |\n",
            "| step  887 | loss: 0.8148 | lr 8.0733e-05 | norm: 0.2497 | dt: 2135.46ms | tok/sec: 245515.37 |\n",
            "| step  888 | loss: 0.8359 | lr 8.0373e-05 | norm: 0.2633 | dt: 2136.18ms | tok/sec: 245432.98 |\n",
            "| step  889 | loss: 0.8247 | lr 8.0015e-05 | norm: 0.2451 | dt: 2136.45ms | tok/sec: 245401.72 |\n",
            "| step  890 | loss: 0.8256 | lr 7.9660e-05 | norm: 0.2606 | dt: 2135.97ms | tok/sec: 245456.45 |\n",
            "| step  891 | loss: 0.8289 | lr 7.9309e-05 | norm: 0.2594 | dt: 2136.66ms | tok/sec: 245377.76 |\n",
            "| step  892 | loss: 0.8356 | lr 7.8960e-05 | norm: 0.2656 | dt: 2136.76ms | tok/sec: 245365.72 |\n",
            "| step  893 | loss: 0.8128 | lr 7.8615e-05 | norm: 0.2553 | dt: 2136.11ms | tok/sec: 245440.07 |\n",
            "| step  894 | loss: 0.8241 | lr 7.8273e-05 | norm: 0.2663 | dt: 2138.14ms | tok/sec: 245207.22 |\n",
            "| step  895 | loss: 0.8135 | lr 7.7933e-05 | norm: 0.2418 | dt: 2136.15ms | tok/sec: 245435.47 |\n",
            "| step  896 | loss: 0.8227 | lr 7.7597e-05 | norm: 0.2674 | dt: 2135.77ms | tok/sec: 245479.14 |\n",
            "| step  897 | loss: 0.8114 | lr 7.7264e-05 | norm: 0.2351 | dt: 2135.30ms | tok/sec: 245533.99 |\n",
            "| step  898 | loss: 0.8165 | lr 7.6934e-05 | norm: 0.2492 | dt: 2136.64ms | tok/sec: 245379.22 |\n",
            "| step  899 | loss: 0.8271 | lr 7.6607e-05 | norm: 0.2595 | dt: 2136.89ms | tok/sec: 245350.61 |\n",
            "| step  900 | loss: 0.8333 | lr 7.6283e-05 | norm: 0.2529 | dt: 2136.71ms | tok/sec: 245371.58 |\n",
            "| step  901 | loss: 0.8159 | lr 7.5962e-05 | norm: 0.2482 | dt: 2136.88ms | tok/sec: 245352.28 |\n",
            "| step  902 | loss: 0.8209 | lr 7.5644e-05 | norm: 0.2554 | dt: 2135.73ms | tok/sec: 245483.94 |\n",
            "| step  903 | loss: 0.8260 | lr 7.5330e-05 | norm: 0.2397 | dt: 2137.15ms | tok/sec: 245320.69 |\n",
            "| step  904 | loss: 0.8221 | lr 7.5018e-05 | norm: 0.2532 | dt: 2136.81ms | tok/sec: 245359.69 |\n",
            "| step  905 | loss: 0.8274 | lr 7.4710e-05 | norm: 0.2408 | dt: 2137.25ms | tok/sec: 245309.41 |\n",
            "| step  906 | loss: 0.8158 | lr 7.4405e-05 | norm: 0.2513 | dt: 2136.01ms | tok/sec: 245451.77 |\n",
            "| step  907 | loss: 0.8296 | lr 7.4103e-05 | norm: 0.2431 | dt: 2136.08ms | tok/sec: 245443.80 |\n",
            "| step  908 | loss: 0.8282 | lr 7.3803e-05 | norm: 0.2504 | dt: 2135.70ms | tok/sec: 245487.31 |\n",
            "| step  909 | loss: 0.8218 | lr 7.3508e-05 | norm: 0.2402 | dt: 2136.01ms | tok/sec: 245451.69 |\n",
            "| step  910 | loss: 0.8171 | lr 7.3215e-05 | norm: 0.2448 | dt: 2137.14ms | tok/sec: 245322.77 |\n",
            "| step  911 | loss: 0.8186 | lr 7.2925e-05 | norm: 0.2439 | dt: 2134.95ms | tok/sec: 245573.44 |\n",
            "| step  912 | loss: 0.8152 | lr 7.2639e-05 | norm: 0.2317 | dt: 2136.52ms | tok/sec: 245393.89 |\n",
            "| step  913 | loss: 0.8128 | lr 7.2355e-05 | norm: 0.2457 | dt: 2137.40ms | tok/sec: 245292.56 |\n",
            "| step  914 | loss: 0.8110 | lr 7.2075e-05 | norm: 0.2385 | dt: 2136.37ms | tok/sec: 245410.35 |\n",
            "| step  915 | loss: 0.8091 | lr 7.1798e-05 | norm: 0.2397 | dt: 2136.66ms | tok/sec: 245377.30 |\n",
            "| step  916 | loss: 0.8157 | lr 7.1524e-05 | norm: 0.2470 | dt: 2167.68ms | tok/sec: 241865.53 |\n",
            "| step  917 | loss: 0.8257 | lr 7.1253e-05 | norm: 0.2385 | dt: 2135.14ms | tok/sec: 245551.64 |\n",
            "| step  918 | loss: 0.8116 | lr 7.0985e-05 | norm: 0.2449 | dt: 2136.73ms | tok/sec: 245369.85 |\n",
            "| step  919 | loss: 0.8125 | lr 7.0721e-05 | norm: 0.2372 | dt: 2136.33ms | tok/sec: 245415.61 |\n",
            "| step  920 | loss: 0.8105 | lr 7.0459e-05 | norm: 0.2393 | dt: 2137.00ms | tok/sec: 245338.04 |\n",
            "| step  921 | loss: 0.8124 | lr 7.0201e-05 | norm: 0.2452 | dt: 2137.15ms | tok/sec: 245321.65 |\n",
            "| step  922 | loss: 0.8249 | lr 6.9946e-05 | norm: 0.2523 | dt: 2135.86ms | tok/sec: 245469.06 |\n",
            "| step  923 | loss: 0.8131 | lr 6.9694e-05 | norm: 0.2487 | dt: 2137.44ms | tok/sec: 245288.32 |\n",
            "| step  924 | loss: 0.8214 | lr 6.9446e-05 | norm: 0.2477 | dt: 2135.60ms | tok/sec: 245498.98 |\n",
            "| step  925 | loss: 0.8023 | lr 6.9200e-05 | norm: 0.2596 | dt: 2137.45ms | tok/sec: 245286.35 |\n",
            "| step  926 | loss: 0.8141 | lr 6.8958e-05 | norm: 0.2425 | dt: 2137.23ms | tok/sec: 245311.36 |\n",
            "| step  927 | loss: 0.8171 | lr 6.8719e-05 | norm: 0.2770 | dt: 2135.78ms | tok/sec: 245479.00 |\n",
            "| step  928 | loss: 0.8164 | lr 6.8483e-05 | norm: 0.2378 | dt: 2136.89ms | tok/sec: 245350.88 |\n",
            "| step  929 | loss: 0.8109 | lr 6.8250e-05 | norm: 0.2588 | dt: 2136.84ms | tok/sec: 245356.85 |\n",
            "| step  930 | loss: 0.8093 | lr 6.8020e-05 | norm: 0.2525 | dt: 2136.45ms | tok/sec: 245402.05 |\n",
            "| step  931 | loss: 0.8171 | lr 6.7794e-05 | norm: 0.2500 | dt: 2137.26ms | tok/sec: 245308.40 |\n",
            "| step  932 | loss: 0.8159 | lr 6.7571e-05 | norm: 0.2442 | dt: 2137.61ms | tok/sec: 245267.80 |\n",
            "| step  933 | loss: 0.8128 | lr 6.7351e-05 | norm: 0.2561 | dt: 2135.02ms | tok/sec: 245565.44 |\n",
            "| step  934 | loss: 0.8014 | lr 6.7134e-05 | norm: 0.2536 | dt: 2137.50ms | tok/sec: 245281.42 |\n",
            "| step  935 | loss: 0.8059 | lr 6.6920e-05 | norm: 0.2430 | dt: 2136.48ms | tok/sec: 245398.11 |\n",
            "| step  936 | loss: 0.8125 | lr 6.6710e-05 | norm: 0.2709 | dt: 2135.75ms | tok/sec: 245481.91 |\n",
            "| step  937 | loss: 0.8163 | lr 6.6502e-05 | norm: 0.2444 | dt: 2137.01ms | tok/sec: 245337.38 |\n",
            "| step  938 | loss: 0.8243 | lr 6.6298e-05 | norm: 0.2533 | dt: 2136.75ms | tok/sec: 245367.14 |\n",
            "| step  939 | loss: 0.8080 | lr 6.6098e-05 | norm: 0.2438 | dt: 2136.04ms | tok/sec: 245448.48 |\n",
            "| step  940 | loss: 0.8225 | lr 6.5900e-05 | norm: 0.2389 | dt: 2136.26ms | tok/sec: 245423.69 |\n",
            "| step  941 | loss: 0.8158 | lr 6.5706e-05 | norm: 0.2438 | dt: 2136.51ms | tok/sec: 245394.85 |\n",
            "| step  942 | loss: 0.8121 | lr 6.5515e-05 | norm: 0.2418 | dt: 2138.21ms | tok/sec: 245199.87 |\n",
            "| step  943 | loss: 0.8058 | lr 6.5327e-05 | norm: 0.2539 | dt: 2136.36ms | tok/sec: 245411.47 |\n",
            "| step  944 | loss: 0.8102 | lr 6.5142e-05 | norm: 0.2510 | dt: 2137.33ms | tok/sec: 245300.38 |\n",
            "| step  945 | loss: 0.8065 | lr 6.4961e-05 | norm: 0.2481 | dt: 2136.70ms | tok/sec: 245373.08 |\n",
            "| step  946 | loss: 0.8179 | lr 6.4782e-05 | norm: 0.2524 | dt: 2135.98ms | tok/sec: 245455.44 |\n",
            "| step  947 | loss: 0.8100 | lr 6.4607e-05 | norm: 0.2611 | dt: 2137.19ms | tok/sec: 245316.86 |\n",
            "| step  948 | loss: 0.8149 | lr 6.4436e-05 | norm: 0.2507 | dt: 2136.33ms | tok/sec: 245415.66 |\n",
            "| step  949 | loss: 0.8025 | lr 6.4267e-05 | norm: 0.2464 | dt: 2136.98ms | tok/sec: 245340.86 |\n",
            "| step  950 | loss: 0.8159 | lr 6.4102e-05 | norm: 0.2659 | dt: 2135.80ms | tok/sec: 245476.10 |\n",
            "| step  951 | loss: 0.8033 | lr 6.3940e-05 | norm: 0.2386 | dt: 2137.13ms | tok/sec: 245323.73 |\n",
            "| step  952 | loss: 0.8089 | lr 6.3781e-05 | norm: 0.2492 | dt: 2137.25ms | tok/sec: 245309.47 |\n",
            "| step  953 | loss: 0.8120 | lr 6.3626e-05 | norm: 0.2485 | dt: 2135.85ms | tok/sec: 245470.89 |\n",
            "| step  954 | loss: 0.8111 | lr 6.3473e-05 | norm: 0.2487 | dt: 2136.35ms | tok/sec: 245412.76 |\n",
            "| step  955 | loss: 0.8177 | lr 6.3324e-05 | norm: 0.2533 | dt: 2135.80ms | tok/sec: 245476.32 |\n",
            "| step  956 | loss: 0.8103 | lr 6.3178e-05 | norm: 0.2484 | dt: 2137.03ms | tok/sec: 245335.19 |\n",
            "| step  957 | loss: 0.8062 | lr 6.3036e-05 | norm: 0.2500 | dt: 2137.04ms | tok/sec: 245333.85 |\n",
            "| step  958 | loss: 0.8069 | lr 6.2896e-05 | norm: 0.2445 | dt: 2135.27ms | tok/sec: 245536.89 |\n",
            "| step  959 | loss: 0.8025 | lr 6.2760e-05 | norm: 0.2495 | dt: 2136.49ms | tok/sec: 245396.90 |\n",
            "| step  960 | loss: 0.8067 | lr 6.2628e-05 | norm: 0.2462 | dt: 2137.52ms | tok/sec: 245279.04 |\n",
            "| step  961 | loss: 0.8053 | lr 6.2498e-05 | norm: 0.2680 | dt: 2137.09ms | tok/sec: 245327.94 |\n",
            "| step  962 | loss: 0.8029 | lr 6.2372e-05 | norm: 0.2455 | dt: 2137.04ms | tok/sec: 245333.22 |\n",
            "| step  963 | loss: 0.8085 | lr 6.2249e-05 | norm: 0.2578 | dt: 2137.24ms | tok/sec: 245310.21 |\n",
            "| step  964 | loss: 0.8090 | lr 6.2129e-05 | norm: 0.2524 | dt: 2134.97ms | tok/sec: 245571.80 |\n",
            "| step  965 | loss: 0.8119 | lr 6.2013e-05 | norm: 0.2596 | dt: 2136.21ms | tok/sec: 245429.17 |\n",
            "| step  966 | loss: 0.8115 | lr 6.1899e-05 | norm: 0.2469 | dt: 2137.29ms | tok/sec: 245305.36 |\n",
            "| step  967 | loss: 0.8071 | lr 6.1789e-05 | norm: 0.2573 | dt: 2136.58ms | tok/sec: 245386.36 |\n",
            "| step  968 | loss: 0.7946 | lr 6.1683e-05 | norm: 0.2395 | dt: 2135.54ms | tok/sec: 245505.89 |\n",
            "| step  969 | loss: 0.8073 | lr 6.1579e-05 | norm: 0.2434 | dt: 2135.60ms | tok/sec: 245499.42 |\n",
            "| step  970 | loss: 0.8134 | lr 6.1479e-05 | norm: 0.2476 | dt: 2136.27ms | tok/sec: 245422.73 |\n",
            "| step  971 | loss: 0.8092 | lr 6.1382e-05 | norm: 0.2423 | dt: 2136.87ms | tok/sec: 245352.80 |\n",
            "| step  972 | loss: 0.8067 | lr 6.1289e-05 | norm: 0.2463 | dt: 2136.01ms | tok/sec: 245452.01 |\n",
            "| step  973 | loss: 0.8117 | lr 6.1198e-05 | norm: 0.2436 | dt: 2136.78ms | tok/sec: 245363.34 |\n",
            "| step  974 | loss: 0.8076 | lr 6.1111e-05 | norm: 0.2477 | dt: 2136.30ms | tok/sec: 245418.24 |\n",
            "| step  975 | loss: 0.8150 | lr 6.1027e-05 | norm: 0.2462 | dt: 2136.10ms | tok/sec: 245441.14 |\n",
            "| step  976 | loss: 0.8064 | lr 6.0947e-05 | norm: 0.2485 | dt: 2136.91ms | tok/sec: 245349.10 |\n",
            "| step  977 | loss: 0.8038 | lr 6.0870e-05 | norm: 0.2451 | dt: 2136.71ms | tok/sec: 245371.11 |\n",
            "| step  978 | loss: 0.8001 | lr 6.0796e-05 | norm: 0.2459 | dt: 2138.80ms | tok/sec: 245131.56 |\n",
            "| step  979 | loss: 0.8047 | lr 6.0725e-05 | norm: 0.2448 | dt: 2136.27ms | tok/sec: 245422.65 |\n",
            "| step  980 | loss: 0.8040 | lr 6.0658e-05 | norm: 0.2599 | dt: 2136.56ms | tok/sec: 245389.18 |\n",
            "| step  981 | loss: 0.8113 | lr 6.0594e-05 | norm: 0.2366 | dt: 2136.38ms | tok/sec: 245410.02 |\n",
            "| step  982 | loss: 0.8076 | lr 6.0533e-05 | norm: 0.2693 | dt: 2138.25ms | tok/sec: 245195.22 |\n",
            "| step  983 | loss: 0.8066 | lr 6.0475e-05 | norm: 0.2438 | dt: 2136.42ms | tok/sec: 245405.20 |\n",
            "| step  984 | loss: 0.8018 | lr 6.0421e-05 | norm: 0.2608 | dt: 2136.70ms | tok/sec: 245372.75 |\n",
            "| step  985 | loss: 0.8164 | lr 6.0370e-05 | norm: 0.2581 | dt: 2136.11ms | tok/sec: 245440.32 |\n",
            "| step  986 | loss: 0.8030 | lr 6.0322e-05 | norm: 0.2406 | dt: 2137.33ms | tok/sec: 245300.33 |\n",
            "| step  987 | loss: 0.8131 | lr 6.0278e-05 | norm: 0.2505 | dt: 2136.93ms | tok/sec: 245345.95 |\n",
            "| step  988 | loss: 0.8056 | lr 6.0237e-05 | norm: 0.2459 | dt: 2135.83ms | tok/sec: 245473.22 |\n",
            "| step  989 | loss: 0.8065 | lr 6.0199e-05 | norm: 0.2411 | dt: 2136.30ms | tok/sec: 245418.70 |\n",
            "| step  990 | loss: 0.7986 | lr 6.0164e-05 | norm: 0.2464 | dt: 2137.35ms | tok/sec: 245297.67 |\n",
            "| step  991 | loss: 0.8156 | lr 6.0133e-05 | norm: 0.2411 | dt: 2136.12ms | tok/sec: 245439.17 |\n",
            "| step  992 | loss: 0.8133 | lr 6.0105e-05 | norm: 0.2479 | dt: 2136.94ms | tok/sec: 245344.77 |\n",
            "| step  993 | loss: 0.8025 | lr 6.0081e-05 | norm: 0.2371 | dt: 2135.71ms | tok/sec: 245486.10 |\n",
            "| step  994 | loss: 0.7977 | lr 6.0059e-05 | norm: 0.2534 | dt: 2135.65ms | tok/sec: 245493.78 |\n",
            "| step  995 | loss: 0.8112 | lr 6.0041e-05 | norm: 0.2488 | dt: 2137.47ms | tok/sec: 245284.30 |\n",
            "| step  996 | loss: 0.8035 | lr 6.0026e-05 | norm: 0.2447 | dt: 2136.14ms | tok/sec: 245436.73 |\n",
            "| step  997 | loss: 0.7963 | lr 6.0015e-05 | norm: 0.2551 | dt: 2137.11ms | tok/sec: 245325.56 |\n",
            "| step  998 | loss: 0.8019 | lr 6.0007e-05 | norm: 0.2567 | dt: 2135.38ms | tok/sec: 245524.12 |\n",
            "| step  999 | loss: 0.8000 | lr 6.0002e-05 | norm: 0.2571 | dt: 2136.10ms | tok/sec: 245441.52 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model(\n",
        "    prompt=\"Goog threw the ball to Lily. Lily threw \",\n",
        "    num_samples=10,\n",
        "    max_tokens=100,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "W_J5Bmb4MAJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d35d96-90c6-4276-ab34-c363476a3ddf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting text into words via regex...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding words as tokens: 100%|██████████| 17/17 [00:00<00:00, 55662.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [0] > Goog threw the ball to Lily. Lily threw her foot far, and it hit the door and the door shut. She waved the doll and shouted.\n",
            "\"Come back, Lily! Thank you, mister!\" the boy said.\n",
            "\"Youre welcome, boy,\" Lily said.\n",
            "\n",
            " [1] > Goog threw the ball to Lily. Lily threw the ball and Sam caught it. They laughed and played until the fragile ball ran away. Then, Lily tried to catch the ball, but it was too far away. Lily and Sam went back home, happy with \n",
            "\n",
            " [2] > Goog threw the ball to Lily. Lily threw the ball very far, and Tom started to feel tired.\n",
            "Lily and Tom walked their homes. They washed their hands and faces. The cold air made their mom feel better. They were not cold anymore. They went\n",
            "\n",
            " [3] > Goog threw the ball to Lily. Lily threw the ball too hard for him.\n",
            "\"Ow! Ow! Ow!\" Tom shouted.\n",
            "Lily tried to jump over the ball, but she was clumsy. She jumped too high and dropped the ball. May ran to \n",
            "\n",
            " [4] > Goog threw the ball to Lily. Lily threw the ball, and Will tried to catch it. But Will did not let her.\n",
            "Lily was sad. She did not like Will anymore. Will was not a toy. Will was Lilys friend. That night W\n",
            "\n",
            " [5] > Goog threw the ball to Lily. Lily threw the ball up, and the dolphins laughed. It was not hurt at all. They became good friends. And they both played with the ball every day.\n",
            "<|endoftext|>\n",
            "Once upon a time, in a small town, there\n",
            "\n",
            " [6] > Goog threw the ball to Lily. Lily threw the ball to Max, and Max caught it with his mouth. They played together for a long time. They were both very tired but happy.\n",
            "<|endoftext|>\n",
            "\n",
            "\n",
            "Lily and Ben like to play with yarn. They have many \n",
            "\n",
            " [7] > Goog threw the ball to Lily. Lily threw the ball too hard can too. It hit a window and the window fell down. She picked up the ball and threw it on the window. She threw the ball again and this time.\n",
            "Lily was happy. She had\n",
            "\n",
            " [8] > Goog threw the ball to Lily. Lily threw the ball very hard. The ball hit the little girl. She was in shock. The girl was sad and hurt.\n",
            "A small boy saw the girl and came to help. The boy went to her and got the \n",
            "\n",
            " [9] > Goog threw the ball to Lily. Lily threw the ball hard. The ball rolled away. Mia did not see the rock in it. She wanted to play again.\n",
            "Just then, a big wind came. It blew the rock far away. Mia was sad. The rock \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mapjHr7jgWL7"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}