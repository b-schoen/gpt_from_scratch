{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPjqeK8+Ist48zF7g/CFsUs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-schoen/gpt_from_scratch/blob/main/colab/gpt2_tinystories_custom_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOq7bHwLIPc7",
        "outputId": "390c9825-50d7-418d-a85f-4071c006b10c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt_from_scratch'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Counting objects: 100% (328/328), done.\u001b[K\n",
            "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "remote: Total 328 (delta 175), reused 215 (delta 72), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (328/328), 4.88 MiB | 6.90 MiB/s, done.\n",
            "Resolving deltas: 100% (175/175), done.\n"
          ]
        }
      ],
      "source": [
        "# clone repo\n",
        "!rm -rf gpt_from_scratch\n",
        "!git clone https://github.com/b-schoen/gpt_from_scratch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into the repo directory\n",
        "import os\n",
        "\n",
        "os.chdir('gpt_from_scratch')\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2heAkYZIp0g",
        "outputId": "254df752-68c7-4d08-b938-fa52c0464394"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content/gpt_from_scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can operate as if this was a local notebook"
      ],
      "metadata": {
        "id": "tUYSCNYTIyWy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "DfSXTCO_I15z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset locally"
      ],
      "metadata": {
        "id": "AGmpQSn2gx-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load tinystories for comparison\n",
        "from gpt_from_scratch.dataset_loaders import tinystories_loader\n",
        "\n",
        "tinystories_version = tinystories_loader.TinyStoriesVersion.V2\n",
        "\n",
        "tinystories_filepaths = tinystories_loader.download_tinystories(tinystories_version)\n",
        "\n",
        "# read train as input text\n",
        "input_text = tinystories_filepaths.train.read_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28TU5lPCJx16",
        "outputId": "af2b9b76-69cf-4adb-8f2c-864f4a43d6a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TinyStoriesV2-GPT4-train.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded TinyStoriesV2-GPT4-train.txt to /root/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-train.txt\n",
            "Downloading TinyStoriesV2-GPT4-valid.txt...\n",
            "Downloaded TinyStoriesV2-GPT4-valid.txt to /root/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-valid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note: if this gets annoying can do an actual pip install requirements\n",
        "!pip install tiktoken\n",
        "!pip install jaxtyping\n",
        "!pip install colored"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_83OHQPJ52z",
        "outputId": "8ee76f73-a7c1-48cd-b3d7-86f470dff0d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.33)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n",
            "Requirement already satisfied: colored in /usr/local/lib/python3.10/dist-packages (2.2.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from gpt_from_scratch.gpt2_from_scratch import data_loader\n",
        "from gpt_from_scratch.gpt2_from_scratch.train_gpt2 import (\n",
        "    GPT,\n",
        "    GPTConfig,\n",
        "    get_best_available_torch_device,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "bQ9dtV0EJ0Ha"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "C6v7v2kHgIv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample some outputs to get an idea of where we are\n",
        "\n",
        "from typing import TYPE_CHECKING\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "  from gpt_from_scratch import tokenizer_utils\n",
        "\n",
        "def sample_model(\n",
        "    prompt: str,\n",
        "    num_samples: int,\n",
        "    max_tokens: int,\n",
        "    model: nn.Module,\n",
        "    tokenizer: 'tokenizer_utils.Tokenizer',\n",
        "    device: torch.device,\n",
        "    stop_token: str | None = None,\n",
        ") -> None:\n",
        "\n",
        "    # tokenize\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "    tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (5, 8)\n",
        "\n",
        "    # tokens in this case is just the prompt, and is small enough to fit on GPU\n",
        "    x = tokens.to(device)\n",
        "\n",
        "    while x.size(1) < max_tokens:\n",
        "\n",
        "        # forward the model to get the logits\n",
        "        with torch.no_grad():\n",
        "\n",
        "            logits, loss = model(x) # (B, T, vocab_size)\n",
        "\n",
        "            # take the logits at the last position\n",
        "            # throw away all the logits from things other than the last position\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "\n",
        "            # get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # do top-k sampling of 50 (huggingface pipeline default)\n",
        "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "            #\n",
        "            # \"anything lower than the 50th, we clamp to 0 and never sample it\"\n",
        "            #\n",
        "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "            # select a token from the top-k probabilities\n",
        "            # note: multinomial does not demand the input to sum to 1\n",
        "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "\n",
        "            # gather the corresponding indices\n",
        "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "\n",
        "            # append to the sequence\n",
        "            x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "    # print the generated text\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        tokens = x[i, :max_tokens].tolist()\n",
        "\n",
        "        decoded = tokenizer.decode(tokens)\n",
        "\n",
        "        # cut off at the first stop token\n",
        "        if stop_token and stop_token in decoded:\n",
        "          position_of_stop_token = decoded.find(stop_token)\n",
        "          decoded = decoded[:position_of_stop_token]\n",
        "\n",
        "        print(f\"\\n [{i}] >\", decoded)"
      ],
      "metadata": {
        "id": "vQnXakW0gH2I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "n0sUP9PagFcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n--- First 1000 characters: ---\\n')\n",
        "print(input_text[:1000])\n",
        "\n",
        "# print('\\n--- Last 1000 characters: ---\\n')\n",
        "# print(input_text[:-1000])"
      ],
      "metadata": {
        "id": "GfNoQtFAh-FU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb611d78-59fc-4991-946f-57cd3cd101c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- First 1000 characters: ---\n",
            "\n",
            "\n",
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
            "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
            "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
            "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
            "And that's how Ben found an amazing vase in the store!\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
            "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his fri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_from_scratch import python_utils\n",
        "\n",
        "# 2,717,700 stories\n",
        "#\n",
        "# 9/10 -> 2,097,152 stories -> 750 M words\n",
        "num_samples = len(input_text.split('<|endoftext|>'))\n",
        "\n",
        "print(f'{num_samples=}')\n",
        "\n",
        "# arbitrarily choosing 9/10 as scale factor (we use this so it's easier to experiment with smaller chunks without changing much code)\n",
        "num_samples = int(num_samples * 0.9)\n",
        "\n",
        "print(f'{num_samples=} after scaling')\n",
        "\n",
        "num_samples = python_utils.closest_power_of_two(num_samples)\n",
        "\n",
        "print(f'{num_samples=} after choosing closest power of 2')"
      ],
      "metadata": {
        "id": "Bj5jzFIFko-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333ce2f7-f6c1-4d48-a640-3c1fb78a2795"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_samples=2717700\n",
            "num_samples=2445930 after scaling\n",
            "num_samples=2097152 after choosing closest power of 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clip the input text at number of samples\n",
        "input_text = python_utils.get_first_n_examples(input_text, n=num_samples, delimiter='<|endoftext|>')"
      ],
      "metadata": {
        "id": "cEbqmb1OluSE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll trim down the dataset to something that loads quickly"
      ],
      "metadata": {
        "id": "aV-7fZA_jm6t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenizer\n",
        "from gpt_from_scratch import (\n",
        "    byte_pair_encoding_tokenizer,\n",
        "    file_utils,\n",
        "    tokenizer_utils,\n",
        ")\n",
        "\n",
        "# load pretrained tokenizer\n",
        "tokenizer_filepath = 'tokenizer_bin/tokenizer__vocab_2048_samples_100000_dataset_tinystories.pkl'\n",
        "tokenizer = file_utils.deserialize_dataclass_from_pickle_file(\n",
        "    cls=byte_pair_encoding_tokenizer.BytePairEncodingWordTokenizer,\n",
        "    file_path=tokenizer_filepath,\n",
        ")\n",
        "print(f\"Loaded tokenizer from {tokenizer_filepath}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL1jIemKRXiI",
        "outputId": "025e2b9b-62e4-45ae-9b16-f86238cfdf9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded tokenizer from tokenizer_bin/tokenizer__vocab_2048_samples_100000_dataset_tinystories.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure we can tokenize some example text\n",
        "tokenizer_utils.show_token_mapping(tokenizer, \"Jack and Jill were doing mechinterp research\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3oz5hTrRcg2",
        "outputId": "3029a50d-a993-4868-8fc7-b4ce50cc5aff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting text into words via regex...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding words as tokens: 100%|██████████| 13/13 [00:00<00:00, 62102.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\t\tJack and Jill were doing mechinterp research\n",
            "Splitting text into words via regex...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding words as tokens: 100%|██████████| 13/13 [00:00<00:00, 53667.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized:\tJack and Jill were doing mechinterp research\n",
            "Token ID | Token Bytes | Token String\n",
            "---------+-------------+--------------\n",
            "     957 | 4A 61 63 6B | 'Jack'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+004A LATIN CAPITAL LETTER J (1 bytes: 4A)\n",
            "          U+0061 LATIN SMALL LETTER A (1 bytes: 61)\n",
            "          U+0063 LATIN SMALL LETTER C (1 bytes: 63)\n",
            "          U+006B LATIN SMALL LETTER K (1 bytes: 6B)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "     263 | 61 6E 64    | 'and'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0061 LATIN SMALL LETTER A (1 bytes: 61)\n",
            "          U+006E LATIN SMALL LETTER N (1 bytes: 6E)\n",
            "          U+0064 LATIN SMALL LETTER D (1 bytes: 64)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "    1890 | 4A 69 6C 6C | 'Jill'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+004A LATIN CAPITAL LETTER J (1 bytes: 4A)\n",
            "          U+0069 LATIN SMALL LETTER I (1 bytes: 69)\n",
            "          U+006C LATIN SMALL LETTER L (1 bytes: 6C)\n",
            "          U+006C LATIN SMALL LETTER L (1 bytes: 6C)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "     377 | 77 65 72 65 | 'were'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0077 LATIN SMALL LETTER W (1 bytes: 77)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "          U+0072 LATIN SMALL LETTER R (1 bytes: 72)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "    1279 | 64 6F 69 6E 67 | 'doing'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0064 LATIN SMALL LETTER D (1 bytes: 64)\n",
            "          U+006F LATIN SMALL LETTER O (1 bytes: 6F)\n",
            "          U+0069 LATIN SMALL LETTER I (1 bytes: 69)\n",
            "          U+006E LATIN SMALL LETTER N (1 bytes: 6E)\n",
            "          U+0067 LATIN SMALL LETTER G (1 bytes: 67)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "     421 | 6D 65       | 'me'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+006D LATIN SMALL LETTER M (1 bytes: 6D)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "     350 | 63 68       | 'ch'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0063 LATIN SMALL LETTER C (1 bytes: 63)\n",
            "          U+0068 LATIN SMALL LETTER H (1 bytes: 68)\n",
            "     264 | 69 6E       | 'in'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0069 LATIN SMALL LETTER I (1 bytes: 69)\n",
            "          U+006E LATIN SMALL LETTER N (1 bytes: 6E)\n",
            "     402 | 74 65 72    | 'ter'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0074 LATIN SMALL LETTER T (1 bytes: 74)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "          U+0072 LATIN SMALL LETTER R (1 bytes: 72)\n",
            "     112 | 70          | 'p'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0070 LATIN SMALL LETTER P (1 bytes: 70)\n",
            "      32 | 20          | ' '\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0020 SPACE (1 bytes: 20)\n",
            "     265 | 72 65       | 're'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0072 LATIN SMALL LETTER R (1 bytes: 72)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "    1708 | 73 65 61 72 | 'sear'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0073 LATIN SMALL LETTER S (1 bytes: 73)\n",
            "          U+0065 LATIN SMALL LETTER E (1 bytes: 65)\n",
            "          U+0061 LATIN SMALL LETTER A (1 bytes: 61)\n",
            "          U+0072 LATIN SMALL LETTER R (1 bytes: 72)\n",
            "     350 | 63 68       | 'ch'\n",
            "          Jack and Jill were doing mechinterp research\n",
            "          U+0063 LATIN SMALL LETTER C (1 bytes: 63)\n",
            "          U+0068 LATIN SMALL LETTER H (1 bytes: 68)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize input text\n",
        "# note: tiktoken is using their implementation of lib.rs in rust, so much faster\n",
        "print(f'Tokenizing input text of length: {len(input_text)}')\n",
        "tokens = tokenizer.encode(input_text)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "print('Finished tokenizing input text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TdxcriXRaFE",
        "outputId": "e0f3e553-c928-4923-d401-b3b074893eee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing input text of length: 1718216655\n",
            "Splitting text into words via regex...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding words as tokens: 100%|██████████| 746873790/746873790 [49:25<00:00, 251891.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished tokenizing input text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gx2Wgn2VGMO",
        "outputId": "e92131b8-db69-4125-86dd-c27e33727af7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.merges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAaNWM_qWePA",
        "outputId": "6d46d798-7e3d-49e7-cb8f-d49749edc0ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2048"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move to GPU, since we can fit it for this dataset\n",
        "device = get_best_available_torch_device()\n",
        "\n",
        "tokens = tokens.to(device)"
      ],
      "metadata": {
        "id": "B9OnivLhd9DW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(bschoen): Shrink other parameters from `config` to match TinyStories paper\n",
        "\n",
        "# vocab_size = 50304 # note: nice number after ~52,000 initially used by GPT-2\n",
        "\n",
        "# TODO(bschoen): It's really 2048 + 1 special tokens lmao\n",
        "vocab_size = 4096\n",
        "\n",
        "# load text via dataloader\n",
        "# TODO(bschoen): Why do we pick this?\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "\n",
        "# let's pick number of tokens closest power of two (above so we get all tokens)\n",
        "# total_batch_size = python_utils.next_power_of_two(len(tokens))\n",
        "\n",
        "B = 128 # micro batch size\n",
        "# T = 1024 # sequence length (from GPT-2)\n",
        "T = 512 # sequence length (matches tinystories paper)\n",
        "\n",
        "assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
        "\n",
        "# compute what our gradient accumulation should be\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "\n",
        "print(f'total length of input text (in characters): {len(input_text)}')\n",
        "print(f'total number of tokens: {len(tokens)}')\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "# create a train loader that will continually give us new batches\n",
        "train_loader = data_loader.DataLoaderLite(B=B, T=T, tokens=tokens)\n",
        "# train_loader = DataLoaderLiteBasedOnPytorch(B=B, T=T, tokens=tokens)\n",
        "# pytorch_train_data_loader = train_loader.get_dataloader()\n",
        "\n",
        "# note: these are computed based on data loading\n",
        "\n",
        "# want to make it through all of our tokens\n",
        "\n",
        "# this seems way too low @ 100, thus the override\n",
        "max_steps = 10000\n",
        "# max_steps = len(tokens) // total_batch_size\n",
        "\n",
        "# chosen fairly arbitrarily\n",
        "# TODO(bschoen): GPT-2 seems to do this as a faction of tokens (proportional)\n",
        "warmup_steps = int(max_steps * 0.1)\n",
        "\n",
        "# learning rate\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "\n",
        "print(f'| {max_steps=} | {warmup_steps=} | {max_lr=:.6f} | {min_lr=:.6f} |')"
      ],
      "metadata": {
        "id": "Piwp_wFzgEPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23840c4b-83b9-43af-fb56-675960777e83"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total length of input text (in characters): 1718216655\n",
            "total number of tokens: 786770743\n",
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 8\n",
            "loaded 786770743 tokens\n",
            "1 epoch = 12005 batches (steps to make one pass through data)\n",
            "| max_steps=10000 | warmup_steps=1000 | max_lr=0.000600 | min_lr=0.000060 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial layer dominates pretty much everything\n",
        "#\n",
        "# Decrease your batch size until things fit\n",
        "# By default you want to max it out with nice numbers\n",
        "#\n",
        "# ... + switching over to tinystories\n",
        "#\n",
        "#   | step   49 | loss: 4.6334 | lr 6.0832e-05 | norm: 0.3634 | dt: 3104.65ms | tok/sec: 168872.06 |\n",
        "#\n",
        "#   * interestingly the same tokens per second\n",
        "#\n",
        "# ... + (B=16) (since was running out of GPU space)\n",
        "#\n",
        "#   | step   49 | loss: 4.2973 | lr 3.0000e-04 | norm: 1.3571 | dt: 3232.42ms | tok/sec: 162196.82 |\n",
        "#   ...\n",
        "#   | step  999 | loss: 1.1815 | lr 6.0002e-05 | norm: 0.3806 | dt: 3234.65ms | tok/sec: 162084.83 |\n",
        "#\n",
        "# ... + custom tokenizer\n",
        "#\n",
        "#   | step   49 | loss: 3.2605 | lr 6.0658e-05 | norm: 0.1714 | dt: 2323.44ms | tok/sec: 225651.47 |\n",
        "#\n",
        "# ... + custom data loader\n",
        "#\n",
        "#   | step   49 | loss: 3.2779 | lr 6.0658e-05 | norm: 0.1305 | dt: 4052.50ms | tok/sec: 129373.97 |\n",
        "#\n",
        "#   * literally slower, reverting in favor of just putting everything on the GPU since can fit it for this dataset\n",
        "#\n",
        "# ... + moving everything in input dataset to GPU first\n",
        "#\n",
        "#   | step   49 | loss: 3.1756 | lr 3.0000e-04 | norm: 2.3773 | dt: 2296.51ms | tok/sec: 228297.49 |\n",
        "#\n",
        "# ... + increasing microbatch size to 64\n",
        "#\n",
        "#   | step   49 | loss: 3.1903 | lr 3.0000e-04 | norm: 1.8769 | dt: 2132.62ms | tok/sec: 245841.70 |\n",
        "#   ...\n",
        "#   | step  999 | loss: 0.8000 | lr 6.0002e-05 | norm: 0.2571 | dt: 2136.10ms | tok/sec: 245441.52 |\n",
        "#\n",
        "# ... + context size 512\n",
        "#\n",
        "#   | step   36 | loss: 3.3956 | lr 1.7902e-04 | norm: 0.5462 | dt: 2095.98ms | tok/sec: 250139.40 |\n",
        "#\n",
        "# ... + batch size 128\n",
        "#\n",
        "#   | step   21 | loss: 3.5883 | lr 4.4836e-04 | norm: 1.9053 | dt: 2023.95ms | tok/sec: 259041.59 |\n",
        "#\n",
        "#"
      ],
      "metadata": {
        "id": "OAWujXuwLwwZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_learning_rate(\n",
        "    step: int,\n",
        "    warmup_steps: int,\n",
        "    max_steps: int,\n",
        "    min_lr: float,\n",
        "    max_lr: float,\n",
        "  ) -> float:\n",
        "\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if step < warmup_steps:\n",
        "        # the +1 is because for the 1st iteration no reason to multiply by 0\n",
        "        return max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if step > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "\n",
        "    # coeff starts at 1 and goes to 0\n",
        "    # TODO(bschoen): Is this cos weight decay?\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "OyEwX6_hXYsF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# reset unused CUDA memory\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# {use F32 multiplication}\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# now we'll try multiple batches\n",
        "device = get_best_available_torch_device()\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "print(\"Creating model...\")\n",
        "config = GPTConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=T,\n",
        ")\n",
        "\n",
        "model = GPT(config)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9JiXn7tXLNy",
        "outputId": "eade5c29-9d3d-463f-c812-952e4380f039"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Creating model...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(4096, 768)\n",
              "    (wpe): Embedding(512, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=4096, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Compiling model...\")\n",
        "model = torch.compile(model)\n",
        "print(\"Done compiling model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfDa8_SFXnvZ",
        "outputId": "2a977275-1e97-4eec-848c-91ae7e50b5e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling model...\n",
            "Done compiling model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Karpathy: \"AdamW is basically a bugfix of Adam\"\n",
        "#\n",
        "# note: pretty good default learning rate for early experimentation\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=max_lr,\n",
        "    device=device.type,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNhEUZuXYT3I",
        "outputId": "f0be4f76-cefc-4f88-a635-cc2647c000c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 88,473,600 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(max_steps):\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # gradient accumulation\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "\n",
        "        # print(f' - {micro_step=}')\n",
        "        x, y = train_loader.next_batch()\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # automatic mixed precision\n",
        "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "\n",
        "          logits, loss = model(x, y)\n",
        "\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        #\n",
        "        # \"accumulation in the gradients is equivalent to the sum in the loss\"\n",
        "        #\n",
        "        # used small self contained version of just this chunk to debug\n",
        "        # since the loss objects etc can be used in isolation\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_learning_rate(\n",
        "        step=i,\n",
        "        warmup_steps=warmup_steps,\n",
        "        max_steps=max_steps,\n",
        "        min_lr=min_lr,\n",
        "        max_lr=max_lr,\n",
        "    )\n",
        "\n",
        "    # update optimizer\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "    print(f\"| step {i:4d} | loss: {loss_accum:.4f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f} |\")"
      ],
      "metadata": {
        "id": "9Tn19WOAJ39u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "604cebe5-2b48-47b8-f839-45ca2b4ce87e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| step    0 | loss: 8.5812 | lr 6.0000e-07 | norm: 95.2466 | dt: 28760.22ms | tok/sec: 18229.62 |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-00bd39892d92>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    299\u001b[0m             )\n\u001b[1;32m    300\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *flat_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompiledFunctionBackward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_compiled_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;31m# TODO: figure out how to refactor the backward properly so I can use aot_dispatch_subclass_wrapper() here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\u001b[0m in \u001b[0;36mcall_compiled_backward\u001b[0;34m()\u001b[0m\n\u001b[1;32m    829\u001b[0m                         )\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                 out = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m    832\u001b[0m                     \u001b[0mCompiledFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_bw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m                     \u001b[0mall_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_current_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mcopy_misaligned_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_to_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36m_run_from_cache\u001b[0;34m(compiled_graph, inputs)\u001b[0m\n\u001b[1;32m    932\u001b[0m         ).call\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torchinductor_root/vc/cvcnojaz6a2jt4a7ndpwjzermfnoovjvqfx3sb33sbwr7nozlfqm.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;31m# Source Nodes: [loss], Original ATen: [aten.nll_loss_backward, aten.nll_loss_forward]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0mtriton_poi_fused_nll_loss_backward_nll_loss_forward_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals_151\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mbuf21\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty_strided_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2097152\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m         \u001b[0;31m# Source Nodes: [loss], Original ATen: [aten._log_softmax_backward_data, aten.add, aten.nll_loss_backward, aten.nll_loss_forward]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0mtriton_red_fused__log_softmax_backward_data_add_nll_loss_backward_nll_loss_forward_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals_151\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_element_type_295\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malias_27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model(\n",
        "    # example from validation set\n",
        "    prompt=\"\"\"Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\"\"\",\n",
        "    num_samples=10,\n",
        "    max_tokens=300,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    stop_token='<|endoftext|>',\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "W_J5Bmb4MAJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A lot of these with the custom tokenizer are actually pretty good\n",
        "\"\"\"\n",
        " [0] > Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
        "Tom asked his friend, Sam, to help him search for the ball. They looked all around the pit, but they could not find it. Then, something unexpected happened. A little bird flew down and said, \"I found your red ball!\" Tom and Sam were happy and surprised. They looked at the bird and had a new friend to play with.\n",
        "<|endoftext|>\n",
        "\n",
        " [1] > Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
        "Tom asked his friend, Sam, to help him search for the ball. They looked near the trees and under the big tree. Finally, they found the red ball near a small pond. Tom was so happy! He said, \"Thank you, Sam! You are a good friend.\"\n",
        "From that day on, Tom and Sam played in the park with the red ball. They were always happy and had lots of fun.\n",
        "<|endoftext|>\n",
        "\n",
        " [2] > Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
        "Tom asked his friend, Sam, to help him search for the ball. Sam nodded and said, \"Lets search together.\" They looked in the pit, under the leaves, and in the yard. And now, they found the red ball in the pit. Tom was so happy, and he thanked Sam for coming to play. They sat on the pit and played together.\n",
        "<|endoftext|>\n",
        "\n",
        " [3] > Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
        "Tom asked his friend, Sam, to help him search for the ball. They looked in the park, in the grass, and behind the rock. They could not find the ball either. They were very sad.\n",
        "Then, Tom had an idea. He asked his friend, Tom, if they could search for the red ball. They looked in his house, behind a tree, and in the yard. With Toms help, Tom searched very high and low. Finally, they found the red ball! Tom was so happy and thanked Tom. From that day on, Tom and Tom played with the red ball all by the pit together.\n",
        "<|endoftext|>\n",
        "\n",
        " [4] > Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
        "Tom asked his friend, Sam, to help him search for the ball. They looked around the big pit and near the big pond. At first, they found a note under a tree. The note said, \"This ball is magic!\"\n",
        "Tom and Sam looked all around the pit. Then, they saw a big red balloon near the pond. Tom picked up the big red balloon. Suddenly, the big red balloon popped! Inside the balloon, there were many small balls with the red balloon inside.\n",
        "Tom and Sam were very happy to have a new toy balloon. They played with the balls all day, taking turns with the magic balloon from the pit. When they played with them, they were not hungry at the red balloon.\n",
        "<|endoftext|>\n",
        "\n",
        "  ...\n",
        "\n",
        " [7] > Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
        "Tom asked his friend, Sam, to help him search for the ball. Sam said, \"Dont worry, Tom. I will help you find it.\" They looked under the trees and behind the trees. They did not find the red ball.\n",
        "At the end of the day, they found the red ball. It was behind a big tree. Tom was very happy. He hugged his red ball and said, \"Thank you, Sam. You are very kind.\" Sam smiled and said, \"Youre welcome! Come back to my home, Tom.\"\n",
        "<|endoftext|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mapjHr7jgWL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}