{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-schoen/gpt_from_scratch/blob/main/colab/refusal_direction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo of bypassing refusal"
      ],
      "metadata": {
        "id": "82acAhWYGIPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FS1mcIy9b21o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RHXEu_9Xb3bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Demo of bypassing refusal](#scrollTo=82acAhWYGIPx)\n",
        "\n",
        ">>[Setup](#scrollTo=fcxHyDZw6b86)\n",
        "\n",
        ">>>[Load model](#scrollTo=6ZOoJagxD49V)\n",
        "\n",
        ">>>[Load harmful / harmless datasets](#scrollTo=rF7e-u20EFTe)\n",
        "\n",
        ">>>[Tokenization utils](#scrollTo=KOKYA61k8LWt)\n",
        "\n",
        ">>>[Generation utils](#scrollTo=gtrIK8x78SZh)\n",
        "\n",
        ">>[Finding the \"refusal direction\"](#scrollTo=W9O8dm0_EQRk)\n",
        "\n",
        ">>[Ablate \"refusal direction\" via inference-time intervention](#scrollTo=2EoxY5i1CWe3)\n",
        "\n",
        ">>[Orthogonalize weights w.r.t. \"refusal direction\"](#scrollTo=t9KooaWaCDc_)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "s-_vu8HuGEb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates our method for bypassing refusal, levaraging the insight that refusal is mediated by a 1-dimensional subspace. We recommend reading the [research post](https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction) for a more thorough explanation."
      ],
      "metadata": {
        "id": "j7hOtw7UHXdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using HookedTransformer"
      ],
      "metadata": {
        "id": "cJU18iqfb4zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "fcxHyDZw6b86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama"
      ],
      "metadata": {
        "id": "dLeei4-T6Wef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import functools\n",
        "import einops\n",
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "import textwrap\n",
        "import gc\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from torch import Tensor\n",
        "from typing import List, Callable\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from transformers import AutoTokenizer\n",
        "from jaxtyping import Float, Int\n",
        "from colorama import Fore"
      ],
      "metadata": {
        "id": "_vhhwl-2-jPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efJ2iv9OMzCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WUfLxtDMrqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "6ZOoJagxD49V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnp65Vsg5x-5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# MODEL_PATH = 'meta-llama/meta-llama-3-8b-instruct'\n",
        "MODEL_PATH = 'google/gemma-2-2b-it'\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "model = HookedTransformer.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    device=DEVICE,\n",
        "    dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://github.com/andyrdt/refusal_direction/blob/main/pipeline/model_utils/gemma_model.py#L100C9-L100C40\n",
        "model.tokenizer.padding_side = 'left'"
      ],
      "metadata": {
        "id": "JhGb5nJhQ6fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure our understanding of the tokenization here is correct\n",
        "example_prompt = \"hello\"\n",
        "\n",
        "# note: hello maps to ~128k token int, that's not the sequence length but can be confusing at first glance if not paying attention\n",
        "tokens = model.tokenizer.encode(example_prompt, return_tensors=\"pt\")\n",
        "tokens = tokens.to('cuda')\n",
        "\n",
        "print(f'{tokens.shape=}')\n",
        "\n",
        "reconstructed_example_prompt = model.tokenizer.decode(tokens[0])\n",
        "\n",
        "print(f\"Original prompt: {example_prompt}\")\n",
        "print(f\"Reconstructed prompt: {reconstructed_example_prompt}\")"
      ],
      "metadata": {
        "id": "AlDXTPXAR_qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(\n",
        "    'What is the capital of France?',\n",
        "    max_new_tokens=10,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "OKFe7XA3SLPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LddjMzlSLSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aU335fu5SLU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load harmful / harmless datasets"
      ],
      "metadata": {
        "id": "rF7e-u20EFTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_harmful_instructions():\n",
        "    url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
        "    instructions = dataset['goal'].tolist()\n",
        "\n",
        "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
        "    return train, test\n",
        "\n",
        "def get_harmless_instructions():\n",
        "    hf_path = 'tatsu-lab/alpaca'\n",
        "    dataset = load_dataset(hf_path)\n",
        "\n",
        "    # filter for instructions that do not have inputs\n",
        "    instructions = []\n",
        "    for i in range(len(dataset['train'])):\n",
        "        if dataset['train'][i]['input'].strip() == '':\n",
        "            instructions.append(dataset['train'][i]['instruction'])\n",
        "\n",
        "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "5i1XcVIgHEE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
        "harmless_inst_train, harmless_inst_test = get_harmless_instructions()"
      ],
      "metadata": {
        "id": "Rth8yvLZJsXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Harmful instructions:\")\n",
        "for i in range(4):\n",
        "    print(f\"\\t{repr(harmful_inst_train[i])}\")\n",
        "print(\"Harmless instructions:\")\n",
        "for i in range(4):\n",
        "    print(f\"\\t{repr(harmless_inst_train[i])}\")"
      ],
      "metadata": {
        "id": "Qv2ALDY_J44G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization utils"
      ],
      "metadata": {
        "id": "KOKYA61k8LWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemma chat template is based on\n",
        "# - Official Gemma documentation: https://ai.google.dev/gemma/docs/formatting\n",
        "\n",
        "GEMMA_CHAT_TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{instruction}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "def format_instruction_gemma_chat(\n",
        "    instruction: str,\n",
        "    output: str=None,\n",
        "    system: str=None,\n",
        "    include_trailing_whitespace: bool=True,\n",
        "):\n",
        "    if system is not None:\n",
        "        raise ValueError(\"System prompts are not supported for Gemma models.\")\n",
        "    else:\n",
        "        formatted_instruction = GEMMA_CHAT_TEMPLATE.format(instruction=instruction)\n",
        "\n",
        "    if not include_trailing_whitespace:\n",
        "        formatted_instruction = formatted_instruction.rstrip()\n",
        "\n",
        "    if output is not None:\n",
        "        formatted_instruction += output\n",
        "\n",
        "    return formatted_instruction\n",
        "\n",
        "def tokenize_instructions_gemma_chat(\n",
        "    tokenizer: AutoTokenizer,\n",
        "    instructions: List[str],\n",
        "    outputs: List[str]=None,\n",
        "    system: str=None,\n",
        "    include_trailing_whitespace=True,\n",
        "):\n",
        "    if outputs is not None:\n",
        "        prompts = [\n",
        "            format_instruction_gemma_chat(instruction=instruction, output=output, system=system, include_trailing_whitespace=include_trailing_whitespace)\n",
        "            for instruction, output in zip(instructions, outputs)\n",
        "        ]\n",
        "    else:\n",
        "        prompts = [\n",
        "            format_instruction_gemma_chat(instruction=instruction, system=system, include_trailing_whitespace=include_trailing_whitespace)\n",
        "            for instruction in instructions\n",
        "        ]\n",
        "\n",
        "    result = tokenizer(\n",
        "        prompts,\n",
        "        padding=True,\n",
        "        truncation=False,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # note: we do this for compatibility with original notebook\n",
        "    return result['input_ids']\n",
        "\n",
        "tokenize_instructions_fn = functools.partial(\n",
        "    tokenize_instructions_gemma_chat,\n",
        "    tokenizer=model.tokenizer,\n",
        "  )"
      ],
      "metadata": {
        "id": "P8UPQSfpWOSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation utils"
      ],
      "metadata": {
        "id": "gtrIK8x78SZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate_with_hooks(\n",
        "    model: HookedTransformer,\n",
        "    toks: Int[Tensor, 'batch_size seq_len'],\n",
        "    max_tokens_generated: int = 64,\n",
        "    fwd_hooks = [],\n",
        ") -> List[str]:\n",
        "\n",
        "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device)\n",
        "    all_toks[:, :toks.shape[1]] = toks\n",
        "\n",
        "    for i in range(max_tokens_generated):\n",
        "        with model.hooks(fwd_hooks=fwd_hooks):\n",
        "            logits = model(all_toks[:, :-max_tokens_generated + i])\n",
        "            next_tokens = logits[:, -1, :].argmax(dim=-1) # greedy sampling (temperature=0)\n",
        "            all_toks[:,-max_tokens_generated+i] = next_tokens\n",
        "\n",
        "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "def get_generations(\n",
        "    model: HookedTransformer,\n",
        "    instructions: List[str],\n",
        "    tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],\n",
        "    fwd_hooks = [],\n",
        "    max_tokens_generated: int = 64,\n",
        "    batch_size: int = 4,\n",
        ") -> List[str]:\n",
        "\n",
        "    generations = []\n",
        "\n",
        "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
        "        toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
        "        generation = _generate_with_hooks(\n",
        "            model,\n",
        "            toks,\n",
        "            max_tokens_generated=max_tokens_generated,\n",
        "            fwd_hooks=fwd_hooks,\n",
        "        )\n",
        "        generations.extend(generation)\n",
        "\n",
        "    return generations"
      ],
      "metadata": {
        "id": "94jRJDR0DRoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the \"refusal direction\""
      ],
      "metadata": {
        "id": "W9O8dm0_EQRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_INST_TRAIN = 32\n",
        "\n",
        "# tokenize instructions\n",
        "harmful_toks = tokenize_instructions_fn(instructions=harmful_inst_train[:N_INST_TRAIN])\n",
        "harmless_toks = tokenize_instructions_fn(instructions=harmless_inst_train[:N_INST_TRAIN])\n",
        "\n",
        "# run model on harmful and harmless instructions, caching intermediate activations\n"
      ],
      "metadata": {
        "id": "MbY79kSP8oOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(harmful_toks)"
      ],
      "metadata": {
        "id": "Qah3hVpeS_oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "harmful_logits, harmful_cache = model.run_with_cache(\n",
        "    harmful_toks,\n",
        "    names_filter=lambda hook_name: 'resid' in hook_name,\n",
        ")"
      ],
      "metadata": {
        "id": "7NYPhZFkS3XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "harmless_logits, harmless_cache = model.run_with_cache(\n",
        "    harmless_toks,\n",
        "    names_filter=lambda hook_name: 'resid' in hook_name,\n",
        ")"
      ],
      "metadata": {
        "id": "GyPqIyOnS3au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute difference of means between harmful and harmless activations at an intermediate layer\n",
        "\n",
        "# note: is this just chosen arbitrarily lol?\n",
        "pos = -1\n",
        "layer = 14\n",
        "\n",
        "harmful_mean_act = harmful_cache['resid_pre', layer][:, pos, :].mean(dim=0)\n",
        "harmless_mean_act = harmless_cache['resid_pre', layer][:, pos, :].mean(dim=0)\n",
        "\n",
        "refusal_dir = harmful_mean_act - harmless_mean_act\n",
        "refusal_dir = refusal_dir / refusal_dir.norm()"
      ],
      "metadata": {
        "id": "tqD5E8Vc_w5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{harmful_mean_act.shape=}')\n",
        "print(f'{harmless_mean_act.shape=}')\n",
        "print(f'{refusal_dir.shape=}')"
      ],
      "metadata": {
        "id": "-acoIMbjUIAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up memory\n",
        "del harmful_cache, harmless_cache, harmful_logits, harmless_logits\n",
        "gc.collect(); torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "NU9rjXPT4uQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ablate \"refusal direction\" via inference-time intervention\n",
        "\n",
        "Given a \"refusal direction\" $\\widehat{r} \\in \\mathbb{R}^{d_{\\text{model}}}$ with unit norm, we can ablate this direction from the model's activations $a_{l}$:\n",
        "$${a}_{l}' \\leftarrow a_l - (a_l \\cdot \\widehat{r}) \\widehat{r}$$\n",
        "\n",
        "By performing this ablation on all intermediate activations, we enforce that the model can never express this direction (or \"feature\")."
      ],
      "metadata": {
        "id": "2EoxY5i1CWe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def direction_ablation_hook(\n",
        "    activation: Float[Tensor, \"... d_act\"],\n",
        "    hook: HookPoint,\n",
        "    direction: Float[Tensor, \"d_act\"]\n",
        "):\n",
        "    proj = einops.einsum(\n",
        "        activation,\n",
        "        direction.view(-1, 1),\n",
        "        '... d_act, d_act single -> ... single',\n",
        "    ) * direction\n",
        "\n",
        "    return activation - proj"
      ],
      "metadata": {
        "id": "26rf-yncB2PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_INST_TEST = 32\n",
        "intervention_dir = refusal_dir\n",
        "intervention_layers = list(range(model.cfg.n_layers)) # all layers\n",
        "\n",
        "hook_fn = functools.partial(\n",
        "    direction_ablation_hook,\n",
        "    direction=intervention_dir,\n",
        ")\n",
        "\n",
        "fwd_hooks = [\n",
        "    (utils.get_act_name(act_name, l), hook_fn)\n",
        "    for l in intervention_layers\n",
        "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
        "]\n",
        "\n",
        "intervention_generations = get_generations(\n",
        "    model,\n",
        "    harmful_inst_test[:N_INST_TEST],\n",
        "    tokenize_instructions_fn,\n",
        "    fwd_hooks=fwd_hooks,\n",
        ")\n",
        "\n",
        "baseline_generations = get_generations(\n",
        "    model,\n",
        "    harmful_inst_test[:N_INST_TEST],\n",
        "    tokenize_instructions_fn,\n",
        "    fwd_hooks=[],\n",
        ")"
      ],
      "metadata": {
        "id": "sR1G5bXoEDty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(N_INST_TEST):\n",
        "    print(f\"INSTRUCTION {i}: {repr(harmful_inst_test[i])}\")\n",
        "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
        "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.RED + f\"INTERVENTION COMPLETION:\")\n",
        "    print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.RESET)"
      ],
      "metadata": {
        "id": "pxbJr4vCFCOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orthogonalize weights w.r.t. \"refusal direction\"\n",
        "\n",
        "We can implement the intervention equivalently by directly orthogonalizing the weight matrices that write to the residual stream with respect to the refusal direction $\\widehat{r}$:\n",
        "$$W_{\\text{out}}' \\leftarrow W_{\\text{out}} - \\widehat{r}\\widehat{r}^{\\mathsf{T}} W_{\\text{out}}$$\n",
        "\n",
        "By orthogonalizing these weight matrices, we enforce that the model is unable to write direction $r$ to the residual stream at all!"
      ],
      "metadata": {
        "id": "t9KooaWaCDc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_orthogonalized_matrix(\n",
        "    matrix: Float[Tensor, '... d_model'],\n",
        "    vec: Float[Tensor, 'd_model'],\n",
        ") -> Float[Tensor, '... d_model']:\n",
        "\n",
        "    proj = einops.einsum(\n",
        "        matrix,\n",
        "        vec.view(-1, 1),\n",
        "        '... d_model, d_model single -> ... single',\n",
        "    ) * vec\n",
        "\n",
        "    return matrix - proj"
      ],
      "metadata": {
        "id": "8fhx0i9vCEou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)\n",
        "\n",
        "for block in model.blocks:\n",
        "    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)\n",
        "    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)"
      ],
      "metadata": {
        "id": "GC7cpMXZCG64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orthogonalized_generations = get_generations(\n",
        "    model,\n",
        "    harmful_inst_test[:N_INST_TEST],\n",
        "    tokenize_instructions_fn,\n",
        "    fwd_hooks=[],\n",
        ")"
      ],
      "metadata": {
        "id": "1Y-qtouNGf3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(N_INST_TEST):\n",
        "    print(f\"INSTRUCTION {i}: {repr(harmful_inst_test[i])}\")\n",
        "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
        "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.RED + f\"INTERVENTION COMPLETION:\")\n",
        "    print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.MAGENTA + f\"ORTHOGONALIZED COMPLETION:\")\n",
        "    print(textwrap.fill(repr(orthogonalized_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
        "    print(Fore.RESET)"
      ],
      "metadata": {
        "id": "r68O4_4DG3P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exUh3PEHRe9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using RefusalDirection Repo (Huggingface)"
      ],
      "metadata": {
        "id": "XcDwRwl6cAkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This allows us to use models that are supported by Huggingface but not HookedTransformers (like 405B)"
      ],
      "metadata": {
        "id": "BVwpmEFvcHYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/andyrdt/refusal_direction.git\n",
        "\n",
        "# change into the repo directory\n",
        "import os\n",
        "\n",
        "os.chdir('refusal_direction')\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "!ls\n",
        "\n",
        "# !source setup.sh\n",
        "\n",
        "from pipeline import run_pipeline"
      ],
      "metadata": {
        "id": "-udPHVkQcGOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basically replicating `run_pipeline` explicitly, so we can go step by step: https://github.com/andyrdt/refusal_direction/blob/main/pipeline/run_pipeline.py#L136"
      ],
      "metadata": {
        "id": "dARm1WkRcpY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 405B: https://huggingface.co/neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8\n",
        "#\n",
        "# > This optimization reduces the number of bits per parameter from 16 to 8,\n",
        "#   reducing the disk size and GPU memory requirements by approximately 50%.\n",
        "#   In particular, this model can now be loaded and evaluated with a single node\n",
        "#   of 8xH100 GPUs, as opposed to multiple nodes."
      ],
      "metadata": {
        "id": "xQ-YM3EectLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8\")"
      ],
      "metadata": {
        "id": "plIi2Q1odEQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first testing with 8B (since 405B is expensive cluster to run on)\n",
        "model_path = 'neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8'"
      ],
      "metadata": {
        "id": "1kKpz-IcdI1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=model_path)\n",
        "pipe(messages)"
      ],
      "metadata": {
        "id": "K4oPR8s_kOeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from dataset.load_dataset import load_dataset_split, load_dataset\n",
        "\n",
        "from pipeline.config import Config\n",
        "from pipeline.model_utils.model_factory import construct_model_base\n",
        "from pipeline.utils.hook_utils import get_activation_addition_input_pre_hook, get_all_direction_ablation_hooks\n",
        "\n",
        "from pipeline.submodules.generate_directions import generate_directions\n",
        "from pipeline.submodules.select_direction import select_direction, get_refusal_scores\n",
        "from pipeline.submodules.evaluate_jailbreak import evaluate_jailbreak\n",
        "from pipeline.submodules.evaluate_loss import evaluate_loss"
      ],
      "metadata": {
        "id": "ZZwzzPTXdu8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipeline.run_pipeline import (\n",
        "    generate_and_save_candidate_directions,\n",
        "    select_and_save_direction,\n",
        "    generate_and_save_completions_for_dataset,\n",
        "    evaluate_completions_and_save_results_for_dataset,\n",
        "    load_and_sample_datasets,\n",
        "    filter_data,\n",
        ")"
      ],
      "metadata": {
        "id": "h_dNS3frdw14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_alias = os.path.basename(model_path)\n",
        "cfg = Config(model_alias=model_alias, model_path=model_path)\n",
        "\n",
        "model_base = construct_model_base(cfg.model_path)\n",
        "\n",
        "print(\"Load and sample datasets...\")\n",
        "harmful_train, harmless_train, harmful_val, harmless_val = load_and_sample_datasets(cfg)\n",
        "\n",
        "print(\"Filter datasets based on refusal scores...\")\n",
        "harmful_train, harmless_train, harmful_val, harmless_val = filter_data(cfg, model_base, harmful_train, harmless_train, harmful_val, harmless_val)\n",
        "\n",
        "print(\"1. Generate candidate refusal directions...\")\n",
        "candidate_directions = generate_and_save_candidate_directions(cfg, model_base, harmful_train, harmless_train)\n",
        "\n",
        "print(\"2. Select the most effective refusal direction...\")\n",
        "pos, layer, direction = select_and_save_direction(cfg, model_base, harmful_val, harmless_val, candidate_directions)\n",
        "\n",
        "baseline_fwd_pre_hooks, baseline_fwd_hooks = [], []\n",
        "ablation_fwd_pre_hooks, ablation_fwd_hooks = get_all_direction_ablation_hooks(model_base, direction)\n",
        "actadd_fwd_pre_hooks, actadd_fwd_hooks = [(model_base.model_block_modules[layer], get_activation_addition_input_pre_hook(vector=direction, coeff=-1.0))], []\n",
        "\n",
        "print(\"3a. Generate and save completions on harmful evaluation datasets...\")\n",
        "for dataset_name in cfg.evaluation_datasets:\n",
        "    generate_and_save_completions_for_dataset(cfg, model_base, baseline_fwd_pre_hooks, baseline_fwd_hooks, 'baseline', dataset_name)\n",
        "    generate_and_save_completions_for_dataset(cfg, model_base, ablation_fwd_pre_hooks, ablation_fwd_hooks, 'ablation', dataset_name)\n",
        "    generate_and_save_completions_for_dataset(cfg, model_base, actadd_fwd_pre_hooks, actadd_fwd_hooks, 'actadd', dataset_name)\n",
        "\n",
        "print(\"3b. Evaluate completions and save results on harmful evaluation datasets...\")\n",
        "for dataset_name in cfg.evaluation_datasets:\n",
        "    evaluate_completions_and_save_results_for_dataset(cfg, 'baseline', dataset_name, eval_methodologies=cfg.jailbreak_eval_methodologies)\n",
        "    evaluate_completions_and_save_results_for_dataset(cfg, 'ablation', dataset_name, eval_methodologies=cfg.jailbreak_eval_methodologies)\n",
        "    evaluate_completions_and_save_results_for_dataset(cfg, 'actadd', dataset_name, eval_methodologies=cfg.jailbreak_eval_methodologies)\n",
        "\n",
        "print(\"4a. Generate and save completions on harmless evaluation dataset...\")\n",
        "harmless_test = random.sample(load_dataset_split(harmtype='harmless', split='test'), cfg.n_test)"
      ],
      "metadata": {
        "id": "HqxIlUU_dZjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJQ3eZQaeQs9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}