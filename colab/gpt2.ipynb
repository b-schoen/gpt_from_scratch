{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOfXjj2jOQTDgAzPydctXo2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-schoen/gpt_from_scratch/blob/main/colab/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOq7bHwLIPc7",
        "outputId": "2f5ec43f-24d1-4945-e385-1531309e484c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'gpt_from_scratch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# clone repo\n",
        "# !rm -rf gpt_from_scratch\n",
        "!git clone https://github.com/b-schoen/gpt_from_scratch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into the repo directory\n",
        "import os\n",
        "\n",
        "os.chdir('gpt_from_scratch')\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2heAkYZIp0g",
        "outputId": "4e5480b2-df8e-4d94-ba60-6e25a0bf1fa4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content/gpt_from_scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can operate as if this was a local notebook"
      ],
      "metadata": {
        "id": "tUYSCNYTIyWy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "DfSXTCO_I15z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_from_scratch import file_utils\n",
        "\n",
        "# load tinyshakespeare\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "input_filepath = file_utils.download_file_from_url(url)\n",
        "\n",
        "# Read all text from the input file\n",
        "input_text = input_filepath.read_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28TU5lPCJx16",
        "outputId": "2f273366-e646-4505-bc24-69f61757f344"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found in cache: download_cache/4acd659e47adc1daeb7aff503accf0a3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note: if this gets annoying can do an actual pip install requirements\n",
        "!pip install tiktoken\n",
        "!pip install jaxtyping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_83OHQPJ52z",
        "outputId": "9282e191-c94e-4a2e-daf6-1fa407af5230"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.33)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting to optimize\n",
        "\n",
        "> ![NOTE] Starting from \"what hardware do I have, and am I fully utilizing it\"\n",
        "\n",
        "Then looking up NVIDIA spec sheet for A100, we see:\n",
        "\n",
        "| Specification | A100 80GB PCIe | A100 80GB SXM |\n",
        "|---------------|----------------|---------------|\n",
        "| FP64 | 9.7 TFLOPS | 9.7 TFLOPS |\n",
        "| FP64 Tensor Core | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "| FP32 | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "| Tensor Float 32 (TF32) | 156 TFLOPS \\| 312 TFLOPS\\* | 156 TFLOPS \\| 312 TFLOPS\\* |\n",
        "| BFLOAT16 Tensor Core | 312 TFLOPS \\| 624 TFLOPS\\* | 312 TFLOPS \\| 624 TFLOPS\\* |\n",
        "| FP16 Tensor Core | 312 TFLOPS \\| 624 TFLOPS\\* | 312 TFLOPS \\| 624 TFLOPS\\* |\n",
        "| INT8 Tensor Core | 624 TOPS \\| 1248 TOPS\\* | 624 TOPS \\| 1248 TOPS\\* |\n",
        "| GPU Memory | 80GB HBM2e | 80GB HBM2e |\n",
        "| GPU Memory Bandwidth | 1,935GB/s | 2,039GB/s |\n",
        "\n",
        "\n",
        "We're currently at:\n",
        "\n",
        "| Specification | A100 80GB PCIe | A100 80GB SXM |\n",
        "|---------------|----------------|---------------|\n",
        "| FP32 | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "\n",
        "but it turns out we don't really need that much precision for deep learning\n",
        "\n",
        "| Format | Sign | Range (exponent) | Precision (mantissa) |\n",
        "|--------|------|------------------|----------------------|\n",
        "| FP32   | 1    | 8                | 23                   |\n",
        "| TF32   | 1    | 8                | 10                   |\n",
        "| FP16   | 1    | 5                | 10                   |\n",
        "| BF16   | 1    | 8                | 7                    |"
      ],
      "metadata": {
        "id": "xFQ-pwEZRV9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from gpt_from_scratch.gpt2_from_scratch import data_loader\n",
        "from gpt_from_scratch.gpt2_from_scratch.train_gpt2 import (\n",
        "    GPT,\n",
        "    GPTConfig,\n",
        "    get_best_available_torch_device,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ9dtV0EJ0Ha",
        "outputId": "ce7c8eb9-1e25-42bd-d014-83bc958571c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 16\n",
            "loaded 338025 tokens\n",
            "1 epoch = 10 batches (steps to make one pass through data)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "C6v7v2kHgIv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample some outputs to get an idea of where we are\n",
        "\n",
        "from typing import TYPE_CHECKING\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "  from gpt_from_scratch import tokenizer_utils\n",
        "\n",
        "def sample_model(\n",
        "    prompt: str,\n",
        "    num_samples: int,\n",
        "    max_tokens: int,\n",
        "    model: nn.Module,\n",
        "    tokenizer: 'tokenizer_utils.Tokenizer',\n",
        "    device: torch.device,\n",
        ") -> None:\n",
        "\n",
        "    # tokenize\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "    tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (5, 8)\n",
        "\n",
        "    # tokens in this case is just the prompt, and is small enough to fit on GPU\n",
        "    x = tokens.to(device)\n",
        "\n",
        "    while x.size(1) < max_tokens:\n",
        "\n",
        "        # forward the model to get the logits\n",
        "        with torch.no_grad():\n",
        "\n",
        "            logits, loss = model(x) # (B, T, vocab_size)\n",
        "\n",
        "            # take the logits at the last position\n",
        "            # throw away all the logits from things other than the last position\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "\n",
        "            # get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # do top-k sampling of 50 (huggingface pipeline default)\n",
        "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "            #\n",
        "            # \"anything lower than the 50th, we clamp to 0 and never sample it\"\n",
        "            #\n",
        "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "            # select a token from the top-k probabilities\n",
        "            # note: multinomial does not demand the input to sum to 1\n",
        "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "\n",
        "            # gather the corresponding indices\n",
        "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "\n",
        "            # append to the sequence\n",
        "            x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "    # print the generated text\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        tokens = x[i, :max_tokens].tolist()\n",
        "\n",
        "        decoded = tokenizer.decode(tokens)\n",
        "\n",
        "        print(f\"\\n [{i}] >\", decoded)"
      ],
      "metadata": {
        "id": "vQnXakW0gH2I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "n0sUP9PagFcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenizer\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# load text via dataloader\n",
        "#\n",
        "# note: we leave these on CPU, so that the dataloader\n",
        "#       isn't trying to hold the whole set on the GPU\n",
        "#\n",
        "#       so is prefetching moving more data to the GPU?\n",
        "tokens = tokenizer.encode(input_text)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "\n",
        "B = 32 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "\n",
        "assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
        "\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "# create a train loader that will continually give us new batches\n",
        "train_loader = data_loader.DataLoaderLite(B=B, T=T, tokens=tokens)"
      ],
      "metadata": {
        "id": "Piwp_wFzgEPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial layer dominates pretty much everything\n",
        "#\n",
        "# Decrease your batch size until things fit\n",
        "# By default you want to max it out with nice numbers\n",
        "#\n",
        "# Tokens / sec is best unit because agnostic to batch size etc, it's the thing we really care about\n",
        "#\n",
        "# Karpathy recommends the `Automatic Mixed Precision` pytorch tutorial specifically, others are confusing\n",
        "#\n",
        "\n",
        "# Initial w/ Float32 - (B=4, T=32) - mps\n",
        "#\n",
        "#   | step 49 | loss: 6.8048 | dt: 136.36ms | tok/sec: 938.68 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=4, T=32) - cpu\n",
        "#\n",
        "#   | step 14 | loss: 7.6758 | dt: 2578.34ms | tok/sec: 49.64 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=4, T=32) - cuda\n",
        "#\n",
        "#   | step 48 | loss: 6.3560 | dt: 31.72ms | tok/sec: 4035.35 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=16, T=1024) - cuda\n",
        "#\n",
        "#   | step 49 | loss: 6.1039 | dt: 1041.67ms | tok/sec: 15728.63 |\n",
        "#\n",
        "#   * Pretty stable\n",
        "#   * Using full 40 GB GPU (~38.5 GB)\n",
        "#\n",
        "# ... + torch.set_float32_matmul_precision('high')\n",
        "#\n",
        "#   | step 49 | loss: 6.2045 | dt: 382.83ms | tok/sec: 42797.34 |\n",
        "#\n",
        "#   {* decrease precision of optimization itself}\n",
        "#\n",
        "# ... + bfloat16 (automatic mixed precision)\n",
        "#\n",
        "#   | step 49 | loss: 6.0319 | dt: 335.56ms | tok/sec: 48826.04 |\n",
        "#\n",
        "#   * decrease amount of storage we're using per float when moving around\n",
        "#   * pytorch docs *specifically* say to only apply to the model's forward pass and loss calculation\n",
        "#\n",
        "# ... + torch.compile\n",
        "#\n",
        "#   | step 49 | loss: 6.0414 | dt: 192.20ms | tok/sec: 85246.46 |\n",
        "#\n",
        "#   * Karpathy: \"Really incredible piece of code from the pytorch team\"\n",
        "#   * Like LLVM for pytorch\n",
        "#   * No reason to not use it\n",
        "#\n",
        "# ... + scaled flash attention\n",
        "#\n",
        "#   | step 49 | loss: 6.1316 | dt: 143.52ms | tok/sec: 114161.25 |\n",
        "#\n",
        "#   * There are operations that torch.compile will not find\n",
        "#   * Kernel fusion, but kernel fusion that torch.compile can't find\n",
        "#   * Flash attention actually more flops! Mindful of memory hierarchy (what's in HBM, shared_memory, min reads/writes)\n",
        "#   * ~7.6x faster\n",
        "#   * Flash attention 3?\n",
        "#   * In particular never materialize the T*T matrix\n",
        "#   * Uses \"online softmax trick\"\n",
        "#   * Allows you to update the softmax value online using intermediate values\n",
        "#   * \"Flops don't matter, the entire memory operation matters\"\n",
        "#   * \"I'm not exactly sure why torch.compile doesn't fuse our original implementation into flash attention operation\"\n",
        "#\n",
        "# ... + nice vocab size\n",
        "#\n",
        "#   | step 49 | loss: 6.1674 | dt: 107.45ms | tok/sec: 152477.50 |\n",
        "#\n",
        "#   * \"The dumbest optimization\"\n",
        "#   * \"In some ways still surprises me\"\n",
        "#   * IN GENERAL, SCAN YOUR CODE AND LOOK FOR UGLY NUMBERS, ex: `3`\n",
        "#   * ex: the `25` as number of heads in GPT2-XL lol\n",
        "#   * basically can always increase the number until it's a nice power of 2\n",
        "#   * 50304 is super divisable by a bunch of different powers of 2\n",
        "#   * this is literally more FLOPS lmao\n",
        "#   * most kernels have a whole second phase where they handle anything that's not blocked as a special case to be correct\n",
        "#   * \"one of my favorite examples of having to know how stuff works under the hood- knowing what to tinker with\"\n",
        "#\n",
        "# ... + AdamW params and grad clipping set\n",
        "#\n",
        "#   | step   49 | loss: 5.9391 | norm: 0.7900 | dt: 109.41ms | tok/sec: 149755.44 |\n",
        "#\n",
        "#   * so a _little_ slower but loss is converging much faster\n",
        "#   * clipping the global norm\n",
        "#   * if you get unlucky in a sample, you don't want a huge loss to throw off your whole batch\n",
        "#   * definitely a hack lmao\n",
        "#   * useful information to view as you train, like spikes or when getting high\n",
        "#   * for example early on high gradients when learning easy dumb stuff\n",
        "#\n",
        "# ... + cosine decay learning schedule with warmup\n",
        "#\n",
        "#   | step   49 | loss: 5.8699 | lr 6.0832e-05 | norm: 0.7640 | dt: 108.81ms | tok/sec: 150577.77 |\n",
        "#\n",
        "#   * a little bit better plus a little bit faster\n",
        "#   * probably matters a lot more later in training? Or is this thinking about it wrong\n",
        "#   * the warmup is _part_ of the process where we eventually decay\n",
        "#   * we're replicating this from GPT-3 paper (since don't know for GPT-2)\n",
        "#\n",
        "# ... + batch size scheduling\n",
        "#\n",
        "#  * Karpathy: \"We skip this, because complicates everything and isn't that big of an improvement\"\n",
        "#  * intuition is that early on you actually don't need huge batches because what you're learning is so dumb\n",
        "#\n",
        "# ... + model.configure_optimizer - add weight decay, only for 2D params, and add fused AdamW\n",
        "#\n",
        "#   | step   49 | loss: 5.8977 | lr 6.0832e-05 | norm: 0.6617 | dt: 103.32ms | tok/sec: 158582.07 |\n",
        "#\n",
        "#  * num decayed parameter tensors: 50, with 124,354,560 parameters\n",
        "#  * num non-decayed parameter tensors: 98, with 121,344 parameters\n",
        "#  * using fused AdamW: True\n",
        "#\n",
        "# ... + gradient accumulation\n",
        "#\n",
        "#   | step   35 | loss: 5.8420 | lr 2.2668e-04 | norm: 0.2565 | dt: 3227.67ms | tok/sec: 162435.59 |\n",
        "#\n",
        "# ... + use batch size 32 instead of 16 for full gpu utilization\n",
        "#\n",
        "#   | step    7 | loss: 8.0427 | lr 4.8000e-04 | norm: 2.0357 | dt: 3084.79ms | tok/sec: 169958.89 |\n",
        "#\n",
        "# ... + DistributedDataParallel (multi gpu, torchrun)\n",
        "#\n",
        "#   * everything looks pretty much the same\n",
        "#   * we skip this, as we only have one GPU\n",
        "#\n"
      ],
      "metadata": {
        "id": "OAWujXuwLwwZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_learning_rate(\n",
        "    step: int,\n",
        "    warmup_steps: int,\n",
        "    max_steps: int,\n",
        "    min_lr: float,\n",
        "    max_lr: float,\n",
        "  ) -> float:\n",
        "\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if step < warmup_steps:\n",
        "        # the +1 is because for the 1st iteration no reason to multiply by 0\n",
        "        return max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if step > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "\n",
        "    # coeff starts at 1 and goes to 0\n",
        "    # TODO(bschoen): Is this cos weight decay?\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "OyEwX6_hXYsF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 50\n",
        "\n",
        "# learning rate\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "\n",
        "# {use F32 multiplication}\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# now we'll try multiple batches\n",
        "device = get_best_available_torch_device()\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# use nice number for vocab size\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model.to(device)\n",
        "\n",
        "print(\"Compiling model...\")\n",
        "model = torch.compile(model)\n",
        "print(\"Done compiling model\")\n",
        "\n",
        "# Karpathy: \"AdamW is basically a bugfix of Adam\"\n",
        "#\n",
        "# note: pretty good default learning rate for early experimentation\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=6e-4,\n",
        "    device=device.type,\n",
        ")\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # gradient accumulation\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "\n",
        "        x, y = train_loader.next_batch()\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # automatic mixed precision\n",
        "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        #\n",
        "        # \"accumulation in the gradients is equivalent to the sum in the loss\"\n",
        "        #\n",
        "        # used small self contained version of just this chunk to debug\n",
        "        # since the loss objects etc can be used in isolation\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_learning_rate(\n",
        "        step=i,\n",
        "        warmup_steps=warmup_steps,\n",
        "        max_steps=max_steps,\n",
        "        min_lr=min_lr,\n",
        "        max_lr=max_lr,\n",
        "    )\n",
        "\n",
        "    # update optimizer\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "    print(f\"| step {i:4d} | loss: {loss_accum:.4f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f} |\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "9Tn19WOAJ39u",
        "outputId": "be980ea7-f6c9-468c-b89f-2593aa2b30bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Compiling model...\n",
            "Done compiling model\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "| step    0 | loss: 11.0142 | lr 6.0000e-05 | norm: 26.2377 | dt: 33385.49ms | tok/sec: 15704.07 |\n",
            "| step    1 | loss: 9.6154 | lr 1.2000e-04 | norm: 9.7345 | dt: 3081.40ms | tok/sec: 170146.17 |\n",
            "| step    2 | loss: 9.2019 | lr 1.8000e-04 | norm: 4.9252 | dt: 3082.28ms | tok/sec: 170097.70 |\n",
            "| step    3 | loss: 9.8561 | lr 2.4000e-04 | norm: 8.8841 | dt: 3082.36ms | tok/sec: 170093.27 |\n",
            "| step    4 | loss: 9.2032 | lr 3.0000e-04 | norm: 4.4389 | dt: 3084.73ms | tok/sec: 169962.45 |\n",
            "| step    5 | loss: 8.6595 | lr 3.6000e-04 | norm: 3.5689 | dt: 3084.69ms | tok/sec: 169964.46 |\n",
            "| step    6 | loss: 8.3249 | lr 4.2000e-04 | norm: 2.3745 | dt: 3083.82ms | tok/sec: 170012.71 |\n",
            "| step    7 | loss: 8.0427 | lr 4.8000e-04 | norm: 2.0357 | dt: 3084.79ms | tok/sec: 169958.89 |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-85a2eadc1061>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# automatic mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model(\n",
        "    prompt=\"Romeo\",\n",
        "    num_samples=5,\n",
        "    max_tokens=30,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_J5Bmb4MAJ-",
        "outputId": "bacd4de0-208e-44f4-bb3e-e5c92dc10375"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [0] > Romeo,\n",
            "\n",
            ",,,,\n",
            "., with,\n",
            ";,,\n",
            "\n",
            " the,,\n",
            ",\n",
            "\n",
            ",\n",
            "\n",
            "\n",
            " [1] > Romeo,;\n",
            ",, with\n",
            ",,,\n",
            ",\n",
            ",\n",
            "\n",
            " the,\n",
            "\n",
            "\n",
            "\n",
            " and,,,\n",
            "\n",
            "\n",
            " [2] > Romeo\n",
            ",,,\n",
            "\n",
            "\n",
            ",,:,H,\n",
            "\n",
            "\n",
            "\n",
            ",\n",
            "\n",
            ",\n",
            ",, thy\n",
            "\n",
            "\n",
            "\n",
            " [3] > Romeo,,,,:,\n",
            "\n",
            "\n",
            "\n",
            ",\n",
            ",,,,,\n",
            ":,,,\n",
            ",,,\n",
            "\n",
            "\n",
            " [4] > Romeo,,,: and,,,\n",
            ",,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ",.\n",
            "\n",
            ",,,,,,,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mapjHr7jgWL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}