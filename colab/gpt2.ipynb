{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOiwyE80JCs0b0hektT1zhu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-schoen/gpt_from_scratch/blob/main/colab/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOq7bHwLIPc7",
        "outputId": "3ba89e76-2d44-46d0-c4b2-e5d278491418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt_from_scratch'...\n",
            "remote: Enumerating objects: 257, done.\u001b[K\n",
            "remote: Counting objects: 100% (257/257), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 257 (delta 129), reused 175 (delta 53), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (257/257), 4.27 MiB | 15.91 MiB/s, done.\n",
            "Resolving deltas: 100% (129/129), done.\n"
          ]
        }
      ],
      "source": [
        "# clone repo\n",
        "!rm -rf gpt_from_scratch\n",
        "!git clone https://github.com/b-schoen/gpt_from_scratch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into the repo directory\n",
        "import os\n",
        "\n",
        "os.chdir('gpt_from_scratch')\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2heAkYZIp0g",
        "outputId": "c89be257-b79a-4758-d08c-668d8ae5b1e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content/gpt_from_scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can operate as if this was a local notebook"
      ],
      "metadata": {
        "id": "tUYSCNYTIyWy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "DfSXTCO_I15z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset locally"
      ],
      "metadata": {
        "id": "AGmpQSn2gx-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load tinystories for comparison\n",
        "#\n",
        "# note: `datasets` can list datasets but is deprecated\n",
        "import huggingface_hub\n",
        "\n",
        "# from https://huggingface.co/docs/huggingface_hub/en/guides/download#from-latest-version\n",
        "import dataclasses\n",
        "from typing import Callable, Generic, TypeVar\n",
        "import pathlib\n",
        "\n",
        "T = TypeVar('T')\n",
        "R = TypeVar('T')\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class TrainAndVal(Generic[T]):\n",
        "    \"\"\"Helper for common pattern of transforming both train and val.\"\"\"\n",
        "\n",
        "    train: T\n",
        "    val: T\n",
        "\n",
        "    def apply(self, func: Callable[[T], R]) -> 'TrainAndVal[R]':\n",
        "        return dataclasses.replace(self,\n",
        "            train=func(self.train),\n",
        "            val=func(self.val),\n",
        "        )\n",
        "\n",
        "def download_file_from_tinystories(filename: str) -> pathlib.Path:\n",
        "\n",
        "    print(f\"Downloading {filename}...\")\n",
        "    filepath = huggingface_hub.hf_hub_download(\n",
        "        repo_id='roneneldan/TinyStories',\n",
        "        filename=filename,\n",
        "        repo_type=\"dataset\",\n",
        "    )\n",
        "\n",
        "    print(f\"Downloaded {filename} to {filepath}\")\n",
        "    return pathlib.Path(filepath)\n",
        "\n",
        "# original in paper\n",
        "# train_filename, val_filename = 'TinyStories-train.txt', 'TinyStories-valid.txt'\n",
        "\n",
        "# GPT-4 only, significantly larger but newer\n",
        "filenames = TrainAndVal('TinyStoriesV2-GPT4-train.txt', 'TinyStoriesV2-GPT4-valid.txt')\n",
        "\n",
        "# download\n",
        "filepaths = filenames.apply(download_file_from_tinystories)\n",
        "\n",
        "# read train as input text\n",
        "input_text = filepaths.train.read_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28TU5lPCJx16",
        "outputId": "bb81d88d-2ca0-428a-d333-17d54efed378"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TinyStoriesV2-GPT4-train.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded TinyStoriesV2-GPT4-train.txt to /root/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-train.txt\n",
            "Downloading TinyStoriesV2-GPT4-valid.txt...\n",
            "Downloaded TinyStoriesV2-GPT4-valid.txt to /root/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-valid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note: if this gets annoying can do an actual pip install requirements\n",
        "!pip install tiktoken\n",
        "!pip install jaxtyping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_83OHQPJ52z",
        "outputId": "67b34413-1ae1-4335-a651-e2bc1f66f8e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.33)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting to optimize\n",
        "\n",
        "> ![NOTE] Starting from \"what hardware do I have, and am I fully utilizing it\"\n",
        "\n",
        "Then looking up NVIDIA spec sheet for A100, we see:\n",
        "\n",
        "| Specification | A100 80GB PCIe | A100 80GB SXM |\n",
        "|---------------|----------------|---------------|\n",
        "| FP64 | 9.7 TFLOPS | 9.7 TFLOPS |\n",
        "| FP64 Tensor Core | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "| FP32 | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "| Tensor Float 32 (TF32) | 156 TFLOPS \\| 312 TFLOPS\\* | 156 TFLOPS \\| 312 TFLOPS\\* |\n",
        "| BFLOAT16 Tensor Core | 312 TFLOPS \\| 624 TFLOPS\\* | 312 TFLOPS \\| 624 TFLOPS\\* |\n",
        "| FP16 Tensor Core | 312 TFLOPS \\| 624 TFLOPS\\* | 312 TFLOPS \\| 624 TFLOPS\\* |\n",
        "| INT8 Tensor Core | 624 TOPS \\| 1248 TOPS\\* | 624 TOPS \\| 1248 TOPS\\* |\n",
        "| GPU Memory | 80GB HBM2e | 80GB HBM2e |\n",
        "| GPU Memory Bandwidth | 1,935GB/s | 2,039GB/s |\n",
        "\n",
        "\n",
        "We're currently at:\n",
        "\n",
        "| Specification | A100 80GB PCIe | A100 80GB SXM |\n",
        "|---------------|----------------|---------------|\n",
        "| FP32 | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "\n",
        "but it turns out we don't really need that much precision for deep learning\n",
        "\n",
        "| Format | Sign | Range (exponent) | Precision (mantissa) |\n",
        "|--------|------|------------------|----------------------|\n",
        "| FP32   | 1    | 8                | 23                   |\n",
        "| TF32   | 1    | 8                | 10                   |\n",
        "| FP16   | 1    | 5                | 10                   |\n",
        "| BF16   | 1    | 8                | 7                    |"
      ],
      "metadata": {
        "id": "xFQ-pwEZRV9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from gpt_from_scratch.gpt2_from_scratch import data_loader\n",
        "from gpt_from_scratch.gpt2_from_scratch.train_gpt2 import (\n",
        "    GPT,\n",
        "    GPTConfig,\n",
        "    get_best_available_torch_device,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "bQ9dtV0EJ0Ha"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "C6v7v2kHgIv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample some outputs to get an idea of where we are\n",
        "\n",
        "from typing import TYPE_CHECKING\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "  from gpt_from_scratch import tokenizer_utils\n",
        "\n",
        "def sample_model(\n",
        "    prompt: str,\n",
        "    num_samples: int,\n",
        "    max_tokens: int,\n",
        "    model: nn.Module,\n",
        "    tokenizer: 'tokenizer_utils.Tokenizer',\n",
        "    device: torch.device,\n",
        ") -> None:\n",
        "\n",
        "    # tokenize\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "    tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (5, 8)\n",
        "\n",
        "    # tokens in this case is just the prompt, and is small enough to fit on GPU\n",
        "    x = tokens.to(device)\n",
        "\n",
        "    while x.size(1) < max_tokens:\n",
        "\n",
        "        # forward the model to get the logits\n",
        "        with torch.no_grad():\n",
        "\n",
        "            logits, loss = model(x) # (B, T, vocab_size)\n",
        "\n",
        "            # take the logits at the last position\n",
        "            # throw away all the logits from things other than the last position\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "\n",
        "            # get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # do top-k sampling of 50 (huggingface pipeline default)\n",
        "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "            #\n",
        "            # \"anything lower than the 50th, we clamp to 0 and never sample it\"\n",
        "            #\n",
        "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "            # select a token from the top-k probabilities\n",
        "            # note: multinomial does not demand the input to sum to 1\n",
        "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "\n",
        "            # gather the corresponding indices\n",
        "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "\n",
        "            # append to the sequence\n",
        "            x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "    # print the generated text\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        tokens = x[i, :max_tokens].tolist()\n",
        "\n",
        "        decoded = tokenizer.decode(tokens)\n",
        "\n",
        "        print(f\"\\n [{i}] >\", decoded)"
      ],
      "metadata": {
        "id": "vQnXakW0gH2I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "n0sUP9PagFcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def closest_power_of_two(n: int) -> int:\n",
        "    # Find the power of 2 less than or equal to n\n",
        "    lower = 2 ** math.floor(math.log2(n))\n",
        "\n",
        "    # Find the power of 2 greater than n\n",
        "    upper = lower * 2\n",
        "\n",
        "    # Return the closest one\n",
        "    return lower if (n - lower) < (upper - n) else upper\n",
        "\n",
        "def next_power_of_two(n: int) -> int:\n",
        "\n",
        "    # Find the power of 2 greater than n\n",
        "    return 2 ** math.ceil(math.log2(n))\n",
        "\n",
        "def get_first_n_examples(input_text: str, n: int) -> str:\n",
        "\n",
        "    delimiter = \"<|endoftext|>\"\n",
        "\n",
        "    examples = input_text.split(delimiter)\n",
        "\n",
        "    # Return all text if n is greater than available examples\n",
        "    if n > len(examples) - 1:\n",
        "        return input_text\n",
        "\n",
        "    result = delimiter.join(examples[:n]) + delimiter\n",
        "    return result.strip()"
      ],
      "metadata": {
        "id": "Ci0gRfnDlYQc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n--- First 1000 characters: ---\\n')\n",
        "print(input_text[:1000])\n",
        "\n",
        "# print('\\n--- Last 1000 characters: ---\\n')\n",
        "# print(input_text[:-1000])"
      ],
      "metadata": {
        "id": "GfNoQtFAh-FU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e18c13-ee62-4c69-c142-853e3fc5cd92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- First 1000 characters: ---\n",
            "\n",
            "\n",
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
            "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
            "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
            "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
            "And that's how Ben found an amazing vase in the store!\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
            "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his fri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2,717,700 stories\n",
        "num_samples = len(input_text.split('<|endoftext|>'))\n",
        "\n",
        "print(f'{num_samples=}')\n",
        "\n",
        "# arbitrarily choosing 1/10 as scale factor\n",
        "num_samples = num_samples // 10\n",
        "\n",
        "print(f'{num_samples=} after scaling')\n",
        "\n",
        "num_samples = closest_power_of_two(num_samples)\n",
        "\n",
        "print(f'{num_samples=} after choosing closest power of 2')"
      ],
      "metadata": {
        "id": "Bj5jzFIFko-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eff5717-6923-40f8-8437-53844d7f600b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_samples=2717700\n",
            "num_samples=271770 after scaling\n",
            "num_samples=262144 after choosing closest power of 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clip the input text at number of samples\n",
        "input_text = get_first_n_examples(input_text, n=num_samples)"
      ],
      "metadata": {
        "id": "cEbqmb1OluSE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll trim down the dataset to something that loads quickly"
      ],
      "metadata": {
        "id": "aV-7fZA_jm6t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenizer\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# tokenize input text\n",
        "# note: the dataset already has `<|endoftext|>` in it, we need to tell the\n",
        "#       encoder that that's okay and that we genuinely do want to treat it\n",
        "#       as `<|endoftext|>`\n",
        "tokens = tokenizer.encode(input_text, allowed_special={'<|endoftext|>'})\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "# load text via dataloader\n",
        "# TODO(bschoen): Why do we pick this?\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "\n",
        "B = 16 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "\n",
        "assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
        "\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "# create a train loader that will continually give us new batches\n",
        "train_loader = data_loader.DataLoaderLite(B=B, T=T, tokens=tokens)\n",
        "\n",
        "# note: these are computed based on data loading\n",
        "\n",
        "# want to make it through all of our tokens\n",
        "max_steps = len(tokens) // total_batch_size\n",
        "\n",
        "# chosen fairly arbitrarily\n",
        "# TODO(bschoen): GPT-2 seems to do this as a faction of tokens (proportional)\n",
        "warmup_steps = int(max_steps * 0.1)\n",
        "\n",
        "# learning rate\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "\n",
        "print(f'| {max_steps=} | {warmup_steps=} | {max_lr=:.6f} | {min_lr=:.6f} |')"
      ],
      "metadata": {
        "id": "Piwp_wFzgEPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70358420-0dc5-4299-d6fb-de31dcf5fcbd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 32\n",
            "loaded 52796537 tokens\n",
            "1 epoch = 3222 batches (steps to make one pass through data)\n",
            "| max_steps=100 | warmup_steps=10 | max_lr=0.000600 | min_lr=0.000060 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial layer dominates pretty much everything\n",
        "#\n",
        "# Decrease your batch size until things fit\n",
        "# By default you want to max it out with nice numbers\n",
        "#\n",
        "# Tokens / sec is best unit because agnostic to batch size etc, it's the thing we really care about\n",
        "#\n",
        "# Karpathy recommends the `Automatic Mixed Precision` pytorch tutorial specifically, others are confusing\n",
        "#\n",
        "\n",
        "# Initial w/ Float32 - (B=4, T=32) - mps\n",
        "#\n",
        "#   | step 49 | loss: 6.8048 | dt: 136.36ms | tok/sec: 938.68 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=4, T=32) - cpu\n",
        "#\n",
        "#   | step 14 | loss: 7.6758 | dt: 2578.34ms | tok/sec: 49.64 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=4, T=32) - cuda\n",
        "#\n",
        "#   | step 48 | loss: 6.3560 | dt: 31.72ms | tok/sec: 4035.35 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=16, T=1024) - cuda\n",
        "#\n",
        "#   | step 49 | loss: 6.1039 | dt: 1041.67ms | tok/sec: 15728.63 |\n",
        "#\n",
        "#   * Pretty stable\n",
        "#   * Using full 40 GB GPU (~38.5 GB)\n",
        "#\n",
        "# ... + torch.set_float32_matmul_precision('high')\n",
        "#\n",
        "#   | step 49 | loss: 6.2045 | dt: 382.83ms | tok/sec: 42797.34 |\n",
        "#\n",
        "#   {* decrease precision of optimization itself}\n",
        "#\n",
        "# ... + bfloat16 (automatic mixed precision)\n",
        "#\n",
        "#   | step 49 | loss: 6.0319 | dt: 335.56ms | tok/sec: 48826.04 |\n",
        "#\n",
        "#   * decrease amount of storage we're using per float when moving around\n",
        "#   * pytorch docs *specifically* say to only apply to the model's forward pass and loss calculation\n",
        "#\n",
        "# ... + torch.compile\n",
        "#\n",
        "#   | step 49 | loss: 6.0414 | dt: 192.20ms | tok/sec: 85246.46 |\n",
        "#\n",
        "#   * Karpathy: \"Really incredible piece of code from the pytorch team\"\n",
        "#   * Like LLVM for pytorch\n",
        "#   * No reason to not use it\n",
        "#\n",
        "# ... + scaled flash attention\n",
        "#\n",
        "#   | step 49 | loss: 6.1316 | dt: 143.52ms | tok/sec: 114161.25 |\n",
        "#\n",
        "#   * There are operations that torch.compile will not find\n",
        "#   * Kernel fusion, but kernel fusion that torch.compile can't find\n",
        "#   * Flash attention actually more flops! Mindful of memory hierarchy (what's in HBM, shared_memory, min reads/writes)\n",
        "#   * ~7.6x faster\n",
        "#   * Flash attention 3?\n",
        "#   * In particular never materialize the T*T matrix\n",
        "#   * Uses \"online softmax trick\"\n",
        "#   * Allows you to update the softmax value online using intermediate values\n",
        "#   * \"Flops don't matter, the entire memory operation matters\"\n",
        "#   * \"I'm not exactly sure why torch.compile doesn't fuse our original implementation into flash attention operation\"\n",
        "#\n",
        "# ... + nice vocab size\n",
        "#\n",
        "#   | step 49 | loss: 6.1674 | dt: 107.45ms | tok/sec: 152477.50 |\n",
        "#\n",
        "#   * \"The dumbest optimization\"\n",
        "#   * \"In some ways still surprises me\"\n",
        "#   * IN GENERAL, SCAN YOUR CODE AND LOOK FOR UGLY NUMBERS, ex: `3`\n",
        "#   * ex: the `25` as number of heads in GPT2-XL lol\n",
        "#   * basically can always increase the number until it's a nice power of 2\n",
        "#   * 50304 is super divisable by a bunch of different powers of 2\n",
        "#   * this is literally more FLOPS lmao\n",
        "#   * most kernels have a whole second phase where they handle anything that's not blocked as a special case to be correct\n",
        "#   * \"one of my favorite examples of having to know how stuff works under the hood- knowing what to tinker with\"\n",
        "#\n",
        "# ... + AdamW params and grad clipping set\n",
        "#\n",
        "#   | step   49 | loss: 5.9391 | norm: 0.7900 | dt: 109.41ms | tok/sec: 149755.44 |\n",
        "#\n",
        "#   * so a _little_ slower but loss is converging much faster\n",
        "#   * clipping the global norm\n",
        "#   * if you get unlucky in a sample, you don't want a huge loss to throw off your whole batch\n",
        "#   * definitely a hack lmao\n",
        "#   * useful information to view as you train, like spikes or when getting high\n",
        "#   * for example early on high gradients when learning easy dumb stuff\n",
        "#\n",
        "# ... + cosine decay learning schedule with warmup\n",
        "#\n",
        "#   | step   49 | loss: 5.8699 | lr 6.0832e-05 | norm: 0.7640 | dt: 108.81ms | tok/sec: 150577.77 |\n",
        "#\n",
        "#   * a little bit better plus a little bit faster\n",
        "#   * probably matters a lot more later in training? Or is this thinking about it wrong\n",
        "#   * the warmup is _part_ of the process where we eventually decay\n",
        "#   * we're replicating this from GPT-3 paper (since don't know for GPT-2)\n",
        "#\n",
        "# ... + batch size scheduling\n",
        "#\n",
        "#  * Karpathy: \"We skip this, because complicates everything and isn't that big of an improvement\"\n",
        "#  * intuition is that early on you actually don't need huge batches because what you're learning is so dumb\n",
        "#\n",
        "# ... + model.configure_optimizer - add weight decay, only for 2D params, and add fused AdamW\n",
        "#\n",
        "#   | step   49 | loss: 5.8977 | lr 6.0832e-05 | norm: 0.6617 | dt: 103.32ms | tok/sec: 158582.07 |\n",
        "#\n",
        "#  * num decayed parameter tensors: 50, with 124,354,560 parameters\n",
        "#  * num non-decayed parameter tensors: 98, with 121,344 parameters\n",
        "#  * using fused AdamW: True\n",
        "#\n",
        "# ... + gradient accumulation\n",
        "#\n",
        "#   | step   35 | loss: 5.8420 | lr 2.2668e-04 | norm: 0.2565 | dt: 3227.67ms | tok/sec: 162435.59 |\n",
        "#\n",
        "# ... + use batch size 32 instead of 16 for full gpu utilization\n",
        "#\n",
        "#   | step    7 | loss: 8.0427 | lr 4.8000e-04 | norm: 2.0357 | dt: 3084.79ms | tok/sec: 169958.89 |\n",
        "#\n",
        "# ... + DistributedDataParallel (multi gpu, torchrun)\n",
        "#\n",
        "#   * everything looks pretty much the same\n",
        "#   * we skip this, as we only have one GPU\n",
        "#   * does bring it up to like 1.5m/sec, but he has 8 GPUs (that seems roughly 169958 * 8)\n",
        "#\n",
        "# ... + switching over to tinystories\n",
        "#\n",
        "#   | step   49 | loss: 4.6334 | lr 6.0832e-05 | norm: 0.3634 | dt: 3104.65ms | tok/sec: 168872.06 |\n",
        "#\n",
        "#   * interestingly the same tokens per second"
      ],
      "metadata": {
        "id": "OAWujXuwLwwZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_learning_rate(\n",
        "    step: int,\n",
        "    warmup_steps: int,\n",
        "    max_steps: int,\n",
        "    min_lr: float,\n",
        "    max_lr: float,\n",
        "  ) -> float:\n",
        "\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if step < warmup_steps:\n",
        "        # the +1 is because for the 1st iteration no reason to multiply by 0\n",
        "        return max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if step > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "\n",
        "    # coeff starts at 1 and goes to 0\n",
        "    # TODO(bschoen): Is this cos weight decay?\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "OyEwX6_hXYsF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {use F32 multiplication}\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# now we'll try multiple batches\n",
        "device = get_best_available_torch_device()\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# use nice number for vocab size\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model.to(device)\n",
        "\n",
        "print(\"Compiling model...\")\n",
        "model = torch.compile(model)\n",
        "print(\"Done compiling model\")\n",
        "\n",
        "# Karpathy: \"AdamW is basically a bugfix of Adam\"\n",
        "#\n",
        "# note: pretty good default learning rate for early experimentation\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=max_lr,\n",
        "    device=device.type,\n",
        ")\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # gradient accumulation\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "\n",
        "        x, y = train_loader.next_batch()\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # automatic mixed precision\n",
        "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        #\n",
        "        # \"accumulation in the gradients is equivalent to the sum in the loss\"\n",
        "        #\n",
        "        # used small self contained version of just this chunk to debug\n",
        "        # since the loss objects etc can be used in isolation\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_learning_rate(\n",
        "        step=i,\n",
        "        warmup_steps=warmup_steps,\n",
        "        max_steps=max_steps,\n",
        "        min_lr=min_lr,\n",
        "        max_lr=max_lr,\n",
        "    )\n",
        "\n",
        "    # update optimizer\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "    print(f\"| step {i:4d} | loss: {loss_accum:.4f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f} |\")"
      ],
      "metadata": {
        "id": "9Tn19WOAJ39u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d1c323-7a06-4786-c71a-5503f237fbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Compiling model...\n",
            "Done compiling model\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "| step    0 | loss: 10.9685 | lr 6.0000e-05 | norm: 21.6754 | dt: 29551.88ms | tok/sec: 17741.28 |\n",
            "| step    1 | loss: 9.7304 | lr 1.2000e-04 | norm: 9.6110 | dt: 3202.36ms | tok/sec: 163719.03 |\n",
            "| step    2 | loss: 9.2743 | lr 1.8000e-04 | norm: 3.8950 | dt: 3208.68ms | tok/sec: 163396.64 |\n",
            "| step    3 | loss: 9.0794 | lr 2.4000e-04 | norm: 3.9895 | dt: 3209.55ms | tok/sec: 163352.39 |\n",
            "| step    4 | loss: 8.7513 | lr 3.0000e-04 | norm: 2.6506 | dt: 3247.58ms | tok/sec: 161439.50 |\n",
            "| step    5 | loss: 8.4270 | lr 3.6000e-04 | norm: 2.5836 | dt: 3211.90ms | tok/sec: 163233.16 |\n",
            "| step    6 | loss: 8.0169 | lr 4.2000e-04 | norm: 2.4977 | dt: 3219.42ms | tok/sec: 162851.58 |\n",
            "| step    7 | loss: 7.6419 | lr 4.8000e-04 | norm: 2.5214 | dt: 3226.98ms | tok/sec: 162470.28 |\n",
            "| step    8 | loss: 8.5072 | lr 5.4000e-04 | norm: 37.5341 | dt: 3224.07ms | tok/sec: 162617.01 |\n",
            "| step    9 | loss: 6.9299 | lr 6.0000e-04 | norm: 1.9174 | dt: 3224.58ms | tok/sec: 162591.34 |\n",
            "| step   10 | loss: 6.5959 | lr 6.0000e-04 | norm: 1.4628 | dt: 3225.03ms | tok/sec: 162568.36 |\n",
            "| step   11 | loss: 6.2623 | lr 5.9984e-04 | norm: 1.2800 | dt: 3226.60ms | tok/sec: 162489.52 |\n",
            "| step   12 | loss: 6.0538 | lr 5.9934e-04 | norm: 2.1517 | dt: 3223.20ms | tok/sec: 162660.77 |\n",
            "| step   13 | loss: 5.8892 | lr 5.9852e-04 | norm: 1.9193 | dt: 3221.16ms | tok/sec: 162763.73 |\n",
            "| step   14 | loss: 5.7888 | lr 5.9737e-04 | norm: 0.9208 | dt: 3226.72ms | tok/sec: 162483.49 |\n",
            "| step   15 | loss: 5.7229 | lr 5.9590e-04 | norm: 1.2839 | dt: 3222.83ms | tok/sec: 162679.19 |\n",
            "| step   16 | loss: 5.6549 | lr 5.9410e-04 | norm: 0.6029 | dt: 3220.80ms | tok/sec: 162781.80 |\n",
            "| step   17 | loss: 5.6141 | lr 5.9198e-04 | norm: 1.7775 | dt: 3221.66ms | tok/sec: 162738.23 |\n",
            "| step   18 | loss: 5.5526 | lr 5.8954e-04 | norm: 1.1564 | dt: 3223.70ms | tok/sec: 162635.25 |\n",
            "| step   19 | loss: 5.5766 | lr 5.8679e-04 | norm: 4.2892 | dt: 3224.68ms | tok/sec: 162586.14 |\n",
            "| step   20 | loss: 5.4402 | lr 5.8372e-04 | norm: 1.3015 | dt: 3224.01ms | tok/sec: 162620.07 |\n",
            "| step   21 | loss: 5.4431 | lr 5.8034e-04 | norm: 2.6862 | dt: 3228.19ms | tok/sec: 162409.24 |\n",
            "| step   22 | loss: 5.4187 | lr 5.7666e-04 | norm: 1.5284 | dt: 3231.92ms | tok/sec: 162221.85 |\n",
            "| step   23 | loss: 5.3472 | lr 5.7267e-04 | norm: 1.1887 | dt: 3225.84ms | tok/sec: 162527.43 |\n",
            "| step   24 | loss: 5.2666 | lr 5.6840e-04 | norm: 1.4122 | dt: 3231.74ms | tok/sec: 162230.97 |\n",
            "| step   25 | loss: 5.2635 | lr 5.6383e-04 | norm: 2.1069 | dt: 3231.05ms | tok/sec: 162265.40 |\n",
            "| step   26 | loss: 5.2045 | lr 5.5897e-04 | norm: 1.6989 | dt: 3232.93ms | tok/sec: 162170.99 |\n",
            "| step   27 | loss: 5.1599 | lr 5.5384e-04 | norm: 1.5511 | dt: 3227.44ms | tok/sec: 162447.03 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model(\n",
        "    prompt=\"Jack took the ball from Jill, Jill was mad because\",\n",
        "    num_samples=5,\n",
        "    max_tokens=30,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "W_J5Bmb4MAJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mapjHr7jgWL7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}