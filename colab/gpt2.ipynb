{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPeDDSFdAW0eZdY5wttz6s8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-schoen/gpt_from_scratch/blob/main/colab/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOq7bHwLIPc7",
        "outputId": "ee1d9e38-8dc0-49ae-862f-6f17642a8de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt_from_scratch'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Counting objects: 100% (261/261), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 261 (delta 132), reused 175 (delta 53), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (261/261), 4.28 MiB | 14.50 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n"
          ]
        }
      ],
      "source": [
        "# clone repo\n",
        "!rm -rf gpt_from_scratch\n",
        "!git clone https://github.com/b-schoen/gpt_from_scratch.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into the repo directory\n",
        "import os\n",
        "\n",
        "os.chdir('gpt_from_scratch')\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2heAkYZIp0g",
        "outputId": "72314e7a-f121-42b2-f630-511e1d1d69c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content/gpt_from_scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can operate as if this was a local notebook"
      ],
      "metadata": {
        "id": "tUYSCNYTIyWy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "DfSXTCO_I15z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset locally"
      ],
      "metadata": {
        "id": "AGmpQSn2gx-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load tinystories for comparison\n",
        "#\n",
        "# note: `datasets` can list datasets but is deprecated\n",
        "import huggingface_hub\n",
        "\n",
        "# from https://huggingface.co/docs/huggingface_hub/en/guides/download#from-latest-version\n",
        "import dataclasses\n",
        "from typing import Callable, Generic, TypeVar\n",
        "import pathlib\n",
        "\n",
        "T = TypeVar('T')\n",
        "R = TypeVar('T')\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class TrainAndVal(Generic[T]):\n",
        "    \"\"\"Helper for common pattern of transforming both train and val.\"\"\"\n",
        "\n",
        "    train: T\n",
        "    val: T\n",
        "\n",
        "    def apply(self, func: Callable[[T], R]) -> 'TrainAndVal[R]':\n",
        "        return dataclasses.replace(self,\n",
        "            train=func(self.train),\n",
        "            val=func(self.val),\n",
        "        )\n",
        "\n",
        "def download_file_from_tinystories(filename: str) -> pathlib.Path:\n",
        "\n",
        "    print(f\"Downloading {filename}...\")\n",
        "    filepath = huggingface_hub.hf_hub_download(\n",
        "        repo_id='roneneldan/TinyStories',\n",
        "        filename=filename,\n",
        "        repo_type=\"dataset\",\n",
        "    )\n",
        "\n",
        "    print(f\"Downloaded {filename} to {filepath}\")\n",
        "    return pathlib.Path(filepath)\n",
        "\n",
        "# original in paper\n",
        "# train_filename, val_filename = 'TinyStories-train.txt', 'TinyStories-valid.txt'\n",
        "\n",
        "# GPT-4 only, significantly larger but newer\n",
        "filenames = TrainAndVal('TinyStoriesV2-GPT4-train.txt', 'TinyStoriesV2-GPT4-valid.txt')\n",
        "\n",
        "# download\n",
        "filepaths = filenames.apply(download_file_from_tinystories)\n",
        "\n",
        "# read train as input text\n",
        "input_text = filepaths.train.read_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28TU5lPCJx16",
        "outputId": "e6b2be89-502d-4f75-ed48-07c0120a7d2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading TinyStoriesV2-GPT4-train.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded TinyStoriesV2-GPT4-train.txt to /root/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-train.txt\n",
            "Downloading TinyStoriesV2-GPT4-valid.txt...\n",
            "Downloaded TinyStoriesV2-GPT4-valid.txt to /root/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-valid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note: if this gets annoying can do an actual pip install requirements\n",
        "!pip install tiktoken\n",
        "!pip install jaxtyping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_83OHQPJ52z",
        "outputId": "f26dabad-9623-4ec5-a7a9-91380555f3e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.33)\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting to optimize\n",
        "\n",
        "> ![NOTE] Starting from \"what hardware do I have, and am I fully utilizing it\"\n",
        "\n",
        "Then looking up NVIDIA spec sheet for A100, we see:\n",
        "\n",
        "| Specification | A100 80GB PCIe | A100 80GB SXM |\n",
        "|---------------|----------------|---------------|\n",
        "| FP64 | 9.7 TFLOPS | 9.7 TFLOPS |\n",
        "| FP64 Tensor Core | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "| FP32 | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "| Tensor Float 32 (TF32) | 156 TFLOPS \\| 312 TFLOPS\\* | 156 TFLOPS \\| 312 TFLOPS\\* |\n",
        "| BFLOAT16 Tensor Core | 312 TFLOPS \\| 624 TFLOPS\\* | 312 TFLOPS \\| 624 TFLOPS\\* |\n",
        "| FP16 Tensor Core | 312 TFLOPS \\| 624 TFLOPS\\* | 312 TFLOPS \\| 624 TFLOPS\\* |\n",
        "| INT8 Tensor Core | 624 TOPS \\| 1248 TOPS\\* | 624 TOPS \\| 1248 TOPS\\* |\n",
        "| GPU Memory | 80GB HBM2e | 80GB HBM2e |\n",
        "| GPU Memory Bandwidth | 1,935GB/s | 2,039GB/s |\n",
        "\n",
        "\n",
        "We're currently at:\n",
        "\n",
        "| Specification | A100 80GB PCIe | A100 80GB SXM |\n",
        "|---------------|----------------|---------------|\n",
        "| FP32 | 19.5 TFLOPS | 19.5 TFLOPS |\n",
        "\n",
        "but it turns out we don't really need that much precision for deep learning\n",
        "\n",
        "| Format | Sign | Range (exponent) | Precision (mantissa) |\n",
        "|--------|------|------------------|----------------------|\n",
        "| FP32   | 1    | 8                | 23                   |\n",
        "| TF32   | 1    | 8                | 10                   |\n",
        "| FP16   | 1    | 5                | 10                   |\n",
        "| BF16   | 1    | 8                | 7                    |"
      ],
      "metadata": {
        "id": "xFQ-pwEZRV9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from gpt_from_scratch.gpt2_from_scratch import data_loader\n",
        "from gpt_from_scratch.gpt2_from_scratch.train_gpt2 import (\n",
        "    GPT,\n",
        "    GPTConfig,\n",
        "    get_best_available_torch_device,\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "bQ9dtV0EJ0Ha"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling"
      ],
      "metadata": {
        "id": "C6v7v2kHgIv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample some outputs to get an idea of where we are\n",
        "\n",
        "from typing import TYPE_CHECKING\n",
        "\n",
        "if TYPE_CHECKING:\n",
        "  from gpt_from_scratch import tokenizer_utils\n",
        "\n",
        "def sample_model(\n",
        "    prompt: str,\n",
        "    num_samples: int,\n",
        "    max_tokens: int,\n",
        "    model: nn.Module,\n",
        "    tokenizer: 'tokenizer_utils.Tokenizer',\n",
        "    device: torch.device,\n",
        ") -> None:\n",
        "\n",
        "    # tokenize\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "    tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (5, 8)\n",
        "\n",
        "    # tokens in this case is just the prompt, and is small enough to fit on GPU\n",
        "    x = tokens.to(device)\n",
        "\n",
        "    while x.size(1) < max_tokens:\n",
        "\n",
        "        # forward the model to get the logits\n",
        "        with torch.no_grad():\n",
        "\n",
        "            logits, loss = model(x) # (B, T, vocab_size)\n",
        "\n",
        "            # take the logits at the last position\n",
        "            # throw away all the logits from things other than the last position\n",
        "            logits = logits[:, -1, :] # (B, vocab_size)\n",
        "\n",
        "            # get the probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # do top-k sampling of 50 (huggingface pipeline default)\n",
        "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "            #\n",
        "            # \"anything lower than the 50th, we clamp to 0 and never sample it\"\n",
        "            #\n",
        "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "            # select a token from the top-k probabilities\n",
        "            # note: multinomial does not demand the input to sum to 1\n",
        "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "\n",
        "            # gather the corresponding indices\n",
        "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "\n",
        "            # append to the sequence\n",
        "            x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "    # print the generated text\n",
        "    for i in range(num_samples):\n",
        "\n",
        "        tokens = x[i, :max_tokens].tolist()\n",
        "\n",
        "        decoded = tokenizer.decode(tokens)\n",
        "\n",
        "        print(f\"\\n [{i}] >\", decoded)"
      ],
      "metadata": {
        "id": "vQnXakW0gH2I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "n0sUP9PagFcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def closest_power_of_two(n: int) -> int:\n",
        "    # Find the power of 2 less than or equal to n\n",
        "    lower = 2 ** math.floor(math.log2(n))\n",
        "\n",
        "    # Find the power of 2 greater than n\n",
        "    upper = lower * 2\n",
        "\n",
        "    # Return the closest one\n",
        "    return lower if (n - lower) < (upper - n) else upper\n",
        "\n",
        "def next_power_of_two(n: int) -> int:\n",
        "\n",
        "    # Find the power of 2 greater than n\n",
        "    return 2 ** math.ceil(math.log2(n))\n",
        "\n",
        "def get_first_n_examples(input_text: str, n: int) -> str:\n",
        "\n",
        "    delimiter = \"<|endoftext|>\"\n",
        "\n",
        "    examples = input_text.split(delimiter)\n",
        "\n",
        "    # Return all text if n is greater than available examples\n",
        "    if n > len(examples) - 1:\n",
        "        return input_text\n",
        "\n",
        "    result = delimiter.join(examples[:n]) + delimiter\n",
        "    return result.strip()"
      ],
      "metadata": {
        "id": "Ci0gRfnDlYQc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n--- First 1000 characters: ---\\n')\n",
        "print(input_text[:1000])\n",
        "\n",
        "# print('\\n--- Last 1000 characters: ---\\n')\n",
        "# print(input_text[:-1000])"
      ],
      "metadata": {
        "id": "GfNoQtFAh-FU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aee6703-a3eb-4e1a-f8e3-8df23dd462f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- First 1000 characters: ---\n",
            "\n",
            "\n",
            "Once upon a time there was a little boy named Ben. Ben loved to explore the world around him. He saw many amazing things, like beautiful vases that were on display in a store. One day, Ben was walking through the store when he came across a very special vase. When Ben saw it he was amazed!  \n",
            "He said, “Wow, that is a really amazing vase! Can I buy it?” \n",
            "The shopkeeper smiled and said, “Of course you can. You can take it home and show all your friends how amazing it is!”\n",
            "So Ben took the vase home and he was so proud of it! He called his friends over and showed them the amazing vase. All his friends thought the vase was beautiful and couldn't believe how lucky Ben was. \n",
            "And that's how Ben found an amazing vase in the store!\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a reliable otter named Ollie. He lived in a river with his family. They all loved to play and swim together.\n",
            "One day, Ollie's mom said, \"Ollie, hurry and get some fish for dinner!\" Ollie swam fast to catch fish. He saw his fri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2,717,700 stories\n",
        "num_samples = len(input_text.split('<|endoftext|>'))\n",
        "\n",
        "print(f'{num_samples=}')\n",
        "\n",
        "# arbitrarily choosing 1/10 as scale factor\n",
        "num_samples = num_samples // 10\n",
        "\n",
        "print(f'{num_samples=} after scaling')\n",
        "\n",
        "num_samples = closest_power_of_two(num_samples)\n",
        "\n",
        "print(f'{num_samples=} after choosing closest power of 2')"
      ],
      "metadata": {
        "id": "Bj5jzFIFko-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88748f46-596f-4003-e189-a9a92b774551"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_samples=2717700\n",
            "num_samples=271770 after scaling\n",
            "num_samples=262144 after choosing closest power of 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clip the input text at number of samples\n",
        "input_text = get_first_n_examples(input_text, n=num_samples)"
      ],
      "metadata": {
        "id": "cEbqmb1OluSE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll trim down the dataset to something that loads quickly"
      ],
      "metadata": {
        "id": "aV-7fZA_jm6t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenizer\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# tokenize input text\n",
        "# note: the dataset already has `<|endoftext|>` in it, we need to tell the\n",
        "#       encoder that that's okay and that we genuinely do want to treat it\n",
        "#       as `<|endoftext|>`\n",
        "tokens = tokenizer.encode(input_text, allowed_special={'<|endoftext|>'})\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "# load text via dataloader\n",
        "# TODO(bschoen): Why do we pick this?\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "\n",
        "B = 16 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "\n",
        "assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
        "\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "# create a train loader that will continually give us new batches\n",
        "train_loader = data_loader.DataLoaderLite(B=B, T=T, tokens=tokens)\n",
        "\n",
        "# note: these are computed based on data loading\n",
        "\n",
        "# want to make it through all of our tokens\n",
        "\n",
        "# this seems way too low @ 100, thus the override\n",
        "max_steps = 1000\n",
        "# max_steps = len(tokens) // total_batch_size\n",
        "\n",
        "# chosen fairly arbitrarily\n",
        "# TODO(bschoen): GPT-2 seems to do this as a faction of tokens (proportional)\n",
        "warmup_steps = int(max_steps * 0.1)\n",
        "\n",
        "# learning rate\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "\n",
        "print(f'| {max_steps=} | {warmup_steps=} | {max_lr=:.6f} | {min_lr=:.6f} |')"
      ],
      "metadata": {
        "id": "Piwp_wFzgEPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0fec5a-5eaa-4fda-8355-6f75a72c0229"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 32\n",
            "loaded 52796537 tokens\n",
            "1 epoch = 3222 batches (steps to make one pass through data)\n",
            "| max_steps=1000 | warmup_steps=100 | max_lr=0.000600 | min_lr=0.000060 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial layer dominates pretty much everything\n",
        "#\n",
        "# Decrease your batch size until things fit\n",
        "# By default you want to max it out with nice numbers\n",
        "#\n",
        "# Tokens / sec is best unit because agnostic to batch size etc, it's the thing we really care about\n",
        "#\n",
        "# Karpathy recommends the `Automatic Mixed Precision` pytorch tutorial specifically, others are confusing\n",
        "#\n",
        "\n",
        "# Initial w/ Float32 - (B=4, T=32) - mps\n",
        "#\n",
        "#   | step 49 | loss: 6.8048 | dt: 136.36ms | tok/sec: 938.68 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=4, T=32) - cpu\n",
        "#\n",
        "#   | step 14 | loss: 7.6758 | dt: 2578.34ms | tok/sec: 49.64 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=4, T=32) - cuda\n",
        "#\n",
        "#   | step 48 | loss: 6.3560 | dt: 31.72ms | tok/sec: 4035.35 |\n",
        "#\n",
        "# Initial w/ Float32 - (B=16, T=1024) - cuda\n",
        "#\n",
        "#   | step 49 | loss: 6.1039 | dt: 1041.67ms | tok/sec: 15728.63 |\n",
        "#\n",
        "#   * Pretty stable\n",
        "#   * Using full 40 GB GPU (~38.5 GB)\n",
        "#\n",
        "# ... + torch.set_float32_matmul_precision('high')\n",
        "#\n",
        "#   | step 49 | loss: 6.2045 | dt: 382.83ms | tok/sec: 42797.34 |\n",
        "#\n",
        "#   {* decrease precision of optimization itself}\n",
        "#\n",
        "# ... + bfloat16 (automatic mixed precision)\n",
        "#\n",
        "#   | step 49 | loss: 6.0319 | dt: 335.56ms | tok/sec: 48826.04 |\n",
        "#\n",
        "#   * decrease amount of storage we're using per float when moving around\n",
        "#   * pytorch docs *specifically* say to only apply to the model's forward pass and loss calculation\n",
        "#\n",
        "# ... + torch.compile\n",
        "#\n",
        "#   | step 49 | loss: 6.0414 | dt: 192.20ms | tok/sec: 85246.46 |\n",
        "#\n",
        "#   * Karpathy: \"Really incredible piece of code from the pytorch team\"\n",
        "#   * Like LLVM for pytorch\n",
        "#   * No reason to not use it\n",
        "#\n",
        "# ... + scaled flash attention\n",
        "#\n",
        "#   | step 49 | loss: 6.1316 | dt: 143.52ms | tok/sec: 114161.25 |\n",
        "#\n",
        "#   * There are operations that torch.compile will not find\n",
        "#   * Kernel fusion, but kernel fusion that torch.compile can't find\n",
        "#   * Flash attention actually more flops! Mindful of memory hierarchy (what's in HBM, shared_memory, min reads/writes)\n",
        "#   * ~7.6x faster\n",
        "#   * Flash attention 3?\n",
        "#   * In particular never materialize the T*T matrix\n",
        "#   * Uses \"online softmax trick\"\n",
        "#   * Allows you to update the softmax value online using intermediate values\n",
        "#   * \"Flops don't matter, the entire memory operation matters\"\n",
        "#   * \"I'm not exactly sure why torch.compile doesn't fuse our original implementation into flash attention operation\"\n",
        "#\n",
        "# ... + nice vocab size\n",
        "#\n",
        "#   | step 49 | loss: 6.1674 | dt: 107.45ms | tok/sec: 152477.50 |\n",
        "#\n",
        "#   * \"The dumbest optimization\"\n",
        "#   * \"In some ways still surprises me\"\n",
        "#   * IN GENERAL, SCAN YOUR CODE AND LOOK FOR UGLY NUMBERS, ex: `3`\n",
        "#   * ex: the `25` as number of heads in GPT2-XL lol\n",
        "#   * basically can always increase the number until it's a nice power of 2\n",
        "#   * 50304 is super divisable by a bunch of different powers of 2\n",
        "#   * this is literally more FLOPS lmao\n",
        "#   * most kernels have a whole second phase where they handle anything that's not blocked as a special case to be correct\n",
        "#   * \"one of my favorite examples of having to know how stuff works under the hood- knowing what to tinker with\"\n",
        "#\n",
        "# ... + AdamW params and grad clipping set\n",
        "#\n",
        "#   | step   49 | loss: 5.9391 | norm: 0.7900 | dt: 109.41ms | tok/sec: 149755.44 |\n",
        "#\n",
        "#   * so a _little_ slower but loss is converging much faster\n",
        "#   * clipping the global norm\n",
        "#   * if you get unlucky in a sample, you don't want a huge loss to throw off your whole batch\n",
        "#   * definitely a hack lmao\n",
        "#   * useful information to view as you train, like spikes or when getting high\n",
        "#   * for example early on high gradients when learning easy dumb stuff\n",
        "#\n",
        "# ... + cosine decay learning schedule with warmup\n",
        "#\n",
        "#   | step   49 | loss: 5.8699 | lr 6.0832e-05 | norm: 0.7640 | dt: 108.81ms | tok/sec: 150577.77 |\n",
        "#\n",
        "#   * a little bit better plus a little bit faster\n",
        "#   * probably matters a lot more later in training? Or is this thinking about it wrong\n",
        "#   * the warmup is _part_ of the process where we eventually decay\n",
        "#   * we're replicating this from GPT-3 paper (since don't know for GPT-2)\n",
        "#\n",
        "# ... + batch size scheduling\n",
        "#\n",
        "#  * Karpathy: \"We skip this, because complicates everything and isn't that big of an improvement\"\n",
        "#  * intuition is that early on you actually don't need huge batches because what you're learning is so dumb\n",
        "#\n",
        "# ... + model.configure_optimizer - add weight decay, only for 2D params, and add fused AdamW\n",
        "#\n",
        "#   | step   49 | loss: 5.8977 | lr 6.0832e-05 | norm: 0.6617 | dt: 103.32ms | tok/sec: 158582.07 |\n",
        "#\n",
        "#  * num decayed parameter tensors: 50, with 124,354,560 parameters\n",
        "#  * num non-decayed parameter tensors: 98, with 121,344 parameters\n",
        "#  * using fused AdamW: True\n",
        "#\n",
        "# ... + gradient accumulation\n",
        "#\n",
        "#   | step   35 | loss: 5.8420 | lr 2.2668e-04 | norm: 0.2565 | dt: 3227.67ms | tok/sec: 162435.59 |\n",
        "#\n",
        "# ... + use batch size 32 instead of 16 for full gpu utilization\n",
        "#\n",
        "#   | step    7 | loss: 8.0427 | lr 4.8000e-04 | norm: 2.0357 | dt: 3084.79ms | tok/sec: 169958.89 |\n",
        "#\n",
        "# ... + DistributedDataParallel (multi gpu, torchrun)\n",
        "#\n",
        "#   * everything looks pretty much the same\n",
        "#   * we skip this, as we only have one GPU\n",
        "#   * does bring it up to like 1.5m/sec, but he has 8 GPUs (that seems roughly 169958 * 8)\n",
        "#\n",
        "# ... + switching over to tinystories\n",
        "#\n",
        "#   | step   49 | loss: 4.6334 | lr 6.0832e-05 | norm: 0.3634 | dt: 3104.65ms | tok/sec: 168872.06 |\n",
        "#\n",
        "#   * interestingly the same tokens per second\n",
        "#\n",
        "# ... + (B=16) (since was running out of GPU space)\n",
        "#\n",
        "#   | step   49 | loss: 4.2973 | lr 3.0000e-04 | norm: 1.3571 | dt: 3232.42ms | tok/sec: 162196.82 |\n",
        "#   ...\n",
        "#   | step  999 | loss: 1.1815 | lr 6.0002e-05 | norm: 0.3806 | dt: 3234.65ms | tok/sec: 162084.83 |"
      ],
      "metadata": {
        "id": "OAWujXuwLwwZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_learning_rate(\n",
        "    step: int,\n",
        "    warmup_steps: int,\n",
        "    max_steps: int,\n",
        "    min_lr: float,\n",
        "    max_lr: float,\n",
        "  ) -> float:\n",
        "\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if step < warmup_steps:\n",
        "        # the +1 is because for the 1st iteration no reason to multiply by 0\n",
        "        return max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if step > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "\n",
        "    # coeff starts at 1 and goes to 0\n",
        "    # TODO(bschoen): Is this cos weight decay?\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "OyEwX6_hXYsF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {use F32 multiplication}\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# now we'll try multiple batches\n",
        "device = get_best_available_torch_device()\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# use nice number for vocab size\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model.to(device)\n",
        "\n",
        "print(\"Compiling model...\")\n",
        "model = torch.compile(model)\n",
        "print(\"Done compiling model\")\n",
        "\n",
        "# Karpathy: \"AdamW is basically a bugfix of Adam\"\n",
        "#\n",
        "# note: pretty good default learning rate for early experimentation\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=0.1,\n",
        "    learning_rate=max_lr,\n",
        "    device=device.type,\n",
        ")\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # gradient accumulation\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "\n",
        "        x, y = train_loader.next_batch()\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # automatic mixed precision\n",
        "        with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        #\n",
        "        # \"accumulation in the gradients is equivalent to the sum in the loss\"\n",
        "        #\n",
        "        # used small self contained version of just this chunk to debug\n",
        "        # since the loss objects etc can be used in isolation\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_learning_rate(\n",
        "        step=i,\n",
        "        warmup_steps=warmup_steps,\n",
        "        max_steps=max_steps,\n",
        "        min_lr=min_lr,\n",
        "        max_lr=max_lr,\n",
        "    )\n",
        "\n",
        "    # update optimizer\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "\n",
        "    print(f\"| step {i:4d} | loss: {loss_accum:.4f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f} |\")"
      ],
      "metadata": {
        "id": "9Tn19WOAJ39u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca10041f-85a3-42a1-ba92-1a8352196e83"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Compiling model...\n",
            "Done compiling model\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "| step    0 | loss: 10.9909 | lr 6.0000e-06 | norm: 20.5855 | dt: 29888.82ms | tok/sec: 17541.27 |\n",
            "| step    1 | loss: 10.5371 | lr 1.2000e-05 | norm: 16.8769 | dt: 3211.99ms | tok/sec: 163228.63 |\n",
            "| step    2 | loss: 10.0238 | lr 1.8000e-05 | norm: 10.4843 | dt: 3215.61ms | tok/sec: 163044.86 |\n",
            "| step    3 | loss: 9.6534 | lr 2.4000e-05 | norm: 7.3438 | dt: 3216.31ms | tok/sec: 163009.13 |\n",
            "| step    4 | loss: 9.3873 | lr 3.0000e-05 | norm: 5.1010 | dt: 3214.80ms | tok/sec: 163085.58 |\n",
            "| step    5 | loss: 9.2134 | lr 3.6000e-05 | norm: 3.9777 | dt: 3216.05ms | tok/sec: 163022.56 |\n",
            "| step    6 | loss: 9.1017 | lr 4.2000e-05 | norm: 3.5790 | dt: 3222.38ms | tok/sec: 162702.19 |\n",
            "| step    7 | loss: 9.0170 | lr 4.8000e-05 | norm: 3.3834 | dt: 3221.52ms | tok/sec: 162745.71 |\n",
            "| step    8 | loss: 8.9233 | lr 5.4000e-05 | norm: 3.5052 | dt: 3225.45ms | tok/sec: 162547.47 |\n",
            "| step    9 | loss: 8.7854 | lr 6.0000e-05 | norm: 3.5620 | dt: 3218.72ms | tok/sec: 162886.97 |\n",
            "| step   10 | loss: 8.6132 | lr 6.6000e-05 | norm: 5.3606 | dt: 3226.10ms | tok/sec: 162514.40 |\n",
            "| step   11 | loss: 8.5953 | lr 7.2000e-05 | norm: 18.7089 | dt: 3224.95ms | tok/sec: 162572.61 |\n",
            "| step   12 | loss: 8.4204 | lr 7.8000e-05 | norm: 4.4425 | dt: 3227.20ms | tok/sec: 162459.07 |\n",
            "| step   13 | loss: 8.2594 | lr 8.4000e-05 | norm: 4.2916 | dt: 3245.65ms | tok/sec: 161535.43 |\n",
            "| step   14 | loss: 8.0805 | lr 9.0000e-05 | norm: 5.2368 | dt: 3225.11ms | tok/sec: 162564.50 |\n",
            "| step   15 | loss: 7.9714 | lr 9.6000e-05 | norm: 7.0814 | dt: 3224.18ms | tok/sec: 162611.04 |\n",
            "| step   16 | loss: 7.8063 | lr 1.0200e-04 | norm: 3.8284 | dt: 3224.36ms | tok/sec: 162602.31 |\n",
            "| step   17 | loss: 7.6969 | lr 1.0800e-04 | norm: 5.2634 | dt: 3226.07ms | tok/sec: 162515.92 |\n",
            "| step   18 | loss: 7.5792 | lr 1.1400e-04 | norm: 8.4247 | dt: 3229.89ms | tok/sec: 162323.99 |\n",
            "| step   19 | loss: 7.4888 | lr 1.2000e-04 | norm: 6.1986 | dt: 3236.99ms | tok/sec: 161967.52 |\n",
            "| step   20 | loss: 7.3111 | lr 1.2600e-04 | norm: 3.1756 | dt: 3233.92ms | tok/sec: 162121.65 |\n",
            "| step   21 | loss: 7.2098 | lr 1.3200e-04 | norm: 9.0812 | dt: 3231.17ms | tok/sec: 162259.52 |\n",
            "| step   22 | loss: 7.0581 | lr 1.3800e-04 | norm: 4.1383 | dt: 3231.70ms | tok/sec: 162233.00 |\n",
            "| step   23 | loss: 6.8962 | lr 1.4400e-04 | norm: 3.0378 | dt: 3229.94ms | tok/sec: 162321.31 |\n",
            "| step   24 | loss: 6.7364 | lr 1.5000e-04 | norm: 2.3763 | dt: 3228.65ms | tok/sec: 162385.90 |\n",
            "| step   25 | loss: 6.6182 | lr 1.5600e-04 | norm: 3.1629 | dt: 3229.41ms | tok/sec: 162348.02 |\n",
            "| step   26 | loss: 6.4551 | lr 1.6200e-04 | norm: 1.9164 | dt: 3229.68ms | tok/sec: 162334.20 |\n",
            "| step   27 | loss: 6.3402 | lr 1.6800e-04 | norm: 4.7377 | dt: 3232.27ms | tok/sec: 162204.09 |\n",
            "| step   28 | loss: 6.2101 | lr 1.7400e-04 | norm: 3.1379 | dt: 3225.22ms | tok/sec: 162559.03 |\n",
            "| step   29 | loss: 6.0629 | lr 1.8000e-04 | norm: 1.8514 | dt: 3229.04ms | tok/sec: 162366.70 |\n",
            "| step   30 | loss: 5.9344 | lr 1.8600e-04 | norm: 3.2903 | dt: 3230.54ms | tok/sec: 162291.25 |\n",
            "| step   31 | loss: 5.7842 | lr 1.9200e-04 | norm: 2.0337 | dt: 3230.20ms | tok/sec: 162308.12 |\n",
            "| step   32 | loss: 5.6396 | lr 1.9800e-04 | norm: 1.7040 | dt: 3230.80ms | tok/sec: 162278.29 |\n",
            "| step   33 | loss: 5.5370 | lr 2.0400e-04 | norm: 2.7994 | dt: 3228.61ms | tok/sec: 162388.19 |\n",
            "| step   34 | loss: 5.4105 | lr 2.1000e-04 | norm: 1.7553 | dt: 3226.09ms | tok/sec: 162514.77 |\n",
            "| step   35 | loss: 5.2933 | lr 2.1600e-04 | norm: 1.5574 | dt: 3226.31ms | tok/sec: 162503.82 |\n",
            "| step   36 | loss: 5.1723 | lr 2.2200e-04 | norm: 1.5329 | dt: 3227.99ms | tok/sec: 162419.41 |\n",
            "| step   37 | loss: 5.0760 | lr 2.2800e-04 | norm: 1.7087 | dt: 3237.60ms | tok/sec: 161937.10 |\n",
            "| step   38 | loss: 4.9703 | lr 2.3400e-04 | norm: 1.4435 | dt: 3230.95ms | tok/sec: 162270.55 |\n",
            "| step   39 | loss: 4.8915 | lr 2.4000e-04 | norm: 1.5112 | dt: 3225.60ms | tok/sec: 162539.47 |\n",
            "| step   40 | loss: 4.8020 | lr 2.4600e-04 | norm: 1.3110 | dt: 3237.93ms | tok/sec: 161920.69 |\n",
            "| step   41 | loss: 4.7388 | lr 2.5200e-04 | norm: 1.4912 | dt: 3232.07ms | tok/sec: 162214.54 |\n",
            "| step   42 | loss: 4.6429 | lr 2.5800e-04 | norm: 0.9787 | dt: 3231.59ms | tok/sec: 162238.25 |\n",
            "| step   43 | loss: 4.6011 | lr 2.6400e-04 | norm: 1.6363 | dt: 3231.33ms | tok/sec: 162251.68 |\n",
            "| step   44 | loss: 4.5227 | lr 2.7000e-04 | norm: 1.2708 | dt: 3235.32ms | tok/sec: 162051.25 |\n",
            "| step   45 | loss: 4.4859 | lr 2.7600e-04 | norm: 1.9059 | dt: 3232.35ms | tok/sec: 162200.47 |\n",
            "| step   46 | loss: 4.4206 | lr 2.8200e-04 | norm: 1.0903 | dt: 3241.01ms | tok/sec: 161766.68 |\n",
            "| step   47 | loss: 4.3744 | lr 2.8800e-04 | norm: 1.9581 | dt: 3232.46ms | tok/sec: 162194.94 |\n",
            "| step   48 | loss: 4.3413 | lr 2.9400e-04 | norm: 1.1977 | dt: 3236.41ms | tok/sec: 161996.94 |\n",
            "| step   49 | loss: 4.2973 | lr 3.0000e-04 | norm: 1.3571 | dt: 3232.42ms | tok/sec: 162196.82 |\n",
            "| step   50 | loss: 4.2986 | lr 3.0600e-04 | norm: 1.5318 | dt: 3236.50ms | tok/sec: 161992.11 |\n",
            "| step   51 | loss: 4.2115 | lr 3.1200e-04 | norm: 0.7550 | dt: 3252.41ms | tok/sec: 161199.86 |\n",
            "| step   52 | loss: 4.2015 | lr 3.1800e-04 | norm: 1.2041 | dt: 3231.99ms | tok/sec: 162218.26 |\n",
            "| step   53 | loss: 4.1774 | lr 3.2400e-04 | norm: 0.6568 | dt: 3234.77ms | tok/sec: 162078.75 |\n",
            "| step   54 | loss: 4.1445 | lr 3.3000e-04 | norm: 0.7835 | dt: 3228.24ms | tok/sec: 162406.54 |\n",
            "| step   55 | loss: 4.0886 | lr 3.3600e-04 | norm: 0.7446 | dt: 3227.72ms | tok/sec: 162433.07 |\n",
            "| step   56 | loss: 4.0704 | lr 3.4200e-04 | norm: 1.2221 | dt: 3229.24ms | tok/sec: 162356.60 |\n",
            "| step   57 | loss: 4.0436 | lr 3.4800e-04 | norm: 1.1158 | dt: 3226.39ms | tok/sec: 162499.77 |\n",
            "| step   58 | loss: 4.0227 | lr 3.5400e-04 | norm: 1.0058 | dt: 3232.65ms | tok/sec: 162185.29 |\n",
            "| step   59 | loss: 4.0188 | lr 3.6000e-04 | norm: 1.5056 | dt: 3234.65ms | tok/sec: 162084.74 |\n",
            "| step   60 | loss: 4.0010 | lr 3.6600e-04 | norm: 1.4807 | dt: 3239.52ms | tok/sec: 161841.42 |\n",
            "| step   61 | loss: 3.9979 | lr 3.7200e-04 | norm: 1.1693 | dt: 3231.07ms | tok/sec: 162264.32 |\n",
            "| step   62 | loss: 3.9803 | lr 3.7800e-04 | norm: 1.2543 | dt: 3234.46ms | tok/sec: 162094.63 |\n",
            "| step   63 | loss: 3.9522 | lr 3.8400e-04 | norm: 0.8440 | dt: 3232.51ms | tok/sec: 162192.25 |\n",
            "| step   64 | loss: 3.9053 | lr 3.9000e-04 | norm: 1.1839 | dt: 3231.07ms | tok/sec: 162264.70 |\n",
            "| step   65 | loss: 3.9317 | lr 3.9600e-04 | norm: 1.0758 | dt: 3237.06ms | tok/sec: 161964.34 |\n",
            "| step   66 | loss: 3.8906 | lr 4.0200e-04 | norm: 0.8861 | dt: 3229.61ms | tok/sec: 162337.82 |\n",
            "| step   67 | loss: 3.9038 | lr 4.0800e-04 | norm: 1.3902 | dt: 3231.31ms | tok/sec: 162252.31 |\n",
            "| step   68 | loss: 3.8820 | lr 4.1400e-04 | norm: 0.7897 | dt: 3230.03ms | tok/sec: 162316.91 |\n",
            "| step   69 | loss: 3.8614 | lr 4.2000e-04 | norm: 0.6124 | dt: 3230.72ms | tok/sec: 162282.33 |\n",
            "| step   70 | loss: 3.8451 | lr 4.2600e-04 | norm: 0.6278 | dt: 3233.14ms | tok/sec: 162160.82 |\n",
            "| step   71 | loss: 3.8308 | lr 4.3200e-04 | norm: 0.7587 | dt: 3233.19ms | tok/sec: 162158.07 |\n",
            "| step   72 | loss: 3.8208 | lr 4.3800e-04 | norm: 0.7352 | dt: 3229.35ms | tok/sec: 162350.96 |\n",
            "| step   73 | loss: 3.8129 | lr 4.4400e-04 | norm: 0.6797 | dt: 3235.45ms | tok/sec: 162044.64 |\n",
            "| step   74 | loss: 3.8004 | lr 4.5000e-04 | norm: 0.7158 | dt: 3238.39ms | tok/sec: 161897.87 |\n",
            "| step   75 | loss: 3.7936 | lr 4.5600e-04 | norm: 0.7683 | dt: 3242.97ms | tok/sec: 161669.05 |\n",
            "| step   76 | loss: 3.7838 | lr 4.6200e-04 | norm: 0.6333 | dt: 3242.20ms | tok/sec: 161707.50 |\n",
            "| step   77 | loss: 3.7600 | lr 4.6800e-04 | norm: 0.7190 | dt: 3233.07ms | tok/sec: 162164.38 |\n",
            "| step   78 | loss: 3.7505 | lr 4.7400e-04 | norm: 0.8266 | dt: 3233.71ms | tok/sec: 162132.06 |\n",
            "| step   79 | loss: 3.7414 | lr 4.8000e-04 | norm: 0.7577 | dt: 3233.65ms | tok/sec: 162134.97 |\n",
            "| step   80 | loss: 3.7617 | lr 4.8600e-04 | norm: 0.8976 | dt: 3231.22ms | tok/sec: 162257.00 |\n",
            "| step   81 | loss: 3.7625 | lr 4.9200e-04 | norm: 1.1448 | dt: 3241.04ms | tok/sec: 161765.18 |\n",
            "| step   82 | loss: 3.7501 | lr 4.9800e-04 | norm: 0.8698 | dt: 3234.56ms | tok/sec: 162089.68 |\n",
            "| step   83 | loss: 3.7537 | lr 5.0400e-04 | norm: 1.1639 | dt: 3232.88ms | tok/sec: 162173.76 |\n",
            "| step   84 | loss: 3.7410 | lr 5.1000e-04 | norm: 1.1615 | dt: 3238.17ms | tok/sec: 161908.74 |\n",
            "| step   85 | loss: 3.7342 | lr 5.1600e-04 | norm: 1.3016 | dt: 3234.68ms | tok/sec: 162083.60 |\n",
            "| step   86 | loss: 3.7523 | lr 5.2200e-04 | norm: 1.3785 | dt: 3236.02ms | tok/sec: 162016.50 |\n",
            "| step   87 | loss: 3.7287 | lr 5.2800e-04 | norm: 1.3735 | dt: 3230.44ms | tok/sec: 162296.13 |\n",
            "| step   88 | loss: 3.7126 | lr 5.3400e-04 | norm: 0.9921 | dt: 3232.63ms | tok/sec: 162186.30 |\n",
            "| step   89 | loss: 3.6984 | lr 5.4000e-04 | norm: 1.0677 | dt: 3239.02ms | tok/sec: 161866.00 |\n",
            "| step   90 | loss: 3.7317 | lr 5.4600e-04 | norm: 1.4721 | dt: 3231.74ms | tok/sec: 162230.88 |\n",
            "| step   91 | loss: 3.6994 | lr 5.5200e-04 | norm: 1.0673 | dt: 3234.18ms | tok/sec: 162108.72 |\n",
            "| step   92 | loss: 3.7057 | lr 5.5800e-04 | norm: 1.1456 | dt: 3233.25ms | tok/sec: 162155.11 |\n",
            "| step   93 | loss: 3.6911 | lr 5.6400e-04 | norm: 1.0201 | dt: 3232.68ms | tok/sec: 162183.91 |\n",
            "| step   94 | loss: 3.6898 | lr 5.7000e-04 | norm: 0.9868 | dt: 3232.03ms | tok/sec: 162216.39 |\n",
            "| step   95 | loss: 3.6762 | lr 5.7600e-04 | norm: 0.8491 | dt: 3233.38ms | tok/sec: 162148.82 |\n",
            "| step   96 | loss: 3.6825 | lr 5.8200e-04 | norm: 0.8233 | dt: 3231.46ms | tok/sec: 162244.77 |\n",
            "| step   97 | loss: 3.6608 | lr 5.8800e-04 | norm: 0.9166 | dt: 3241.67ms | tok/sec: 161734.15 |\n",
            "| step   98 | loss: 3.6636 | lr 5.9400e-04 | norm: 0.8422 | dt: 3232.59ms | tok/sec: 162188.20 |\n",
            "| step   99 | loss: 3.6381 | lr 6.0000e-04 | norm: 0.7528 | dt: 3235.24ms | tok/sec: 162055.55 |\n",
            "| step  100 | loss: 3.6347 | lr 6.0000e-04 | norm: 0.6869 | dt: 3230.02ms | tok/sec: 162317.39 |\n",
            "| step  101 | loss: 3.6381 | lr 6.0000e-04 | norm: 0.8156 | dt: 3242.70ms | tok/sec: 161682.61 |\n",
            "| step  102 | loss: 3.6584 | lr 5.9999e-04 | norm: 1.2854 | dt: 3231.91ms | tok/sec: 162222.51 |\n",
            "| step  103 | loss: 3.6390 | lr 5.9999e-04 | norm: 0.9321 | dt: 3228.37ms | tok/sec: 162400.41 |\n",
            "| step  104 | loss: 3.6125 | lr 5.9997e-04 | norm: 0.7568 | dt: 3227.45ms | tok/sec: 162446.73 |\n",
            "| step  105 | loss: 3.6002 | lr 5.9996e-04 | norm: 0.7418 | dt: 3240.74ms | tok/sec: 161780.41 |\n",
            "| step  106 | loss: 3.6012 | lr 5.9994e-04 | norm: 0.7565 | dt: 3231.41ms | tok/sec: 162247.37 |\n",
            "| step  107 | loss: 3.5746 | lr 5.9992e-04 | norm: 0.7470 | dt: 3231.42ms | tok/sec: 162246.79 |\n",
            "| step  108 | loss: 3.5710 | lr 5.9989e-04 | norm: 0.6784 | dt: 3233.35ms | tok/sec: 162149.98 |\n",
            "| step  109 | loss: 3.5710 | lr 5.9987e-04 | norm: 0.6290 | dt: 3232.70ms | tok/sec: 162182.88 |\n",
            "| step  110 | loss: 3.5615 | lr 5.9984e-04 | norm: 0.6892 | dt: 3231.41ms | tok/sec: 162247.50 |\n",
            "| step  111 | loss: 3.5557 | lr 5.9980e-04 | norm: 0.7715 | dt: 3232.80ms | tok/sec: 162177.79 |\n",
            "| step  112 | loss: 3.5348 | lr 5.9976e-04 | norm: 0.9008 | dt: 3231.64ms | tok/sec: 162235.90 |\n",
            "| step  113 | loss: 3.5473 | lr 5.9972e-04 | norm: 0.8093 | dt: 3240.43ms | tok/sec: 161796.06 |\n",
            "| step  114 | loss: 3.5390 | lr 5.9968e-04 | norm: 0.8424 | dt: 3231.95ms | tok/sec: 162220.27 |\n",
            "| step  115 | loss: 3.5317 | lr 5.9963e-04 | norm: 0.8284 | dt: 3231.38ms | tok/sec: 162249.05 |\n",
            "| step  116 | loss: 3.5244 | lr 5.9958e-04 | norm: 0.8420 | dt: 3230.47ms | tok/sec: 162294.61 |\n",
            "| step  117 | loss: 3.5160 | lr 5.9952e-04 | norm: 0.9206 | dt: 3229.01ms | tok/sec: 162368.25 |\n",
            "| step  118 | loss: 3.5036 | lr 5.9947e-04 | norm: 0.6927 | dt: 3236.68ms | tok/sec: 161983.37 |\n",
            "| step  119 | loss: 3.5063 | lr 5.9941e-04 | norm: 0.7486 | dt: 3243.58ms | tok/sec: 161638.73 |\n",
            "| step  120 | loss: 3.4800 | lr 5.9934e-04 | norm: 0.8120 | dt: 3238.90ms | tok/sec: 161872.34 |\n",
            "| step  121 | loss: 3.4907 | lr 5.9927e-04 | norm: 0.8311 | dt: 3231.14ms | tok/sec: 162260.78 |\n",
            "| step  122 | loss: 3.4531 | lr 5.9920e-04 | norm: 0.7641 | dt: 3240.49ms | tok/sec: 161792.78 |\n",
            "| step  123 | loss: 3.4302 | lr 5.9913e-04 | norm: 0.6677 | dt: 3234.19ms | tok/sec: 162108.04 |\n",
            "| step  124 | loss: 3.4389 | lr 5.9905e-04 | norm: 0.9397 | dt: 3235.79ms | tok/sec: 162027.87 |\n",
            "| step  125 | loss: 3.4330 | lr 5.9897e-04 | norm: 0.9181 | dt: 3236.12ms | tok/sec: 162011.14 |\n",
            "| step  126 | loss: 3.4329 | lr 5.9889e-04 | norm: 0.9159 | dt: 3231.13ms | tok/sec: 162261.27 |\n",
            "| step  127 | loss: 3.4185 | lr 5.9880e-04 | norm: 1.0152 | dt: 3237.12ms | tok/sec: 161961.22 |\n",
            "| step  128 | loss: 3.4204 | lr 5.9871e-04 | norm: 1.1268 | dt: 3231.73ms | tok/sec: 162231.14 |\n",
            "| step  129 | loss: 3.4018 | lr 5.9862e-04 | norm: 0.9810 | dt: 3239.86ms | tok/sec: 161824.52 |\n",
            "| step  130 | loss: 3.3882 | lr 5.9852e-04 | norm: 0.8384 | dt: 3231.72ms | tok/sec: 162231.91 |\n",
            "| step  131 | loss: 3.3676 | lr 5.9842e-04 | norm: 1.0312 | dt: 3232.97ms | tok/sec: 162169.15 |\n",
            "| step  132 | loss: 3.3678 | lr 5.9832e-04 | norm: 0.9721 | dt: 3231.57ms | tok/sec: 162239.30 |\n",
            "| step  133 | loss: 3.3439 | lr 5.9821e-04 | norm: 0.9667 | dt: 3236.42ms | tok/sec: 161996.51 |\n",
            "| step  134 | loss: 3.3399 | lr 5.9810e-04 | norm: 1.0233 | dt: 3231.45ms | tok/sec: 162245.67 |\n",
            "| step  135 | loss: 3.3324 | lr 5.9799e-04 | norm: 1.2906 | dt: 3234.29ms | tok/sec: 162102.77 |\n",
            "| step  136 | loss: 3.3125 | lr 5.9787e-04 | norm: 0.7481 | dt: 3238.13ms | tok/sec: 161910.67 |\n",
            "| step  137 | loss: 3.2911 | lr 5.9775e-04 | norm: 0.7229 | dt: 3234.99ms | tok/sec: 162067.80 |\n",
            "| step  138 | loss: 3.2810 | lr 5.9763e-04 | norm: 0.5911 | dt: 3234.61ms | tok/sec: 162086.92 |\n",
            "| step  139 | loss: 3.2624 | lr 5.9750e-04 | norm: 0.6051 | dt: 3235.31ms | tok/sec: 162051.62 |\n",
            "| step  140 | loss: 3.2449 | lr 5.9737e-04 | norm: 0.6268 | dt: 3226.30ms | tok/sec: 162504.45 |\n",
            "| step  141 | loss: 3.2327 | lr 5.9724e-04 | norm: 0.6572 | dt: 3231.73ms | tok/sec: 162231.33 |\n",
            "| step  142 | loss: 3.2268 | lr 5.9710e-04 | norm: 0.7322 | dt: 3236.26ms | tok/sec: 162004.42 |\n",
            "| step  143 | loss: 3.2153 | lr 5.9696e-04 | norm: 0.9574 | dt: 3235.28ms | tok/sec: 162053.20 |\n",
            "| step  144 | loss: 3.2181 | lr 5.9682e-04 | norm: 1.0045 | dt: 3237.99ms | tok/sec: 161917.87 |\n",
            "| step  145 | loss: 3.1793 | lr 5.9668e-04 | norm: 0.9171 | dt: 3231.87ms | tok/sec: 162224.24 |\n",
            "| step  146 | loss: 3.2003 | lr 5.9653e-04 | norm: 1.0933 | dt: 3234.83ms | tok/sec: 162075.92 |\n",
            "| step  147 | loss: 3.1926 | lr 5.9637e-04 | norm: 1.0212 | dt: 3229.85ms | tok/sec: 162325.83 |\n",
            "| step  148 | loss: 3.1716 | lr 5.9622e-04 | norm: 0.9899 | dt: 3235.89ms | tok/sec: 162023.03 |\n",
            "| step  149 | loss: 3.1506 | lr 5.9606e-04 | norm: 0.8487 | dt: 3235.54ms | tok/sec: 162040.36 |\n",
            "| step  150 | loss: 3.1472 | lr 5.9590e-04 | norm: 0.9998 | dt: 3236.17ms | tok/sec: 162008.85 |\n",
            "| step  151 | loss: 3.1436 | lr 5.9573e-04 | norm: 0.9401 | dt: 3233.39ms | tok/sec: 162148.24 |\n",
            "| step  152 | loss: 3.1110 | lr 5.9556e-04 | norm: 0.6442 | dt: 3236.82ms | tok/sec: 161976.08 |\n",
            "| step  153 | loss: 3.1075 | lr 5.9539e-04 | norm: 0.7233 | dt: 3232.47ms | tok/sec: 162194.19 |\n",
            "| step  154 | loss: 3.1037 | lr 5.9522e-04 | norm: 0.8049 | dt: 3236.03ms | tok/sec: 162015.79 |\n",
            "| step  155 | loss: 3.1039 | lr 5.9504e-04 | norm: 1.2457 | dt: 3230.87ms | tok/sec: 162274.78 |\n",
            "| step  156 | loss: 3.0755 | lr 5.9486e-04 | norm: 1.0961 | dt: 3242.53ms | tok/sec: 161691.16 |\n",
            "| step  157 | loss: 3.0511 | lr 5.9467e-04 | norm: 0.8708 | dt: 3238.45ms | tok/sec: 161894.98 |\n",
            "| step  158 | loss: 3.0460 | lr 5.9449e-04 | norm: 1.0660 | dt: 3242.95ms | tok/sec: 161669.90 |\n",
            "| step  159 | loss: 3.0615 | lr 5.9429e-04 | norm: 1.0391 | dt: 3241.36ms | tok/sec: 161749.17 |\n",
            "| step  160 | loss: 3.0301 | lr 5.9410e-04 | norm: 0.9106 | dt: 3242.79ms | tok/sec: 161677.97 |\n",
            "| step  161 | loss: 3.0375 | lr 5.9390e-04 | norm: 1.1184 | dt: 3240.55ms | tok/sec: 161789.84 |\n",
            "| step  162 | loss: 3.0479 | lr 5.9370e-04 | norm: 1.1533 | dt: 3235.57ms | tok/sec: 162038.90 |\n",
            "| step  163 | loss: 3.0252 | lr 5.9350e-04 | norm: 1.0001 | dt: 3236.96ms | tok/sec: 161969.23 |\n",
            "| step  164 | loss: 3.0014 | lr 5.9329e-04 | norm: 0.8368 | dt: 3236.58ms | tok/sec: 161988.36 |\n",
            "| step  165 | loss: 2.9615 | lr 5.9308e-04 | norm: 0.8707 | dt: 3235.69ms | tok/sec: 162032.67 |\n",
            "| step  166 | loss: 2.9722 | lr 5.9287e-04 | norm: 0.7014 | dt: 3232.97ms | tok/sec: 162169.29 |\n",
            "| step  167 | loss: 2.9527 | lr 5.9265e-04 | norm: 0.6050 | dt: 3229.77ms | tok/sec: 162329.89 |\n",
            "| step  168 | loss: 2.9251 | lr 5.9243e-04 | norm: 0.5960 | dt: 3231.24ms | tok/sec: 162255.76 |\n",
            "| step  169 | loss: 2.9295 | lr 5.9221e-04 | norm: 0.8046 | dt: 3232.69ms | tok/sec: 162183.16 |\n",
            "| step  170 | loss: 2.9222 | lr 5.9198e-04 | norm: 1.0519 | dt: 3229.85ms | tok/sec: 162325.98 |\n",
            "| step  171 | loss: 2.9143 | lr 5.9175e-04 | norm: 0.9653 | dt: 3232.29ms | tok/sec: 162203.28 |\n",
            "| step  172 | loss: 2.9040 | lr 5.9152e-04 | norm: 1.1693 | dt: 3236.78ms | tok/sec: 161978.23 |\n",
            "| step  173 | loss: 2.9145 | lr 5.9128e-04 | norm: 1.3623 | dt: 3232.57ms | tok/sec: 162189.29 |\n",
            "| step  174 | loss: 2.8700 | lr 5.9104e-04 | norm: 0.8006 | dt: 3229.74ms | tok/sec: 162331.21 |\n",
            "| step  175 | loss: 2.8773 | lr 5.9080e-04 | norm: 1.0015 | dt: 3236.12ms | tok/sec: 162011.25 |\n",
            "| step  176 | loss: 2.8652 | lr 5.9055e-04 | norm: 0.9359 | dt: 3229.99ms | tok/sec: 162318.76 |\n",
            "| step  177 | loss: 2.8684 | lr 5.9031e-04 | norm: 0.9540 | dt: 3240.77ms | tok/sec: 161778.77 |\n",
            "| step  178 | loss: 2.8344 | lr 5.9005e-04 | norm: 1.1299 | dt: 3234.83ms | tok/sec: 162075.75 |\n",
            "| step  179 | loss: 2.8183 | lr 5.8980e-04 | norm: 1.0982 | dt: 3228.03ms | tok/sec: 162417.10 |\n",
            "| step  180 | loss: 2.8101 | lr 5.8954e-04 | norm: 1.0032 | dt: 3234.27ms | tok/sec: 162103.96 |\n",
            "| step  181 | loss: 2.8251 | lr 5.8928e-04 | norm: 0.8653 | dt: 3236.95ms | tok/sec: 161969.60 |\n",
            "| step  182 | loss: 2.7946 | lr 5.8901e-04 | norm: 1.0264 | dt: 3238.80ms | tok/sec: 161877.21 |\n",
            "| step  183 | loss: 2.8138 | lr 5.8875e-04 | norm: 1.0994 | dt: 3233.60ms | tok/sec: 162137.67 |\n",
            "| step  184 | loss: 2.7846 | lr 5.8848e-04 | norm: 0.8474 | dt: 3235.03ms | tok/sec: 162065.96 |\n",
            "| step  185 | loss: 2.7736 | lr 5.8820e-04 | norm: 0.7801 | dt: 3228.58ms | tok/sec: 162389.83 |\n",
            "| step  186 | loss: 2.7600 | lr 5.8793e-04 | norm: 0.7459 | dt: 3231.80ms | tok/sec: 162227.65 |\n",
            "| step  187 | loss: 2.7529 | lr 5.8764e-04 | norm: 0.9389 | dt: 3232.18ms | tok/sec: 162208.57 |\n",
            "| step  188 | loss: 2.7414 | lr 5.8736e-04 | norm: 1.3783 | dt: 3235.27ms | tok/sec: 162053.79 |\n",
            "| step  189 | loss: 2.7168 | lr 5.8707e-04 | norm: 0.9596 | dt: 3235.04ms | tok/sec: 162065.47 |\n",
            "| step  190 | loss: 2.7422 | lr 5.8679e-04 | norm: 1.3059 | dt: 3234.75ms | tok/sec: 162080.08 |\n",
            "| step  191 | loss: 2.7277 | lr 5.8649e-04 | norm: 1.4501 | dt: 3231.91ms | tok/sec: 162222.42 |\n",
            "| step  192 | loss: 2.7082 | lr 5.8620e-04 | norm: 1.1682 | dt: 3234.04ms | tok/sec: 162115.48 |\n",
            "| step  193 | loss: 2.7105 | lr 5.8590e-04 | norm: 0.8460 | dt: 3233.14ms | tok/sec: 162160.39 |\n",
            "| step  194 | loss: 2.7043 | lr 5.8560e-04 | norm: 1.0952 | dt: 3234.35ms | tok/sec: 162100.10 |\n",
            "| step  195 | loss: 2.6832 | lr 5.8529e-04 | norm: 1.1064 | dt: 3230.52ms | tok/sec: 162292.13 |\n",
            "| step  196 | loss: 2.6740 | lr 5.8498e-04 | norm: 0.8841 | dt: 3233.21ms | tok/sec: 162157.00 |\n",
            "| step  197 | loss: 2.6717 | lr 5.8467e-04 | norm: 0.9984 | dt: 3230.65ms | tok/sec: 162285.36 |\n",
            "| step  198 | loss: 2.6555 | lr 5.8436e-04 | norm: 1.1304 | dt: 3233.06ms | tok/sec: 162164.70 |\n",
            "| step  199 | loss: 2.6670 | lr 5.8404e-04 | norm: 1.0969 | dt: 3238.30ms | tok/sec: 161902.16 |\n",
            "| step  200 | loss: 2.6214 | lr 5.8372e-04 | norm: 0.7812 | dt: 3230.79ms | tok/sec: 162278.62 |\n",
            "| step  201 | loss: 2.6358 | lr 5.8339e-04 | norm: 0.8380 | dt: 3229.54ms | tok/sec: 162341.19 |\n",
            "| step  202 | loss: 2.6313 | lr 5.8307e-04 | norm: 0.8958 | dt: 3226.57ms | tok/sec: 162491.03 |\n",
            "| step  203 | loss: 2.6256 | lr 5.8274e-04 | norm: 0.9335 | dt: 3247.14ms | tok/sec: 161461.46 |\n",
            "| step  204 | loss: 2.6146 | lr 5.8240e-04 | norm: 0.9518 | dt: 3232.66ms | tok/sec: 162184.91 |\n",
            "| step  205 | loss: 2.5974 | lr 5.8207e-04 | norm: 0.9941 | dt: 3229.19ms | tok/sec: 162358.76 |\n",
            "| step  206 | loss: 2.6003 | lr 5.8173e-04 | norm: 1.1842 | dt: 3228.90ms | tok/sec: 162373.69 |\n",
            "| step  207 | loss: 2.5817 | lr 5.8139e-04 | norm: 1.0467 | dt: 3239.89ms | tok/sec: 161822.67 |\n",
            "| step  208 | loss: 2.5868 | lr 5.8104e-04 | norm: 1.0019 | dt: 3232.13ms | tok/sec: 162211.47 |\n",
            "| step  209 | loss: 2.5664 | lr 5.8069e-04 | norm: 0.9285 | dt: 3231.86ms | tok/sec: 162224.67 |\n",
            "| step  210 | loss: 2.5632 | lr 5.8034e-04 | norm: 0.9068 | dt: 3234.29ms | tok/sec: 162102.77 |\n",
            "| step  211 | loss: 2.5544 | lr 5.7999e-04 | norm: 0.8648 | dt: 3232.48ms | tok/sec: 162193.98 |\n",
            "| step  212 | loss: 2.5397 | lr 5.7963e-04 | norm: 0.9053 | dt: 3231.59ms | tok/sec: 162238.31 |\n",
            "| step  213 | loss: 2.5298 | lr 5.7927e-04 | norm: 0.9137 | dt: 3233.01ms | tok/sec: 162167.03 |\n",
            "| step  214 | loss: 2.5175 | lr 5.7890e-04 | norm: 0.8860 | dt: 3234.53ms | tok/sec: 162091.18 |\n",
            "| step  215 | loss: 2.5087 | lr 5.7854e-04 | norm: 0.9623 | dt: 3227.19ms | tok/sec: 162459.40 |\n",
            "| step  216 | loss: 2.5108 | lr 5.7817e-04 | norm: 0.9771 | dt: 3227.06ms | tok/sec: 162465.99 |\n",
            "| step  217 | loss: 2.5106 | lr 5.7779e-04 | norm: 0.9944 | dt: 3233.07ms | tok/sec: 162164.21 |\n",
            "| step  218 | loss: 2.4865 | lr 5.7742e-04 | norm: 1.0411 | dt: 3239.61ms | tok/sec: 161836.80 |\n",
            "| step  219 | loss: 2.5135 | lr 5.7704e-04 | norm: 1.4265 | dt: 3232.53ms | tok/sec: 162191.29 |\n",
            "| step  220 | loss: 2.5043 | lr 5.7666e-04 | norm: 0.9575 | dt: 3240.13ms | tok/sec: 161810.78 |\n",
            "| step  221 | loss: 2.4826 | lr 5.7627e-04 | norm: 1.0130 | dt: 3231.16ms | tok/sec: 162259.85 |\n",
            "| step  222 | loss: 2.4850 | lr 5.7588e-04 | norm: 0.8701 | dt: 3234.48ms | tok/sec: 162093.54 |\n",
            "| step  223 | loss: 2.4398 | lr 5.7549e-04 | norm: 0.6988 | dt: 3241.80ms | tok/sec: 161727.47 |\n",
            "| step  224 | loss: 2.4523 | lr 5.7510e-04 | norm: 0.8447 | dt: 3237.29ms | tok/sec: 161952.94 |\n",
            "| step  225 | loss: 2.4412 | lr 5.7470e-04 | norm: 0.9142 | dt: 3240.01ms | tok/sec: 161816.60 |\n",
            "| step  226 | loss: 2.4497 | lr 5.7430e-04 | norm: 0.9319 | dt: 3233.70ms | tok/sec: 162132.37 |\n",
            "| step  227 | loss: 2.4549 | lr 5.7390e-04 | norm: 1.0469 | dt: 3230.93ms | tok/sec: 162271.58 |\n",
            "| step  228 | loss: 2.4510 | lr 5.7349e-04 | norm: 1.1291 | dt: 3233.16ms | tok/sec: 162159.48 |\n",
            "| step  229 | loss: 2.4308 | lr 5.7309e-04 | norm: 1.0009 | dt: 3234.28ms | tok/sec: 162103.39 |\n",
            "| step  230 | loss: 2.4494 | lr 5.7267e-04 | norm: 1.1791 | dt: 3231.65ms | tok/sec: 162235.31 |\n",
            "| step  231 | loss: 2.4297 | lr 5.7226e-04 | norm: 0.8886 | dt: 3234.91ms | tok/sec: 162071.76 |\n",
            "| step  232 | loss: 2.4117 | lr 5.7184e-04 | norm: 0.8727 | dt: 3237.11ms | tok/sec: 161961.98 |\n",
            "| step  233 | loss: 2.3990 | lr 5.7142e-04 | norm: 1.0749 | dt: 3232.54ms | tok/sec: 162190.53 |\n",
            "| step  234 | loss: 2.3848 | lr 5.7100e-04 | norm: 0.9803 | dt: 3229.82ms | tok/sec: 162327.46 |\n",
            "| step  235 | loss: 2.3885 | lr 5.7057e-04 | norm: 1.1011 | dt: 3230.33ms | tok/sec: 162301.61 |\n",
            "| step  236 | loss: 2.3850 | lr 5.7014e-04 | norm: 0.8868 | dt: 3233.70ms | tok/sec: 162132.32 |\n",
            "| step  237 | loss: 2.3693 | lr 5.6971e-04 | norm: 0.8725 | dt: 3231.17ms | tok/sec: 162259.25 |\n",
            "| step  238 | loss: 2.3760 | lr 5.6927e-04 | norm: 1.0646 | dt: 3233.42ms | tok/sec: 162146.53 |\n",
            "| step  239 | loss: 2.3618 | lr 5.6884e-04 | norm: 1.0463 | dt: 3231.75ms | tok/sec: 162230.33 |\n",
            "| step  240 | loss: 2.3747 | lr 5.6840e-04 | norm: 0.8565 | dt: 3234.80ms | tok/sec: 162077.55 |\n",
            "| step  241 | loss: 2.3549 | lr 5.6795e-04 | norm: 1.0776 | dt: 3238.27ms | tok/sec: 161903.63 |\n",
            "| step  242 | loss: 2.3561 | lr 5.6751e-04 | norm: 1.0474 | dt: 3240.24ms | tok/sec: 161805.46 |\n",
            "| step  243 | loss: 2.3354 | lr 5.6706e-04 | norm: 0.8144 | dt: 3235.18ms | tok/sec: 162058.39 |\n",
            "| step  244 | loss: 2.3229 | lr 5.6660e-04 | norm: 0.7357 | dt: 3233.15ms | tok/sec: 162160.13 |\n",
            "| step  245 | loss: 2.3214 | lr 5.6615e-04 | norm: 0.7153 | dt: 3230.97ms | tok/sec: 162269.48 |\n",
            "| step  246 | loss: 2.3089 | lr 5.6569e-04 | norm: 0.7578 | dt: 3229.10ms | tok/sec: 162363.28 |\n",
            "| step  247 | loss: 2.3157 | lr 5.6523e-04 | norm: 0.8003 | dt: 3229.15ms | tok/sec: 162360.89 |\n",
            "| step  248 | loss: 2.2967 | lr 5.6476e-04 | norm: 0.9790 | dt: 3241.38ms | tok/sec: 161748.63 |\n",
            "| step  249 | loss: 2.3049 | lr 5.6430e-04 | norm: 1.0212 | dt: 3232.01ms | tok/sec: 162217.49 |\n",
            "| step  250 | loss: 2.2865 | lr 5.6383e-04 | norm: 0.9739 | dt: 3228.45ms | tok/sec: 162396.28 |\n",
            "| step  251 | loss: 2.2951 | lr 5.6335e-04 | norm: 0.7882 | dt: 3231.22ms | tok/sec: 162257.02 |\n",
            "| step  252 | loss: 2.2798 | lr 5.6288e-04 | norm: 0.7433 | dt: 3233.00ms | tok/sec: 162167.90 |\n",
            "| step  253 | loss: 2.2746 | lr 5.6240e-04 | norm: 0.9394 | dt: 3234.10ms | tok/sec: 162112.72 |\n",
            "| step  254 | loss: 2.3015 | lr 5.6192e-04 | norm: 1.0534 | dt: 3232.49ms | tok/sec: 162193.41 |\n",
            "| step  255 | loss: 2.2897 | lr 5.6144e-04 | norm: 0.9681 | dt: 3236.21ms | tok/sec: 162006.73 |\n",
            "| step  256 | loss: 2.2821 | lr 5.6095e-04 | norm: 1.1400 | dt: 3235.94ms | tok/sec: 162020.29 |\n",
            "| step  257 | loss: 2.2470 | lr 5.6046e-04 | norm: 0.7964 | dt: 3238.18ms | tok/sec: 161908.22 |\n",
            "| step  258 | loss: 2.2451 | lr 5.5997e-04 | norm: 0.9683 | dt: 3234.54ms | tok/sec: 162090.22 |\n",
            "| step  259 | loss: 2.2372 | lr 5.5947e-04 | norm: 0.8353 | dt: 3234.55ms | tok/sec: 162089.98 |\n",
            "| step  260 | loss: 2.2475 | lr 5.5897e-04 | norm: 0.8091 | dt: 3232.71ms | tok/sec: 162181.98 |\n",
            "| step  261 | loss: 2.2275 | lr 5.5847e-04 | norm: 0.7450 | dt: 3231.47ms | tok/sec: 162244.48 |\n",
            "| step  262 | loss: 2.2369 | lr 5.5797e-04 | norm: 0.7999 | dt: 3230.55ms | tok/sec: 162290.76 |\n",
            "| step  263 | loss: 2.2630 | lr 5.5746e-04 | norm: 0.8066 | dt: 3231.15ms | tok/sec: 162260.28 |\n",
            "| step  264 | loss: 2.2300 | lr 5.5695e-04 | norm: 0.9009 | dt: 3231.36ms | tok/sec: 162249.95 |\n",
            "| step  265 | loss: 2.2182 | lr 5.5644e-04 | norm: 0.9198 | dt: 3236.44ms | tok/sec: 161995.08 |\n",
            "| step  266 | loss: 2.1988 | lr 5.5593e-04 | norm: 0.9690 | dt: 3235.43ms | tok/sec: 162045.87 |\n",
            "| step  267 | loss: 2.2127 | lr 5.5541e-04 | norm: 0.8681 | dt: 3241.60ms | tok/sec: 161737.47 |\n",
            "| step  268 | loss: 2.2120 | lr 5.5489e-04 | norm: 0.8139 | dt: 3233.06ms | tok/sec: 162164.73 |\n",
            "| step  269 | loss: 2.2022 | lr 5.5437e-04 | norm: 0.9868 | dt: 3228.80ms | tok/sec: 162378.51 |\n",
            "| step  270 | loss: 2.1965 | lr 5.5384e-04 | norm: 0.9626 | dt: 3231.13ms | tok/sec: 162261.54 |\n",
            "| step  271 | loss: 2.1837 | lr 5.5331e-04 | norm: 0.9046 | dt: 3236.61ms | tok/sec: 161987.01 |\n",
            "| step  272 | loss: 2.1937 | lr 5.5278e-04 | norm: 0.9450 | dt: 3243.75ms | tok/sec: 161630.20 |\n",
            "| step  273 | loss: 2.1878 | lr 5.5225e-04 | norm: 1.0265 | dt: 3230.13ms | tok/sec: 162311.82 |\n",
            "| step  274 | loss: 2.1708 | lr 5.5171e-04 | norm: 0.8299 | dt: 3233.74ms | tok/sec: 162130.73 |\n",
            "| step  275 | loss: 2.1666 | lr 5.5117e-04 | norm: 0.7188 | dt: 3231.73ms | tok/sec: 162231.61 |\n",
            "| step  276 | loss: 2.1604 | lr 5.5063e-04 | norm: 0.6352 | dt: 3238.68ms | tok/sec: 161883.15 |\n",
            "| step  277 | loss: 2.1596 | lr 5.5008e-04 | norm: 0.6279 | dt: 3232.11ms | tok/sec: 162212.06 |\n",
            "| step  278 | loss: 2.1478 | lr 5.4954e-04 | norm: 0.6937 | dt: 3232.06ms | tok/sec: 162214.79 |\n",
            "| step  279 | loss: 2.1390 | lr 5.4899e-04 | norm: 0.8181 | dt: 3230.65ms | tok/sec: 162285.41 |\n",
            "| step  280 | loss: 2.1329 | lr 5.4843e-04 | norm: 0.9013 | dt: 3232.30ms | tok/sec: 162202.91 |\n",
            "| step  281 | loss: 2.1458 | lr 5.4788e-04 | norm: 1.0370 | dt: 3236.18ms | tok/sec: 162008.42 |\n",
            "| step  282 | loss: 2.1646 | lr 5.4732e-04 | norm: 0.9892 | dt: 3239.19ms | tok/sec: 161857.85 |\n",
            "| step  283 | loss: 2.1411 | lr 5.4676e-04 | norm: 0.9503 | dt: 3232.67ms | tok/sec: 162184.07 |\n",
            "| step  284 | loss: 2.1445 | lr 5.4620e-04 | norm: 0.8639 | dt: 3227.39ms | tok/sec: 162449.29 |\n",
            "| step  285 | loss: 2.1271 | lr 5.4563e-04 | norm: 0.7646 | dt: 3233.63ms | tok/sec: 162135.88 |\n",
            "| step  286 | loss: 2.1399 | lr 5.4506e-04 | norm: 0.7023 | dt: 3236.62ms | tok/sec: 161986.38 |\n",
            "| step  287 | loss: 2.1111 | lr 5.4449e-04 | norm: 0.6187 | dt: 3232.44ms | tok/sec: 162195.81 |\n",
            "| step  288 | loss: 2.1059 | lr 5.4392e-04 | norm: 0.6933 | dt: 3230.58ms | tok/sec: 162289.14 |\n",
            "| step  289 | loss: 2.0901 | lr 5.4334e-04 | norm: 0.8490 | dt: 3233.31ms | tok/sec: 162151.97 |\n",
            "| step  290 | loss: 2.1005 | lr 5.4276e-04 | norm: 0.9719 | dt: 3231.42ms | tok/sec: 162246.82 |\n",
            "| step  291 | loss: 2.1003 | lr 5.4218e-04 | norm: 0.8611 | dt: 3231.24ms | tok/sec: 162255.87 |\n",
            "| step  292 | loss: 2.0898 | lr 5.4160e-04 | norm: 0.7353 | dt: 3231.22ms | tok/sec: 162256.93 |\n",
            "| step  293 | loss: 2.0747 | lr 5.4101e-04 | norm: 0.7975 | dt: 3233.33ms | tok/sec: 162151.13 |\n",
            "| step  294 | loss: 2.0904 | lr 5.4042e-04 | norm: 0.7623 | dt: 3230.61ms | tok/sec: 162287.77 |\n",
            "| step  295 | loss: 2.0765 | lr 5.3983e-04 | norm: 0.6767 | dt: 3231.07ms | tok/sec: 162264.27 |\n",
            "| step  296 | loss: 2.0832 | lr 5.3924e-04 | norm: 0.8606 | dt: 3230.25ms | tok/sec: 162305.73 |\n",
            "| step  297 | loss: 2.0708 | lr 5.3864e-04 | norm: 0.8844 | dt: 3231.82ms | tok/sec: 162226.70 |\n",
            "| step  298 | loss: 2.0662 | lr 5.3804e-04 | norm: 0.7856 | dt: 3236.28ms | tok/sec: 162003.36 |\n",
            "| step  299 | loss: 2.0587 | lr 5.3744e-04 | norm: 0.6630 | dt: 3237.21ms | tok/sec: 161956.74 |\n",
            "| step  300 | loss: 2.0418 | lr 5.3683e-04 | norm: 0.7863 | dt: 3233.37ms | tok/sec: 162149.29 |\n",
            "| step  301 | loss: 2.0436 | lr 5.3622e-04 | norm: 0.7996 | dt: 3234.03ms | tok/sec: 162116.02 |\n",
            "| step  302 | loss: 2.0618 | lr 5.3562e-04 | norm: 0.8257 | dt: 3232.36ms | tok/sec: 162199.92 |\n",
            "| step  303 | loss: 2.0486 | lr 5.3500e-04 | norm: 0.7901 | dt: 3235.13ms | tok/sec: 162060.87 |\n",
            "| step  304 | loss: 2.0692 | lr 5.3439e-04 | norm: 0.9107 | dt: 3236.07ms | tok/sec: 162014.04 |\n",
            "| step  305 | loss: 2.0394 | lr 5.3377e-04 | norm: 0.9674 | dt: 3239.15ms | tok/sec: 161859.52 |\n",
            "| step  306 | loss: 2.0284 | lr 5.3315e-04 | norm: 0.7236 | dt: 3235.07ms | tok/sec: 162063.89 |\n",
            "| step  307 | loss: 2.0295 | lr 5.3253e-04 | norm: 0.7764 | dt: 3232.04ms | tok/sec: 162215.90 |\n",
            "| step  308 | loss: 2.0263 | lr 5.3191e-04 | norm: 0.8493 | dt: 3235.19ms | tok/sec: 162057.67 |\n",
            "| step  309 | loss: 2.0285 | lr 5.3128e-04 | norm: 0.8947 | dt: 3239.34ms | tok/sec: 161850.11 |\n",
            "| step  310 | loss: 2.0293 | lr 5.3065e-04 | norm: 0.8335 | dt: 3232.04ms | tok/sec: 162215.94 |\n",
            "| step  311 | loss: 2.0320 | lr 5.3002e-04 | norm: 0.8226 | dt: 3238.22ms | tok/sec: 161906.36 |\n",
            "| step  312 | loss: 2.0355 | lr 5.2938e-04 | norm: 0.8576 | dt: 3233.72ms | tok/sec: 162131.34 |\n",
            "| step  313 | loss: 2.0162 | lr 5.2875e-04 | norm: 0.8696 | dt: 3232.28ms | tok/sec: 162203.85 |\n",
            "| step  314 | loss: 2.0114 | lr 5.2811e-04 | norm: 0.8166 | dt: 3233.52ms | tok/sec: 162141.73 |\n",
            "| step  315 | loss: 2.0010 | lr 5.2747e-04 | norm: 0.7866 | dt: 3231.11ms | tok/sec: 162262.37 |\n",
            "| step  316 | loss: 1.9982 | lr 5.2682e-04 | norm: 0.6781 | dt: 3230.10ms | tok/sec: 162313.22 |\n",
            "| step  317 | loss: 1.9969 | lr 5.2618e-04 | norm: 0.6171 | dt: 3229.95ms | tok/sec: 162320.96 |\n",
            "| step  318 | loss: 1.9881 | lr 5.2553e-04 | norm: 0.6458 | dt: 3231.89ms | tok/sec: 162223.20 |\n",
            "| step  319 | loss: 1.9857 | lr 5.2488e-04 | norm: 0.6846 | dt: 3231.38ms | tok/sec: 162248.86 |\n",
            "| step  320 | loss: 1.9936 | lr 5.2422e-04 | norm: 0.7101 | dt: 3232.15ms | tok/sec: 162210.15 |\n",
            "| step  321 | loss: 1.9890 | lr 5.2357e-04 | norm: 0.7585 | dt: 3231.59ms | tok/sec: 162238.58 |\n",
            "| step  322 | loss: 1.9914 | lr 5.2291e-04 | norm: 0.8379 | dt: 3232.77ms | tok/sec: 162179.04 |\n",
            "| step  323 | loss: 1.9727 | lr 5.2225e-04 | norm: 0.7450 | dt: 3232.69ms | tok/sec: 162183.17 |\n",
            "| step  324 | loss: 1.9624 | lr 5.2158e-04 | norm: 0.7972 | dt: 3242.62ms | tok/sec: 161686.43 |\n",
            "| step  325 | loss: 1.9735 | lr 5.2092e-04 | norm: 0.6979 | dt: 3235.92ms | tok/sec: 162021.34 |\n",
            "| step  326 | loss: 1.9613 | lr 5.2025e-04 | norm: 0.6013 | dt: 3229.54ms | tok/sec: 162341.49 |\n",
            "| step  327 | loss: 1.9635 | lr 5.1958e-04 | norm: 0.6027 | dt: 3235.57ms | tok/sec: 162038.60 |\n",
            "| step  328 | loss: 1.9618 | lr 5.1891e-04 | norm: 0.5751 | dt: 3237.54ms | tok/sec: 161940.05 |\n",
            "| step  329 | loss: 1.9525 | lr 5.1823e-04 | norm: 0.5926 | dt: 3236.25ms | tok/sec: 162005.01 |\n",
            "| step  330 | loss: 1.9511 | lr 5.1756e-04 | norm: 0.6892 | dt: 3236.86ms | tok/sec: 161974.26 |\n",
            "| step  331 | loss: 1.9708 | lr 5.1688e-04 | norm: 0.7503 | dt: 3233.42ms | tok/sec: 162146.38 |\n",
            "| step  332 | loss: 1.9415 | lr 5.1620e-04 | norm: 0.7034 | dt: 3231.51ms | tok/sec: 162242.23 |\n",
            "| step  333 | loss: 1.9408 | lr 5.1551e-04 | norm: 0.6997 | dt: 3231.78ms | tok/sec: 162228.67 |\n",
            "| step  334 | loss: 1.9211 | lr 5.1483e-04 | norm: 0.7002 | dt: 3232.32ms | tok/sec: 162201.70 |\n",
            "| step  335 | loss: 1.9240 | lr 5.1414e-04 | norm: 0.9132 | dt: 3230.56ms | tok/sec: 162290.10 |\n",
            "| step  336 | loss: 1.9375 | lr 5.1345e-04 | norm: 1.0558 | dt: 3231.95ms | tok/sec: 162220.58 |\n",
            "| step  337 | loss: 1.9460 | lr 5.1276e-04 | norm: 0.8790 | dt: 3230.54ms | tok/sec: 162291.07 |\n",
            "| step  338 | loss: 1.9398 | lr 5.1206e-04 | norm: 1.1257 | dt: 3230.78ms | tok/sec: 162278.86 |\n",
            "| step  339 | loss: 1.9294 | lr 5.1136e-04 | norm: 0.9988 | dt: 3233.78ms | tok/sec: 162128.55 |\n",
            "| step  340 | loss: 1.9394 | lr 5.1067e-04 | norm: 0.9672 | dt: 3227.14ms | tok/sec: 162461.87 |\n",
            "| step  341 | loss: 1.9359 | lr 5.0996e-04 | norm: 0.8185 | dt: 3229.85ms | tok/sec: 162325.95 |\n",
            "| step  342 | loss: 1.9281 | lr 5.0926e-04 | norm: 0.7644 | dt: 3234.86ms | tok/sec: 162074.57 |\n",
            "| step  343 | loss: 1.9132 | lr 5.0855e-04 | norm: 0.6841 | dt: 3231.47ms | tok/sec: 162244.24 |\n",
            "| step  344 | loss: 1.9052 | lr 5.0785e-04 | norm: 0.6344 | dt: 3227.39ms | tok/sec: 162449.43 |\n",
            "| step  345 | loss: 1.8952 | lr 5.0714e-04 | norm: 0.5753 | dt: 3233.62ms | tok/sec: 162136.65 |\n",
            "| step  346 | loss: 1.8904 | lr 5.0642e-04 | norm: 0.5238 | dt: 3232.42ms | tok/sec: 162196.57 |\n",
            "| step  347 | loss: 1.8876 | lr 5.0571e-04 | norm: 0.5448 | dt: 3236.37ms | tok/sec: 161998.98 |\n",
            "| step  348 | loss: 1.8971 | lr 5.0499e-04 | norm: 0.5625 | dt: 3232.83ms | tok/sec: 162175.93 |\n",
            "| step  349 | loss: 1.8710 | lr 5.0427e-04 | norm: 0.5730 | dt: 3232.39ms | tok/sec: 162198.36 |\n",
            "| step  350 | loss: 1.8767 | lr 5.0355e-04 | norm: 0.7205 | dt: 3241.46ms | tok/sec: 161744.29 |\n",
            "| step  351 | loss: 1.8758 | lr 5.0283e-04 | norm: 0.7540 | dt: 3232.66ms | tok/sec: 162184.70 |\n",
            "| step  352 | loss: 1.8988 | lr 5.0210e-04 | norm: 0.7658 | dt: 3231.16ms | tok/sec: 162259.96 |\n",
            "| step  353 | loss: 1.8720 | lr 5.0138e-04 | norm: 0.7042 | dt: 3233.42ms | tok/sec: 162146.56 |\n",
            "| step  354 | loss: 1.8708 | lr 5.0065e-04 | norm: 0.6185 | dt: 3236.91ms | tok/sec: 161971.80 |\n",
            "| step  355 | loss: 1.8924 | lr 4.9992e-04 | norm: 0.7289 | dt: 3238.98ms | tok/sec: 161868.19 |\n",
            "| step  356 | loss: 1.8713 | lr 4.9918e-04 | norm: 0.5857 | dt: 3233.42ms | tok/sec: 162146.34 |\n",
            "| step  357 | loss: 1.8580 | lr 4.9845e-04 | norm: 0.5964 | dt: 3233.29ms | tok/sec: 162153.09 |\n",
            "| step  358 | loss: 1.8429 | lr 4.9771e-04 | norm: 0.6234 | dt: 3236.29ms | tok/sec: 162002.62 |\n",
            "| step  359 | loss: 1.8537 | lr 4.9697e-04 | norm: 0.8269 | dt: 3232.33ms | tok/sec: 162201.46 |\n",
            "| step  360 | loss: 1.8552 | lr 4.9623e-04 | norm: 0.8444 | dt: 3236.44ms | tok/sec: 161995.48 |\n",
            "| step  361 | loss: 1.8620 | lr 4.9548e-04 | norm: 0.7361 | dt: 3239.79ms | tok/sec: 161827.66 |\n",
            "| step  362 | loss: 1.8461 | lr 4.9474e-04 | norm: 0.7798 | dt: 3228.59ms | tok/sec: 162389.03 |\n",
            "| step  363 | loss: 1.8676 | lr 4.9399e-04 | norm: 0.7570 | dt: 3230.20ms | tok/sec: 162307.97 |\n",
            "| step  364 | loss: 1.8700 | lr 4.9324e-04 | norm: 0.6900 | dt: 3233.00ms | tok/sec: 162167.60 |\n",
            "| step  365 | loss: 1.8546 | lr 4.9249e-04 | norm: 0.7333 | dt: 3236.74ms | tok/sec: 161980.08 |\n",
            "| step  366 | loss: 1.8116 | lr 4.9174e-04 | norm: 0.7228 | dt: 3238.15ms | tok/sec: 161909.89 |\n",
            "| step  367 | loss: 1.8405 | lr 4.9098e-04 | norm: 0.7043 | dt: 3235.73ms | tok/sec: 162031.05 |\n",
            "| step  368 | loss: 1.8322 | lr 4.9022e-04 | norm: 0.6773 | dt: 3227.56ms | tok/sec: 162441.06 |\n",
            "| step  369 | loss: 1.8302 | lr 4.8946e-04 | norm: 0.5890 | dt: 3234.67ms | tok/sec: 162083.76 |\n",
            "| step  370 | loss: 1.8321 | lr 4.8870e-04 | norm: 0.5319 | dt: 3227.33ms | tok/sec: 162452.51 |\n",
            "| step  371 | loss: 1.8145 | lr 4.8794e-04 | norm: 0.4769 | dt: 3236.78ms | tok/sec: 161978.13 |\n",
            "| step  372 | loss: 1.8053 | lr 4.8717e-04 | norm: 0.5502 | dt: 3243.03ms | tok/sec: 161666.07 |\n",
            "| step  373 | loss: 1.8139 | lr 4.8641e-04 | norm: 0.6446 | dt: 3234.34ms | tok/sec: 162100.32 |\n",
            "| step  374 | loss: 1.8093 | lr 4.8564e-04 | norm: 0.6783 | dt: 3235.96ms | tok/sec: 162019.50 |\n",
            "| step  375 | loss: 1.8165 | lr 4.8487e-04 | norm: 0.7776 | dt: 3236.53ms | tok/sec: 161990.70 |\n",
            "| step  376 | loss: 1.8100 | lr 4.8409e-04 | norm: 0.6739 | dt: 3236.06ms | tok/sec: 162014.19 |\n",
            "| step  377 | loss: 1.8076 | lr 4.8332e-04 | norm: 0.5762 | dt: 3241.55ms | tok/sec: 161739.85 |\n",
            "| step  378 | loss: 1.8192 | lr 4.8254e-04 | norm: 0.6144 | dt: 3233.15ms | tok/sec: 162160.25 |\n",
            "| step  379 | loss: 1.7952 | lr 4.8176e-04 | norm: 0.7103 | dt: 3238.05ms | tok/sec: 161914.60 |\n",
            "| step  380 | loss: 1.7881 | lr 4.8098e-04 | norm: 0.6676 | dt: 3236.39ms | tok/sec: 161997.73 |\n",
            "| step  381 | loss: 1.7775 | lr 4.8020e-04 | norm: 0.5741 | dt: 3237.22ms | tok/sec: 161956.09 |\n",
            "| step  382 | loss: 1.7988 | lr 4.7942e-04 | norm: 0.6551 | dt: 3230.32ms | tok/sec: 162302.37 |\n",
            "| step  383 | loss: 1.7997 | lr 4.7863e-04 | norm: 0.7338 | dt: 3229.93ms | tok/sec: 162321.82 |\n",
            "| step  384 | loss: 1.8040 | lr 4.7784e-04 | norm: 0.7651 | dt: 3236.24ms | tok/sec: 162005.44 |\n",
            "| step  385 | loss: 1.8020 | lr 4.7705e-04 | norm: 0.7625 | dt: 3235.04ms | tok/sec: 162065.60 |\n",
            "| step  386 | loss: 1.8045 | lr 4.7626e-04 | norm: 0.7594 | dt: 3232.18ms | tok/sec: 162208.73 |\n",
            "| step  387 | loss: 1.7956 | lr 4.7547e-04 | norm: 0.6246 | dt: 3228.46ms | tok/sec: 162395.89 |\n",
            "| step  388 | loss: 1.7903 | lr 4.7467e-04 | norm: 0.6328 | dt: 3234.03ms | tok/sec: 162116.01 |\n",
            "| step  389 | loss: 1.7674 | lr 4.7388e-04 | norm: 0.5464 | dt: 3238.10ms | tok/sec: 161912.33 |\n",
            "| step  390 | loss: 1.7567 | lr 4.7308e-04 | norm: 0.5707 | dt: 3240.61ms | tok/sec: 161786.84 |\n",
            "| step  391 | loss: 1.7559 | lr 4.7228e-04 | norm: 0.5281 | dt: 3240.01ms | tok/sec: 161816.62 |\n",
            "| step  392 | loss: 1.7663 | lr 4.7148e-04 | norm: 0.5466 | dt: 3239.82ms | tok/sec: 161826.04 |\n",
            "| step  393 | loss: 1.7513 | lr 4.7067e-04 | norm: 0.5663 | dt: 3265.96ms | tok/sec: 160530.90 |\n",
            "| step  394 | loss: 1.7582 | lr 4.6987e-04 | norm: 0.6264 | dt: 3237.79ms | tok/sec: 161927.86 |\n",
            "| step  395 | loss: 1.7653 | lr 4.6906e-04 | norm: 0.6523 | dt: 3232.19ms | tok/sec: 162208.29 |\n",
            "| step  396 | loss: 1.7505 | lr 4.6825e-04 | norm: 0.6505 | dt: 3231.82ms | tok/sec: 162226.92 |\n",
            "| step  397 | loss: 1.7577 | lr 4.6744e-04 | norm: 0.6928 | dt: 3234.87ms | tok/sec: 162073.80 |\n",
            "| step  398 | loss: 1.7689 | lr 4.6663e-04 | norm: 0.8747 | dt: 3236.94ms | tok/sec: 161970.43 |\n",
            "| step  399 | loss: 1.7583 | lr 4.6582e-04 | norm: 0.8326 | dt: 3234.94ms | tok/sec: 162070.36 |\n",
            "| step  400 | loss: 1.7608 | lr 4.6500e-04 | norm: 0.7277 | dt: 3237.25ms | tok/sec: 161954.61 |\n",
            "| step  401 | loss: 1.7372 | lr 4.6418e-04 | norm: 0.5945 | dt: 3238.41ms | tok/sec: 161896.66 |\n",
            "| step  402 | loss: 1.7381 | lr 4.6336e-04 | norm: 0.5844 | dt: 3230.64ms | tok/sec: 162286.02 |\n",
            "| step  403 | loss: 1.7538 | lr 4.6254e-04 | norm: 0.6051 | dt: 3236.31ms | tok/sec: 162001.98 |\n",
            "| step  404 | loss: 1.7590 | lr 4.6172e-04 | norm: 0.6853 | dt: 3240.38ms | tok/sec: 161798.13 |\n",
            "| step  405 | loss: 1.7446 | lr 4.6090e-04 | norm: 0.5255 | dt: 3233.67ms | tok/sec: 162134.05 |\n",
            "| step  406 | loss: 1.7233 | lr 4.6007e-04 | norm: 0.5421 | dt: 3235.49ms | tok/sec: 162042.74 |\n",
            "| step  407 | loss: 1.7210 | lr 4.5925e-04 | norm: 0.5255 | dt: 3238.34ms | tok/sec: 161900.38 |\n",
            "| step  408 | loss: 1.7357 | lr 4.5842e-04 | norm: 0.6720 | dt: 3236.84ms | tok/sec: 161975.16 |\n",
            "| step  409 | loss: 1.7145 | lr 4.5759e-04 | norm: 0.6176 | dt: 3236.10ms | tok/sec: 162012.31 |\n",
            "| step  410 | loss: 1.7199 | lr 4.5676e-04 | norm: 0.5758 | dt: 3245.49ms | tok/sec: 161543.62 |\n",
            "| step  411 | loss: 1.7257 | lr 4.5592e-04 | norm: 0.5684 | dt: 3234.81ms | tok/sec: 162076.84 |\n",
            "| step  412 | loss: 1.7259 | lr 4.5509e-04 | norm: 0.5687 | dt: 3233.82ms | tok/sec: 162126.42 |\n",
            "| step  413 | loss: 1.7237 | lr 4.5425e-04 | norm: 0.5856 | dt: 3233.75ms | tok/sec: 162130.03 |\n",
            "| step  414 | loss: 1.7008 | lr 4.5342e-04 | norm: 0.4812 | dt: 3232.85ms | tok/sec: 162175.35 |\n",
            "| step  415 | loss: 1.7079 | lr 4.5258e-04 | norm: 0.5295 | dt: 3239.61ms | tok/sec: 161836.58 |\n",
            "| step  416 | loss: 1.6964 | lr 4.5174e-04 | norm: 0.5542 | dt: 3236.16ms | tok/sec: 162009.29 |\n",
            "| step  417 | loss: 1.6962 | lr 4.5089e-04 | norm: 0.5586 | dt: 3237.01ms | tok/sec: 161966.91 |\n",
            "| step  418 | loss: 1.7111 | lr 4.5005e-04 | norm: 0.5379 | dt: 3239.68ms | tok/sec: 161833.25 |\n",
            "| step  419 | loss: 1.6913 | lr 4.4921e-04 | norm: 0.7070 | dt: 3233.39ms | tok/sec: 162148.15 |\n",
            "| step  420 | loss: 1.7186 | lr 4.4836e-04 | norm: 0.8613 | dt: 3237.20ms | tok/sec: 161957.50 |\n",
            "| step  421 | loss: 1.7174 | lr 4.4751e-04 | norm: 0.7811 | dt: 3239.06ms | tok/sec: 161864.44 |\n",
            "| step  422 | loss: 1.7094 | lr 4.4666e-04 | norm: 0.7253 | dt: 3235.88ms | tok/sec: 162023.35 |\n",
            "| step  423 | loss: 1.7205 | lr 4.4581e-04 | norm: 0.6553 | dt: 3241.74ms | tok/sec: 161730.58 |\n",
            "| step  424 | loss: 1.6919 | lr 4.4496e-04 | norm: 0.6450 | dt: 3232.34ms | tok/sec: 162200.77 |\n",
            "| step  425 | loss: 1.6964 | lr 4.4411e-04 | norm: 0.6031 | dt: 3233.84ms | tok/sec: 162125.56 |\n",
            "| step  426 | loss: 1.6967 | lr 4.4325e-04 | norm: 0.5604 | dt: 3237.36ms | tok/sec: 161949.18 |\n",
            "| step  427 | loss: 1.6996 | lr 4.4240e-04 | norm: 0.5850 | dt: 3239.28ms | tok/sec: 161853.24 |\n",
            "| step  428 | loss: 1.6965 | lr 4.4154e-04 | norm: 0.5286 | dt: 3241.60ms | tok/sec: 161737.42 |\n",
            "| step  429 | loss: 1.6836 | lr 4.4068e-04 | norm: 0.4887 | dt: 3245.18ms | tok/sec: 161558.89 |\n",
            "| step  430 | loss: 1.6914 | lr 4.3982e-04 | norm: 0.5026 | dt: 3238.96ms | tok/sec: 161869.11 |\n",
            "| step  431 | loss: 1.6870 | lr 4.3896e-04 | norm: 0.5180 | dt: 3234.91ms | tok/sec: 162071.68 |\n",
            "| step  432 | loss: 1.6870 | lr 4.3809e-04 | norm: 0.4850 | dt: 3234.13ms | tok/sec: 162110.85 |\n",
            "| step  433 | loss: 1.6657 | lr 4.3723e-04 | norm: 0.5650 | dt: 3233.14ms | tok/sec: 162160.76 |\n",
            "| step  434 | loss: 1.6790 | lr 4.3636e-04 | norm: 0.5900 | dt: 3230.58ms | tok/sec: 162288.94 |\n",
            "| step  435 | loss: 1.6632 | lr 4.3550e-04 | norm: 0.5626 | dt: 3233.47ms | tok/sec: 162144.32 |\n",
            "| step  436 | loss: 1.6644 | lr 4.3463e-04 | norm: 0.5922 | dt: 3233.63ms | tok/sec: 162136.26 |\n",
            "| step  437 | loss: 1.6760 | lr 4.3376e-04 | norm: 0.6773 | dt: 3229.24ms | tok/sec: 162356.39 |\n",
            "| step  438 | loss: 1.6643 | lr 4.3289e-04 | norm: 0.5538 | dt: 3229.53ms | tok/sec: 162341.92 |\n",
            "| step  439 | loss: 1.6618 | lr 4.3202e-04 | norm: 0.6159 | dt: 3231.04ms | tok/sec: 162265.79 |\n",
            "| step  440 | loss: 1.6681 | lr 4.3114e-04 | norm: 0.6723 | dt: 3237.69ms | tok/sec: 161932.67 |\n",
            "| step  441 | loss: 1.6661 | lr 4.3027e-04 | norm: 0.7156 | dt: 3230.75ms | tok/sec: 162280.72 |\n",
            "| step  442 | loss: 1.6623 | lr 4.2939e-04 | norm: 0.6667 | dt: 3232.96ms | tok/sec: 162169.84 |\n",
            "| step  443 | loss: 1.6645 | lr 4.2852e-04 | norm: 0.5181 | dt: 3230.10ms | tok/sec: 162313.05 |\n",
            "| step  444 | loss: 1.6514 | lr 4.2764e-04 | norm: 0.5463 | dt: 3233.41ms | tok/sec: 162146.85 |\n",
            "| step  445 | loss: 1.6509 | lr 4.2676e-04 | norm: 0.6210 | dt: 3230.21ms | tok/sec: 162307.79 |\n",
            "| step  446 | loss: 1.6542 | lr 4.2588e-04 | norm: 0.6319 | dt: 3230.27ms | tok/sec: 162304.75 |\n",
            "| step  447 | loss: 1.6445 | lr 4.2500e-04 | norm: 0.6847 | dt: 3233.36ms | tok/sec: 162149.46 |\n",
            "| step  448 | loss: 1.6506 | lr 4.2411e-04 | norm: 0.6023 | dt: 3243.82ms | tok/sec: 161626.87 |\n",
            "| step  449 | loss: 1.6471 | lr 4.2323e-04 | norm: 0.5624 | dt: 3242.39ms | tok/sec: 161698.06 |\n",
            "| step  450 | loss: 1.6417 | lr 4.2235e-04 | norm: 0.7644 | dt: 3243.82ms | tok/sec: 161626.63 |\n",
            "| step  451 | loss: 1.6539 | lr 4.2146e-04 | norm: 0.7782 | dt: 3242.04ms | tok/sec: 161715.59 |\n",
            "| step  452 | loss: 1.6412 | lr 4.2057e-04 | norm: 0.7061 | dt: 3240.62ms | tok/sec: 161786.45 |\n",
            "| step  453 | loss: 1.6491 | lr 4.1968e-04 | norm: 0.5478 | dt: 3240.25ms | tok/sec: 161804.63 |\n",
            "| step  454 | loss: 1.6413 | lr 4.1879e-04 | norm: 0.5289 | dt: 3235.25ms | tok/sec: 162055.10 |\n",
            "| step  455 | loss: 1.6414 | lr 4.1790e-04 | norm: 0.4623 | dt: 3238.43ms | tok/sec: 161895.72 |\n",
            "| step  456 | loss: 1.6368 | lr 4.1701e-04 | norm: 0.4346 | dt: 3235.00ms | tok/sec: 162067.30 |\n",
            "| step  457 | loss: 1.6349 | lr 4.1612e-04 | norm: 0.3975 | dt: 3230.39ms | tok/sec: 162298.67 |\n",
            "| step  458 | loss: 1.6146 | lr 4.1523e-04 | norm: 0.4202 | dt: 3227.72ms | tok/sec: 162433.08 |\n",
            "| step  459 | loss: 1.6017 | lr 4.1433e-04 | norm: 0.4274 | dt: 3239.49ms | tok/sec: 161842.77 |\n",
            "| step  460 | loss: 1.5998 | lr 4.1343e-04 | norm: 0.4794 | dt: 3233.26ms | tok/sec: 162154.39 |\n",
            "| step  461 | loss: 1.6262 | lr 4.1254e-04 | norm: 0.4867 | dt: 3234.38ms | tok/sec: 162098.30 |\n",
            "| step  462 | loss: 1.6079 | lr 4.1164e-04 | norm: 0.5310 | dt: 3235.43ms | tok/sec: 162046.09 |\n",
            "| step  463 | loss: 1.6088 | lr 4.1074e-04 | norm: 0.6662 | dt: 3238.18ms | tok/sec: 161908.27 |\n",
            "| step  464 | loss: 1.6542 | lr 4.0984e-04 | norm: 0.8312 | dt: 3231.02ms | tok/sec: 162267.20 |\n",
            "| step  465 | loss: 1.6410 | lr 4.0894e-04 | norm: 0.7855 | dt: 3229.39ms | tok/sec: 162348.90 |\n",
            "| step  466 | loss: 1.6235 | lr 4.0804e-04 | norm: 0.6768 | dt: 3231.00ms | tok/sec: 162268.20 |\n",
            "| step  467 | loss: 1.5973 | lr 4.0714e-04 | norm: 0.6362 | dt: 3234.21ms | tok/sec: 162106.74 |\n",
            "| step  468 | loss: 1.6149 | lr 4.0623e-04 | norm: 0.5386 | dt: 3231.69ms | tok/sec: 162233.52 |\n",
            "| step  469 | loss: 1.6064 | lr 4.0533e-04 | norm: 0.4689 | dt: 3235.08ms | tok/sec: 162063.36 |\n",
            "| step  470 | loss: 1.6064 | lr 4.0442e-04 | norm: 0.4783 | dt: 3232.88ms | tok/sec: 162173.59 |\n",
            "| step  471 | loss: 1.6086 | lr 4.0352e-04 | norm: 0.4683 | dt: 3231.13ms | tok/sec: 162261.55 |\n",
            "| step  472 | loss: 1.5923 | lr 4.0261e-04 | norm: 0.4991 | dt: 3239.72ms | tok/sec: 161831.11 |\n",
            "| step  473 | loss: 1.5985 | lr 4.0170e-04 | norm: 0.4589 | dt: 3235.48ms | tok/sec: 162043.57 |\n",
            "| step  474 | loss: 1.5918 | lr 4.0079e-04 | norm: 0.4700 | dt: 3238.86ms | tok/sec: 161874.46 |\n",
            "| step  475 | loss: 1.5953 | lr 3.9988e-04 | norm: 0.4572 | dt: 3233.37ms | tok/sec: 162148.86 |\n",
            "| step  476 | loss: 1.5772 | lr 3.9897e-04 | norm: 0.5061 | dt: 3234.52ms | tok/sec: 162091.68 |\n",
            "| step  477 | loss: 1.6024 | lr 3.9806e-04 | norm: 0.4904 | dt: 3231.20ms | tok/sec: 162257.96 |\n",
            "| step  478 | loss: 1.5913 | lr 3.9715e-04 | norm: 0.4815 | dt: 3230.29ms | tok/sec: 162303.54 |\n",
            "| step  479 | loss: 1.6026 | lr 3.9623e-04 | norm: 0.4809 | dt: 3229.01ms | tok/sec: 162368.17 |\n",
            "| step  480 | loss: 1.5744 | lr 3.9532e-04 | norm: 0.5099 | dt: 3231.28ms | tok/sec: 162254.18 |\n",
            "| step  481 | loss: 1.5799 | lr 3.9440e-04 | norm: 0.5489 | dt: 3232.42ms | tok/sec: 162196.54 |\n",
            "| step  482 | loss: 1.5675 | lr 3.9349e-04 | norm: 0.6128 | dt: 3232.31ms | tok/sec: 162202.32 |\n",
            "| step  483 | loss: 1.6085 | lr 3.9257e-04 | norm: 0.5392 | dt: 3231.05ms | tok/sec: 162265.67 |\n",
            "| step  484 | loss: 1.5716 | lr 3.9165e-04 | norm: 0.5245 | dt: 3231.16ms | tok/sec: 162259.91 |\n",
            "| step  485 | loss: 1.5925 | lr 3.9074e-04 | norm: 0.5519 | dt: 3232.19ms | tok/sec: 162208.17 |\n",
            "| step  486 | loss: 1.5882 | lr 3.8982e-04 | norm: 0.5707 | dt: 3231.68ms | tok/sec: 162233.66 |\n",
            "| step  487 | loss: 1.5864 | lr 3.8890e-04 | norm: 0.6125 | dt: 3231.35ms | tok/sec: 162250.54 |\n",
            "| step  488 | loss: 1.5926 | lr 3.8798e-04 | norm: 0.6500 | dt: 3232.08ms | tok/sec: 162213.94 |\n",
            "| step  489 | loss: 1.5750 | lr 3.8706e-04 | norm: 0.5529 | dt: 3236.43ms | tok/sec: 161996.02 |\n",
            "| step  490 | loss: 1.5583 | lr 3.8614e-04 | norm: 0.5196 | dt: 3236.83ms | tok/sec: 161975.90 |\n",
            "| step  491 | loss: 1.5473 | lr 3.8521e-04 | norm: 0.5134 | dt: 3238.79ms | tok/sec: 161877.59 |\n",
            "| step  492 | loss: 1.5709 | lr 3.8429e-04 | norm: 0.4439 | dt: 3235.61ms | tok/sec: 162036.65 |\n",
            "| step  493 | loss: 1.5584 | lr 3.8337e-04 | norm: 0.4559 | dt: 3230.66ms | tok/sec: 162285.04 |\n",
            "| step  494 | loss: 1.5477 | lr 3.8244e-04 | norm: 0.4267 | dt: 3227.78ms | tok/sec: 162429.77 |\n",
            "| step  495 | loss: 1.5646 | lr 3.8152e-04 | norm: 0.5284 | dt: 3232.54ms | tok/sec: 162190.64 |\n",
            "| step  496 | loss: 1.5668 | lr 3.8059e-04 | norm: 0.5611 | dt: 3240.46ms | tok/sec: 161794.53 |\n",
            "| step  497 | loss: 1.5558 | lr 3.7967e-04 | norm: 0.6691 | dt: 3232.26ms | tok/sec: 162204.63 |\n",
            "| step  498 | loss: 1.5638 | lr 3.7874e-04 | norm: 0.6784 | dt: 3232.55ms | tok/sec: 162189.97 |\n",
            "| step  499 | loss: 1.5610 | lr 3.7781e-04 | norm: 0.5558 | dt: 3230.27ms | tok/sec: 162304.90 |\n",
            "| step  500 | loss: 1.5547 | lr 3.7689e-04 | norm: 0.5389 | dt: 3231.27ms | tok/sec: 162254.57 |\n",
            "| step  501 | loss: 1.5561 | lr 3.7596e-04 | norm: 0.4801 | dt: 3231.84ms | tok/sec: 162225.75 |\n",
            "| step  502 | loss: 1.5349 | lr 3.7503e-04 | norm: 0.4677 | dt: 3229.80ms | tok/sec: 162328.09 |\n",
            "| step  503 | loss: 1.5501 | lr 3.7410e-04 | norm: 0.4283 | dt: 3232.32ms | tok/sec: 162201.90 |\n",
            "| step  504 | loss: 1.5561 | lr 3.7317e-04 | norm: 0.4677 | dt: 3231.04ms | tok/sec: 162265.91 |\n",
            "| step  505 | loss: 1.5554 | lr 3.7224e-04 | norm: 0.4309 | dt: 3233.37ms | tok/sec: 162149.11 |\n",
            "| step  506 | loss: 1.5444 | lr 3.7131e-04 | norm: 0.4321 | dt: 3232.42ms | tok/sec: 162196.53 |\n",
            "| step  507 | loss: 1.5355 | lr 3.7037e-04 | norm: 0.4861 | dt: 3234.40ms | tok/sec: 162097.57 |\n",
            "| step  508 | loss: 1.5420 | lr 3.6944e-04 | norm: 0.4252 | dt: 3239.65ms | tok/sec: 161834.91 |\n",
            "| step  509 | loss: 1.5352 | lr 3.6851e-04 | norm: 0.4108 | dt: 3237.97ms | tok/sec: 161918.61 |\n",
            "| step  510 | loss: 1.5323 | lr 3.6758e-04 | norm: 0.4204 | dt: 3230.72ms | tok/sec: 162281.91 |\n",
            "| step  511 | loss: 1.5253 | lr 3.6664e-04 | norm: 0.4044 | dt: 3236.02ms | tok/sec: 162016.14 |\n",
            "| step  512 | loss: 1.5325 | lr 3.6571e-04 | norm: 0.4186 | dt: 3243.26ms | tok/sec: 161654.59 |\n",
            "| step  513 | loss: 1.5437 | lr 3.6477e-04 | norm: 0.4549 | dt: 3238.58ms | tok/sec: 161888.19 |\n",
            "| step  514 | loss: 1.5299 | lr 3.6384e-04 | norm: 0.4785 | dt: 3236.37ms | tok/sec: 161998.81 |\n",
            "| step  515 | loss: 1.5254 | lr 3.6290e-04 | norm: 0.5127 | dt: 3229.87ms | tok/sec: 162324.59 |\n",
            "| step  516 | loss: 1.5294 | lr 3.6197e-04 | norm: 0.5581 | dt: 3235.66ms | tok/sec: 162034.14 |\n",
            "| step  517 | loss: 1.5329 | lr 3.6103e-04 | norm: 0.7249 | dt: 3232.21ms | tok/sec: 162207.18 |\n",
            "| step  518 | loss: 1.5362 | lr 3.6010e-04 | norm: 0.6442 | dt: 3234.80ms | tok/sec: 162077.64 |\n",
            "| step  519 | loss: 1.5304 | lr 3.5916e-04 | norm: 0.5294 | dt: 3236.15ms | tok/sec: 162009.70 |\n",
            "| step  520 | loss: 1.5180 | lr 3.5822e-04 | norm: 0.5215 | dt: 3233.08ms | tok/sec: 162163.85 |\n",
            "| step  521 | loss: 1.5403 | lr 3.5729e-04 | norm: 0.5368 | dt: 3232.45ms | tok/sec: 162195.26 |\n",
            "| step  522 | loss: 1.5345 | lr 3.5635e-04 | norm: 0.5447 | dt: 3234.22ms | tok/sec: 162106.60 |\n",
            "| step  523 | loss: 1.5270 | lr 3.5541e-04 | norm: 0.5344 | dt: 3233.53ms | tok/sec: 162140.94 |\n",
            "| step  524 | loss: 1.5314 | lr 3.5447e-04 | norm: 0.5082 | dt: 3233.30ms | tok/sec: 162152.47 |\n",
            "| step  525 | loss: 1.5118 | lr 3.5353e-04 | norm: 0.5007 | dt: 3234.08ms | tok/sec: 162113.38 |\n",
            "| step  526 | loss: 1.5218 | lr 3.5259e-04 | norm: 0.4713 | dt: 3240.33ms | tok/sec: 161800.81 |\n",
            "| step  527 | loss: 1.5096 | lr 3.5165e-04 | norm: 0.4939 | dt: 3239.66ms | tok/sec: 161834.22 |\n",
            "| step  528 | loss: 1.5234 | lr 3.5071e-04 | norm: 0.4243 | dt: 3235.87ms | tok/sec: 162023.57 |\n",
            "| step  529 | loss: 1.5251 | lr 3.4977e-04 | norm: 0.4922 | dt: 3233.56ms | tok/sec: 162139.75 |\n",
            "| step  530 | loss: 1.5246 | lr 3.4883e-04 | norm: 0.4570 | dt: 3239.91ms | tok/sec: 161822.00 |\n",
            "| step  531 | loss: 1.5105 | lr 3.4789e-04 | norm: 0.4220 | dt: 3237.24ms | tok/sec: 161955.05 |\n",
            "| step  532 | loss: 1.5184 | lr 3.4695e-04 | norm: 0.4522 | dt: 3235.49ms | tok/sec: 162042.72 |\n",
            "| step  533 | loss: 1.5187 | lr 3.4601e-04 | norm: 0.4988 | dt: 3237.51ms | tok/sec: 161941.98 |\n",
            "| step  534 | loss: 1.5015 | lr 3.4507e-04 | norm: 0.4810 | dt: 3226.90ms | tok/sec: 162474.28 |\n",
            "| step  535 | loss: 1.5036 | lr 3.4413e-04 | norm: 0.4613 | dt: 3232.06ms | tok/sec: 162214.81 |\n",
            "| step  536 | loss: 1.4929 | lr 3.4319e-04 | norm: 0.3927 | dt: 3235.59ms | tok/sec: 162037.76 |\n",
            "| step  537 | loss: 1.4992 | lr 3.4225e-04 | norm: 0.4161 | dt: 3236.71ms | tok/sec: 161981.77 |\n",
            "| step  538 | loss: 1.4985 | lr 3.4131e-04 | norm: 0.3967 | dt: 3235.80ms | tok/sec: 162027.23 |\n",
            "| step  539 | loss: 1.4972 | lr 3.4036e-04 | norm: 0.4211 | dt: 3231.48ms | tok/sec: 162244.05 |\n",
            "| step  540 | loss: 1.5066 | lr 3.3942e-04 | norm: 0.4585 | dt: 3232.21ms | tok/sec: 162207.41 |\n",
            "| step  541 | loss: 1.4868 | lr 3.3848e-04 | norm: 0.5070 | dt: 3231.95ms | tok/sec: 162220.21 |\n",
            "| step  542 | loss: 1.5147 | lr 3.3754e-04 | norm: 0.5699 | dt: 3231.46ms | tok/sec: 162244.77 |\n",
            "| step  543 | loss: 1.4975 | lr 3.3660e-04 | norm: 0.5933 | dt: 3230.72ms | tok/sec: 162282.22 |\n",
            "| step  544 | loss: 1.4961 | lr 3.3565e-04 | norm: 0.6049 | dt: 3231.58ms | tok/sec: 162238.69 |\n",
            "| step  545 | loss: 1.4975 | lr 3.3471e-04 | norm: 0.5825 | dt: 3247.44ms | tok/sec: 161446.79 |\n",
            "| step  546 | loss: 1.4885 | lr 3.3377e-04 | norm: 0.5343 | dt: 3235.60ms | tok/sec: 162037.19 |\n",
            "| step  547 | loss: 1.4949 | lr 3.3283e-04 | norm: 0.5829 | dt: 3230.17ms | tok/sec: 162309.75 |\n",
            "| step  548 | loss: 1.4847 | lr 3.3188e-04 | norm: 0.5916 | dt: 3231.60ms | tok/sec: 162238.13 |\n",
            "| step  549 | loss: 1.4980 | lr 3.3094e-04 | norm: 0.5065 | dt: 3231.06ms | tok/sec: 162264.99 |\n",
            "| step  550 | loss: 1.4823 | lr 3.3000e-04 | norm: 0.4499 | dt: 3235.40ms | tok/sec: 162047.23 |\n",
            "| step  551 | loss: 1.4813 | lr 3.2906e-04 | norm: 0.4645 | dt: 3233.02ms | tok/sec: 162166.85 |\n",
            "| step  552 | loss: 1.4780 | lr 3.2812e-04 | norm: 0.4881 | dt: 3235.07ms | tok/sec: 162063.73 |\n",
            "| step  553 | loss: 1.4822 | lr 3.2717e-04 | norm: 0.4177 | dt: 3234.75ms | tok/sec: 162080.06 |\n",
            "| step  554 | loss: 1.4909 | lr 3.2623e-04 | norm: 0.4082 | dt: 3233.14ms | tok/sec: 162160.85 |\n",
            "| step  555 | loss: 1.4717 | lr 3.2529e-04 | norm: 0.4087 | dt: 3230.25ms | tok/sec: 162305.87 |\n",
            "| step  556 | loss: 1.4892 | lr 3.2435e-04 | norm: 0.3948 | dt: 3233.02ms | tok/sec: 162166.78 |\n",
            "| step  557 | loss: 1.4822 | lr 3.2340e-04 | norm: 0.4097 | dt: 3232.80ms | tok/sec: 162177.51 |\n",
            "| step  558 | loss: 1.4785 | lr 3.2246e-04 | norm: 0.3961 | dt: 3235.61ms | tok/sec: 162036.86 |\n",
            "| step  559 | loss: 1.4590 | lr 3.2152e-04 | norm: 0.3925 | dt: 3232.44ms | tok/sec: 162195.50 |\n",
            "| step  560 | loss: 1.4539 | lr 3.2058e-04 | norm: 0.4138 | dt: 3238.75ms | tok/sec: 161879.92 |\n",
            "| step  561 | loss: 1.4567 | lr 3.1964e-04 | norm: 0.4837 | dt: 3234.41ms | tok/sec: 162096.90 |\n",
            "| step  562 | loss: 1.4700 | lr 3.1869e-04 | norm: 0.4604 | dt: 3234.11ms | tok/sec: 162112.07 |\n",
            "| step  563 | loss: 1.4623 | lr 3.1775e-04 | norm: 0.5048 | dt: 3233.91ms | tok/sec: 162122.03 |\n",
            "| step  564 | loss: 1.4655 | lr 3.1681e-04 | norm: 0.4628 | dt: 3233.38ms | tok/sec: 162148.56 |\n",
            "| step  565 | loss: 1.5018 | lr 3.1587e-04 | norm: 0.4537 | dt: 3237.63ms | tok/sec: 161935.55 |\n",
            "| step  566 | loss: 1.4676 | lr 3.1493e-04 | norm: 0.4744 | dt: 3234.24ms | tok/sec: 162105.29 |\n",
            "| step  567 | loss: 1.4666 | lr 3.1399e-04 | norm: 0.5270 | dt: 3235.45ms | tok/sec: 162044.73 |\n",
            "| step  568 | loss: 1.4454 | lr 3.1305e-04 | norm: 0.5432 | dt: 3231.74ms | tok/sec: 162230.66 |\n",
            "| step  569 | loss: 1.4549 | lr 3.1211e-04 | norm: 0.4503 | dt: 3231.12ms | tok/sec: 162261.94 |\n",
            "| step  570 | loss: 1.4694 | lr 3.1117e-04 | norm: 0.4285 | dt: 3230.95ms | tok/sec: 162270.56 |\n",
            "| step  571 | loss: 1.4526 | lr 3.1023e-04 | norm: 0.3969 | dt: 3233.82ms | tok/sec: 162126.62 |\n",
            "| step  572 | loss: 1.4646 | lr 3.0929e-04 | norm: 0.4796 | dt: 3235.57ms | tok/sec: 162038.98 |\n",
            "| step  573 | loss: 1.4489 | lr 3.0835e-04 | norm: 0.4684 | dt: 3236.48ms | tok/sec: 161993.26 |\n",
            "| step  574 | loss: 1.4610 | lr 3.0741e-04 | norm: 0.5333 | dt: 3230.51ms | tok/sec: 162292.87 |\n",
            "| step  575 | loss: 1.4526 | lr 3.0647e-04 | norm: 0.5973 | dt: 3225.93ms | tok/sec: 162522.97 |\n",
            "| step  576 | loss: 1.4425 | lr 3.0553e-04 | norm: 0.4857 | dt: 3227.41ms | tok/sec: 162448.51 |\n",
            "| step  577 | loss: 1.4497 | lr 3.0459e-04 | norm: 0.4680 | dt: 3228.28ms | tok/sec: 162404.92 |\n",
            "| step  578 | loss: 1.4547 | lr 3.0365e-04 | norm: 0.4885 | dt: 3228.32ms | tok/sec: 162402.67 |\n",
            "| step  579 | loss: 1.4628 | lr 3.0271e-04 | norm: 0.4893 | dt: 3228.38ms | tok/sec: 162399.95 |\n",
            "| step  580 | loss: 1.4534 | lr 3.0178e-04 | norm: 0.4433 | dt: 3234.04ms | tok/sec: 162115.53 |\n",
            "| step  581 | loss: 1.4385 | lr 3.0084e-04 | norm: 0.4381 | dt: 3236.29ms | tok/sec: 162002.66 |\n",
            "| step  582 | loss: 1.4321 | lr 2.9990e-04 | norm: 0.4663 | dt: 3234.80ms | tok/sec: 162077.51 |\n",
            "| step  583 | loss: 1.4419 | lr 2.9897e-04 | norm: 0.4505 | dt: 3246.66ms | tok/sec: 161485.10 |\n",
            "| step  584 | loss: 1.4524 | lr 2.9803e-04 | norm: 0.4164 | dt: 3235.75ms | tok/sec: 162029.72 |\n",
            "| step  585 | loss: 1.4402 | lr 2.9710e-04 | norm: 0.4343 | dt: 3230.57ms | tok/sec: 162289.38 |\n",
            "| step  586 | loss: 1.4432 | lr 2.9616e-04 | norm: 0.4224 | dt: 3231.52ms | tok/sec: 162241.69 |\n",
            "| step  587 | loss: 1.4457 | lr 2.9523e-04 | norm: 0.4278 | dt: 3231.36ms | tok/sec: 162249.92 |\n",
            "| step  588 | loss: 1.4567 | lr 2.9429e-04 | norm: 0.4291 | dt: 3232.56ms | tok/sec: 162189.70 |\n",
            "| step  589 | loss: 1.4402 | lr 2.9336e-04 | norm: 0.4052 | dt: 3231.20ms | tok/sec: 162257.91 |\n",
            "| step  590 | loss: 1.4401 | lr 2.9242e-04 | norm: 0.4060 | dt: 3229.96ms | tok/sec: 162320.52 |\n",
            "| step  591 | loss: 1.4196 | lr 2.9149e-04 | norm: 0.4641 | dt: 3233.67ms | tok/sec: 162133.97 |\n",
            "| step  592 | loss: 1.4234 | lr 2.9056e-04 | norm: 0.5417 | dt: 3231.72ms | tok/sec: 162232.06 |\n",
            "| step  593 | loss: 1.4353 | lr 2.8963e-04 | norm: 0.5428 | dt: 3231.65ms | tok/sec: 162235.17 |\n",
            "| step  594 | loss: 1.4263 | lr 2.8869e-04 | norm: 0.4262 | dt: 3235.95ms | tok/sec: 162020.02 |\n",
            "| step  595 | loss: 1.4180 | lr 2.8776e-04 | norm: 0.4026 | dt: 3241.34ms | tok/sec: 161750.15 |\n",
            "| step  596 | loss: 1.4315 | lr 2.8683e-04 | norm: 0.4533 | dt: 3233.69ms | tok/sec: 162132.96 |\n",
            "| step  597 | loss: 1.4297 | lr 2.8590e-04 | norm: 0.4386 | dt: 3231.46ms | tok/sec: 162244.76 |\n",
            "| step  598 | loss: 1.4263 | lr 2.8497e-04 | norm: 0.4877 | dt: 3237.00ms | tok/sec: 161967.48 |\n",
            "| step  599 | loss: 1.4248 | lr 2.8404e-04 | norm: 0.4574 | dt: 3244.52ms | tok/sec: 161591.82 |\n",
            "| step  600 | loss: 1.4176 | lr 2.8311e-04 | norm: 0.4016 | dt: 3234.39ms | tok/sec: 162098.08 |\n",
            "| step  601 | loss: 1.4267 | lr 2.8219e-04 | norm: 0.4181 | dt: 3235.98ms | tok/sec: 162018.40 |\n",
            "| step  602 | loss: 1.4171 | lr 2.8126e-04 | norm: 0.4178 | dt: 3239.87ms | tok/sec: 161824.02 |\n",
            "| step  603 | loss: 1.4079 | lr 2.8033e-04 | norm: 0.3884 | dt: 3234.43ms | tok/sec: 162095.73 |\n",
            "| step  604 | loss: 1.4292 | lr 2.7941e-04 | norm: 0.3876 | dt: 3233.64ms | tok/sec: 162135.33 |\n",
            "| step  605 | loss: 1.4159 | lr 2.7848e-04 | norm: 0.3816 | dt: 3237.09ms | tok/sec: 161962.77 |\n",
            "| step  606 | loss: 1.4341 | lr 2.7756e-04 | norm: 0.3861 | dt: 3234.92ms | tok/sec: 162071.41 |\n",
            "| step  607 | loss: 1.4057 | lr 2.7663e-04 | norm: 0.3763 | dt: 3232.52ms | tok/sec: 162191.83 |\n",
            "| step  608 | loss: 1.4099 | lr 2.7571e-04 | norm: 0.3909 | dt: 3231.41ms | tok/sec: 162247.51 |\n",
            "| step  609 | loss: 1.4177 | lr 2.7479e-04 | norm: 0.4130 | dt: 3233.59ms | tok/sec: 162138.04 |\n",
            "| step  610 | loss: 1.4088 | lr 2.7386e-04 | norm: 0.4479 | dt: 3232.93ms | tok/sec: 162171.22 |\n",
            "| step  611 | loss: 1.4068 | lr 2.7294e-04 | norm: 0.4322 | dt: 3229.75ms | tok/sec: 162330.65 |\n",
            "| step  612 | loss: 1.4104 | lr 2.7202e-04 | norm: 0.4647 | dt: 3231.38ms | tok/sec: 162249.09 |\n",
            "| step  613 | loss: 1.4204 | lr 2.7110e-04 | norm: 0.4606 | dt: 3232.20ms | tok/sec: 162207.69 |\n",
            "| step  614 | loss: 1.4211 | lr 2.7018e-04 | norm: 0.4556 | dt: 3233.95ms | tok/sec: 162120.05 |\n",
            "| step  615 | loss: 1.4065 | lr 2.6926e-04 | norm: 0.4926 | dt: 3232.50ms | tok/sec: 162192.57 |\n",
            "| step  616 | loss: 1.4069 | lr 2.6835e-04 | norm: 0.5241 | dt: 3229.62ms | tok/sec: 162337.37 |\n",
            "| step  617 | loss: 1.3995 | lr 2.6743e-04 | norm: 0.4667 | dt: 3229.36ms | tok/sec: 162350.35 |\n",
            "| step  618 | loss: 1.4060 | lr 2.6651e-04 | norm: 0.3951 | dt: 3233.33ms | tok/sec: 162151.01 |\n",
            "| step  619 | loss: 1.4040 | lr 2.6560e-04 | norm: 0.4038 | dt: 3234.30ms | tok/sec: 162102.23 |\n",
            "| step  620 | loss: 1.4061 | lr 2.6468e-04 | norm: 0.4260 | dt: 3240.22ms | tok/sec: 161806.25 |\n",
            "| step  621 | loss: 1.3982 | lr 2.6377e-04 | norm: 0.4043 | dt: 3236.85ms | tok/sec: 161974.71 |\n",
            "| step  622 | loss: 1.4135 | lr 2.6285e-04 | norm: 0.4188 | dt: 3233.69ms | tok/sec: 162133.03 |\n",
            "| step  623 | loss: 1.4099 | lr 2.6194e-04 | norm: 0.4149 | dt: 3230.59ms | tok/sec: 162288.74 |\n",
            "| step  624 | loss: 1.4097 | lr 2.6103e-04 | norm: 0.3882 | dt: 3234.20ms | tok/sec: 162107.67 |\n",
            "| step  625 | loss: 1.3938 | lr 2.6012e-04 | norm: 0.3478 | dt: 3237.97ms | tok/sec: 161918.54 |\n",
            "| step  626 | loss: 1.3913 | lr 2.5921e-04 | norm: 0.3585 | dt: 3234.68ms | tok/sec: 162083.26 |\n",
            "| step  627 | loss: 1.4044 | lr 2.5830e-04 | norm: 0.3691 | dt: 3230.32ms | tok/sec: 162301.95 |\n",
            "| step  628 | loss: 1.3991 | lr 2.5739e-04 | norm: 0.3889 | dt: 3232.58ms | tok/sec: 162188.83 |\n",
            "| step  629 | loss: 1.4004 | lr 2.5648e-04 | norm: 0.3993 | dt: 3231.92ms | tok/sec: 162222.06 |\n",
            "| step  630 | loss: 1.4058 | lr 2.5558e-04 | norm: 0.3972 | dt: 3228.89ms | tok/sec: 162373.90 |\n",
            "| step  631 | loss: 1.4007 | lr 2.5467e-04 | norm: 0.4157 | dt: 3230.28ms | tok/sec: 162304.01 |\n",
            "| step  632 | loss: 1.3978 | lr 2.5377e-04 | norm: 0.3684 | dt: 3231.83ms | tok/sec: 162226.31 |\n",
            "| step  633 | loss: 1.4087 | lr 2.5286e-04 | norm: 0.4046 | dt: 3234.77ms | tok/sec: 162079.00 |\n",
            "| step  634 | loss: 1.3877 | lr 2.5196e-04 | norm: 0.4079 | dt: 3239.77ms | tok/sec: 161828.57 |\n",
            "| step  635 | loss: 1.3997 | lr 2.5106e-04 | norm: 0.4038 | dt: 3231.40ms | tok/sec: 162247.97 |\n",
            "| step  636 | loss: 1.3813 | lr 2.5016e-04 | norm: 0.4540 | dt: 3234.46ms | tok/sec: 162094.56 |\n",
            "| step  637 | loss: 1.3864 | lr 2.4926e-04 | norm: 0.4863 | dt: 3243.13ms | tok/sec: 161660.86 |\n",
            "| step  638 | loss: 1.3906 | lr 2.4836e-04 | norm: 0.5494 | dt: 3238.87ms | tok/sec: 161873.76 |\n",
            "| step  639 | loss: 1.3976 | lr 2.4746e-04 | norm: 0.4955 | dt: 3245.40ms | tok/sec: 161548.27 |\n",
            "| step  640 | loss: 1.3852 | lr 2.4657e-04 | norm: 0.4751 | dt: 3235.87ms | tok/sec: 162023.86 |\n",
            "| step  641 | loss: 1.3859 | lr 2.4567e-04 | norm: 0.4521 | dt: 3237.42ms | tok/sec: 161946.20 |\n",
            "| step  642 | loss: 1.3882 | lr 2.4477e-04 | norm: 0.4758 | dt: 3236.26ms | tok/sec: 162004.39 |\n",
            "| step  643 | loss: 1.3941 | lr 2.4388e-04 | norm: 0.4475 | dt: 3231.84ms | tok/sec: 162225.62 |\n",
            "| step  644 | loss: 1.3856 | lr 2.4299e-04 | norm: 0.4187 | dt: 3238.58ms | tok/sec: 161888.45 |\n",
            "| step  645 | loss: 1.3833 | lr 2.4210e-04 | norm: 0.4264 | dt: 3232.14ms | tok/sec: 162210.61 |\n",
            "| step  646 | loss: 1.3789 | lr 2.4121e-04 | norm: 0.4102 | dt: 3232.60ms | tok/sec: 162187.68 |\n",
            "| step  647 | loss: 1.3807 | lr 2.4032e-04 | norm: 0.4180 | dt: 3232.45ms | tok/sec: 162195.01 |\n",
            "| step  648 | loss: 1.3787 | lr 2.3943e-04 | norm: 0.3748 | dt: 3234.70ms | tok/sec: 162082.23 |\n",
            "| step  649 | loss: 1.3680 | lr 2.3854e-04 | norm: 0.3860 | dt: 3232.70ms | tok/sec: 162182.85 |\n",
            "| step  650 | loss: 1.3902 | lr 2.3765e-04 | norm: 0.4151 | dt: 3231.40ms | tok/sec: 162248.09 |\n",
            "| step  651 | loss: 1.3716 | lr 2.3677e-04 | norm: 0.4027 | dt: 3231.53ms | tok/sec: 162241.60 |\n",
            "| step  652 | loss: 1.3700 | lr 2.3589e-04 | norm: 0.3803 | dt: 3232.60ms | tok/sec: 162187.53 |\n",
            "| step  653 | loss: 1.3665 | lr 2.3500e-04 | norm: 0.3800 | dt: 3230.65ms | tok/sec: 162285.77 |\n",
            "| step  654 | loss: 1.3880 | lr 2.3412e-04 | norm: 0.3768 | dt: 3231.96ms | tok/sec: 162220.07 |\n",
            "| step  655 | loss: 1.3744 | lr 2.3324e-04 | norm: 0.3895 | dt: 3230.84ms | tok/sec: 162276.31 |\n",
            "| step  656 | loss: 1.3667 | lr 2.3236e-04 | norm: 0.3914 | dt: 3234.92ms | tok/sec: 162071.29 |\n",
            "| step  657 | loss: 1.3863 | lr 2.3148e-04 | norm: 0.3904 | dt: 3232.06ms | tok/sec: 162214.94 |\n",
            "| step  658 | loss: 1.3727 | lr 2.3061e-04 | norm: 0.3699 | dt: 3240.57ms | tok/sec: 161788.98 |\n",
            "| step  659 | loss: 1.3646 | lr 2.2973e-04 | norm: 0.3813 | dt: 3233.80ms | tok/sec: 162127.69 |\n",
            "| step  660 | loss: 1.3546 | lr 2.2886e-04 | norm: 0.4402 | dt: 3228.52ms | tok/sec: 162392.81 |\n",
            "| step  661 | loss: 1.3603 | lr 2.2798e-04 | norm: 0.4567 | dt: 3229.01ms | tok/sec: 162368.17 |\n",
            "| step  662 | loss: 1.3546 | lr 2.2711e-04 | norm: 0.4009 | dt: 3233.80ms | tok/sec: 162127.42 |\n",
            "| step  663 | loss: 1.3622 | lr 2.2624e-04 | norm: 0.4680 | dt: 3232.78ms | tok/sec: 162178.65 |\n",
            "| step  664 | loss: 1.3573 | lr 2.2537e-04 | norm: 0.4458 | dt: 3229.92ms | tok/sec: 162322.32 |\n",
            "| step  665 | loss: 1.3722 | lr 2.2450e-04 | norm: 0.4233 | dt: 3237.88ms | tok/sec: 161923.15 |\n",
            "| step  666 | loss: 1.3870 | lr 2.2364e-04 | norm: 0.4141 | dt: 3234.11ms | tok/sec: 162112.16 |\n",
            "| step  667 | loss: 1.3723 | lr 2.2277e-04 | norm: 0.4064 | dt: 3233.31ms | tok/sec: 162152.23 |\n",
            "| step  668 | loss: 1.3407 | lr 2.2191e-04 | norm: 0.4210 | dt: 3229.45ms | tok/sec: 162346.15 |\n",
            "| step  669 | loss: 1.3538 | lr 2.2104e-04 | norm: 0.4019 | dt: 3233.58ms | tok/sec: 162138.52 |\n",
            "| step  670 | loss: 1.3587 | lr 2.2018e-04 | norm: 0.4198 | dt: 3232.16ms | tok/sec: 162209.76 |\n",
            "| step  671 | loss: 1.3573 | lr 2.1932e-04 | norm: 0.3964 | dt: 3232.61ms | tok/sec: 162187.39 |\n",
            "| step  672 | loss: 1.3672 | lr 2.1846e-04 | norm: 0.3585 | dt: 3232.55ms | tok/sec: 162190.20 |\n",
            "| step  673 | loss: 1.3562 | lr 2.1760e-04 | norm: 0.3849 | dt: 3234.93ms | tok/sec: 162071.08 |\n",
            "| step  674 | loss: 1.3494 | lr 2.1675e-04 | norm: 0.4126 | dt: 3234.74ms | tok/sec: 162080.63 |\n",
            "| step  675 | loss: 1.3524 | lr 2.1589e-04 | norm: 0.4055 | dt: 3229.86ms | tok/sec: 162325.41 |\n",
            "| step  676 | loss: 1.3502 | lr 2.1504e-04 | norm: 0.4052 | dt: 3231.68ms | tok/sec: 162233.65 |\n",
            "| step  677 | loss: 1.3460 | lr 2.1419e-04 | norm: 0.4208 | dt: 3232.82ms | tok/sec: 162176.64 |\n",
            "| step  678 | loss: 1.3527 | lr 2.1334e-04 | norm: 0.3982 | dt: 3237.16ms | tok/sec: 161959.11 |\n",
            "| step  679 | loss: 1.3587 | lr 2.1249e-04 | norm: 0.4542 | dt: 3228.89ms | tok/sec: 162373.96 |\n",
            "| step  680 | loss: 1.3659 | lr 2.1164e-04 | norm: 0.4645 | dt: 3227.79ms | tok/sec: 162429.64 |\n",
            "| step  681 | loss: 1.3412 | lr 2.1079e-04 | norm: 0.4366 | dt: 3229.68ms | tok/sec: 162334.19 |\n",
            "| step  682 | loss: 1.3427 | lr 2.0995e-04 | norm: 0.4153 | dt: 3234.57ms | tok/sec: 162088.94 |\n",
            "| step  683 | loss: 1.3375 | lr 2.0911e-04 | norm: 0.4223 | dt: 3228.00ms | tok/sec: 162418.62 |\n",
            "| step  684 | loss: 1.3519 | lr 2.0826e-04 | norm: 0.3859 | dt: 3233.29ms | tok/sec: 162152.95 |\n",
            "| step  685 | loss: 1.3495 | lr 2.0742e-04 | norm: 0.3806 | dt: 3235.70ms | tok/sec: 162032.13 |\n",
            "| step  686 | loss: 1.3467 | lr 2.0658e-04 | norm: 0.3750 | dt: 3230.88ms | tok/sec: 162274.29 |\n",
            "| step  687 | loss: 1.3526 | lr 2.0575e-04 | norm: 0.3689 | dt: 3233.40ms | tok/sec: 162147.56 |\n",
            "| step  688 | loss: 1.3483 | lr 2.0491e-04 | norm: 0.3769 | dt: 3230.55ms | tok/sec: 162290.76 |\n",
            "| step  689 | loss: 1.3537 | lr 2.0408e-04 | norm: 0.3736 | dt: 3228.41ms | tok/sec: 162398.27 |\n",
            "| step  690 | loss: 1.3498 | lr 2.0324e-04 | norm: 0.3845 | dt: 3234.31ms | tok/sec: 162102.05 |\n",
            "| step  691 | loss: 1.3386 | lr 2.0241e-04 | norm: 0.3950 | dt: 3232.07ms | tok/sec: 162214.14 |\n",
            "| step  692 | loss: 1.3270 | lr 2.0158e-04 | norm: 0.3876 | dt: 3236.33ms | tok/sec: 162000.93 |\n",
            "| step  693 | loss: 1.3261 | lr 2.0075e-04 | norm: 0.3758 | dt: 3236.25ms | tok/sec: 162004.97 |\n",
            "| step  694 | loss: 1.3434 | lr 1.9993e-04 | norm: 0.3645 | dt: 3231.24ms | tok/sec: 162256.01 |\n",
            "| step  695 | loss: 1.3341 | lr 1.9910e-04 | norm: 0.3967 | dt: 3237.50ms | tok/sec: 161942.18 |\n",
            "| step  696 | loss: 1.3293 | lr 1.9828e-04 | norm: 0.3854 | dt: 3230.71ms | tok/sec: 162282.57 |\n",
            "| step  697 | loss: 1.3370 | lr 1.9746e-04 | norm: 0.3834 | dt: 3241.34ms | tok/sec: 161750.53 |\n",
            "| step  698 | loss: 1.3265 | lr 1.9664e-04 | norm: 0.3723 | dt: 3235.00ms | tok/sec: 162067.26 |\n",
            "| step  699 | loss: 1.3330 | lr 1.9582e-04 | norm: 0.3749 | dt: 3241.40ms | tok/sec: 161747.48 |\n",
            "| step  700 | loss: 1.3311 | lr 1.9500e-04 | norm: 0.4119 | dt: 3242.54ms | tok/sec: 161690.34 |\n",
            "| step  701 | loss: 1.3298 | lr 1.9418e-04 | norm: 0.3808 | dt: 3242.45ms | tok/sec: 161695.00 |\n",
            "| step  702 | loss: 1.3344 | lr 1.9337e-04 | norm: 0.3641 | dt: 3237.87ms | tok/sec: 161923.63 |\n",
            "| step  703 | loss: 1.3165 | lr 1.9256e-04 | norm: 0.3808 | dt: 3238.74ms | tok/sec: 161880.16 |\n",
            "| step  704 | loss: 1.3265 | lr 1.9175e-04 | norm: 0.3986 | dt: 3239.62ms | tok/sec: 161836.15 |\n",
            "| step  705 | loss: 1.3363 | lr 1.9094e-04 | norm: 0.4048 | dt: 3239.17ms | tok/sec: 161858.53 |\n",
            "| step  706 | loss: 1.3377 | lr 1.9013e-04 | norm: 0.4193 | dt: 3233.32ms | tok/sec: 162151.60 |\n",
            "| step  707 | loss: 1.3340 | lr 1.8933e-04 | norm: 0.4109 | dt: 3232.55ms | tok/sec: 162190.25 |\n",
            "| step  708 | loss: 1.3188 | lr 1.8852e-04 | norm: 0.3964 | dt: 3232.85ms | tok/sec: 162175.32 |\n",
            "| step  709 | loss: 1.3257 | lr 1.8772e-04 | norm: 0.3715 | dt: 3234.07ms | tok/sec: 162114.18 |\n",
            "| step  710 | loss: 1.3300 | lr 1.8692e-04 | norm: 0.3776 | dt: 3231.95ms | tok/sec: 162220.42 |\n",
            "| step  711 | loss: 1.3163 | lr 1.8612e-04 | norm: 0.3749 | dt: 3231.27ms | tok/sec: 162254.38 |\n",
            "| step  712 | loss: 1.3180 | lr 1.8533e-04 | norm: 0.3750 | dt: 3232.31ms | tok/sec: 162202.09 |\n",
            "| step  713 | loss: 1.3223 | lr 1.8453e-04 | norm: 0.3680 | dt: 3233.57ms | tok/sec: 162139.03 |\n",
            "| step  714 | loss: 1.3301 | lr 1.8374e-04 | norm: 0.3924 | dt: 3233.51ms | tok/sec: 162141.83 |\n",
            "| step  715 | loss: 1.3287 | lr 1.8295e-04 | norm: 0.3871 | dt: 3228.48ms | tok/sec: 162394.77 |\n",
            "| step  716 | loss: 1.3140 | lr 1.8216e-04 | norm: 0.4035 | dt: 3236.81ms | tok/sec: 161976.51 |\n",
            "| step  717 | loss: 1.3242 | lr 1.8137e-04 | norm: 0.4215 | dt: 3237.35ms | tok/sec: 161949.67 |\n",
            "| step  718 | loss: 1.3117 | lr 1.8058e-04 | norm: 0.4053 | dt: 3231.97ms | tok/sec: 162219.19 |\n",
            "| step  719 | loss: 1.3199 | lr 1.7980e-04 | norm: 0.4441 | dt: 3234.06ms | tok/sec: 162114.43 |\n",
            "| step  720 | loss: 1.3247 | lr 1.7902e-04 | norm: 0.4216 | dt: 3233.95ms | tok/sec: 162120.23 |\n",
            "| step  721 | loss: 1.3045 | lr 1.7824e-04 | norm: 0.3648 | dt: 3233.98ms | tok/sec: 162118.58 |\n",
            "| step  722 | loss: 1.3247 | lr 1.7746e-04 | norm: 0.3827 | dt: 3240.72ms | tok/sec: 161781.15 |\n",
            "| step  723 | loss: 1.3237 | lr 1.7668e-04 | norm: 0.4255 | dt: 3234.04ms | tok/sec: 162115.27 |\n",
            "| step  724 | loss: 1.3207 | lr 1.7591e-04 | norm: 0.3882 | dt: 3234.47ms | tok/sec: 162094.14 |\n",
            "| step  725 | loss: 1.3315 | lr 1.7513e-04 | norm: 0.3804 | dt: 3233.68ms | tok/sec: 162133.45 |\n",
            "| step  726 | loss: 1.3072 | lr 1.7436e-04 | norm: 0.3956 | dt: 3234.25ms | tok/sec: 162105.21 |\n",
            "| step  727 | loss: 1.3071 | lr 1.7359e-04 | norm: 0.3770 | dt: 3231.29ms | tok/sec: 162253.65 |\n",
            "| step  728 | loss: 1.3167 | lr 1.7283e-04 | norm: 0.3717 | dt: 3231.78ms | tok/sec: 162228.98 |\n",
            "| step  729 | loss: 1.3188 | lr 1.7206e-04 | norm: 0.3788 | dt: 3231.07ms | tok/sec: 162264.58 |\n",
            "| step  730 | loss: 1.3223 | lr 1.7130e-04 | norm: 0.3700 | dt: 3228.55ms | tok/sec: 162391.14 |\n",
            "| step  731 | loss: 1.3108 | lr 1.7054e-04 | norm: 0.3419 | dt: 3234.34ms | tok/sec: 162100.34 |\n",
            "| step  732 | loss: 1.3193 | lr 1.6978e-04 | norm: 0.3555 | dt: 3237.08ms | tok/sec: 161963.40 |\n",
            "| step  733 | loss: 1.3162 | lr 1.6902e-04 | norm: 0.3892 | dt: 3231.17ms | tok/sec: 162259.27 |\n",
            "| step  734 | loss: 1.3212 | lr 1.6826e-04 | norm: 0.3652 | dt: 3232.50ms | tok/sec: 162192.56 |\n",
            "| step  735 | loss: 1.3007 | lr 1.6751e-04 | norm: 0.3747 | dt: 3231.91ms | tok/sec: 162222.35 |\n",
            "| step  736 | loss: 1.3154 | lr 1.6676e-04 | norm: 0.3630 | dt: 3230.70ms | tok/sec: 162283.14 |\n",
            "| step  737 | loss: 1.3031 | lr 1.6601e-04 | norm: 0.3640 | dt: 3235.21ms | tok/sec: 162056.76 |\n",
            "| step  738 | loss: 1.3025 | lr 1.6526e-04 | norm: 0.3811 | dt: 3236.38ms | tok/sec: 161998.18 |\n",
            "| step  739 | loss: 1.3084 | lr 1.6452e-04 | norm: 0.3746 | dt: 3233.06ms | tok/sec: 162164.74 |\n",
            "| step  740 | loss: 1.3058 | lr 1.6377e-04 | norm: 0.3955 | dt: 3233.36ms | tok/sec: 162149.68 |\n",
            "| step  741 | loss: 1.2973 | lr 1.6303e-04 | norm: 0.3906 | dt: 3229.07ms | tok/sec: 162365.14 |\n",
            "| step  742 | loss: 1.3100 | lr 1.6229e-04 | norm: 0.4314 | dt: 3230.15ms | tok/sec: 162310.65 |\n",
            "| step  743 | loss: 1.3066 | lr 1.6155e-04 | norm: 0.4091 | dt: 3235.92ms | tok/sec: 162021.17 |\n",
            "| step  744 | loss: 1.3003 | lr 1.6082e-04 | norm: 0.3740 | dt: 3231.75ms | tok/sec: 162230.44 |\n",
            "| step  745 | loss: 1.3089 | lr 1.6008e-04 | norm: 0.3810 | dt: 3229.40ms | tok/sec: 162348.22 |\n",
            "| step  746 | loss: 1.3008 | lr 1.5935e-04 | norm: 0.3923 | dt: 3236.33ms | tok/sec: 162000.90 |\n",
            "| step  747 | loss: 1.2991 | lr 1.5862e-04 | norm: 0.4251 | dt: 3240.74ms | tok/sec: 161780.20 |\n",
            "| step  748 | loss: 1.3023 | lr 1.5790e-04 | norm: 0.4239 | dt: 3235.38ms | tok/sec: 162048.11 |\n",
            "| step  749 | loss: 1.2943 | lr 1.5717e-04 | norm: 0.3766 | dt: 3233.80ms | tok/sec: 162127.51 |\n",
            "| step  750 | loss: 1.2994 | lr 1.5645e-04 | norm: 0.3941 | dt: 3239.97ms | tok/sec: 161818.65 |\n",
            "| step  751 | loss: 1.3019 | lr 1.5573e-04 | norm: 0.4314 | dt: 3238.03ms | tok/sec: 161915.53 |\n",
            "| step  752 | loss: 1.2908 | lr 1.5501e-04 | norm: 0.3828 | dt: 3234.64ms | tok/sec: 162085.54 |\n",
            "| step  753 | loss: 1.2999 | lr 1.5429e-04 | norm: 0.3873 | dt: 3235.66ms | tok/sec: 162034.41 |\n",
            "| step  754 | loss: 1.2908 | lr 1.5358e-04 | norm: 0.4248 | dt: 3254.34ms | tok/sec: 161104.22 |\n",
            "| step  755 | loss: 1.3060 | lr 1.5286e-04 | norm: 0.4104 | dt: 3235.50ms | tok/sec: 162042.59 |\n",
            "| step  756 | loss: 1.2958 | lr 1.5215e-04 | norm: 0.3939 | dt: 3237.83ms | tok/sec: 161925.58 |\n",
            "| step  757 | loss: 1.2990 | lr 1.5145e-04 | norm: 0.4101 | dt: 3241.45ms | tok/sec: 161745.13 |\n",
            "| step  758 | loss: 1.3034 | lr 1.5074e-04 | norm: 0.4074 | dt: 3236.61ms | tok/sec: 161986.76 |\n",
            "| step  759 | loss: 1.2984 | lr 1.5004e-04 | norm: 0.3810 | dt: 3233.46ms | tok/sec: 162144.55 |\n",
            "| step  760 | loss: 1.2853 | lr 1.4933e-04 | norm: 0.3861 | dt: 3232.60ms | tok/sec: 162187.52 |\n",
            "| step  761 | loss: 1.2761 | lr 1.4864e-04 | norm: 0.3694 | dt: 3234.58ms | tok/sec: 162088.57 |\n",
            "| step  762 | loss: 1.2755 | lr 1.4794e-04 | norm: 0.3733 | dt: 3238.78ms | tok/sec: 161878.42 |\n",
            "| step  763 | loss: 1.2914 | lr 1.4724e-04 | norm: 0.3932 | dt: 3243.00ms | tok/sec: 161667.50 |\n",
            "| step  764 | loss: 1.2783 | lr 1.4655e-04 | norm: 0.3862 | dt: 3233.90ms | tok/sec: 162122.43 |\n",
            "| step  765 | loss: 1.2809 | lr 1.4586e-04 | norm: 0.3879 | dt: 3244.97ms | tok/sec: 161569.25 |\n",
            "| step  766 | loss: 1.3117 | lr 1.4517e-04 | norm: 0.3806 | dt: 3233.21ms | tok/sec: 162157.31 |\n",
            "| step  767 | loss: 1.3013 | lr 1.4449e-04 | norm: 0.3914 | dt: 3236.63ms | tok/sec: 161985.54 |\n",
            "| step  768 | loss: 1.2813 | lr 1.4380e-04 | norm: 0.3737 | dt: 3229.53ms | tok/sec: 162342.04 |\n",
            "| step  769 | loss: 1.2675 | lr 1.4312e-04 | norm: 0.3770 | dt: 3242.34ms | tok/sec: 161700.35 |\n",
            "| step  770 | loss: 1.2832 | lr 1.4244e-04 | norm: 0.3767 | dt: 3237.34ms | tok/sec: 161949.99 |\n",
            "| step  771 | loss: 1.2799 | lr 1.4177e-04 | norm: 0.3650 | dt: 3236.66ms | tok/sec: 161984.05 |\n",
            "| step  772 | loss: 1.2878 | lr 1.4109e-04 | norm: 0.3895 | dt: 3239.04ms | tok/sec: 161865.08 |\n",
            "| step  773 | loss: 1.2886 | lr 1.4042e-04 | norm: 0.3574 | dt: 3236.69ms | tok/sec: 161982.64 |\n",
            "| step  774 | loss: 1.2744 | lr 1.3975e-04 | norm: 0.3543 | dt: 3238.19ms | tok/sec: 161907.90 |\n",
            "| step  775 | loss: 1.2788 | lr 1.3908e-04 | norm: 0.3840 | dt: 3235.65ms | tok/sec: 162035.04 |\n",
            "| step  776 | loss: 1.2769 | lr 1.3842e-04 | norm: 0.3721 | dt: 3234.79ms | tok/sec: 162077.80 |\n",
            "| step  777 | loss: 1.2772 | lr 1.3775e-04 | norm: 0.3770 | dt: 3236.04ms | tok/sec: 162015.28 |\n",
            "| step  778 | loss: 1.2689 | lr 1.3709e-04 | norm: 0.3890 | dt: 3241.22ms | tok/sec: 161756.26 |\n",
            "| step  779 | loss: 1.2873 | lr 1.3643e-04 | norm: 0.3939 | dt: 3232.64ms | tok/sec: 162185.60 |\n",
            "| step  780 | loss: 1.2823 | lr 1.3578e-04 | norm: 0.3650 | dt: 3234.99ms | tok/sec: 162067.81 |\n",
            "| step  781 | loss: 1.2921 | lr 1.3512e-04 | norm: 0.3789 | dt: 3232.51ms | tok/sec: 162192.43 |\n",
            "| step  782 | loss: 1.2677 | lr 1.3447e-04 | norm: 0.3888 | dt: 3230.55ms | tok/sec: 162290.78 |\n",
            "| step  783 | loss: 1.2648 | lr 1.3382e-04 | norm: 0.3786 | dt: 3238.45ms | tok/sec: 161894.96 |\n",
            "| step  784 | loss: 1.2607 | lr 1.3318e-04 | norm: 0.3742 | dt: 3233.97ms | tok/sec: 162119.12 |\n",
            "| step  785 | loss: 1.2889 | lr 1.3253e-04 | norm: 0.3670 | dt: 3229.14ms | tok/sec: 162361.63 |\n",
            "| step  786 | loss: 1.2667 | lr 1.3189e-04 | norm: 0.3703 | dt: 3233.24ms | tok/sec: 162155.51 |\n",
            "| step  787 | loss: 1.2825 | lr 1.3125e-04 | norm: 0.3637 | dt: 3244.13ms | tok/sec: 161611.12 |\n",
            "| step  788 | loss: 1.2755 | lr 1.3062e-04 | norm: 0.3809 | dt: 3240.85ms | tok/sec: 161775.08 |\n",
            "| step  789 | loss: 1.2820 | lr 1.2998e-04 | norm: 0.4083 | dt: 3241.21ms | tok/sec: 161756.67 |\n",
            "| step  790 | loss: 1.2819 | lr 1.2935e-04 | norm: 0.3692 | dt: 3235.08ms | tok/sec: 162063.15 |\n",
            "| step  791 | loss: 1.2720 | lr 1.2872e-04 | norm: 0.3585 | dt: 3236.92ms | tok/sec: 161971.13 |\n",
            "| step  792 | loss: 1.2636 | lr 1.2809e-04 | norm: 0.3829 | dt: 3241.01ms | tok/sec: 161766.63 |\n",
            "| step  793 | loss: 1.2537 | lr 1.2747e-04 | norm: 0.3541 | dt: 3234.58ms | tok/sec: 162088.29 |\n",
            "| step  794 | loss: 1.2682 | lr 1.2685e-04 | norm: 0.3738 | dt: 3229.56ms | tok/sec: 162340.29 |\n",
            "| step  795 | loss: 1.2665 | lr 1.2623e-04 | norm: 0.3586 | dt: 3233.16ms | tok/sec: 162159.78 |\n",
            "| step  796 | loss: 1.2599 | lr 1.2561e-04 | norm: 0.3673 | dt: 3232.37ms | tok/sec: 162199.47 |\n",
            "| step  797 | loss: 1.2690 | lr 1.2500e-04 | norm: 0.3800 | dt: 3231.27ms | tok/sec: 162254.66 |\n",
            "| step  798 | loss: 1.2617 | lr 1.2438e-04 | norm: 0.3548 | dt: 3242.33ms | tok/sec: 161700.78 |\n",
            "| step  799 | loss: 1.2560 | lr 1.2378e-04 | norm: 0.3675 | dt: 3234.32ms | tok/sec: 162101.42 |\n",
            "| step  800 | loss: 1.2594 | lr 1.2317e-04 | norm: 0.3802 | dt: 3233.22ms | tok/sec: 162156.64 |\n",
            "| step  801 | loss: 1.2684 | lr 1.2256e-04 | norm: 0.3979 | dt: 3232.24ms | tok/sec: 162205.83 |\n",
            "| step  802 | loss: 1.2615 | lr 1.2196e-04 | norm: 0.3654 | dt: 3242.77ms | tok/sec: 161679.27 |\n",
            "| step  803 | loss: 1.2644 | lr 1.2136e-04 | norm: 0.3792 | dt: 3242.90ms | tok/sec: 161672.57 |\n",
            "| step  804 | loss: 1.2451 | lr 1.2076e-04 | norm: 0.3730 | dt: 3242.47ms | tok/sec: 161693.96 |\n",
            "| step  805 | loss: 1.2611 | lr 1.2017e-04 | norm: 0.3929 | dt: 3241.09ms | tok/sec: 161762.69 |\n",
            "| step  806 | loss: 1.2678 | lr 1.1958e-04 | norm: 0.4078 | dt: 3239.23ms | tok/sec: 161855.67 |\n",
            "| step  807 | loss: 1.2659 | lr 1.1899e-04 | norm: 0.3755 | dt: 3240.63ms | tok/sec: 161785.64 |\n",
            "| step  808 | loss: 1.2633 | lr 1.1840e-04 | norm: 0.3744 | dt: 3232.24ms | tok/sec: 162205.59 |\n",
            "| step  809 | loss: 1.2523 | lr 1.1782e-04 | norm: 0.3836 | dt: 3227.35ms | tok/sec: 162451.76 |\n",
            "| step  810 | loss: 1.2613 | lr 1.1724e-04 | norm: 0.3693 | dt: 3239.67ms | tok/sec: 161833.93 |\n",
            "| step  811 | loss: 1.2579 | lr 1.1666e-04 | norm: 0.3584 | dt: 3234.64ms | tok/sec: 162085.67 |\n",
            "| step  812 | loss: 1.2496 | lr 1.1608e-04 | norm: 0.3744 | dt: 3232.92ms | tok/sec: 162171.57 |\n",
            "| step  813 | loss: 1.2522 | lr 1.1551e-04 | norm: 0.3594 | dt: 3234.31ms | tok/sec: 162101.87 |\n",
            "| step  814 | loss: 1.2585 | lr 1.1494e-04 | norm: 0.3725 | dt: 3235.71ms | tok/sec: 162031.65 |\n",
            "| step  815 | loss: 1.2658 | lr 1.1437e-04 | norm: 0.3523 | dt: 3229.66ms | tok/sec: 162335.54 |\n",
            "| step  816 | loss: 1.2582 | lr 1.1380e-04 | norm: 0.3687 | dt: 3231.56ms | tok/sec: 162239.75 |\n",
            "| step  817 | loss: 1.2450 | lr 1.1324e-04 | norm: 0.3580 | dt: 3233.28ms | tok/sec: 162153.61 |\n",
            "| step  818 | loss: 1.2528 | lr 1.1268e-04 | norm: 0.3640 | dt: 3234.01ms | tok/sec: 162117.14 |\n",
            "| step  819 | loss: 1.2479 | lr 1.1212e-04 | norm: 0.3666 | dt: 3232.38ms | tok/sec: 162198.78 |\n",
            "| step  820 | loss: 1.2582 | lr 1.1157e-04 | norm: 0.3776 | dt: 3231.77ms | tok/sec: 162229.59 |\n",
            "| step  821 | loss: 1.2491 | lr 1.1101e-04 | norm: 0.3669 | dt: 3234.00ms | tok/sec: 162117.41 |\n",
            "| step  822 | loss: 1.2475 | lr 1.1046e-04 | norm: 0.3865 | dt: 3232.58ms | tok/sec: 162188.77 |\n",
            "| step  823 | loss: 1.2593 | lr 1.0992e-04 | norm: 0.3582 | dt: 3232.60ms | tok/sec: 162187.58 |\n",
            "| step  824 | loss: 1.2577 | lr 1.0937e-04 | norm: 0.3689 | dt: 3236.71ms | tok/sec: 161981.83 |\n",
            "| step  825 | loss: 1.2536 | lr 1.0883e-04 | norm: 0.3677 | dt: 3233.14ms | tok/sec: 162160.41 |\n",
            "| step  826 | loss: 1.2627 | lr 1.0829e-04 | norm: 0.3702 | dt: 3236.77ms | tok/sec: 161978.55 |\n",
            "| step  827 | loss: 1.2378 | lr 1.0775e-04 | norm: 0.3626 | dt: 3231.96ms | tok/sec: 162219.69 |\n",
            "| step  828 | loss: 1.2532 | lr 1.0722e-04 | norm: 0.3564 | dt: 3232.97ms | tok/sec: 162169.31 |\n",
            "| step  829 | loss: 1.2435 | lr 1.0669e-04 | norm: 0.3655 | dt: 3244.61ms | tok/sec: 161587.44 |\n",
            "| step  830 | loss: 1.2537 | lr 1.0616e-04 | norm: 0.3784 | dt: 3259.60ms | tok/sec: 160844.11 |\n",
            "| step  831 | loss: 1.2590 | lr 1.0563e-04 | norm: 0.3701 | dt: 3229.83ms | tok/sec: 162326.71 |\n",
            "| step  832 | loss: 1.2595 | lr 1.0511e-04 | norm: 0.3815 | dt: 3231.40ms | tok/sec: 162248.03 |\n",
            "| step  833 | loss: 1.2555 | lr 1.0459e-04 | norm: 0.3906 | dt: 3232.17ms | tok/sec: 162209.12 |\n",
            "| step  834 | loss: 1.2494 | lr 1.0407e-04 | norm: 0.3508 | dt: 3230.78ms | tok/sec: 162278.95 |\n",
            "| step  835 | loss: 1.2530 | lr 1.0356e-04 | norm: 0.3808 | dt: 3232.92ms | tok/sec: 162171.47 |\n",
            "| step  836 | loss: 1.2394 | lr 1.0305e-04 | norm: 0.3846 | dt: 3233.27ms | tok/sec: 162153.89 |\n",
            "| step  837 | loss: 1.2466 | lr 1.0254e-04 | norm: 0.3753 | dt: 3237.76ms | tok/sec: 161929.05 |\n",
            "| step  838 | loss: 1.2414 | lr 1.0203e-04 | norm: 0.3788 | dt: 3233.27ms | tok/sec: 162153.94 |\n",
            "| step  839 | loss: 1.2432 | lr 1.0153e-04 | norm: 0.4110 | dt: 3231.32ms | tok/sec: 162252.16 |\n",
            "| step  840 | loss: 1.2487 | lr 1.0103e-04 | norm: 0.3847 | dt: 3229.25ms | tok/sec: 162355.82 |\n",
            "| step  841 | loss: 1.2375 | lr 1.0053e-04 | norm: 0.3607 | dt: 3231.88ms | tok/sec: 162223.61 |\n",
            "| step  842 | loss: 1.2535 | lr 1.0003e-04 | norm: 0.3897 | dt: 3233.17ms | tok/sec: 162159.07 |\n",
            "| step  843 | loss: 1.2343 | lr 9.9541e-05 | norm: 0.3544 | dt: 3238.33ms | tok/sec: 161900.87 |\n",
            "| step  844 | loss: 1.2532 | lr 9.9052e-05 | norm: 0.3784 | dt: 3237.89ms | tok/sec: 161922.49 |\n",
            "| step  845 | loss: 1.2366 | lr 9.8565e-05 | norm: 0.3907 | dt: 3235.64ms | tok/sec: 162035.19 |\n",
            "| step  846 | loss: 1.2464 | lr 9.8081e-05 | norm: 0.3610 | dt: 3233.89ms | tok/sec: 162122.85 |\n",
            "| step  847 | loss: 1.2399 | lr 9.7600e-05 | norm: 0.3659 | dt: 3233.19ms | tok/sec: 162158.11 |\n",
            "| step  848 | loss: 1.2357 | lr 9.7121e-05 | norm: 0.3546 | dt: 3238.82ms | tok/sec: 161876.20 |\n",
            "| step  849 | loss: 1.2367 | lr 9.6646e-05 | norm: 0.3698 | dt: 3233.37ms | tok/sec: 162149.28 |\n",
            "| step  850 | loss: 1.2364 | lr 9.6173e-05 | norm: 0.3514 | dt: 3232.28ms | tok/sec: 162203.74 |\n",
            "| step  851 | loss: 1.2421 | lr 9.5703e-05 | norm: 0.3492 | dt: 3231.32ms | tok/sec: 162251.80 |\n",
            "| step  852 | loss: 1.2364 | lr 9.5236e-05 | norm: 0.3650 | dt: 3232.95ms | tok/sec: 162170.10 |\n",
            "| step  853 | loss: 1.2354 | lr 9.4772e-05 | norm: 0.3552 | dt: 3231.75ms | tok/sec: 162230.42 |\n",
            "| step  854 | loss: 1.2380 | lr 9.4311e-05 | norm: 0.3833 | dt: 3233.68ms | tok/sec: 162133.36 |\n",
            "| step  855 | loss: 1.2365 | lr 9.3853e-05 | norm: 0.3698 | dt: 3233.35ms | tok/sec: 162149.97 |\n",
            "| step  856 | loss: 1.2455 | lr 9.3397e-05 | norm: 0.3548 | dt: 3231.13ms | tok/sec: 162261.49 |\n",
            "| step  857 | loss: 1.2300 | lr 9.2945e-05 | norm: 0.3577 | dt: 3231.25ms | tok/sec: 162255.68 |\n",
            "| step  858 | loss: 1.2483 | lr 9.2495e-05 | norm: 0.3618 | dt: 3229.76ms | tok/sec: 162330.49 |\n",
            "| step  859 | loss: 1.2367 | lr 9.2048e-05 | norm: 0.3774 | dt: 3233.87ms | tok/sec: 162124.24 |\n",
            "| step  860 | loss: 1.2412 | lr 9.1604e-05 | norm: 0.3810 | dt: 3229.42ms | tok/sec: 162347.55 |\n",
            "| step  861 | loss: 1.2251 | lr 9.1163e-05 | norm: 0.3906 | dt: 3232.57ms | tok/sec: 162189.45 |\n",
            "| step  862 | loss: 1.2188 | lr 9.0725e-05 | norm: 0.3752 | dt: 3230.94ms | tok/sec: 162270.88 |\n",
            "| step  863 | loss: 1.2162 | lr 9.0290e-05 | norm: 0.3972 | dt: 3233.35ms | tok/sec: 162150.16 |\n",
            "| step  864 | loss: 1.2358 | lr 8.9858e-05 | norm: 0.3792 | dt: 3228.86ms | tok/sec: 162375.56 |\n",
            "| step  865 | loss: 1.2230 | lr 8.9428e-05 | norm: 0.3883 | dt: 3227.62ms | tok/sec: 162437.86 |\n",
            "| step  866 | loss: 1.2310 | lr 8.9002e-05 | norm: 0.3833 | dt: 3232.74ms | tok/sec: 162180.66 |\n",
            "| step  867 | loss: 1.2616 | lr 8.8578e-05 | norm: 0.3786 | dt: 3234.61ms | tok/sec: 162086.69 |\n",
            "| step  868 | loss: 1.2339 | lr 8.8158e-05 | norm: 0.4000 | dt: 3259.75ms | tok/sec: 160836.70 |\n",
            "| step  869 | loss: 1.2236 | lr 8.7740e-05 | norm: 0.3605 | dt: 3233.49ms | tok/sec: 162143.19 |\n",
            "| step  870 | loss: 1.2082 | lr 8.7326e-05 | norm: 0.4057 | dt: 3234.98ms | tok/sec: 162068.17 |\n",
            "| step  871 | loss: 1.2260 | lr 8.6914e-05 | norm: 0.3739 | dt: 3238.72ms | tok/sec: 161881.45 |\n",
            "| step  872 | loss: 1.2350 | lr 8.6505e-05 | norm: 0.3961 | dt: 3238.47ms | tok/sec: 161893.62 |\n",
            "| step  873 | loss: 1.2226 | lr 8.6099e-05 | norm: 0.3891 | dt: 3234.97ms | tok/sec: 162068.93 |\n",
            "| step  874 | loss: 1.2326 | lr 8.5697e-05 | norm: 0.3903 | dt: 3232.03ms | tok/sec: 162216.27 |\n",
            "| step  875 | loss: 1.2218 | lr 8.5297e-05 | norm: 0.3938 | dt: 3231.71ms | tok/sec: 162232.43 |\n",
            "| step  876 | loss: 1.2284 | lr 8.4900e-05 | norm: 0.3657 | dt: 3229.41ms | tok/sec: 162348.15 |\n",
            "| step  877 | loss: 1.2196 | lr 8.4506e-05 | norm: 0.4108 | dt: 3231.63ms | tok/sec: 162236.15 |\n",
            "| step  878 | loss: 1.2173 | lr 8.4115e-05 | norm: 0.3677 | dt: 3233.32ms | tok/sec: 162151.74 |\n",
            "| step  879 | loss: 1.2199 | lr 8.3728e-05 | norm: 0.4008 | dt: 3235.17ms | tok/sec: 162058.88 |\n",
            "| step  880 | loss: 1.2292 | lr 8.3343e-05 | norm: 0.3773 | dt: 3240.02ms | tok/sec: 161816.40 |\n",
            "| step  881 | loss: 1.2318 | lr 8.2961e-05 | norm: 0.3860 | dt: 3230.93ms | tok/sec: 162271.61 |\n",
            "| step  882 | loss: 1.2298 | lr 8.2582e-05 | norm: 0.3988 | dt: 3232.67ms | tok/sec: 162184.30 |\n",
            "| step  883 | loss: 1.2131 | lr 8.2206e-05 | norm: 0.3555 | dt: 3230.05ms | tok/sec: 162315.62 |\n",
            "| step  884 | loss: 1.2121 | lr 8.1833e-05 | norm: 0.3801 | dt: 3235.08ms | tok/sec: 162063.48 |\n",
            "| step  885 | loss: 1.2149 | lr 8.1464e-05 | norm: 0.3778 | dt: 3235.55ms | tok/sec: 162039.81 |\n",
            "| step  886 | loss: 1.2342 | lr 8.1097e-05 | norm: 0.3880 | dt: 3241.81ms | tok/sec: 161727.15 |\n",
            "| step  887 | loss: 1.2149 | lr 8.0733e-05 | norm: 0.3865 | dt: 3239.24ms | tok/sec: 161855.27 |\n",
            "| step  888 | loss: 1.2228 | lr 8.0373e-05 | norm: 0.3921 | dt: 3243.75ms | tok/sec: 161630.23 |\n",
            "| step  889 | loss: 1.2240 | lr 8.0015e-05 | norm: 0.3653 | dt: 3237.96ms | tok/sec: 161919.40 |\n",
            "| step  890 | loss: 1.2332 | lr 7.9660e-05 | norm: 0.3930 | dt: 3236.82ms | tok/sec: 161976.43 |\n",
            "| step  891 | loss: 1.2224 | lr 7.9309e-05 | norm: 0.3819 | dt: 3231.32ms | tok/sec: 162252.12 |\n",
            "| step  892 | loss: 1.2239 | lr 7.8960e-05 | norm: 0.3815 | dt: 3236.12ms | tok/sec: 162011.26 |\n",
            "| step  893 | loss: 1.2009 | lr 7.8615e-05 | norm: 0.3798 | dt: 3239.16ms | tok/sec: 161859.09 |\n",
            "| step  894 | loss: 1.2092 | lr 7.8273e-05 | norm: 0.3684 | dt: 3230.85ms | tok/sec: 162275.66 |\n",
            "| step  895 | loss: 1.2164 | lr 7.7933e-05 | norm: 0.3834 | dt: 3229.28ms | tok/sec: 162354.70 |\n",
            "| step  896 | loss: 1.2113 | lr 7.7597e-05 | norm: 0.3510 | dt: 3232.23ms | tok/sec: 162206.22 |\n",
            "| step  897 | loss: 1.2040 | lr 7.7264e-05 | norm: 0.3762 | dt: 3233.69ms | tok/sec: 162133.14 |\n",
            "| step  898 | loss: 1.2164 | lr 7.6934e-05 | norm: 0.3523 | dt: 3228.80ms | tok/sec: 162378.39 |\n",
            "| step  899 | loss: 1.2136 | lr 7.6607e-05 | norm: 0.3818 | dt: 3235.46ms | tok/sec: 162044.44 |\n",
            "| step  900 | loss: 1.2056 | lr 7.6283e-05 | norm: 0.3664 | dt: 3230.26ms | tok/sec: 162305.24 |\n",
            "| step  901 | loss: 1.2132 | lr 7.5962e-05 | norm: 0.4252 | dt: 3240.93ms | tok/sec: 161770.89 |\n",
            "| step  902 | loss: 1.2063 | lr 7.5644e-05 | norm: 0.3745 | dt: 3234.73ms | tok/sec: 162081.13 |\n",
            "| step  903 | loss: 1.2152 | lr 7.5330e-05 | norm: 0.3992 | dt: 3232.15ms | tok/sec: 162210.21 |\n",
            "| step  904 | loss: 1.2076 | lr 7.5018e-05 | norm: 0.3660 | dt: 3232.98ms | tok/sec: 162168.65 |\n",
            "| step  905 | loss: 1.1987 | lr 7.4710e-05 | norm: 0.3855 | dt: 3233.47ms | tok/sec: 162144.29 |\n",
            "| step  906 | loss: 1.2203 | lr 7.4405e-05 | norm: 0.3739 | dt: 3232.66ms | tok/sec: 162184.53 |\n",
            "| step  907 | loss: 1.2080 | lr 7.4103e-05 | norm: 0.3682 | dt: 3230.22ms | tok/sec: 162307.22 |\n",
            "| step  908 | loss: 1.2207 | lr 7.3803e-05 | norm: 0.3806 | dt: 3234.43ms | tok/sec: 162096.15 |\n",
            "| step  909 | loss: 1.2017 | lr 7.3508e-05 | norm: 0.3688 | dt: 3235.13ms | tok/sec: 162060.81 |\n",
            "| step  910 | loss: 1.2069 | lr 7.3215e-05 | norm: 0.3627 | dt: 3230.96ms | tok/sec: 162270.26 |\n",
            "| step  911 | loss: 1.2116 | lr 7.2925e-05 | norm: 0.3816 | dt: 3232.56ms | tok/sec: 162189.71 |\n",
            "| step  912 | loss: 1.2040 | lr 7.2639e-05 | norm: 0.3596 | dt: 3234.90ms | tok/sec: 162072.58 |\n",
            "| step  913 | loss: 1.2028 | lr 7.2355e-05 | norm: 0.3703 | dt: 3232.47ms | tok/sec: 162194.10 |\n",
            "| step  914 | loss: 1.2049 | lr 7.2075e-05 | norm: 0.3654 | dt: 3228.30ms | tok/sec: 162403.79 |\n",
            "| step  915 | loss: 1.2131 | lr 7.1798e-05 | norm: 0.3689 | dt: 3238.38ms | tok/sec: 161898.27 |\n",
            "| step  916 | loss: 1.2167 | lr 7.1524e-05 | norm: 0.3699 | dt: 3233.33ms | tok/sec: 162150.94 |\n",
            "| step  917 | loss: 1.2066 | lr 7.1253e-05 | norm: 0.3616 | dt: 3233.74ms | tok/sec: 162130.39 |\n",
            "| step  918 | loss: 1.1989 | lr 7.0985e-05 | norm: 0.3613 | dt: 3231.46ms | tok/sec: 162245.03 |\n",
            "| step  919 | loss: 1.2008 | lr 7.0721e-05 | norm: 0.3676 | dt: 3236.87ms | tok/sec: 161973.96 |\n",
            "| step  920 | loss: 1.2069 | lr 7.0459e-05 | norm: 0.3620 | dt: 3234.38ms | tok/sec: 162098.69 |\n",
            "| step  921 | loss: 1.2060 | lr 7.0201e-05 | norm: 0.3562 | dt: 3239.72ms | tok/sec: 161831.47 |\n",
            "| step  922 | loss: 1.2029 | lr 6.9946e-05 | norm: 0.3501 | dt: 3230.54ms | tok/sec: 162291.04 |\n",
            "| step  923 | loss: 1.1994 | lr 6.9694e-05 | norm: 0.3653 | dt: 3233.47ms | tok/sec: 162144.17 |\n",
            "| step  924 | loss: 1.2110 | lr 6.9446e-05 | norm: 0.3579 | dt: 3228.89ms | tok/sec: 162374.21 |\n",
            "| step  925 | loss: 1.2133 | lr 6.9200e-05 | norm: 0.3630 | dt: 3232.09ms | tok/sec: 162213.29 |\n",
            "| step  926 | loss: 1.2057 | lr 6.8958e-05 | norm: 0.3761 | dt: 3236.53ms | tok/sec: 161990.83 |\n",
            "| step  927 | loss: 1.2046 | lr 6.8719e-05 | norm: 0.3655 | dt: 3241.78ms | tok/sec: 161728.48 |\n",
            "| step  928 | loss: 1.1949 | lr 6.8483e-05 | norm: 0.3610 | dt: 3237.44ms | tok/sec: 161945.00 |\n",
            "| step  929 | loss: 1.2064 | lr 6.8250e-05 | norm: 0.3631 | dt: 3235.11ms | tok/sec: 162062.03 |\n",
            "| step  930 | loss: 1.2049 | lr 6.8020e-05 | norm: 0.3646 | dt: 3230.95ms | tok/sec: 162270.39 |\n",
            "| step  931 | loss: 1.2038 | lr 6.7794e-05 | norm: 0.3649 | dt: 3238.31ms | tok/sec: 161901.91 |\n",
            "| step  932 | loss: 1.2106 | lr 6.7571e-05 | norm: 0.3716 | dt: 3232.72ms | tok/sec: 162181.72 |\n",
            "| step  933 | loss: 1.2132 | lr 6.7351e-05 | norm: 0.3702 | dt: 3232.94ms | tok/sec: 162170.78 |\n",
            "| step  934 | loss: 1.2024 | lr 6.7134e-05 | norm: 0.3607 | dt: 3231.19ms | tok/sec: 162258.34 |\n",
            "| step  935 | loss: 1.2137 | lr 6.6920e-05 | norm: 0.3587 | dt: 3234.46ms | tok/sec: 162094.68 |\n",
            "| step  936 | loss: 1.1971 | lr 6.6710e-05 | norm: 0.3580 | dt: 3233.71ms | tok/sec: 162131.85 |\n",
            "| step  937 | loss: 1.2042 | lr 6.6502e-05 | norm: 0.3729 | dt: 3233.87ms | tok/sec: 162124.04 |\n",
            "| step  938 | loss: 1.1966 | lr 6.6298e-05 | norm: 0.3759 | dt: 3237.38ms | tok/sec: 161948.01 |\n",
            "| step  939 | loss: 1.1961 | lr 6.6098e-05 | norm: 0.3608 | dt: 3233.99ms | tok/sec: 162118.23 |\n",
            "| step  940 | loss: 1.1969 | lr 6.5900e-05 | norm: 0.4012 | dt: 3230.22ms | tok/sec: 162307.23 |\n",
            "| step  941 | loss: 1.2044 | lr 6.5706e-05 | norm: 0.3725 | dt: 3230.56ms | tok/sec: 162289.97 |\n",
            "| step  942 | loss: 1.2021 | lr 6.5515e-05 | norm: 0.3820 | dt: 3237.14ms | tok/sec: 161960.20 |\n",
            "| step  943 | loss: 1.1949 | lr 6.5327e-05 | norm: 0.3559 | dt: 3231.66ms | tok/sec: 162234.83 |\n",
            "| step  944 | loss: 1.1971 | lr 6.5142e-05 | norm: 0.3613 | dt: 3233.12ms | tok/sec: 162161.59 |\n",
            "| step  945 | loss: 1.2076 | lr 6.4961e-05 | norm: 0.3600 | dt: 3234.49ms | tok/sec: 162092.86 |\n",
            "| step  946 | loss: 1.1970 | lr 6.4782e-05 | norm: 0.3541 | dt: 3233.14ms | tok/sec: 162160.83 |\n",
            "| step  947 | loss: 1.2009 | lr 6.4607e-05 | norm: 0.3589 | dt: 3232.83ms | tok/sec: 162176.10 |\n",
            "| step  948 | loss: 1.1907 | lr 6.4436e-05 | norm: 0.3686 | dt: 3233.43ms | tok/sec: 162145.87 |\n",
            "| step  949 | loss: 1.1999 | lr 6.4267e-05 | norm: 0.3656 | dt: 3238.77ms | tok/sec: 161878.61 |\n",
            "| step  950 | loss: 1.1956 | lr 6.4102e-05 | norm: 0.3580 | dt: 3237.25ms | tok/sec: 161954.79 |\n",
            "| step  951 | loss: 1.1907 | lr 6.3940e-05 | norm: 0.3655 | dt: 3231.20ms | tok/sec: 162258.15 |\n",
            "| step  952 | loss: 1.2065 | lr 6.3781e-05 | norm: 0.3645 | dt: 3231.15ms | tok/sec: 162260.32 |\n",
            "| step  953 | loss: 1.1918 | lr 6.3626e-05 | norm: 0.3629 | dt: 3234.82ms | tok/sec: 162076.42 |\n",
            "| step  954 | loss: 1.1907 | lr 6.3473e-05 | norm: 0.3708 | dt: 3243.87ms | tok/sec: 161624.01 |\n",
            "| step  955 | loss: 1.1905 | lr 6.3324e-05 | norm: 0.3670 | dt: 3231.75ms | tok/sec: 162230.37 |\n",
            "| step  956 | loss: 1.2070 | lr 6.3178e-05 | norm: 0.3579 | dt: 3236.46ms | tok/sec: 161994.14 |\n",
            "| step  957 | loss: 1.2018 | lr 6.3036e-05 | norm: 0.3620 | dt: 3235.63ms | tok/sec: 162035.79 |\n",
            "| step  958 | loss: 1.1871 | lr 6.2896e-05 | norm: 0.3738 | dt: 3234.17ms | tok/sec: 162109.21 |\n",
            "| step  959 | loss: 1.2103 | lr 6.2760e-05 | norm: 0.3759 | dt: 3231.02ms | tok/sec: 162267.24 |\n",
            "| step  960 | loss: 1.1981 | lr 6.2628e-05 | norm: 0.3742 | dt: 3231.68ms | tok/sec: 162233.89 |\n",
            "| step  961 | loss: 1.1904 | lr 6.2498e-05 | norm: 0.3902 | dt: 3233.75ms | tok/sec: 162129.91 |\n",
            "| step  962 | loss: 1.1803 | lr 6.2372e-05 | norm: 0.3931 | dt: 3231.70ms | tok/sec: 162232.94 |\n",
            "| step  963 | loss: 1.1858 | lr 6.2249e-05 | norm: 0.3979 | dt: 3229.83ms | tok/sec: 162326.94 |\n",
            "| step  964 | loss: 1.1820 | lr 6.2129e-05 | norm: 0.4063 | dt: 3231.46ms | tok/sec: 162244.75 |\n",
            "| step  965 | loss: 1.1876 | lr 6.2013e-05 | norm: 0.3852 | dt: 3243.19ms | tok/sec: 161658.21 |\n",
            "| step  966 | loss: 1.1852 | lr 6.1899e-05 | norm: 0.3938 | dt: 3236.24ms | tok/sec: 162005.20 |\n",
            "| step  967 | loss: 1.1972 | lr 6.1789e-05 | norm: 0.3705 | dt: 3234.97ms | tok/sec: 162068.77 |\n",
            "| step  968 | loss: 1.2155 | lr 6.1683e-05 | norm: 0.3932 | dt: 3235.49ms | tok/sec: 162042.74 |\n",
            "| step  969 | loss: 1.1947 | lr 6.1579e-05 | norm: 0.3804 | dt: 3236.71ms | tok/sec: 161981.75 |\n",
            "| step  970 | loss: 1.1776 | lr 6.1479e-05 | norm: 0.4010 | dt: 3239.93ms | tok/sec: 161820.58 |\n",
            "| step  971 | loss: 1.1796 | lr 6.1382e-05 | norm: 0.3737 | dt: 3237.60ms | tok/sec: 161937.45 |\n",
            "| step  972 | loss: 1.1877 | lr 6.1289e-05 | norm: 0.3760 | dt: 3239.83ms | tok/sec: 161825.54 |\n",
            "| step  973 | loss: 1.1881 | lr 6.1198e-05 | norm: 0.3563 | dt: 3236.84ms | tok/sec: 161975.21 |\n",
            "| step  974 | loss: 1.1945 | lr 6.1111e-05 | norm: 0.3780 | dt: 3231.02ms | tok/sec: 162266.99 |\n",
            "| step  975 | loss: 1.1933 | lr 6.1027e-05 | norm: 0.3785 | dt: 3237.94ms | tok/sec: 161920.35 |\n",
            "| step  976 | loss: 1.1818 | lr 6.0947e-05 | norm: 0.3796 | dt: 3230.04ms | tok/sec: 162316.10 |\n",
            "| step  977 | loss: 1.1875 | lr 6.0870e-05 | norm: 0.3732 | dt: 3230.29ms | tok/sec: 162303.63 |\n",
            "| step  978 | loss: 1.1825 | lr 6.0796e-05 | norm: 0.3791 | dt: 3238.52ms | tok/sec: 161891.24 |\n",
            "| step  979 | loss: 1.1825 | lr 6.0725e-05 | norm: 0.3817 | dt: 3235.95ms | tok/sec: 162019.93 |\n",
            "| step  980 | loss: 1.1857 | lr 6.0658e-05 | norm: 0.3745 | dt: 3234.21ms | tok/sec: 162107.05 |\n",
            "| step  981 | loss: 1.1923 | lr 6.0594e-05 | norm: 0.4129 | dt: 3232.88ms | tok/sec: 162173.53 |\n",
            "| step  982 | loss: 1.1982 | lr 6.0533e-05 | norm: 0.3692 | dt: 3232.74ms | tok/sec: 162180.84 |\n",
            "| step  983 | loss: 1.1798 | lr 6.0475e-05 | norm: 0.4115 | dt: 3241.37ms | tok/sec: 161748.67 |\n",
            "| step  984 | loss: 1.1828 | lr 6.0421e-05 | norm: 0.3928 | dt: 3235.43ms | tok/sec: 162045.65 |\n",
            "| step  985 | loss: 1.1759 | lr 6.0370e-05 | norm: 0.4113 | dt: 3231.74ms | tok/sec: 162230.94 |\n",
            "| step  986 | loss: 1.1890 | lr 6.0322e-05 | norm: 0.3997 | dt: 3232.29ms | tok/sec: 162203.28 |\n",
            "| step  987 | loss: 1.1868 | lr 6.0278e-05 | norm: 0.4049 | dt: 3231.04ms | tok/sec: 162265.90 |\n",
            "| step  988 | loss: 1.1885 | lr 6.0237e-05 | norm: 0.3953 | dt: 3231.85ms | tok/sec: 162225.14 |\n",
            "| step  989 | loss: 1.1921 | lr 6.0199e-05 | norm: 0.3869 | dt: 3233.04ms | tok/sec: 162165.85 |\n",
            "| step  990 | loss: 1.1874 | lr 6.0164e-05 | norm: 0.3901 | dt: 3233.22ms | tok/sec: 162156.65 |\n",
            "| step  991 | loss: 1.1936 | lr 6.0133e-05 | norm: 0.3884 | dt: 3235.56ms | tok/sec: 162039.53 |\n",
            "| step  992 | loss: 1.1906 | lr 6.0105e-05 | norm: 0.3683 | dt: 3245.51ms | tok/sec: 161542.44 |\n",
            "| step  993 | loss: 1.1797 | lr 6.0081e-05 | norm: 0.3780 | dt: 3236.25ms | tok/sec: 162004.77 |\n",
            "| step  994 | loss: 1.1699 | lr 6.0059e-05 | norm: 0.3804 | dt: 3236.12ms | tok/sec: 162011.25 |\n",
            "| step  995 | loss: 1.1747 | lr 6.0041e-05 | norm: 0.3778 | dt: 3242.68ms | tok/sec: 161683.70 |\n",
            "| step  996 | loss: 1.1834 | lr 6.0026e-05 | norm: 0.3800 | dt: 3240.70ms | tok/sec: 161782.40 |\n",
            "| step  997 | loss: 1.1802 | lr 6.0015e-05 | norm: 0.3730 | dt: 3237.19ms | tok/sec: 161957.62 |\n",
            "| step  998 | loss: 1.1772 | lr 6.0007e-05 | norm: 0.3761 | dt: 3230.89ms | tok/sec: 162273.49 |\n",
            "| step  999 | loss: 1.1815 | lr 6.0002e-05 | norm: 0.3806 | dt: 3234.65ms | tok/sec: 162084.83 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_model(\n",
        "    prompt=\"Goog threw the ball to Lily. Lily threw it back to him.\",\n",
        "    num_samples=10,\n",
        "    max_tokens=100,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "id": "W_J5Bmb4MAJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088ca2cd-6233-49b5-83bc-5ea397951723"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " [0] > Goog threw the ball to Lily. Lily threw it back to him. It flew away fast. It landed on a pile of leaves. Sara and Ben ran to get the ball. They took it to Mom and Dad.\n",
            "Mom and Dad looked at each other. They called them. They said, \"That was a good kick. But you are glad you played nicely. When you are older, you can have a good time. We love you?\"\n",
            "Sara and Ben nodded. They\n",
            "\n",
            " [1] > Goog threw the ball to Lily. Lily threw it back to him. It hit her hard and she started to cry.\n",
            "Sam stopped crying and ran to Lily. He hugged her and said, \"It's okay, Lily. I'm sorry I didn't watch the ball. It's mean. Can we play together again?\"\n",
            "Lily looked at Sam. She looked at him and said, \"No, we can't. The ball is too big. It might roll on us.\"\n",
            "\n",
            " [2] > Goog threw the ball to Lily. Lily threw it back to him. She landed on the ball. The ball flew through the air and landed on Ben's head. Ben fell back on the ball. He scraped his knee and cried.\n",
            "Lily ran to Ben and picked him up. She felt sorry for him and tried to get the ball back. She said, \"I'm sorry, Ben. I didn't mean to throw balls.\"\n",
            "Ben looked at Lily and saw her head\n",
            "\n",
            " [3] > Goog threw the ball to Lily. Lily threw it back to him. They smiled and said, \"Thank you, Tim!\" They both enjoyed playing with the ball. Tim was happy to help his new friend.\n",
            "From then on, Tim and Lily played more gently in the swamp. They had a lot of fun together. And they were the best of friends.\n",
            "<|endoftext|>\n",
            "Once upon a time, there was a little boy named Tim. Tim loved to play with his blocks. He had\n",
            "\n",
            " [4] > Goog threw the ball to Lily. Lily threw it back to him. Lily was happy and said, \"Thank you, Ben. You are a good friend.\" Ben nodded and threw the ball. Lily joined them and they all played happily.\n",
            "<|endoftext|>\n",
            "\n",
            "Once upon a time there were two friends: Sally and Jack. Sally had a large case so she could keep it safe from Jack's friends. Jack was the first, as he was not carrying a case. \n",
            "Every day,\n",
            "\n",
            " [5] > Goog threw the ball to Lily. Lily threw it back to him. Max ran to catch it. He thought it was fun.\n",
            "But Lily did not say sorry. She was angry. She wanted to play with the ball. She kicked it to Max. She said, \"Max, no! That is our ball. Go away!\"\n",
            "Max did not get out of the room. He ran away with the ball. He did not care about Lily or the dog. He chased after Max\n",
            "\n",
            " [6] > Goog threw the ball to Lily. Lily threw it back to him. He wanted to keep it.\n",
            "\"Give it back, Lily. It's my ball,\" Tom said.\n",
            "\"No, it's mine. You have your own ball. Leave me alone,\" Lily said.\n",
            "The ball did not seem to grab. It bounced away from Tom and rolled away. It rolled until it was out of sight. Then it stopped. It disappeared.\n",
            "Tom and Lily were surprised. They\n",
            "\n",
            " [7] > Goog threw the ball to Lily. Lily threw it back to him. He kicked it hard. He knocked over the ball.\n",
            "\"Hey, that's my ball!\" Ben shouted.\n",
            "He ran away from the ball. He tried to fit the ball in the air. He saw the hole in the tree. He felt sad. It was too big to fit the ball in his hand.\n",
            "\"Ow, that hurt!\" Ben said. \"Let me.\"\n",
            "He ran to the\n",
            "\n",
            " [8] > Goog threw the ball to Lily. Lily threw it back to him. Ben felt embarrassed. He said, \"I'm sorry, Lily. I just want to play.\"\n",
            "Lily looked sad. She said, \"You are right, Ben. I'm sorry for being rude.\" Ben smiled. He said, \"It's okay, Lily. I forgive you. But next time, ask me for help.\"\n",
            "Lily nodded. She said, \"OK, Ben. And be nice\n",
            "\n",
            " [9] > Goog threw the ball to Lily. Lily threw it back to him. The cat ran away and Lily could not get it back. She said, \"I'm sorry, Tom. I thought you missed me. Here is your ball.\"\n",
            "<|endoftext|>\n",
            "One day, a little boy named Tim found a pipe in his yard. He was upset because he could not play with the pipe. He looked at it and thought, \"What if I play with the pipe?\"\n",
            "Tim's friend, Sam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mapjHr7jgWL7"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}