{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMjfbr0a62wuKJbdtbTJrDU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-schoen/gpt_from_scratch/blob/main/colab/bigram_with_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NC9lP0tG4raC"
      },
      "outputs": [],
      "source": [
        "# preamble\n",
        "\n",
        "import dataclasses\n",
        "from typing import Iterable\n",
        "\n",
        "# since python represents these this way\n",
        "Char = str\n",
        "EncodedChar = int\n",
        "\n",
        "\n",
        "# TODO(bschoen): in general would want to abstract this based on type\n",
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, value: Iterable[str]) -> None:\n",
        "\n",
        "        self.unique_elements: list[Char] = sorted(list(set(value)))\n",
        "\n",
        "        # create encoding / decoding mapping\n",
        "        self._char_to_int = {char: i for i, char in enumerate(self.unique_elements)}\n",
        "        self._int_to_char = {i: char for i, char in enumerate(self.unique_elements)}\n",
        "\n",
        "    def encode(self, string: str) -> list[EncodedChar]:\n",
        "        return [self._char_to_int[c] for c in string]\n",
        "\n",
        "    def decode(self, encoded_chars: list[EncodedChar]) -> str:\n",
        "        return \"\".join([self._int_to_char[ec] for ec in encoded_chars])\n",
        "\n",
        "    def decode_single(self, encoded_char: EncodedChar) -> Char:\n",
        "        return self._int_to_char[encoded_char]\n",
        "\n",
        "    def encode_single(self, char: Char) -> EncodedChar:\n",
        "        return self._char_to_int[char]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import hashlib\n",
        "import pathlib\n",
        "\n",
        "\n",
        "def _generate_cached_filename_for_url(url: str) -> str:\n",
        "    return hashlib.md5(url.encode()).hexdigest()\n",
        "\n",
        "\n",
        "def download_file_from_url(\n",
        "    url: str,\n",
        "    cache_dir: pathlib.Path = pathlib.Path(\"download_cache\"),\n",
        ") -> pathlib.Path:\n",
        "    \"\"\"Download file, using local cache to avoid repeated downloads.\"\"\"\n",
        "\n",
        "    # create cachedir if not exists\n",
        "    cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Generate a unique filename based on the URL\n",
        "    filename = _generate_cached_filename_for_url(url)\n",
        "    filepath = cache_dir / filename\n",
        "\n",
        "    # Check if the file is already cached\n",
        "    if filepath.exists():\n",
        "        print(f\"File found in cache: {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    # If not cached, download the file\n",
        "    print(f\"Downloading file from {url}\")\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Raise an exception for HTTP errors\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Save the file to cache\n",
        "    with open(filepath, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(f\"File downloaded and cached: {filepath}\")\n",
        "    return filepath\n"
      ],
      "metadata": {
        "id": "c0sVMD5G45I0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "eDO8nzw75jzR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CG0bL0h252re"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FLQRMevU6YHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jaxtyping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg-7CWSt52vq",
        "outputId": "ca925f8d-ba19-4521-b86a-b71969ee4093"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.2.33-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading jaxtyping-0.2.33-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, jaxtyping\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.3.1 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jaxtyping-0.2.33 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# imported for typechecking\n",
        "#\n",
        "# note: can't easily alias via jaxtyping annotations, as it's a string literal and\n",
        "#       likely plays weirdly with typing.Annotation to forward a payload\n",
        "# note: torchtyping is deprecated in favor of jaxtyping, as torchtyping doesn't have mypy integration\n",
        "#\n",
        "# note: jaxtyping does support prepending\n",
        "#\n",
        "#   Image = Float[Array, \"channels height width\"]\n",
        "#   BatchImage = Float[Image, \"batch\"]\n",
        "#\n",
        "#    -->\n",
        "#\n",
        "#   BatchImage = Float[Array, \"batch channels height width\"]\n",
        "#\n",
        "# so we can compose aliases\n",
        "#\n",
        "from torch import Tensor\n",
        "import jaxtyping\n",
        "from jaxtyping import jaxtyped, Float32, Int64\n",
        "from typeguard import typechecked as typechecker\n",
        "\n",
        "# now we still want to batch\n",
        "# we seed it so it's always the same\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# type aliases\n",
        "Block = Int64[Tensor, \"block_size\"]\n",
        "\n",
        "# equivalent to `Int64[Tensor, \"batch_size block_size\"]`\n",
        "BatchedBlocks = Int64[Block, \"batch_size\"]\n"
      ],
      "metadata": {
        "id": "dx7N8X-35mMz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BrAiHK55mcM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmReQRmk5mgZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classes and functions"
      ],
      "metadata": {
        "id": "2EbhNuuP6bmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO(bschoen): Understanding dropout\n",
        "\"\"\"\n",
        "Adding dropout\n",
        "\n",
        "[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
        "\n",
        "Essentially causes the model to train a handful of ensemble networks\n",
        "\n",
        "This seems super hand wavy.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"One head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_embed: int,\n",
        "        block_size: int,\n",
        "        dropout: float,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embed = n_embed\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.key = nn.Linear(n_embed, d_model, bias=False)\n",
        "        self.query = nn.Linear(n_embed, d_model, bias=False)\n",
        "        self.value = nn.Linear(n_embed, d_model, bias=False)\n",
        "\n",
        "        # TODO(bschoen): Why?\n",
        "        tril: Float32[Tensor, \"block_size block_size\"] = torch.ones(\n",
        "            block_size, block_size\n",
        "        )\n",
        "        tril = torch.tril(tril)\n",
        "        self.register_buffer(\"tril\", tril)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # @jaxtyped(typechecker=typechecker)\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Float32[Tensor, \"batch_size block_size n_embed\"],\n",
        "    ) -> Float32[Tensor, \"batch_size block_size d_model\"]:\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        k: Float32[Tensor, \"batch_size block_size d_model\"] = self.key(x)\n",
        "        q: Float32[Tensor, \"batch_size block_size d_model\"] = self.query(x)\n",
        "\n",
        "        # compute attention scores (\"affinities\"), this is our QK matrix\n",
        "        # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        weights: Float32[Tensor, \"batch_size block_size block_size\"] = q @ k.transpose(\n",
        "            -2, -1\n",
        "        )\n",
        "\n",
        "        # normalize the attention scores\n",
        "        weights = weights / math.sqrt(self.n_embed)\n",
        "\n",
        "        # masked attention so can only see previous tokens\n",
        "        # note: `T` is used instead of `block_size` to allow for sequences shorter than `block_size`\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "\n",
        "        # (B, T, T)\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "\n",
        "        # TODO(bschoen): Why apply dropout here? Is it always right before it actually gets used?\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v: Float32[Tensor, \"batch_size block_size d_model\"] = self.value(x)\n",
        "\n",
        "        out = weights @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attention in parallel, then concatenating the results\n",
        "\n",
        "    \"these tokens have a lot to talk about\"\n",
        "\n",
        "    Helpful to think of these as multiple independent channels.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_heads: int,\n",
        "        d_model: int,\n",
        "        n_embed: int,\n",
        "        block_size: int,\n",
        "        dropout: float,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_embed = n_embed\n",
        "\n",
        "        if n_heads * d_model != n_embed:\n",
        "            raise ValueError(\n",
        "                f\"{n_heads * d_model=} ({n_heads=}, {d_model=}) != {n_embed=}. \"\n",
        "                f\"Must be able to stack the outputs of {n_heads=} of dimension {d_model=} \"\n",
        "                f\"into an output that's back in the embedding space {n_embed=}\"\n",
        "            )\n",
        "\n",
        "        self.heads = nn.ModuleList(\n",
        "            [\n",
        "                Head(\n",
        "                    d_model=d_model,\n",
        "                    n_embed=n_embed,\n",
        "                    block_size=block_size,\n",
        "                    dropout=dropout,\n",
        "                )\n",
        "                for _ in range(n_heads)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # TODO(bschoen): Don't we want this to go from `d_model` to `n_embed`?\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # @jaxtyped(typechecker=typechecker)\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Float32[Tensor, \"batch_size block_size n_embed\"],\n",
        "    ) -> Float32[Tensor, \"batch_size block_size n_embed\"]:\n",
        "\n",
        "        head_outputs: list[Float32[Tensor, \"batch_size block_size d_model\"]] = [\n",
        "            head.forward(x) for head in self.heads\n",
        "        ]\n",
        "\n",
        "        # assertion just for readability, we already check this during construction\n",
        "        assert len(head_outputs) * self.d_model == self.n_embed\n",
        "\n",
        "        # concatenate them over the last dimension\n",
        "        out: Float32[Tensor, \"batch_size block_size n_embed\"] = torch.cat(\n",
        "            head_outputs,\n",
        "            dim=-1,\n",
        "        )\n",
        "\n",
        "        # TODO(bschoen): Why this projection?\n",
        "        out = self.proj(out)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"\n",
        "    a simple linear layer followed by a non-linearity\n",
        "\n",
        "    Want to add computation into the network\n",
        "\n",
        "    Want that computation to be able to operate at a per token level.\n",
        "\n",
        "    \"Tokens looked at each other, but didn't really have a lot of time to 'think on' (a lot of compute)\n",
        "     what they found.\"\n",
        "\n",
        "    Note:\n",
        "     - This is on a per token level\n",
        "     - Self-attention is the communication\n",
        "     - Now they need to think on that data individually\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # note: this just comes directly from attention is all you need paper\n",
        "    # TODO(bschoen): ???\n",
        "    INNER_DIMENSIONALITY_FACTOR = 4\n",
        "\n",
        "    def __init__(self, n_embed: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        d_model = self.INNER_DIMENSIONALITY_FACTOR * n_embed\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, d_model),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.proj = nn.Linear(d_model, n_embed)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Float32[Tensor, \"batch_size block_size n_embed\"],\n",
        "    ) -> Float32[Tensor, \"batch_size block_size n_embed\"]:\n",
        "\n",
        "        out_model: Float32[Tensor, \"batch_size block_size d_model\"] = self.net.forward(\n",
        "            x\n",
        "        )\n",
        "\n",
        "        out: Float32[Tensor, \"batch_size block_size n_embed\"] = self.proj.forward(\n",
        "            out_model\n",
        "        )\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block: communication followed by computation\n",
        "\n",
        "    Now that these are getting deep, they're getting harder to optimize\n",
        "\n",
        "    (1) Let's use residual connections:\n",
        "\n",
        "    [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
        "\n",
        "    Initialize residual blocks to near zero, and let them over time represent\n",
        "    how much should contribute\n",
        "\n",
        "    So:\n",
        "\n",
        "        x = self.sa_heads(x)\n",
        "        x = self.ffwd(x)\n",
        "\n",
        "    Becomes:\n",
        "\n",
        "        x = x + self.sa_heads(x)\n",
        "        x = x + self.ffwd(x)\n",
        "\n",
        "    # TODO(bschoen): Why do we need this?\n",
        "\n",
        "    Now both `MultiHeadAttention` and `FeedFoward` need an additional\n",
        "    projection `proj` (nn.Linear(n_embed, n_embed))\n",
        "\n",
        "    (2) Now let's add in layer normalization\n",
        "\n",
        "    [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
        "\n",
        "    Note: Layer norm has it's own `gamma` and `beta` parameters, which are\n",
        "          learnable.\n",
        "\n",
        "          def layer_norm(x):\n",
        "\n",
        "            x_normalized = (x - torch.mean(x)) / math.sqrt(torch.var(x) + epsilon)\n",
        "\n",
        "            out = x_normalized * gamma + beta\n",
        "\n",
        "            return out\n",
        "\n",
        "    Note: In the original `attention` paper, this was applied after attention / feed forward,\n",
        "          now it's common to apply them before\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_heads: int,\n",
        "        d_model: int,\n",
        "        n_embed: int,\n",
        "        block_size: int,\n",
        "        dropout: float,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # note: normalization happening over embedding space\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "\n",
        "        # self attention head\n",
        "        # ex: 4 heads, 8-dimensional self attention\n",
        "        self.sa_heads = MultiHeadAttention(\n",
        "            n_heads=n_heads,\n",
        "            n_embed=n_embed,\n",
        "            d_model=d_model,\n",
        "            block_size=block_size,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "        self.ffwd = FeedFoward(n_embed=n_embed, dropout=dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Float32[Tensor, \"batch_size block_size n_embed\"],\n",
        "    ) -> Float32[Tensor, \"batch_size block_size n_embed\"]:\n",
        "\n",
        "        # apply self attention\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.sa_heads(x)\n",
        "\n",
        "        # feed forward layer to \"think on\" the results of the self attention\n",
        "        x = self.ln2(x)\n",
        "        x = x + self.ffwd(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Adding in token embeddings means we need a linear layer\n",
        "# (to go from token embeddings to logits, basically to undo the linear embedding layer)\n",
        "# Is this just an unembedding layer?\n",
        "#\n",
        "# We're basically add a bunch of stuff onto just bigram *first*\n",
        "#\n",
        "class BigramWithSelfAttentionLanguageModel(nn.Module):\n",
        "\n",
        "    # @jaxtyped(typechecker=typechecker)\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        n_embed: int,\n",
        "        n_heads: int,\n",
        "        n_transformer_blocks: int,\n",
        "        block_size: int,\n",
        "        dropout: float,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "\n",
        "        # add position embeddings\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "\n",
        "        # sequential decoder only transformer blocks\n",
        "        self.transformer_blocks = nn.Sequential(\n",
        "            *[\n",
        "                TransformerBlock(\n",
        "                    n_heads=n_heads,\n",
        "                    n_embed=n_embed,\n",
        "                    d_model=n_embed // n_heads,\n",
        "                    block_size=block_size,\n",
        "                    dropout=dropout,\n",
        "                )\n",
        "                for _ in range(n_transformer_blocks)\n",
        "            ],\n",
        "            nn.LayerNorm(n_embed),\n",
        "        )\n",
        "\n",
        "        # map from token embeddings back to logits\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    # @jaxtyped(typechecker=typechecker)\n",
        "    def forward(\n",
        "        self,\n",
        "        idx: BatchedBlocks,\n",
        "        targets: BatchedBlocks | None = None,\n",
        "    ) -> tuple[\n",
        "        Float32[Tensor, \"batch_size block_size vocab_size\"]\n",
        "        | Float32[Tensor, \"batch_size*block_size vocab_size\"],\n",
        "        Float32[Tensor, \"\"] | None,\n",
        "    ]:\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_embeddings: Float32[Tensor, \"batch_size block_size n_embed\"] = (\n",
        "            self.token_embedding_table(idx)\n",
        "        )\n",
        "\n",
        "        # ex: [0, 1, 2, 3, ...]\n",
        "        pos_indices = torch.arange(idx.shape[1], device=idx.device)\n",
        "        pos_embeddings: Float32[Tensor, \"batch_size block_size n_embed\"] = (\n",
        "            self.position_embedding_table(pos_indices)\n",
        "        )\n",
        "\n",
        "        # concat\n",
        "        x = token_embeddings + pos_embeddings\n",
        "\n",
        "        x = self.transformer_blocks(x)\n",
        "\n",
        "        # takes them back from token embedding space to logits\n",
        "        logits: Float32[Tensor, \"batch_size block_size vocab_size\"] = self.lm_head(x)\n",
        "\n",
        "        # if no targets, nothing to calculate\n",
        "        if targets is None:\n",
        "            return logits, None\n",
        "\n",
        "        B, T, C = logits.shape\n",
        "\n",
        "        # strech them out into 1d sequence, just because of quirks of what pytorch expects\n",
        "        # for the cross_entropy calculation\n",
        "        reshaped_logits: Float32[Tensor, \"batch_size*block_size vocab_size\"] = (\n",
        "            logits.view(B * T, C)\n",
        "        )\n",
        "\n",
        "        reshaped_targets: Float32[Tensor, \"batch_size*block_size\"] = targets.view(B * T)\n",
        "\n",
        "        loss: Float32[Tensor, \"\"] = F.cross_entropy(reshaped_logits, reshaped_targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # @jaxtyped(typechecker=typechecker)\n",
        "    def generate(\n",
        "        self,\n",
        "        idx: BatchedBlocks,\n",
        "        max_new_tokens: int,\n",
        "    ) -> BatchedBlocks:\n",
        "\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop `idx` to the last `block_size` tokens, since that's\n",
        "            # all our positional embedding supports\n",
        "            idx_cond = idx[:, -self.block_size :]\n",
        "\n",
        "            # get the predictions\n",
        "            logits, loss = self.forward(idx_cond)\n",
        "\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            # essentially gets the highest probability logit\n",
        "            probs: Float32[Tensor, \"batch_size vocab_size\"] = F.softmax(\n",
        "                logits, dim=-1\n",
        "            )  # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next: Float32[Tensor, \"batch_size 1\"] = torch.multinomial(\n",
        "                probs, num_samples=1\n",
        "            )  # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            # and move `idx` forward\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "# want to average loss over a few batches\n",
        "# tells pytorch doesn't have to store intermediate values\n",
        "@torch.no_grad()\n",
        "def estimate_loss(\n",
        "    model: nn.Module,\n",
        "    data: Int64[Tensor, \"num_samples\"],\n",
        "    num_batches_to_eval: int,\n",
        "    batch_size: int,\n",
        "    block_size: int,\n",
        "    device: torch.device,\n",
        ") -> float:\n",
        "\n",
        "    print('Estimating loss...')\n",
        "\n",
        "    # set to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # compute loss over `eval_interval` batches, then average it\n",
        "    losses = torch.zeros(num_batches_to_eval)\n",
        "\n",
        "    for i in range(num_batches_to_eval):\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch(\n",
        "            data,\n",
        "            batch_size=batch_size,\n",
        "            block_size=block_size,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        # compute loss\n",
        "        _, loss = model(xb, yb)\n",
        "\n",
        "        losses[i] = loss\n",
        "\n",
        "    # convert back to train mode\n",
        "    model.train()\n",
        "\n",
        "    # return average loss\n",
        "    return losses.mean().item()\n",
        "\n",
        "\n",
        "# note: usually want to stack into a batch\n",
        "# @jaxtyped(typechecker=typechecker)\n",
        "def get_batch(\n",
        "    data: Int64[Tensor, \"num_samples\"],\n",
        "    batch_size: int,\n",
        "    block_size: int,\n",
        "    device: torch.device,\n",
        ") -> tuple[BatchedBlocks, BatchedBlocks]:\n",
        "    \"\"\"Generate a small batch of data of inputs x and targets y.\"\"\"\n",
        "\n",
        "    # Generate 'batch_size' random indices. Each index is the start of a sequence.\n",
        "    # The upper bound (len(data) - block_size) ensures we have enough room for a full sequence.\n",
        "    max_batch_start_index = len(data) - block_size\n",
        "\n",
        "    # choose `batch_size` random starting indices for where to start each batch\n",
        "    batch_start_indices: Int64[Tensor, \"batch_size\"] = torch.randint(\n",
        "        max_batch_start_index, (batch_size,)\n",
        "    )\n",
        "\n",
        "    # For each random start index, extract a sequence of length 'block_size'.\n",
        "    x_blocks: list[Int64[Tensor, \"block_size\"]] = [\n",
        "        data[i : i + block_size] for i in batch_start_indices\n",
        "    ]\n",
        "\n",
        "    # Similar to x, but shifted one position to the right (next-token prediction).\n",
        "    # This creates the targets for each input sequence.\n",
        "    y_blocks: list[Int64[Tensor, \"block_size\"]] = [\n",
        "        data[i + 1 : i + block_size + 1] for i in batch_start_indices\n",
        "    ]\n",
        "\n",
        "    # Stack these sequences into a single tensor of shape (batch_size, block_size).\n",
        "    x_batch: BatchedBlocks = torch.stack(x_blocks)\n",
        "    y_batch: BatchedBlocks = torch.stack(y_blocks)\n",
        "\n",
        "    # move to device\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    return x_batch, y_batch\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_and_decode_text(\n",
        "    m: nn.Module,\n",
        "    vocab: Vocabulary,\n",
        "    device: torch.device,\n",
        "    max_new_tokens: int = 500,\n",
        ") -> str:\n",
        "\n",
        "    # set to eval mode\n",
        "    m.eval()\n",
        "\n",
        "    # note: we're using batch size\n",
        "    generated_text_batch = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "    generated_text_batch = m.generate(\n",
        "        idx=generated_text_batch,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "    )\n",
        "\n",
        "    # check that we only generated one batch (since we only had one batch)\n",
        "    assert len(generated_text_batch) == 1\n",
        "\n",
        "    generated_text = generated_text_batch[0]\n",
        "\n",
        "    # decode\n",
        "    decoded_generated_text = vocab.decode(generated_text.tolist())\n",
        "\n",
        "    # convert back to train mode\n",
        "    m.train()\n",
        "\n",
        "    return decoded_generated_text"
      ],
      "metadata": {
        "id": "0mobAvUl46Mk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# constants"
      ],
      "metadata": {
        "id": "0Nji5Te16Upp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -- now we get to the part specific for our problem\n",
        "\n",
        "# note: these numbers are from Karpathy\n",
        "# LMAO took 15 minutes on an A100 for him\n",
        "\n",
        "# how many independent sequences will we process in parallel?\n",
        "BATCH_SIZE = 256 # 64\n",
        "\n",
        "# what is the maximum context length for predictions?\n",
        "BLOCK_SIZE = 256\n",
        "\n",
        "# how many batches to look at when running `evaluate_loss`\n",
        "# 300 comes from karpathy\n",
        "# note: this is likely why our numbers are a bit different\n",
        "NUM_BATCHES_TO_EVAL = 100\n",
        "\n",
        "# how often to evaluate the model\n",
        "EVALUATE_EVERY_N_STEPS = 100\n",
        "\n",
        "MAX_STEPS = 10000\n",
        "\n",
        "N_EMBED = 384 # -> every head has 64\n",
        "\n",
        "LEARNING_RATE = 3e-4\n",
        "\n",
        "# number of self attention heads\n",
        "N_HEADS = 6\n",
        "\n",
        "DROPOUT = 0.2 # -> 20% of each intermediate computation is converted to 0\n",
        "\n",
        "N_TRANSFORMER_BLOCKS = 6"
      ],
      "metadata": {
        "id": "2VHnuIpi6AXh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "AX351Ft36SbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## device setup"
      ],
      "metadata": {
        "id": "IsHuXE-P67-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note: does this cover the attention head too? presumably every sub module\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm7OR2pd68eY",
        "outputId": "00eaeb24-2d30-47e9-ac2c-10b23876b75b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dataset setup"
      ],
      "metadata": {
        "id": "adrXVEfa685A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tinyshakespeare\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "input_filepath = download_file_from_url(url)\n",
        "\n",
        "# Read all text from the input file\n",
        "input_text = input_filepath.read_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SR42ilzt69Dq",
        "outputId": "a53624b4-f03f-46b7-a0bb-ffe3da65ad0a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File found in cache: download_cache/4acd659e47adc1daeb7aff503accf0a3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocabulary(input_text)\n",
        "\n",
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "encoded_input_text: Int64[Tensor, \"num_samples\"] = torch.tensor(\n",
        "    vocab.encode(input_text),\n",
        "    dtype=torch.long,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "\n",
        "# first 90% will be train, rest val\n",
        "train_val_ratio = 0.9\n",
        "\n",
        "n = int(train_val_ratio * len(encoded_input_text))\n",
        "\n",
        "train_data: Int64[Tensor, \"num_samples\"] = encoded_input_text[:n]\n",
        "val_data: Int64[Tensor, \"num_samples\"] = encoded_input_text[n:]\n",
        "\n",
        "# note: train loss getting lower than val means we're overfitting\n",
        "print(f\"Splitting {len(encoded_input_text)} input tokens into\")\n",
        "print(f\" - train: {len(train_data)}\")\n",
        "print(f\" - val: {len(val_data)}\")\n",
        "\n",
        "m = BigramWithSelfAttentionLanguageModel(\n",
        "    vocab_size=len(vocab.unique_elements),\n",
        "    block_size=BLOCK_SIZE,\n",
        "    n_embed=N_EMBED,\n",
        "    n_heads=N_HEADS,\n",
        "    n_transformer_blocks=N_TRANSFORMER_BLOCKS,\n",
        "    dropout=DROPOUT,\n",
        ")\n",
        "m.to(device)\n",
        "\n",
        "print(\"Output before optimizing:\\n---\")\n",
        "print(generate_and_decode_text(m, vocab, device))\n",
        "print(\"---\")\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# increase number of steps for good results...\n",
        "print(\n",
        "    f\"Running for {MAX_STEPS}, evaluating every {EVALUATE_EVERY_N_STEPS} steps...\"\n",
        ")\n",
        "\n",
        "def evaluate_and_show_loss(step: int) -> None:\n",
        "    # evaluate the loss\n",
        "    loss_by_dataset_name: dict[str, float] = {}\n",
        "    for name, data in [(\"train\", train_data), (\"val\", val_data)]:\n",
        "        loss_by_dataset_name[name] = estimate_loss(\n",
        "            m,\n",
        "            data,\n",
        "            num_batches_to_eval=NUM_BATCHES_TO_EVAL,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            block_size=BLOCK_SIZE,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "    train_loss = loss_by_dataset_name[\"train\"]\n",
        "    val_loss = loss_by_dataset_name[\"val\"]\n",
        "\n",
        "    print(f\"Step {step}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    print(f\"Sampling at {step}:\\n---\")\n",
        "    print(print(generate_and_decode_text(m, vocab, device, max_new_tokens=100)))\n",
        "    print(\"---\")\n",
        "\n",
        "# note: Karpathy is printing at the end\n",
        "for step in range(MAX_STEPS):\n",
        "\n",
        "    if step % EVALUATE_EVERY_N_STEPS == 0:\n",
        "        evaluate_and_show_loss(step=step)\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(\n",
        "        train_data,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        block_size=BLOCK_SIZE,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Final loss:\")\n",
        "evaluate_and_show_loss(step=MAX_STEPS)\n",
        "\n",
        "print(\"Output after optimizing:\\n---\")\n",
        "print(print(generate_and_decode_text(m, vocab, device)))\n",
        "print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gi_CfBi6Aad",
        "outputId": "2ec066e2-288c-4f29-a5e2-78e663f26860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting 1115394 input tokens into\n",
            " - train: 1003854\n",
            " - val: 111540\n",
            "Output before optimizing:\n",
            "---\n",
            "\n",
            "ZchmlFEQpQFIjODD:ZsefZOdW-hL:hMokC$  wReKEW!A.,hl3RGrKkNFf'As\n",
            "Wsz:mnj3\n",
            "r!3v kW:ZI\n",
            "KjgpAh!Jvc:ju Bu.\n",
            "k  ncf&gWf\n",
            "f\n",
            "aK,K3wQDktoUjmtV&KbxgEJ3P;ESHYDeDeAB:f?xLwLDjD$;oILuqT$hOCjyvvuauh!w&rdrYbAq;AB&ouEWXO;,D gKxMaIqVwN:LVgZpq\n",
            "Ed!K$O!vhCwxs3TSZNRATrDOEb;P,Xpa qMEr\n",
            "\n",
            "!'h.s3ru,RN $;3&PW&CvJSvur'rOokuWs'ug$rFD!EADGIHeT!ijfWPDbP,wS:dM.PDH!CpbDAuAJ'?'3UFq \n",
            "t?!DaIXB,EqO:\n",
            "3PDlXGcXzWx;mbcNeFg.3O'vOgCM$kDE$uIJylnl.$uPjNFRIRtku'-dwlO'bEu:ZuO\n",
            "Px3bK'ct$!A33;DBMFWH,Wuf:sr.'ROBD jDjiOuEP.UMH''bWkWDl;JfDMdqMW&ysNi'XD\n",
            "---\n",
            "Running for 10000, evaluating every 100 steps...\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 0, Train Loss: 4.2971, Val Loss: 4.2863\n",
            "Sampling at 0:\n",
            "---\n",
            "\n",
            "wzH-MrGlcOkTzIJ&KCO;unbzhIDCJh3kuJjfaEncTKu3IEnajOwUL;lyy.M.sVWE?!HAYXkVj-dJ'dug-!ZW!pqLz$$yhUYIOEI-\n",
            "None\n",
            "---\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 100, Train Loss: 2.4636, Val Loss: 2.4804\n",
            "Sampling at 100:\n",
            "---\n",
            "\n",
            "SIULOrdrnolis arenolI'd andapod!\n",
            "\n",
            "S;\n",
            "\n",
            "Sist!\n",
            "CEThalders ather atha nd brershalyou: win Lin hendat? cu\n",
            "None\n",
            "---\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 200, Train Loss: 2.3894, Val Loss: 2.4139\n",
            "Sampling at 200:\n",
            "---\n",
            "\n",
            "EO:utwe hes gafouperkint fuct't t!\n",
            "Lifr Inod tirer\n",
            "Pathin ned pothallt i'sw horelear; hef aieme iler\n",
            "None\n",
            "---\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 300, Train Loss: 2.2237, Val Loss: 2.2592\n",
            "Sampling at 300:\n",
            "---\n",
            "\n",
            "CENBY:\n",
            "OROr, soughad ip geng shen fee. theverik! nethe hour, tha bupl she st I'tso sornn mecese\n",
            "Whal\n",
            "None\n",
            "---\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 400, Train Loss: 2.0270, Val Loss: 2.0976\n",
            "Sampling at 400:\n",
            "---\n",
            "\n",
            "CAMIUS:\n",
            "I a gear and your deertigh, moce of lay Of mine thigh?\n",
            "\n",
            "RUCHISS:\n",
            "Dicle! hou bo mard dind of \n",
            "None\n",
            "---\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 500, Train Loss: 1.8678, Val Loss: 1.9859\n",
            "Sampling at 500:\n",
            "---\n",
            "\n",
            "Aso! sakciont! it Ye hand my thy 'Tou,\n",
            "Lord shall ong her and tir rents; fathou\n",
            "I had thim have nad \n",
            "None\n",
            "---\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 600, Train Loss: 1.7431, Val Loss: 1.8853\n",
            "Sampling at 600:\n",
            "---\n",
            "\n",
            "Beirs is mon;\n",
            "the fizit her swell enst I forth of moy you lady\n",
            "Engle so so thou wmephing and long ma\n",
            "None\n",
            "---\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 700, Train Loss: 1.6486, Val Loss: 1.8162\n",
            "Sampling at 700:\n",
            "---\n",
            "\n",
            "ANGE:\n",
            "Mord up herene, and the beavent?\n",
            "\n",
            "KING RICHARD II:\n",
            "God to me, is thathou areed begay deed;\n",
            "Whi\n",
            "None\n",
            "---\n",
            "Estimating loss...\n",
            "Estimating loss...\n",
            "Step 800, Train Loss: 1.5666, Val Loss: 1.7455\n",
            "Sampling at 800:\n",
            "---\n",
            "\n",
            "And firgest in the it thought art pawby his\n",
            "Of ouber-bodisphest king.\n",
            "\n",
            "BAUTUS:\n",
            "I came, you a no have\n",
            "None\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swmIrka96AdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMfIX7Uy5eFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}