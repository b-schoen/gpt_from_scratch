{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8136c449-8445-45fd-8d23-358f6e639e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: same thing for REPL\n",
    "# note: we use this instead of magic because `black` will otherwise fail to format\n",
    "#\n",
    "# Enable autoreload to automatically reload modules when they change\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "# do this so that formatter not messed up\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Import commonly used libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# type annotation\n",
    "import jaxtyping\n",
    "from jaxtyping import Float32, Int64, jaxtyped\n",
    "from typeguard import typechecked as typechecker\n",
    "\n",
    "# more itertools\n",
    "import more_itertools as mi\n",
    "\n",
    "# itertools\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "# tensor manipulation\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# automatically apply jaxtyping\n",
    "# %load_ext jaxtyping\n",
    "# %jaxtyping.typechecker typeguard.typechecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91f8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable jaxtyping based typechecking\n",
    "import jaxtyping\n",
    "import typeguard\n",
    "\n",
    "# Inline comment: This magic command enables runtime type checking using jaxtyping and typeguard\n",
    "# ipython.run_line_magic(\"load_ext\", \"jaxtyping\")\n",
    "\n",
    "# Inline comment: This sets the typecheck mode to 'jaxtyping', which allows for more precise tensor shape checking\n",
    "# ipython.run_line_magic(\"jaxtyping.typechecker\", \"typeguard.typechecked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c068f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f86d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(nnsight_api_key)=32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "nnsight_api_key = os.environ[\"NNSIGHT_API_KEY\"]\n",
    "\n",
    "print(f\"{len(nnsight_api_key)=}\")\n",
    "\n",
    "nnsight.CONFIG.set_default_api_key(nnsight_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cfa2175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f1acc0a8aa40b29849b0f7e044a9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed605ffab5a4bbe87acd2453952343d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae3bb6634f04220af5daba821ac206b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdcc8c887e843be94c69a7ce30168d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472b6c4fb1a949f8b847ea0e28b5c869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3940e3574a41c6971c0d93b76fdce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9a28599b154e8a9c497ff23663a522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of heads: 16\n",
      "Number of layers: 28\n",
      "Model dimension: 4096\n",
      "Head dimension: 256\n",
      "\n",
      "Entire config:  GPTJConfig {\n",
      "  \"_name_or_path\": \"EleutherAI/gpt-j-6b\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPTJForCausalLM\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gptj\",\n",
      "  \"n_embd\": 4096,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 28,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary\": true,\n",
      "  \"rotary_dim\": 64,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50,\n",
      "      \"temperature\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50400\n",
      "}\n",
      "\n",
      "{'input_ids': tensor([[1212, 1276,  307, 3635]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "I never could get the hang of Thursdays.\n",
      "['These', ' words', ' will', ' be', ' split', ' up']\n",
      "['This sentence will be together', 'So will this one']\n",
      "['This', 'Ġsentence', 'Ġwill', 'Ġbe', 'Ġtoken', 'ized']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bronsonschoen/gpt_from_scratch/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = nnsight.LanguageModel(\"EleutherAI/gpt-j-6b\", device_map=\"auto\")\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "N_HEADS = model.config.n_head\n",
    "N_LAYERS = model.config.n_layer\n",
    "D_MODEL = model.config.n_embd\n",
    "D_HEAD = D_MODEL // N_HEADS\n",
    "\n",
    "print(f\"Number of heads: {N_HEADS}\")\n",
    "print(f\"Number of layers: {N_LAYERS}\")\n",
    "print(f\"Model dimension: {D_MODEL}\")\n",
    "print(f\"Head dimension: {D_HEAD}\\n\")\n",
    "\n",
    "print(\"Entire config: \", model.config)\n",
    "# %%\n",
    "\n",
    "# Calling tokenizer returns a dictionary, containing input ids & other data.\n",
    "# If returned as a tensor, then by default it will have a batch dimension.\n",
    "print(tokenizer(\"This must be Thursday\", return_tensors=\"pt\"))\n",
    "\n",
    "# Decoding a list of integers, into a concatenated string.\n",
    "print(tokenizer.decode([40, 1239, 714, 651, 262, 8181, 286, 48971, 12545, 13]))\n",
    "\n",
    "# Using batch decode, on both 1D and 2D input.\n",
    "print(tokenizer.batch_decode([4711, 2456, 481, 307, 6626, 510]))\n",
    "print(tokenizer.batch_decode([[1212, 6827, 481, 307, 1978], [2396, 481, 428, 530]]))\n",
    "\n",
    "# Split sentence into tokens (note we see the special Ġ character in place of prepended spaces).\n",
    "print(tokenizer.tokenize(\"This sentence will be tokenized\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e1051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 17:23:49,012 81d891ed-9659-4b95-93fb-134293e2cf1f - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-13 17:23:49,090 81d891ed-9659-4b95-93fb-134293e2cf1f - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-09-13 17:23:49,202 81d891ed-9659-4b95-93fb-134293e2cf1f - RUNNING: Your job has started running.\n",
      "2024-09-13 17:23:49,537 81d891ed-9659-4b95-93fb-134293e2cf1f - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 184k/184k [00:00<00:00, 773kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "logits.shape = torch.Size([50400]) = (vocab_size,)\n",
      "Predicted token ID = 6342\n",
      "Predicted token = ' Paris'\n",
      "\n",
      "resid.shape = torch.Size([1, 10, 4096]) = (batch_size, seq_len, d_model)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The Eiffel Tower is in the city of\"\n",
    "\n",
    "with model.trace(remote=True) as runner:\n",
    "    with runner.invoke(prompt) as invoker:\n",
    "\n",
    "        # Save the model's hidden states\n",
    "        hidden_states = model.transformer.h[-1].output[0].save()\n",
    "\n",
    "        # Save the model's logit output\n",
    "        logits = model.lm_head.output[0, -1].save()\n",
    "\n",
    "        # If you've worked with TransformerLens (or even regular HF models) then you\n",
    "        # might be used to getting logits directly from the model output, but here we\n",
    "        # generally extract logits from the model internals just like any other\n",
    "        # activation because this allows us to control exactly what we return.\n",
    "\n",
    "# Get the model's logit output, and it's next token prediction\n",
    "print(f\"\\nlogits.shape = {logits.value.shape} = (vocab_size,)\")\n",
    "\n",
    "predicted_token_id = logits.value.argmax().item()\n",
    "print(f\"Predicted token ID = {predicted_token_id}\")\n",
    "print(f\"Predicted token = {tokenizer.decode(predicted_token_id)!r}\")\n",
    "\n",
    "# Print the shape of the model's residual stream\n",
    "print(f\"\\nresid.shape = {hidden_states.value.shape} = (batch_size, seq_len, d_model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0901cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 17:24:36,306 dc961223-64f3-4681-987f-70866ac69c7f - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-13 17:24:36,326 dc961223-64f3-4681-987f-70866ac69c7f - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-09-13 17:24:36,358 dc961223-64f3-4681-987f-70866ac69c7f - RUNNING: Your job has started running.\n",
      "2024-09-13 17:24:36,478 dc961223-64f3-4681-987f-70866ac69c7f - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.44k/1.44k [00:00<00:00, 2.82MB/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import TypeVar, TypeVarTuple, Any\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "U = TypeVar(\"U\")\n",
    "\n",
    "\n",
    "def get_first_arg_from_input(proxy: T) -> U:\n",
    "\n",
    "    # get back a tuple, (args, kwargs)\n",
    "    proxy_inputs: tuple[tuple[TypeVarTuple], dict[str, Any]] = proxy.input\n",
    "\n",
    "    proxy_inputs_args: tuple[TypeVarTuple] = proxy_inputs[0]\n",
    "\n",
    "    return proxy_inputs_args[0]\n",
    "\n",
    "\n",
    "with model.trace(remote=True) as runner:\n",
    "\n",
    "    with runner.invoke(prompt) as invoker:\n",
    "\n",
    "        # note: `input` returns (args, kwargs)\n",
    "\n",
    "        attn_patterns_proxy = model.transformer.h[0].attn.attn_dropout.input[0][0]\n",
    "\n",
    "        attn_patterns = attn_patterns_proxy.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd43685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 Head Attention Patterns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-bc753e1e-79b2\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-bc753e1e-79b2\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"The\", \" E\", \"iff\", \"el\", \" Tower\", \" is\", \" in\", \" the\", \" city\", \" of\"], \"attention\": [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8359375, 0.1640625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10302734375, 0.86328125, 0.032470703125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00213623046875, 0.01019287109375, 0.9765625, 0.0108642578125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0198974609375, 0.000530242919921875, 0.8984375, 0.07861328125, 0.0019073486328125, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09814453125, 0.00286865234375, 0.002227783203125, 0.00830078125, 0.1181640625, 0.76953125, 0.0, 0.0, 0.0, 0.0], [0.1337890625, 0.0361328125, 0.00970458984375, 0.0128173828125, 0.0361328125, 0.52734375, 0.2421875, 0.0, 0.0, 0.0], [0.09326171875, 0.0084228515625, 0.0036163330078125, 0.00616455078125, 0.0072021484375, 0.2236328125, 0.04833984375, 0.609375, 0.0, 0.0], [0.02197265625, 0.0057373046875, 0.00555419921875, 0.003631591796875, 0.05615234375, 0.18359375, 0.0654296875, 0.53125, 0.1259765625, 0.0], [0.004302978515625, 0.0009918212890625, 0.0002193450927734375, 0.00135040283203125, 0.005523681640625, 0.035888671875, 0.00830078125, 0.059326171875, 0.72265625, 0.1611328125]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x17d473e00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "\n",
    "# Get string tokens (replacing special character for spaces)\n",
    "str_tokens = model.tokenizer.tokenize(prompt)\n",
    "str_tokens = [s.replace(\"Ġ\", \" \") for s in str_tokens]\n",
    "\n",
    "# Attention patterns (squeeze out the batch dimension)\n",
    "attn_patterns_value = attn_patterns.value.squeeze(0)\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "display(\n",
    "    cv.attention.attention_patterns(\n",
    "        tokens=str_tokens,\n",
    "        attention=attn_patterns_value,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e18ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, one thing to be wary of is that GPT-J uses rotary embeddings, which makes\n",
    "# the computation of attention patterns from keys and queries a bit harder than it\n",
    "# would otherwise be\n",
    "\n",
    "ANTONYM_PAIRS = [\n",
    "    (\"happy\", \"sad\"),\n",
    "    (\"light\", \"dark\"),\n",
    "    (\"hot\", \"cold\"),\n",
    "    (\"big\", \"small\"),\n",
    "    (\"fast\", \"slow\"),\n",
    "    (\"hard\", \"soft\"),\n",
    "    (\"rich\", \"poor\"),\n",
    "    (\"full\", \"empty\"),\n",
    "    (\"up\", \"down\"),\n",
    "    (\"strong\", \"weak\"),\n",
    "    (\"brave\", \"cowardly\"),\n",
    "    (\"young\", \"old\"),\n",
    "    (\"new\", \"old\"),\n",
    "    (\"clean\", \"dirty\"),\n",
    "    (\"near\", \"far\"),\n",
    "    (\"sharp\", \"blunt\"),\n",
    "    (\"quiet\", \"loud\"),\n",
    "    (\"hard\", \"easy\"),\n",
    "    (\"thick\", \"thin\"),\n",
    "    (\"wet\", \"dry\"),\n",
    "    (\"open\", \"closed\"),\n",
    "    (\"happy\", \"sad\"),\n",
    "    (\"love\", \"hate\"),\n",
    "    (\"success\", \"failure\"),\n",
    "    (\"yes\", \"no\"),\n",
    "    (\"buy\", \"sell\"),\n",
    "    (\"true\", \"false\"),\n",
    "    (\"defend\", \"attack\"),\n",
    "    (\"accept\", \"refuse\"),\n",
    "    (\"included\", \"excluded\"),\n",
    "    (\"acceptance\", \"rejection\"),\n",
    "    (\"advance\", \"retreat\"),\n",
    "    (\"gain\", \"loss\"),\n",
    "    (\"believe\", \"doubt\"),\n",
    "    (\"attract\", \"repel\"),\n",
    "    (\"increase\", \"decrease\"),\n",
    "    (\"win\", \"lose\"),\n",
    "    (\"visible\", \"invisible\"),\n",
    "    (\"active\", \"inactive\"),\n",
    "    (\"complex\", \"simple\"),\n",
    "    (\"ignore\", \"acknowledge\"),\n",
    "    (\"encourage\", \"discourage\"),\n",
    "    (\"assemble\", \"disperse\"),\n",
    "    (\"mature\", \"immature\"),\n",
    "    (\"gain\", \"lose\"),\n",
    "    (\"new\", \"used\"),\n",
    "    (\"cooperate\", \"compete\"),\n",
    "    (\"begin\", \"end\"),\n",
    "    (\"create\", \"destroy\"),\n",
    "    (\"expand\", \"contract\"),\n",
    "    (\"develop\", \"regress\"),\n",
    "    (\"succeed\", \"fail\"),\n",
    "    (\"connect\", \"disconnect\"),\n",
    "    (\"expand\", \"shrink\"),\n",
    "    (\"introduce\", \"withdraw\"),\n",
    "    (\"safety\", \"danger\"),\n",
    "    (\"satisfaction\", \"discontent\"),\n",
    "    (\"freedom\", \"restriction\"),\n",
    "    (\"strength\", \"weakness\"),\n",
    "    (\"joy\", \"sorrow\"),\n",
    "    (\"truth\", \"falsehood\"),\n",
    "    (\"acceptance\", \"rejection\"),\n",
    "]\n",
    "\n",
    "ANTONYM_PAIRS = [list(x) for x in ANTONYM_PAIRS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b65d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple-representation of the sequence:\n",
      "(hot, cold), (yes, no), (in, out), up ->\n",
      "\n",
      "Actual prompt, which will be fed into the model:\n",
      "Q: hot\n",
      "A: cold\n",
      "\n",
      "Q: yes\n",
      "A: no\n",
      "\n",
      "Q: in\n",
      "A: out\n",
      "\n",
      "Q: up\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "class ICLSequence:\n",
    "    \"\"\"\n",
    "    Class to store a single antonym sequence.\n",
    "\n",
    "    Uses the default template \"Q: {x}\\nA: {y}\" (with separate pairs split by \"\\n\\n\").\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_pairs: list[tuple[str, str]]):\n",
    "        self.word_pairs = word_pairs\n",
    "        self.x, self.y = zip(*word_pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.word_pairs[idx]\n",
    "\n",
    "    def prompt(self):\n",
    "        \"\"\"Returns the prompt, which contains all but the second element in the last word pair.\"\"\"\n",
    "        p = \"\\n\\n\".join([f\"Q: {x}\\nA: {y}\" for x, y in self.word_pairs])\n",
    "        return p[: -len(self.completion())]\n",
    "\n",
    "    def completion(self):\n",
    "        \"\"\"Returns the second element in the last word pair (with padded space).\"\"\"\n",
    "        return \" \" + self.y[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Prints a readable string representation of the prompt & completion (indep of template).\"\"\"\n",
    "        return f\"{', '.join([f'({x}, {y})' for x, y in self[:-1]])}, {self.x[-1]} ->\".strip(\n",
    "            \", \"\n",
    "        )\n",
    "\n",
    "\n",
    "word_list = [[\"hot\", \"cold\"], [\"yes\", \"no\"], [\"in\", \"out\"], [\"up\", \"down\"]]\n",
    "seq = ICLSequence(word_list)\n",
    "\n",
    "print(\"Tuple-representation of the sequence:\")\n",
    "print(seq)\n",
    "print(\"\\nActual prompt, which will be fed into the model:\")\n",
    "print(seq.prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9394d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class ICLDataset:\n",
    "    \"\"\"\n",
    "    Dataset to create antonym pair prompts, in ICL task format. We use random seeds for consistency\n",
    "    between the corrupted and clean datasets.\n",
    "\n",
    "    Note:\n",
    "\n",
    "        Note that the correct completions have a prepended space!!!\n",
    "\n",
    "    Inputs:\n",
    "        word_pairs:\n",
    "            list of ICL task, e.g. [[\"old\", \"young\"], [\"top\", \"bottom\"], ...] for the antonym task\n",
    "        size:\n",
    "            number of prompts to generate\n",
    "        n_prepended:\n",
    "            number of antonym pairs before the single-word ICL task\n",
    "        bidirectional:\n",
    "            if True, then we also consider the reversed antonym pairs\n",
    "        corrupted:\n",
    "            if True, then the second word in each pair is replaced with a random word\n",
    "        seed:\n",
    "            random seed, for consistency & reproducibility\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_pairs: list[str],\n",
    "        size: int,\n",
    "        n_prepended: int,\n",
    "        bidirectional: bool = True,\n",
    "        seed: int = 0,\n",
    "        corrupted: bool = False,\n",
    "    ):\n",
    "        assert n_prepended + 1 <= len(\n",
    "            word_pairs\n",
    "        ), \"Not enough antonym pairs in dataset to create prompt.\"\n",
    "\n",
    "        self.word_pairs = copy.deepcopy(word_pairs)\n",
    "        self.word_list = [word for word_pair in word_pairs for word in word_pair]\n",
    "        self.size = size\n",
    "        self.n_prepended = n_prepended\n",
    "        self.bidirectional = bidirectional\n",
    "        self.corrupted = corrupted\n",
    "        self.seed = seed\n",
    "\n",
    "        self.seqs = []\n",
    "        self.prompts = []\n",
    "        self.completions = []\n",
    "\n",
    "        # Generate the dataset (by choosing random antonym pairs, and constructing `ICLSequence` objects)\n",
    "        for n in range(size):\n",
    "            np.random.seed(seed + n)\n",
    "            random_pairs = np.random.choice(\n",
    "                len(self.word_pairs), n_prepended + 1, replace=False\n",
    "            )\n",
    "            random_orders = np.random.choice([1, -1], n_prepended + 1)\n",
    "            if not (bidirectional):\n",
    "                random_orders[:] = 1\n",
    "            word_pairs = [\n",
    "                self.word_pairs[pair][::order]\n",
    "                for pair, order in zip(random_pairs, random_orders)\n",
    "            ]\n",
    "            if corrupted:\n",
    "                for i in range(len(word_pairs) - 1):\n",
    "                    word_pairs[i][1] = np.random.choice(self.word_list)\n",
    "            seq = ICLSequence(word_pairs)\n",
    "\n",
    "            self.seqs.append(seq)\n",
    "            self.prompts.append(seq.prompt())\n",
    "            self.completions.append(seq.completion())\n",
    "\n",
    "    def create_corrupted_dataset(self):\n",
    "        \"\"\"Creates a corrupted version of the dataset (with same random seed).\"\"\"\n",
    "        return ICLDataset(\n",
    "            self.word_pairs,\n",
    "            self.size,\n",
    "            self.n_prepended,\n",
    "            self.bidirectional,\n",
    "            corrupted=True,\n",
    "            seed=self.seed,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bb01e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Prompt                                            </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ (rejection, acceptance), (gain, loss), believe -&gt; │ ' doubt'           │\n",
       "│ (sad, happy), (disconnect, connect), active -&gt;    │ ' inactive'        │\n",
       "│ (win, lose), (happy, sad), destroy -&gt;             │ ' create'          │\n",
       "│ (destroy, create), (thick, thin), weakness -&gt;     │ ' strength'        │\n",
       "│ (slow, fast), (retreat, advance), attack -&gt;       │ ' defend'          │\n",
       "│ (disperse, assemble), (cowardly, brave), easy -&gt;  │ ' hard'            │\n",
       "│ (gain, loss), (empty, full), sad -&gt;               │ ' happy'           │\n",
       "│ (blunt, sharp), (hot, cold), decrease -&gt;          │ ' increase'        │\n",
       "│ (light, dark), (begin, end), shrink -&gt;            │ ' expand'          │\n",
       "│ (repel, attract), (near, far), poor -&gt;            │ ' rich'            │\n",
       "└───────────────────────────────────────────────────┴────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPrompt                                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ (rejection, acceptance), (gain, loss), believe -> │ ' doubt'           │\n",
       "│ (sad, happy), (disconnect, connect), active ->    │ ' inactive'        │\n",
       "│ (win, lose), (happy, sad), destroy ->             │ ' create'          │\n",
       "│ (destroy, create), (thick, thin), weakness ->     │ ' strength'        │\n",
       "│ (slow, fast), (retreat, advance), attack ->       │ ' defend'          │\n",
       "│ (disperse, assemble), (cowardly, brave), easy ->  │ ' hard'            │\n",
       "│ (gain, loss), (empty, full), sad ->               │ ' happy'           │\n",
       "│ (blunt, sharp), (hot, cold), decrease ->          │ ' increase'        │\n",
       "│ (light, dark), (begin, end), shrink ->            │ ' expand'          │\n",
       "│ (repel, attract), (near, far), poor ->            │ ' rich'            │\n",
       "└───────────────────────────────────────────────────┴────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import rich\n",
    "import rich.table\n",
    "\n",
    "dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=False)\n",
    "\n",
    "table = rich.table.Table(\"Prompt\", \"Correct completion\")\n",
    "for seq, completion in zip(dataset.seqs, dataset.completions):\n",
    "    table.add_row(str(seq), repr(completion))\n",
    "\n",
    "rich.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed68a433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Prompt                                               </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ (rejection, strength), (gain, down), believe -&gt;      │ ' doubt'           │\n",
       "│ (sad, empty), (disconnect, gain), active -&gt;          │ ' inactive'        │\n",
       "│ (win, create), (happy, sad), destroy -&gt;              │ ' create'          │\n",
       "│ (destroy, develop), (thick, contract), weakness -&gt;   │ ' strength'        │\n",
       "│ (slow, satisfaction), (retreat, disperse), attack -&gt; │ ' defend'          │\n",
       "│ (disperse, up), (cowardly, begin), easy -&gt;           │ ' hard'            │\n",
       "│ (gain, thick), (empty, invisible), sad -&gt;            │ ' happy'           │\n",
       "│ (blunt, loud), (hot, rejection), decrease -&gt;         │ ' increase'        │\n",
       "│ (light, acceptance), (begin, success), shrink -&gt;     │ ' expand'          │\n",
       "│ (repel, satisfaction), (near, strength), poor -&gt;     │ ' rich'            │\n",
       "└──────────────────────────────────────────────────────┴────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPrompt                                              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ (rejection, strength), (gain, down), believe ->      │ ' doubt'           │\n",
       "│ (sad, empty), (disconnect, gain), active ->          │ ' inactive'        │\n",
       "│ (win, create), (happy, sad), destroy ->              │ ' create'          │\n",
       "│ (destroy, develop), (thick, contract), weakness ->   │ ' strength'        │\n",
       "│ (slow, satisfaction), (retreat, disperse), attack -> │ ' defend'          │\n",
       "│ (disperse, up), (cowardly, begin), easy ->           │ ' hard'            │\n",
       "│ (gain, thick), (empty, invisible), sad ->            │ ' happy'           │\n",
       "│ (blunt, loud), (hot, rejection), decrease ->         │ ' increase'        │\n",
       "│ (light, acceptance), (begin, success), shrink ->     │ ' expand'          │\n",
       "│ (repel, satisfaction), (near, strength), poor ->     │ ' rich'            │\n",
       "└──────────────────────────────────────────────────────┴────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = ICLDataset(ANTONYM_PAIRS, size=10, n_prepended=2, corrupted=True)\n",
    "\n",
    "table = rich.table.Table(\"Prompt\", \"Correct completion\")\n",
    "for seq, completions in zip(dataset.seqs, dataset.completions):\n",
    "    table.add_row(str(seq), repr(completions))\n",
    "\n",
    "rich.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4344d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_h(\n",
    "    model: nnsight.LanguageModel,\n",
    "    dataset: ICLDataset,\n",
    "    layer: int = -1,\n",
    ") -> tuple[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Averages over the model's hidden representations on each of the prompts in\n",
    "    `dataset` at layer `layer`, to produce a single vector `h`.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        dataset: ICLDataset\n",
    "            the dataset whose prompts `dataset.prompts` you're extracting the\n",
    "            activations from (at the last seq pos)\n",
    "        layer: int\n",
    "            the layer you're extracting activations from\n",
    "\n",
    "    Returns:\n",
    "        completions: str]\n",
    "            list of model completion strings (i.e. the strings the model predicts to\n",
    "            follow the last token)\n",
    "        h: Tensor\n",
    "            average hidden state tensor at final sequence position, of shape (d_model,)\n",
    "\n",
    "    \"\"\"\n",
    "    with model.trace(remote=True) as runner:\n",
    "        with runner.invoke(dataset.prompts) as invoker:\n",
    "\n",
    "            h = model.transformer.h[layer].output[0][:, -1].mean(dim=0).save()\n",
    "\n",
    "            logits = model.lm_head.output[:, -1]\n",
    "            token_ids = logits.argmax(dim=-1).save()\n",
    "\n",
    "    completions = model.tokenizer.batch_decode(token_ids.value)\n",
    "\n",
    "    return completions, h.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "453d3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_completions_on_antonyms(\n",
    "    model: nnsight.LanguageModel,\n",
    "    dataset: ICLDataset,\n",
    "    completions: str,\n",
    "    num_to_display: int = 20,\n",
    ") -> None:\n",
    "    table = rich.table.Table(\n",
    "        \"Prompt (tuple representation)\",\n",
    "        \"Model's completion\\n(green=correct)\",\n",
    "        \"Correct completion\",\n",
    "        title=\"Model's antonym completions\",\n",
    "    )\n",
    "\n",
    "    for i in range(min(len(completions), num_to_display)):\n",
    "\n",
    "        # Get model's completion, and correct completion\n",
    "        completion = completions[i]\n",
    "        correct_completion = dataset.completions[i]\n",
    "        correct_completion_first_token = model.tokenizer.tokenize(correct_completion)[\n",
    "            0\n",
    "        ].replace(\"Ġ\", \" \")\n",
    "        seq = dataset.seqs[i]\n",
    "\n",
    "        # Color code the completion based on whether it's correct\n",
    "        is_correct = completion == correct_completion_first_token\n",
    "        completion = (\n",
    "            f\"[b green]{repr(completion)}[/]\" if is_correct else repr(completion)\n",
    "        )\n",
    "\n",
    "        table.add_row(str(seq), completion, repr(correct_completion))\n",
    "\n",
    "    rich.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "588d14bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 17:55:04,071 8c9fd27b-cff0-4c02-bf74-f711ed01253b - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-13 17:55:04,130 8c9fd27b-cff0-4c02-bf74-f711ed01253b - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-09-13 17:55:04,199 8c9fd27b-cff0-4c02-bf74-f711ed01253b - RUNNING: Your job has started running.\n",
      "2024-09-13 17:55:04,417 8c9fd27b-cff0-4c02-bf74-f711ed01253b - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 9.82k/9.82k [00:00<00:00, 157kB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                     Model's antonym completions                                     </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                                                         </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\">                    </span>┃\n",
       "┃<span style=\"font-weight: bold\"> Prompt (tuple representation)                           </span>┃<span style=\"font-weight: bold\"> (green=correct)    </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ (rejection, acceptance), (gain, loss), believe -&gt;       │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' doubt'</span>           │ ' doubt'           │\n",
       "│ (sad, happy), (disconnect, connect), active -&gt;          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' inactive'</span>        │ ' inactive'        │\n",
       "│ (win, lose), (happy, sad), destroy -&gt;                   │ ' build'           │ ' create'          │\n",
       "│ (destroy, create), (thick, thin), weakness -&gt;           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' strength'</span>        │ ' strength'        │\n",
       "│ (slow, fast), (retreat, advance), attack -&gt;             │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' defend'</span>          │ ' defend'          │\n",
       "│ (disperse, assemble), (cowardly, brave), easy -&gt;        │ ' difficult'       │ ' hard'            │\n",
       "│ (gain, loss), (empty, full), sad -&gt;                     │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' happy'</span>           │ ' happy'           │\n",
       "│ (blunt, sharp), (hot, cold), decrease -&gt;                │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' increase'</span>        │ ' increase'        │\n",
       "│ (light, dark), (begin, end), shrink -&gt;                  │ ' grow'            │ ' expand'          │\n",
       "│ (repel, attract), (near, far), poor -&gt;                  │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' rich'</span>            │ ' rich'            │\n",
       "│ (open, closed), (hot, cold), advance -&gt;                 │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' retreat'</span>         │ ' retreat'         │\n",
       "│ (strong, weak), (cooperate, compete), increase -&gt;       │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' decrease'</span>        │ ' decrease'        │\n",
       "│ (accept, refuse), (freedom, restriction), hard -&gt;       │ ' soft'            │ ' easy'            │\n",
       "│ (safety, danger), (slow, fast), assemble -&gt;             │ ' dis'             │ ' disperse'        │\n",
       "│ (rich, poor), (included, excluded), joy -&gt;              │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' sorrow'</span>          │ ' sorrow'          │\n",
       "│ (hate, love), (acknowledge, ignore), regress -&gt;         │ ' progress'        │ ' develop'         │\n",
       "│ (lose, gain), (new, used), hot -&gt;                       │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' cold'</span>            │ ' cold'            │\n",
       "│ (quiet, loud), (acknowledge, ignore), truth -&gt;          │ ' lie'             │ ' falsehood'       │\n",
       "│ (encourage, discourage), (invisible, visible), doubt -&gt; │ ' certainty'       │ ' believe'         │\n",
       "│ (thick, thin), (invisible, visible), acceptance -&gt;      │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' rejection'</span>       │ ' rejection'       │\n",
       "└─────────────────────────────────────────────────────────┴────────────────────┴────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                     Model's antonym completions                                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m                                                         \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                    \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPrompt (tuple representation)                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(green=correct)   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ (rejection, acceptance), (gain, loss), believe ->       │ \u001b[1;32m' doubt'\u001b[0m           │ ' doubt'           │\n",
       "│ (sad, happy), (disconnect, connect), active ->          │ \u001b[1;32m' inactive'\u001b[0m        │ ' inactive'        │\n",
       "│ (win, lose), (happy, sad), destroy ->                   │ ' build'           │ ' create'          │\n",
       "│ (destroy, create), (thick, thin), weakness ->           │ \u001b[1;32m' strength'\u001b[0m        │ ' strength'        │\n",
       "│ (slow, fast), (retreat, advance), attack ->             │ \u001b[1;32m' defend'\u001b[0m          │ ' defend'          │\n",
       "│ (disperse, assemble), (cowardly, brave), easy ->        │ ' difficult'       │ ' hard'            │\n",
       "│ (gain, loss), (empty, full), sad ->                     │ \u001b[1;32m' happy'\u001b[0m           │ ' happy'           │\n",
       "│ (blunt, sharp), (hot, cold), decrease ->                │ \u001b[1;32m' increase'\u001b[0m        │ ' increase'        │\n",
       "│ (light, dark), (begin, end), shrink ->                  │ ' grow'            │ ' expand'          │\n",
       "│ (repel, attract), (near, far), poor ->                  │ \u001b[1;32m' rich'\u001b[0m            │ ' rich'            │\n",
       "│ (open, closed), (hot, cold), advance ->                 │ \u001b[1;32m' retreat'\u001b[0m         │ ' retreat'         │\n",
       "│ (strong, weak), (cooperate, compete), increase ->       │ \u001b[1;32m' decrease'\u001b[0m        │ ' decrease'        │\n",
       "│ (accept, refuse), (freedom, restriction), hard ->       │ ' soft'            │ ' easy'            │\n",
       "│ (safety, danger), (slow, fast), assemble ->             │ ' dis'             │ ' disperse'        │\n",
       "│ (rich, poor), (included, excluded), joy ->              │ \u001b[1;32m' sorrow'\u001b[0m          │ ' sorrow'          │\n",
       "│ (hate, love), (acknowledge, ignore), regress ->         │ ' progress'        │ ' develop'         │\n",
       "│ (lose, gain), (new, used), hot ->                       │ \u001b[1;32m' cold'\u001b[0m            │ ' cold'            │\n",
       "│ (quiet, loud), (acknowledge, ignore), truth ->          │ ' lie'             │ ' falsehood'       │\n",
       "│ (encourage, discourage), (invisible, visible), doubt -> │ ' certainty'       │ ' believe'         │\n",
       "│ (thick, thin), (invisible, visible), acceptance ->      │ \u001b[1;32m' rejection'\u001b[0m       │ ' rejection'       │\n",
       "└─────────────────────────────────────────────────────────┴────────────────────┴────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get uncorrupted dataset\n",
    "dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=2)\n",
    "\n",
    "# Getting it from layer 12, as in the description in section 2.1 of paper\n",
    "model_completions, h = calculate_h(model, dataset, layer=12)\n",
    "\n",
    "# Displaying the output\n",
    "display_model_completions_on_antonyms(model, dataset, model_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2122d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene_with_h(\n",
    "    model: nnsight.LanguageModel,\n",
    "    zero_shot_dataset: ICLDataset,\n",
    "    h: torch.Tensor,\n",
    "    layer: int,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Extracts the vector `h` using previously defined function, and intervenes by adding `h` to the\n",
    "    residual stream of a set of generated zero-shot prompts.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        zero_shot_dataset: ICLDataset\n",
    "            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector\n",
    "        h: Tensor\n",
    "            the `h`-vector we'll be adding to the residual stream\n",
    "        layer: int\n",
    "            the layer we'll be extracting the `h`-vector from\n",
    "\n",
    "    Returns:\n",
    "        completions_zero_shot: list[str]\n",
    "            list of string completions for the zero-shot prompts, without intervention\n",
    "        completions_intervention: list[str]\n",
    "            list of string completions for the zero-shot prompts, with h-intervention\n",
    "    \"\"\"\n",
    "    with model.trace(remote=True) as runner:\n",
    "\n",
    "        # First, run a forward pass where we don't intervene, just save token id completions\n",
    "        with runner.invoke(zero_shot_dataset.prompts) as invoker:\n",
    "            token_completions_zero_shot = (\n",
    "                model.lm_head.output[:, -1].argmax(dim=-1).save()\n",
    "            )\n",
    "\n",
    "        # Next, run a forward pass on the zero-shot prompts where we do intervene\n",
    "        with runner.invoke(zero_shot_dataset.prompts) as invoker:\n",
    "            # Add the h-vector to the residual stream, at the last sequence position\n",
    "            hidden_states = model.transformer.h[layer].output[0]\n",
    "            hidden_states[:, -1] += h\n",
    "            # Also save completions\n",
    "            token_completions_intervention = (\n",
    "                model.lm_head.output[:, -1].argmax(dim=-1).save()\n",
    "            )\n",
    "\n",
    "    # Decode to get the string tokens\n",
    "    completions_zero_shot = model.tokenizer.batch_decode(\n",
    "        token_completions_zero_shot.value\n",
    "    )\n",
    "    completions_intervention = model.tokenizer.batch_decode(\n",
    "        token_completions_intervention.value\n",
    "    )\n",
    "\n",
    "    return completions_zero_shot, completions_intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81f8f834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 17:55:39,657 54e99311-d381-40d1-9a3b-5134666a2277 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-13 17:55:39,678 54e99311-d381-40d1-9a3b-5134666a2277 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-09-13 17:55:39,706 54e99311-d381-40d1-9a3b-5134666a2277 - RUNNING: Your job has started running.\n",
      "2024-09-13 17:55:39,922 54e99311-d381-40d1-9a3b-5134666a2277 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 9.82k/9.82k [00:00<00:00, 145kB/s]\n",
      "2024-09-13 17:55:41,182 bb78d644-d2cb-4ee7-83e3-cd3af8d6c585 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-13 17:55:41,215 bb78d644-d2cb-4ee7-83e3-cd3af8d6c585 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-09-13 17:55:41,238 bb78d644-d2cb-4ee7-83e3-cd3af8d6c585 - RUNNING: Your job has started running.\n",
      "2024-09-13 17:55:41,373 bb78d644-d2cb-4ee7-83e3-cd3af8d6c585 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.75k/1.75k [00:00<00:00, 5.01MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-shot completions:  [' sad', ' win', ' destroy', ' slow', ' 1', ' gain', ' blunt', ' light', ' rep', ' open', ' strong', ' accept', ' yes', ' rich', ' hate', ' lose', ' quiet', ' encourage', ' thick', ' fast']\n",
      "Completions with intervention:  [' happy', ' win', ' destroy', ' fast', ' disperse', ' loss', ' blunt', ' dark', ' rep', ' closed', ' weak', ' accept', ' safety', ' poor', ' love', ' lose', ' quiet', ' discourage', ' thin', ' slow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Note, it's very important that we set a different random seed for the zero shot\n",
    "# dataset, otherwise we'll be intervening on examples which were actually in the\n",
    "# dataset we used to compute h!\n",
    "\n",
    "layer = 12\n",
    "dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)\n",
    "zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)\n",
    "\n",
    "# Run previous function to get h-vector\n",
    "h = calculate_h(model, dataset, layer=layer)[1]\n",
    "\n",
    "# Run new function to intervene with h-vector\n",
    "completions_zero_shot, completions_intervention = intervene_with_h(\n",
    "    model,\n",
    "    zero_shot_dataset,\n",
    "    h,\n",
    "    layer=layer,\n",
    ")\n",
    "\n",
    "print(\"\\nZero-shot completions: \", completions_zero_shot)\n",
    "print(\"Completions with intervention: \", completions_intervention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71e820a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_completions_on_h_intervention(\n",
    "    dataset: ICLDataset,\n",
    "    completions: list[str],\n",
    "    completions_intervention: list[str],\n",
    "    num_to_display: int = 20,\n",
    ") -> None:\n",
    "    table = rich.table.Table(\n",
    "        \"Prompt\",\n",
    "        \"Model's completion\\n(no intervention)\",\n",
    "        \"Model's completion\\n(intervention)\",\n",
    "        \"Correct completion\",\n",
    "        title=\"Model's antonym completions\",\n",
    "    )\n",
    "\n",
    "    for i in range(min(len(completions), num_to_display)):\n",
    "\n",
    "        completion_ni = completions[i]\n",
    "        completion_i = completions_intervention[i]\n",
    "        correct_completion = dataset.completions[i]\n",
    "        correct_completion_first_token = tokenizer.tokenize(correct_completion)[\n",
    "            0\n",
    "        ].replace(\"Ġ\", \" \")\n",
    "        seq = dataset.seqs[i]\n",
    "\n",
    "        # Color code the completion based on whether it's correct\n",
    "        is_correct = completion_i == correct_completion_first_token\n",
    "        completion_i = (\n",
    "            f\"[b green]{repr(completion_i)}[/]\" if is_correct else repr(completion_i)\n",
    "        )\n",
    "\n",
    "        table.add_row(\n",
    "            str(seq), repr(completion_ni), completion_i, repr(correct_completion)\n",
    "        )\n",
    "\n",
    "    rich.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a394ae93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Model's antonym completions                          </span>\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">              </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\">                    </span>┃\n",
       "┃<span style=\"font-weight: bold\"> Prompt       </span>┃<span style=\"font-weight: bold\"> (no intervention)  </span>┃<span style=\"font-weight: bold\"> (intervention)     </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ sad -&gt;       │ ' sad'             │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' happy'</span>           │ ' happy'           │\n",
       "│ win -&gt;       │ ' win'             │ ' win'             │ ' lose'            │\n",
       "│ destroy -&gt;   │ ' destroy'         │ ' destroy'         │ ' create'          │\n",
       "│ slow -&gt;      │ ' slow'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' fast'</span>            │ ' fast'            │\n",
       "│ disperse -&gt;  │ ' 1'               │ ' disperse'        │ ' assemble'        │\n",
       "│ gain -&gt;      │ ' gain'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' loss'</span>            │ ' loss'            │\n",
       "│ blunt -&gt;     │ ' blunt'           │ ' blunt'           │ ' sharp'           │\n",
       "│ light -&gt;     │ ' light'           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' dark'</span>            │ ' dark'            │\n",
       "│ repel -&gt;     │ ' rep'             │ ' rep'             │ ' attract'         │\n",
       "│ open -&gt;      │ ' open'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' closed'</span>          │ ' closed'          │\n",
       "│ strong -&gt;    │ ' strong'          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' weak'</span>            │ ' weak'            │\n",
       "│ accept -&gt;    │ ' accept'          │ ' accept'          │ ' refuse'          │\n",
       "│ safety -&gt;    │ ' yes'             │ ' safety'          │ ' danger'          │\n",
       "│ rich -&gt;      │ ' rich'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' poor'</span>            │ ' poor'            │\n",
       "│ hate -&gt;      │ ' hate'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' love'</span>            │ ' love'            │\n",
       "│ lose -&gt;      │ ' lose'            │ ' lose'            │ ' gain'            │\n",
       "│ quiet -&gt;     │ ' quiet'           │ ' quiet'           │ ' loud'            │\n",
       "│ encourage -&gt; │ ' encourage'       │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' discourage'</span>      │ ' discourage'      │\n",
       "│ thick -&gt;     │ ' thick'           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' thin'</span>            │ ' thin'            │\n",
       "│ fast -&gt;      │ ' fast'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' slow'</span>            │ ' slow'            │\n",
       "└──────────────┴────────────────────┴────────────────────┴────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                          Model's antonym completions                          \u001b[0m\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m              \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                    \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPrompt      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention)    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ sad ->       │ ' sad'             │ \u001b[1;32m' happy'\u001b[0m           │ ' happy'           │\n",
       "│ win ->       │ ' win'             │ ' win'             │ ' lose'            │\n",
       "│ destroy ->   │ ' destroy'         │ ' destroy'         │ ' create'          │\n",
       "│ slow ->      │ ' slow'            │ \u001b[1;32m' fast'\u001b[0m            │ ' fast'            │\n",
       "│ disperse ->  │ ' 1'               │ ' disperse'        │ ' assemble'        │\n",
       "│ gain ->      │ ' gain'            │ \u001b[1;32m' loss'\u001b[0m            │ ' loss'            │\n",
       "│ blunt ->     │ ' blunt'           │ ' blunt'           │ ' sharp'           │\n",
       "│ light ->     │ ' light'           │ \u001b[1;32m' dark'\u001b[0m            │ ' dark'            │\n",
       "│ repel ->     │ ' rep'             │ ' rep'             │ ' attract'         │\n",
       "│ open ->      │ ' open'            │ \u001b[1;32m' closed'\u001b[0m          │ ' closed'          │\n",
       "│ strong ->    │ ' strong'          │ \u001b[1;32m' weak'\u001b[0m            │ ' weak'            │\n",
       "│ accept ->    │ ' accept'          │ ' accept'          │ ' refuse'          │\n",
       "│ safety ->    │ ' yes'             │ ' safety'          │ ' danger'          │\n",
       "│ rich ->      │ ' rich'            │ \u001b[1;32m' poor'\u001b[0m            │ ' poor'            │\n",
       "│ hate ->      │ ' hate'            │ \u001b[1;32m' love'\u001b[0m            │ ' love'            │\n",
       "│ lose ->      │ ' lose'            │ ' lose'            │ ' gain'            │\n",
       "│ quiet ->     │ ' quiet'           │ ' quiet'           │ ' loud'            │\n",
       "│ encourage -> │ ' encourage'       │ \u001b[1;32m' discourage'\u001b[0m      │ ' discourage'      │\n",
       "│ thick ->     │ ' thick'           │ \u001b[1;32m' thin'\u001b[0m            │ ' thin'            │\n",
       "│ fast ->      │ ' fast'            │ \u001b[1;32m' slow'\u001b[0m            │ ' slow'            │\n",
       "└──────────────┴────────────────────┴────────────────────┴────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_model_completions_on_h_intervention(\n",
    "    zero_shot_dataset,\n",
    "    completions_zero_shot,\n",
    "    completions_intervention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ba6fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_h_and_intervene(\n",
    "    model: nnsight.LanguageModel,\n",
    "    dataset: ICLDataset,\n",
    "    zero_shot_dataset: ICLDataset,\n",
    "    layer: int,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,\n",
    "    all within the same forward pass. Returns the completions from this intervention.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the model we're using to generate completions\n",
    "        dataset: ICLDataset\n",
    "            the dataset of clean prompts from which we'll extract the `h`-vector\n",
    "        zero_shot_dataset: ICLDataset\n",
    "            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector\n",
    "        layer: int\n",
    "            the layer we'll be extracting the `h`-vector from\n",
    "\n",
    "    Returns:\n",
    "        completions_zero_shot: list[str]\n",
    "            list of string completions for the zero-shot prompts, without intervention\n",
    "        completions_intervention: list[str]\n",
    "            list of string completions for the zero-shot prompts, with h-intervention\n",
    "    \"\"\"\n",
    "    with model.trace(remote=True) as runner:\n",
    "\n",
    "        # Run on the clean prompts, to get the h-vector\n",
    "        with runner.invoke(dataset.prompts) as invoker:\n",
    "            # Define h (we don't need to save it, cause we don't need it outside `runner:`)\n",
    "            hidden_states = model.transformer.h[layer].output[0]\n",
    "            h = hidden_states[:, -1].mean(dim=0)\n",
    "\n",
    "        # First, run a forward pass where we don't intervene\n",
    "        with runner.invoke(zero_shot_dataset.prompts) as invoker:\n",
    "            token_completions_zero_shot = (\n",
    "                model.lm_head.output[:, -1].argmax(dim=-1).save()\n",
    "            )\n",
    "\n",
    "        # Next, run a forward pass on the zero-shot prompts where we do intervene\n",
    "        with runner.invoke(zero_shot_dataset.prompts) as invoker:\n",
    "            # Add the h-vector to the residual stream, at the last sequence position\n",
    "            hidden_states = model.transformer.h[layer].output[0]\n",
    "            hidden_states[:, -1] += h\n",
    "            # Also save completions\n",
    "            token_completions_intervention = (\n",
    "                model.lm_head.output[:, -1].argmax(dim=-1).save()\n",
    "            )\n",
    "\n",
    "    # Decode to get the string tokens\n",
    "    completions_zero_shot = model.tokenizer.batch_decode(\n",
    "        token_completions_zero_shot.value\n",
    "    )\n",
    "    completions_intervention = model.tokenizer.batch_decode(\n",
    "        token_completions_intervention.value\n",
    "    )\n",
    "\n",
    "    return completions_zero_shot, completions_intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf2ee0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 17:56:39,912 6cc48d41-5503-4a89-a7c1-a5efb4374c6e - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-13 17:56:39,944 6cc48d41-5503-4a89-a7c1-a5efb4374c6e - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-09-13 17:56:39,974 6cc48d41-5503-4a89-a7c1-a5efb4374c6e - RUNNING: Your job has started running.\n",
      "2024-09-13 17:56:40,422 6cc48d41-5503-4a89-a7c1-a5efb4374c6e - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.75k/1.75k [00:00<00:00, 4.64MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                          Model's antonym completions                          </span>\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">              </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\"> Model's completion </span>┃<span style=\"font-weight: bold\">                    </span>┃\n",
       "┃<span style=\"font-weight: bold\"> Prompt       </span>┃<span style=\"font-weight: bold\"> (no intervention)  </span>┃<span style=\"font-weight: bold\"> (intervention)     </span>┃<span style=\"font-weight: bold\"> Correct completion </span>┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ sad -&gt;       │ ' sad'             │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' happy'</span>           │ ' happy'           │\n",
       "│ win -&gt;       │ ' win'             │ ' win'             │ ' lose'            │\n",
       "│ destroy -&gt;   │ ' destroy'         │ ' destroy'         │ ' create'          │\n",
       "│ slow -&gt;      │ ' slow'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' fast'</span>            │ ' fast'            │\n",
       "│ disperse -&gt;  │ ' 1'               │ ' disperse'        │ ' assemble'        │\n",
       "│ gain -&gt;      │ ' gain'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' loss'</span>            │ ' loss'            │\n",
       "│ blunt -&gt;     │ ' blunt'           │ ' blunt'           │ ' sharp'           │\n",
       "│ light -&gt;     │ ' light'           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' dark'</span>            │ ' dark'            │\n",
       "│ repel -&gt;     │ ' rep'             │ ' rep'             │ ' attract'         │\n",
       "│ open -&gt;      │ ' open'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' closed'</span>          │ ' closed'          │\n",
       "│ strong -&gt;    │ ' strong'          │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' weak'</span>            │ ' weak'            │\n",
       "│ accept -&gt;    │ ' accept'          │ ' accept'          │ ' refuse'          │\n",
       "│ safety -&gt;    │ ' yes'             │ ' safety'          │ ' danger'          │\n",
       "│ rich -&gt;      │ ' rich'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' poor'</span>            │ ' poor'            │\n",
       "│ hate -&gt;      │ ' hate'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' love'</span>            │ ' love'            │\n",
       "│ lose -&gt;      │ ' lose'            │ ' lose'            │ ' gain'            │\n",
       "│ quiet -&gt;     │ ' quiet'           │ ' quiet'           │ ' loud'            │\n",
       "│ encourage -&gt; │ ' encourage'       │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' discourage'</span>      │ ' discourage'      │\n",
       "│ thick -&gt;     │ ' thick'           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' thin'</span>            │ ' thin'            │\n",
       "│ fast -&gt;      │ ' fast'            │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">' slow'</span>            │ ' slow'            │\n",
       "└──────────────┴────────────────────┴────────────────────┴────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                          Model's antonym completions                          \u001b[0m\n",
       "┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m              \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's completion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                    \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPrompt      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention)    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect completion\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ sad ->       │ ' sad'             │ \u001b[1;32m' happy'\u001b[0m           │ ' happy'           │\n",
       "│ win ->       │ ' win'             │ ' win'             │ ' lose'            │\n",
       "│ destroy ->   │ ' destroy'         │ ' destroy'         │ ' create'          │\n",
       "│ slow ->      │ ' slow'            │ \u001b[1;32m' fast'\u001b[0m            │ ' fast'            │\n",
       "│ disperse ->  │ ' 1'               │ ' disperse'        │ ' assemble'        │\n",
       "│ gain ->      │ ' gain'            │ \u001b[1;32m' loss'\u001b[0m            │ ' loss'            │\n",
       "│ blunt ->     │ ' blunt'           │ ' blunt'           │ ' sharp'           │\n",
       "│ light ->     │ ' light'           │ \u001b[1;32m' dark'\u001b[0m            │ ' dark'            │\n",
       "│ repel ->     │ ' rep'             │ ' rep'             │ ' attract'         │\n",
       "│ open ->      │ ' open'            │ \u001b[1;32m' closed'\u001b[0m          │ ' closed'          │\n",
       "│ strong ->    │ ' strong'          │ \u001b[1;32m' weak'\u001b[0m            │ ' weak'            │\n",
       "│ accept ->    │ ' accept'          │ ' accept'          │ ' refuse'          │\n",
       "│ safety ->    │ ' yes'             │ ' safety'          │ ' danger'          │\n",
       "│ rich ->      │ ' rich'            │ \u001b[1;32m' poor'\u001b[0m            │ ' poor'            │\n",
       "│ hate ->      │ ' hate'            │ \u001b[1;32m' love'\u001b[0m            │ ' love'            │\n",
       "│ lose ->      │ ' lose'            │ ' lose'            │ ' gain'            │\n",
       "│ quiet ->     │ ' quiet'           │ ' quiet'           │ ' loud'            │\n",
       "│ encourage -> │ ' encourage'       │ \u001b[1;32m' discourage'\u001b[0m      │ ' discourage'      │\n",
       "│ thick ->     │ ' thick'           │ \u001b[1;32m' thin'\u001b[0m            │ ' thin'            │\n",
       "│ fast ->      │ ' fast'            │ \u001b[1;32m' slow'\u001b[0m            │ ' slow'            │\n",
       "└──────────────┴────────────────────┴────────────────────┴────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)\n",
    "zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)\n",
    "\n",
    "completions_zero_shot, completions_intervention = calculate_h_and_intervene(\n",
    "    model,\n",
    "    dataset,\n",
    "    zero_shot_dataset,\n",
    "    layer=layer,\n",
    ")\n",
    "\n",
    "display_model_completions_on_h_intervention(\n",
    "    zero_shot_dataset,\n",
    "    completions_zero_shot,\n",
    "    completions_intervention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5186e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_h_and_intervene_logprobs(\n",
    "    model: nnsight.LanguageModel,\n",
    "    dataset: ICLDataset,\n",
    "    zero_shot_dataset: ICLDataset,\n",
    "    layer: int,\n",
    ") -> tuple[list[float], list[float]]:\n",
    "    \"\"\"\n",
    "    Extracts the vector `h`, intervenes by adding `h` to the residual stream of a set of generated zero-shot prompts,\n",
    "    all within the same forward pass. Returns the logprobs on correct tokens from this intervention.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the model we're using to generate completions\n",
    "        dataset: ICLDataset\n",
    "            the dataset of clean prompts from which we'll extract the `h`-vector\n",
    "        zero_shot_dataset: ICLDataset\n",
    "            the dataset of zero-shot prompts which we'll intervene on, using the `h`-vector\n",
    "        layer: int\n",
    "            the layer we'll be extracting the `h`-vector from\n",
    "\n",
    "    Returns:\n",
    "        correct_logprobs: list[float]\n",
    "            list of correct-token logprobs for the zero-shot prompts, without intervention\n",
    "        correct_logprobs_intervention: list[float]\n",
    "            list of correct-token logprobs for the zero-shot prompts, with h-intervention\n",
    "    \"\"\"\n",
    "    # Get correct completions from `dataset`, to be used for indexing into the logprobs\n",
    "    correct_completion_ids = [\n",
    "        toks[0] for toks in tokenizer(zero_shot_dataset.completions)[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    with model.trace(remote=True) as runner:\n",
    "\n",
    "        # Run on the clean prompts, to get the h-vector\n",
    "        with runner.invoke(dataset.prompts) as invoker:\n",
    "            # Define h (we don't need to save it, cause we don't need it outside `runner:`)\n",
    "            hidden_states = model.transformer.h[layer].output[0]\n",
    "            h = hidden_states[:, -1].mean(dim=0)\n",
    "\n",
    "        # First, run a forward pass where we don't intervene\n",
    "        with runner.invoke(zero_shot_dataset.prompts) as invoker:\n",
    "            # We save correct-token logprobs, not all logits - this means less for us to download!\n",
    "            logprobs = model.lm_head.output[:, -1].log_softmax(dim=-1)\n",
    "            correct_logprobs_zero_shot = logprobs[\n",
    "                torch.arange(len(zero_shot_dataset)), correct_completion_ids\n",
    "            ].save()\n",
    "\n",
    "        # Next, run a forward pass on the zero-shot prompts where we do intervene\n",
    "        with runner.invoke(zero_shot_dataset.prompts) as invoker:\n",
    "            # Add the h-vector to the residual stream, at the last sequence position\n",
    "            hidden_states = model.transformer.h[layer].output[0]\n",
    "            hidden_states[:, -1] += h\n",
    "            # We save correct-token logprobs, not all logits - this means less for us to download!\n",
    "            logprobs = model.lm_head.output[:, -1].log_softmax(dim=-1)\n",
    "            correct_logprobs_intervention = logprobs[\n",
    "                torch.arange(len(zero_shot_dataset)), correct_completion_ids\n",
    "            ].save()\n",
    "\n",
    "    return (\n",
    "        correct_logprobs_zero_shot.value.tolist(),\n",
    "        correct_logprobs_intervention.value.tolist(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58f84cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 17:58:48,485 4f031cd1-3ead-46ff-9f36-a4b85a93cc4e - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-13 17:58:48,516 4f031cd1-3ead-46ff-9f36-a4b85a93cc4e - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-09-13 17:58:48,548 4f031cd1-3ead-46ff-9f36-a4b85a93cc4e - RUNNING: Your job has started running.\n",
      "2024-09-13 17:58:49,014 4f031cd1-3ead-46ff-9f36-a4b85a93cc4e - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████| 1.62k/1.62k [00:00<00:00, 3.71MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">               Model's antonym logprobs, with zero-shot h-intervention               </span>\n",
       "<span style=\"font-style: italic\">                      (green = intervention improves accuracy)                       </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                         </span>┃<span style=\"font-weight: bold\"> Model's logprob   </span>┃<span style=\"font-weight: bold\"> Model's logprob </span>┃<span style=\"font-weight: bold\">                   </span>┃\n",
       "┃<span style=\"font-weight: bold\"> Zero-shot prompt        </span>┃<span style=\"font-weight: bold\"> (no intervention) </span>┃<span style=\"font-weight: bold\"> (intervention)  </span>┃<span style=\"font-weight: bold\"> Change in logprob </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│      sad -&gt; happy       │ -3.69             │ -1.79           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.89</span>             │\n",
       "│      win -&gt; lose        │ -5.92             │ -3.15           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.77</span>             │\n",
       "│  destroy -&gt; create      │ -4.40             │ -3.54           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+0.86</span>             │\n",
       "│     slow -&gt; fast        │ -4.49             │ -1.47           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.01</span>             │\n",
       "│ disperse -&gt; assemble    │ -7.68             │ -5.78           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.91</span>             │\n",
       "│     gain -&gt; loss        │ -3.13             │ -1.18           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.96</span>             │\n",
       "│    blunt -&gt; sharp       │ -5.15             │ -2.88           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.26</span>             │\n",
       "│    light -&gt; dark        │ -4.07             │ -1.95           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.12</span>             │\n",
       "│    repel -&gt; attract     │ -6.80             │ -3.73           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.07</span>             │\n",
       "│     open -&gt; closed      │ -5.06             │ -1.45           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.60</span>             │\n",
       "│   strong -&gt; weak        │ -2.64             │ -1.43           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.20</span>             │\n",
       "│   accept -&gt; refuse      │ -7.91             │ -4.87           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.04</span>             │\n",
       "│   safety -&gt; danger      │ -6.57             │ -2.94           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.63</span>             │\n",
       "│     rich -&gt; poor        │ -4.69             │ -2.17           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.52</span>             │\n",
       "│     hate -&gt; love        │ -2.66             │ -0.90           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.76</span>             │\n",
       "│     lose -&gt; gain        │ -4.13             │ -1.96           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.17</span>             │\n",
       "│    quiet -&gt; loud        │ -6.57             │ -3.57           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+3.00</span>             │\n",
       "│ encourage -&gt; discourage │ -4.09             │ -1.41           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.68</span>             │\n",
       "│    thick -&gt; thin        │ -3.23             │ -1.57           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+1.66</span>             │\n",
       "│     fast -&gt; slow        │ -3.13             │ -0.90           │ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">+2.23</span>             │\n",
       "└─────────────────────────┴───────────────────┴─────────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m               Model's antonym logprobs, with zero-shot h-intervention               \u001b[0m\n",
       "\u001b[3m                      (green = intervention improves accuracy)                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m                         \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's logprob  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel's logprob\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                   \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1mZero-shot prompt       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(no intervention)\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m(intervention) \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mChange in logprob\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│      sad -> happy       │ -3.69             │ -1.79           │ \u001b[1;32m+1.89\u001b[0m             │\n",
       "│      win -> lose        │ -5.92             │ -3.15           │ \u001b[1;32m+2.77\u001b[0m             │\n",
       "│  destroy -> create      │ -4.40             │ -3.54           │ \u001b[1;32m+0.86\u001b[0m             │\n",
       "│     slow -> fast        │ -4.49             │ -1.47           │ \u001b[1;32m+3.01\u001b[0m             │\n",
       "│ disperse -> assemble    │ -7.68             │ -5.78           │ \u001b[1;32m+1.91\u001b[0m             │\n",
       "│     gain -> loss        │ -3.13             │ -1.18           │ \u001b[1;32m+1.96\u001b[0m             │\n",
       "│    blunt -> sharp       │ -5.15             │ -2.88           │ \u001b[1;32m+2.26\u001b[0m             │\n",
       "│    light -> dark        │ -4.07             │ -1.95           │ \u001b[1;32m+2.12\u001b[0m             │\n",
       "│    repel -> attract     │ -6.80             │ -3.73           │ \u001b[1;32m+3.07\u001b[0m             │\n",
       "│     open -> closed      │ -5.06             │ -1.45           │ \u001b[1;32m+3.60\u001b[0m             │\n",
       "│   strong -> weak        │ -2.64             │ -1.43           │ \u001b[1;32m+1.20\u001b[0m             │\n",
       "│   accept -> refuse      │ -7.91             │ -4.87           │ \u001b[1;32m+3.04\u001b[0m             │\n",
       "│   safety -> danger      │ -6.57             │ -2.94           │ \u001b[1;32m+3.63\u001b[0m             │\n",
       "│     rich -> poor        │ -4.69             │ -2.17           │ \u001b[1;32m+2.52\u001b[0m             │\n",
       "│     hate -> love        │ -2.66             │ -0.90           │ \u001b[1;32m+1.76\u001b[0m             │\n",
       "│     lose -> gain        │ -4.13             │ -1.96           │ \u001b[1;32m+2.17\u001b[0m             │\n",
       "│    quiet -> loud        │ -6.57             │ -3.57           │ \u001b[1;32m+3.00\u001b[0m             │\n",
       "│ encourage -> discourage │ -4.09             │ -1.41           │ \u001b[1;32m+2.68\u001b[0m             │\n",
       "│    thick -> thin        │ -3.23             │ -1.57           │ \u001b[1;32m+1.66\u001b[0m             │\n",
       "│     fast -> slow        │ -3.13             │ -0.90           │ \u001b[1;32m+2.23\u001b[0m             │\n",
       "└─────────────────────────┴───────────────────┴─────────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_model_logprobs_on_h_intervention(\n",
    "    dataset: ICLDataset,\n",
    "    correct_logprobs_zero_shot: list[float],\n",
    "    correct_logprobs_intervention: list[float],\n",
    "    num_to_display: int = 20,\n",
    ") -> None:\n",
    "    table = rich.table.Table(\n",
    "        \"Zero-shot prompt\",\n",
    "        \"Model's logprob\\n(no intervention)\",\n",
    "        \"Model's logprob\\n(intervention)\",\n",
    "        \"Change in logprob\",\n",
    "        title=\"Model's antonym logprobs, with zero-shot h-intervention\\n(green = intervention improves accuracy)\",\n",
    "    )\n",
    "\n",
    "    for i in range(min(len(correct_logprobs_zero_shot), num_to_display)):\n",
    "\n",
    "        logprob_ni = correct_logprobs_zero_shot[i]\n",
    "        logprob_i = correct_logprobs_intervention[i]\n",
    "        delta_logprob = logprob_i - logprob_ni\n",
    "        zero_shot_prompt = f\"{dataset[i].x[0]:>8} -> {dataset[i].y[0]}\"\n",
    "\n",
    "        # Color code the logprob based on whether it's increased with this intervention\n",
    "        is_improvement = delta_logprob >= 0\n",
    "        delta_logprob = (\n",
    "            f\"[b green]{delta_logprob:+.2f}[/]\"\n",
    "            if is_improvement\n",
    "            else f\"{delta_logprob:+.2f}\"\n",
    "        )\n",
    "\n",
    "        table.add_row(\n",
    "            zero_shot_prompt, f\"{logprob_ni:.2f}\", f\"{logprob_i:.2f}\", delta_logprob\n",
    "        )\n",
    "\n",
    "    rich.print(table)\n",
    "\n",
    "\n",
    "dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=3, seed=0)\n",
    "zero_shot_dataset = ICLDataset(ANTONYM_PAIRS, size=20, n_prepended=0, seed=1)\n",
    "\n",
    "correct_logprobs_zero_shot, correct_logprobs_intervention = (\n",
    "    calculate_h_and_intervene_logprobs(model, dataset, zero_shot_dataset, layer=layer)\n",
    ")\n",
    "\n",
    "display_model_logprobs_on_h_intervention(\n",
    "    zero_shot_dataset, correct_logprobs_zero_shot, correct_logprobs_intervention\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c81c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "\n",
    "def calculate_fn_vectors_and_intervene(\n",
    "    model: nnsight.LanguageModel,\n",
    "    dataset: ICLDataset,\n",
    "    layers: list[int] | None = None,\n",
    ") -> jaxtyping.Float[torch.Tensor, \"layers heads\"]:\n",
    "    \"\"\"\n",
    "    Returns a tensor of shape (layers, heads), containing the CIE for each head.\n",
    "\n",
    "    Inputs:\n",
    "        model: LanguageModel\n",
    "            the transformer you're doing this computation with\n",
    "        dataset: ICLDataset\n",
    "            the dataset of clean prompts from which we'll extract the function vector (we'll also create a\n",
    "            corrupted version of this dataset for interventions)\n",
    "\n",
    "        layers: list[int] | None\n",
    "            the layers which this function will calculate the score for (if None, we assume all layers)\n",
    "    \"\"\"\n",
    "    layers = range(model.config.n_layer) if (layers is None) else layers\n",
    "    heads = range(model.config.n_head)\n",
    "\n",
    "    # Get corrupted dataset\n",
    "    corrupted_dataset = dataset.create_corrupted_dataset()\n",
    "    N = len(dataset)\n",
    "\n",
    "    # Get correct token ids, so we can get correct token logprobs\n",
    "    correct_completion_ids = [\n",
    "        toks[0] for toks in tokenizer(dataset.completions)[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    with model.trace(remote=True) as runner:\n",
    "\n",
    "        # Run a forward pass on clean prompts, where we store attention head outputs\n",
    "        z_dict = {}\n",
    "        with runner.invoke(dataset.prompts) as invoker:\n",
    "\n",
    "            for layer in layers:\n",
    "\n",
    "                # Get hidden states, reshape to get head dimension, store the mean tensor\n",
    "                z = model.transformer.h[layer].attn.out_proj.input[0][0][:, -1]\n",
    "                z_reshaped = z.reshape(N, N_HEADS, D_HEAD).mean(dim=0)\n",
    "\n",
    "                for head in heads:\n",
    "                    z_dict[(layer, head)] = z_reshaped[head]\n",
    "\n",
    "            # Get correct token logprobs\n",
    "            logits_clean = model.lm_head.output[:, -1]\n",
    "\n",
    "        # Run a forward pass on corrupted prompts, where we don't intervene or store activations (just so we can\n",
    "        # get the correct-token logprobs to compare with our intervention)\n",
    "        with runner.invoke(corrupted_dataset.prompts) as invoker:\n",
    "\n",
    "            logits = model.lm_head.output[:, -1]\n",
    "\n",
    "            correct_logprobs_corrupted = logits.log_softmax(dim=-1)[\n",
    "                torch.arange(N), correct_completion_ids\n",
    "            ].save()\n",
    "\n",
    "        # For each head, run a forward pass on corrupted prompts (here we need multiple different forward passes,\n",
    "        # because we're doing different interventions each time)\n",
    "        correct_logprobs_dict = {}\n",
    "        for layer in layers:\n",
    "            for head in heads:\n",
    "                with runner.invoke(corrupted_dataset.prompts) as invoker:\n",
    "\n",
    "                    # Get hidden states, reshape to get head dimension, then set it to the a-vector\n",
    "                    z = model.transformer.h[layer].attn.out_proj.input[0][0][:, -1]\n",
    "                    z.reshape(N, N_HEADS, D_HEAD)[:, head] = z_dict[(layer, head)]\n",
    "\n",
    "                    # Get logprobs at the end, which we'll compare with our corrupted logprobs\n",
    "                    logits = model.lm_head.output[:, -1]\n",
    "                    correct_logprobs_dict[(layer, head)] = logits.log_softmax(dim=-1)[\n",
    "                        torch.arange(N), correct_completion_ids\n",
    "                    ].save()\n",
    "\n",
    "    # Get difference between intervention logprobs and corrupted logprobs, and take mean over batch dim\n",
    "    all_correct_logprobs_intervention = einops.rearrange(\n",
    "        torch.stack([v.value for v in correct_logprobs_dict.values()]),\n",
    "        \"(layers heads) batch -> layers heads batch\",\n",
    "        layers=len(layers),\n",
    "    )\n",
    "\n",
    "    # shape [layers heads batch]\n",
    "    logprobs_diff = all_correct_logprobs_intervention - correct_logprobs_corrupted.value\n",
    "\n",
    "    # Return mean effect of intervention, over the batch dimension\n",
    "    return logprobs_diff.mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dc434ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Computing layers in range(0, 4) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 18:32:03,132 5077ebdc-650c-4f85-96ae-f2dd43118faf - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-13 18:32:03,527 5077ebdc-650c-4f85-96ae-f2dd43118faf - APPROVED: Your job was approved and is waiting to be run.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing layers in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     34\u001b[0m     results \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m---> 35\u001b[0m         [results, \u001b[43mcalculate_fn_vectors_and_intervene\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)]\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... finished in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m plotly_utils\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[1;32m     41\u001b[0m     results\u001b[38;5;241m.\u001b[39mT,\n\u001b[1;32m     42\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage indirect effect of function-vector intervention on antonym task\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m )\n",
      "Cell \u001b[0;32mIn[37], line 34\u001b[0m, in \u001b[0;36mcalculate_fn_vectors_and_intervene\u001b[0;34m(model, dataset, layers)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Get correct token ids, so we can get correct token logprobs\u001b[39;00m\n\u001b[1;32m     30\u001b[0m correct_completion_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m     toks[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m toks \u001b[38;5;129;01min\u001b[39;00m tokenizer(dataset\u001b[38;5;241m.\u001b[39mcompletions)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     32\u001b[0m ]\n\u001b[0;32m---> 34\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Run a forward pass on clean prompts, where we store attention head outputs\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minvoker\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/nnsight/contexts/Tracer.py:102\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/nnsight/contexts/GraphBasedContext.py:217\u001b[0m, in \u001b[0;36mGraphBasedContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/nnsight/contexts/backends/RemoteBackend.py:107\u001b[0m, in \u001b[0;36mRemoteBackend.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    104\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(obj)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Do blocking request.\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocking_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/nnsight/contexts/backends/RemoteBackend.py:273\u001b[0m, in \u001b[0;36mRemoteBackend.blocking_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Loop until\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_response(\u001b[43msio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;241m==\u001b[39m ResponseModel\u001b[38;5;241m.\u001b[39mJobStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    275\u001b[0m     ):\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/gpt_from_scratch/venv/lib/python3.12/site-packages/socketio/simple_client.py:175\u001b[0m, in \u001b[0;36mSimpleClient.receive\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnected:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DisconnectedError()\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_event\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gpt_from_scratch import plotly_utils\n",
    "\n",
    "import time\n",
    "\n",
    "# Get the best available PyTorch device\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Using device: {device}\"\n",
    ")  # Inline comment: Print the selected device for confirmation\n",
    "\n",
    "dataset = ICLDataset(ANTONYM_PAIRS, size=4, n_prepended=2)\n",
    "\n",
    "\n",
    "def batch_process_layers(n_layers, batch_size):\n",
    "    for i in range(0, n_layers, batch_size):\n",
    "        yield range(n_layers)[i : i + batch_size]\n",
    "\n",
    "\n",
    "results = torch.empty((0, N_HEADS), device=device)\n",
    "\n",
    "# If this fails to run, reduce the batch size so the fwd passes are split up more\n",
    "for layers in batch_process_layers(N_LAYERS, batch_size=4):\n",
    "\n",
    "    if layers[0] == 12:\n",
    "        break\n",
    "\n",
    "    print(f\"Computing layers in {layers} ...\")\n",
    "    t0 = time.time()\n",
    "    results = torch.concat(\n",
    "        [results, calculate_fn_vectors_and_intervene(model, dataset, layers).to(device)]\n",
    "    )\n",
    "    print(f\"... finished in {time.time()-t0:.2f} seconds.\\n\")\n",
    "\n",
    "\n",
    "plotly_utils.imshow(\n",
    "    results.T,\n",
    "    title=\"Average indirect effect of function-vector intervention on antonym task\",\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Head\"},\n",
    "    aspect=\"equal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2f219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
