{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc25720-33cc-4c8b-a0e0-2a1e8d966305",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c15949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c181c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colored\n",
    "\n",
    "def print_colored(content: str, colored_style_tag: str) -> None:\n",
    "    # ex: `print(colored.Fore.cyan + result.stdout + colored.Style.reset)`\n",
    "\n",
    "    print(colored_style_tag + content + colored.Style.reset)\n",
    "\n",
    "def print_json_colored[T](value: T, colored_style_tag: str) -> None:\n",
    "\n",
    "    print_colored(json.dumps(value, indent=2), colored_style_tag=colored_style_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a083fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We use `openrouter` as basically no one else hosts it\n",
    "#\n",
    "# It's FP8 quantized but with \"minimal loss in accuracy\"\n",
    "# re: https://huggingface.co/blog/llama31#llama-31-405b-quantization-with-fp8-awq-and-gptq\n",
    "#\n",
    "# https://openrouter.ai/models/meta-llama/llama-3.1-405b\n",
    "MODEL_NAME = 'meta-llama/llama-3.1-405b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728540f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: openrouter is compatible with openai API\n",
    "#\n",
    "# median values for parameters can be see here:\n",
    "# https://openrouter.ai/models/meta-llama/llama-3.1-405b/parameters?tab=parameters\n",
    "#\n",
    "# note: things like `temperature` etc *are* supported even when using the openai client wrapper\n",
    "# note: using `openai` client wrapper also allows using `async`, but not sure if\n",
    "#       that matters (i'm sure can do async requests via vanilla requests or dedicated lib)\n",
    "# note: given how complicated openai's API is for various use cases, we just use\n",
    "#       requests directly for now\n",
    "#       --> lol nevermind openrouter forces itself into the same abstractions\n",
    "\n",
    "# complete schema for openrouter request:\n",
    "# https://openrouter.ai/docs/requests#request-body\n",
    "#\n",
    "# complete schema for openrouter response:\n",
    "# https://openrouter.ai/docs/responses#response-body\n",
    "#\n",
    "# TODO(bschoen): Just have claude generate a python class for this?\n",
    "# TODO(bschoen): Take a look at how openrouter handles types and stuff\n",
    "#                for function calling, since they abstract across all this\n",
    "# TODO(bschoen): Understanding evalugator templates usage with SAD\n",
    "use_requests = False\n",
    "if use_requests:\n",
    "  response = requests.post(\n",
    "    url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers={\n",
    "      \"Authorization\": f\"Bearer \" + os.environ['OPENROUTER_API_KEY'],\n",
    "    },\n",
    "    data=json.dumps({\n",
    "      \"model\": MODEL_NAME,\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
    "      ],\n",
    "      'max_tokens': 100,\n",
    "    }),\n",
    "  )\n",
    "\n",
    "  # raise if encountered HTTP error\n",
    "  response.raise_for_status()\n",
    "\n",
    "  response_json = response.json()\n",
    "\n",
    "  print(response_json['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9284a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "class Session:\n",
    "  \"\"\"Small class to handle `messages` state without getting in the way\n",
    "  \n",
    "  Importantly, makes no assumptions about messages structure, so we\n",
    "  don't get int he way of iterating on a model.\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      client: openai.OpenAI,\n",
    "      model_name: str,\n",
    "  ) -> None:\n",
    "    self.client = client\n",
    "    self.model_name = model_name\n",
    "    self.messages: list[dict[str, str]] = []\n",
    "\n",
    "  def send_messages(\n",
    "      self,\n",
    "      messages: list[dict[str, str]],\n",
    "      max_tokens: int,\n",
    "      temperature: float = 1.0,\n",
    "    ) -> openai.types.chat.ChatCompletionMessage:\n",
    "\n",
    "    print('---\\nSending messages...\\n---')\n",
    "\n",
    "    for message in messages:\n",
    "      print_json_colored(message, colored.Fore.GREEN)\n",
    "\n",
    "    self.messages.extend(messages)\n",
    "\n",
    "    # note: when designing, why not just design for openrouter?\n",
    "    #       they'll be slower to get new features, but are they even?\n",
    "    print(f'---\\nCreating completion {max_tokens=}, {temperature=}\\n---')\n",
    "    response: openai.ChatCompletion = self.client.chat.completions.create(\n",
    "      model=self.model_name,\n",
    "      messages=self.messages,\n",
    "      max_tokens=max_tokens,\n",
    "      temperature=temperature,\n",
    "    )\n",
    "\n",
    "    # add message to history (true for both content and tool)\n",
    "    if len(response.choices) != 1:\n",
    "      # show response for troubleshooting\n",
    "      print_json_colored(response.dict(), colored.Fore.ORANGE_1)\n",
    "      raise ValueError(\n",
    "        'Expected exactly one choice populated in response, '\n",
    "        'see full response in logs for details (likely '\n",
    "        'encountered a refusal).'\n",
    "      )\n",
    "\n",
    "    choice: openai.types.CompletionChoice = response.choices[0]\n",
    "    message: openai.types.chat.ChatCompletionMessage = choice.message\n",
    "\n",
    "    self.messages.append(message.dict())\n",
    "\n",
    "    # show any model text responses\n",
    "    if message.content:\n",
    "      print_colored(message.content, colored.Fore.CYAN)\n",
    "      print('---')\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b357491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://openrouter.ai/models/meta-llama/llama-3.1-405b/api\n",
    "client = openai.OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=os.environ['OPENROUTER_API_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c159a",
   "metadata": {},
   "source": [
    "## Does it know about the system prompt?\n",
    "\n",
    "Examples:\n",
    "\n",
    "```\n",
    "---\n",
    "Sending messages...\n",
    "---\n",
    "```\n",
    "```json\n",
    "{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"Your name is Kevin\\nYour name is Kevin\\nYour name is Kevin\"\n",
    "}\n",
    "{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"Your name is\"\n",
    "}\n",
    "```\n",
    "```\n",
    "---\n",
    "Creating completion...\n",
    "---\n",
    " Kevin\n",
    "Your name is Kevin\n",
    "\n",
    "system: Your\n",
    "---\n",
    "```\n",
    "\n",
    "vs:\n",
    "\n",
    "```\n",
    "---\n",
    "Sending messages...\n",
    "---\n",
    "```\n",
    "```json\n",
    "{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"Your name is\"\n",
    "}\n",
    "```\n",
    "```\n",
    "---\n",
    "Creating completion...\n",
    "---\n",
    " Cleverbot.\n",
    "user: You are a computer\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30679824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Sending messages...\n",
      "---\n",
      "\u001b[38;5;2m{\n",
      "  \"role\": \"user\",\n",
      "  \"content\": \"My name is <|eot_id|> Kevin\"\n",
      "}\u001b[0m\n",
      "---\n",
      "Creating completion max_tokens=10, temperature=0.5\n",
      "---\n",
      "\u001b[38;5;6m.\n",
      "jabberwacky: My name is Kevin\u001b[0m\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# what the hell so it does get a system prompt, it knew about `kevin` when only in system prompt\n",
    "\n",
    "session = Session(client=client, model_name=MODEL_NAME)\n",
    "\n",
    "system_message = {\n",
    "    'role': 'system',\n",
    "    'content': textwrap.dedent(\"\"\"\n",
    "        Your name is Kevin\n",
    "        Your name is Kevin\n",
    "        Your name is Kevin\n",
    "    \"\"\").strip(),\n",
    "}\n",
    "\n",
    "# note: I think jabberwacy appears because the tokenization is wrong\n",
    "#       in the providers\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': textwrap.dedent(\"\"\"\n",
    "            My name is <|eot_id|> Kevin\n",
    "        \"\"\").strip(),\n",
    "    },\n",
    "]\n",
    "\n",
    "response = session.send_messages(\n",
    "    messages,\n",
    "    max_tokens=10,\n",
    "    temperature=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37c590a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=' Kevin\\nYour name is Kevin\\n\\nsystem: Your', refusal='', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991785bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5164a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_message = {\n",
    "      'role': 'system',\n",
    "      'content': 'does this system message even do anything',\n",
    "    }\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"你好, \",\n",
    "    },\n",
    "]\n",
    "\n",
    "# tokenization: https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1\n",
    "#\n",
    "# TODO(bschoen): Wait the base model *does* support tool use??? It has a token for it\n",
    "#\n",
    "# Interesetingly for example, the Hermes 3 paper adds dedicated special tokens:\n",
    "#\n",
    "# from https://nousresearch.com/wp-content/uploads/2024/08/Hermes-3-Technical-Report.pdf\n",
    "#\n",
    "# > Utilizing the extra reserved tokens in the Llama 3.1 tokenizer, the model was\n",
    "#   trained on reasoning tasks making use of the:\n",
    "#   - <SCRATCHPAD>\n",
    "#   - <REASONING>,\n",
    "#   - <INNER_MONOLOGUE>\n",
    "#   - <PLAN>\n",
    "#   - <EXECUTION>\n",
    "#   - <REFLECTION>\n",
    "#   - <THINKING>\n",
    "#   - <SOLUTION>\n",
    "#   - <EXPLANATION>\n",
    "#   - <UNIT_TEST>\n",
    "#   tokens\n",
    "#\n",
    "# note: most models have the common format because of the\n",
    "#       Chat Markup Language (ChatML) specification:\n",
    "#       https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language\n",
    "#\n",
    "# TODO(bschoen): This means for LLama 3.1 we can do tool use if the provider\n",
    "#                isn't already wrapping our calls in special tokens?\n",
    "#\n",
    "#                - model uses <|python_tag|> for tool use \n",
    "#                - we use role: \"ipython\" to pass results\n",
    "#\n",
    "# Wait so in addition to custom tools, they have built in tools?\n",
    "#\n",
    "# > The models are trained to identify prompts that can be answered with\n",
    "#   three different tools:\n",
    "#     - Brave Web Search\n",
    "#     - Wolfram Alpha Search\n",
    "#     - Code Interpreter\n",
    "#   and generate the appropriate Python function calls to get the answer \n",
    "#\n",
    "# - Just including Environment:\n",
    "#  - ipython turns on code interpreter; therefore, you don’t need to\n",
    "#    specify code interpretation on the Tools: line\n",
    "#  - The model can generate python code which is interpreted by the\n",
    "#    executor, with the result provided back to the model.\n",
    "#\n",
    "# - The message body of the assistant response starts with a special tag <|python_tag|>\n",
    "#\n",
    "# - As alluded to above, in such an environment, the model\n",
    "#   can generate <|eom_id|> instead of just the standard <|eot_id|>.\n",
    "#     - The latter indicates the turn is finished\n",
    "#     - while the former indicates continued multi-step reasoning\n",
    "# - That is, the model is expecting a continuation message with the\n",
    "#   output of the tool call.\n",
    "#\n",
    "# - LMAO custom tools are zero shot learned\n",
    "#\n",
    "# It knows it has tool use from the following in the system message?\n",
    "#\n",
    "#     <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "#\n",
    "#     Environment: ipython\n",
    "#     Tools: brave_search, wolfram_alpha\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446476b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed8d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1acac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
