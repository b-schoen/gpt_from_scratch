{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8a7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# today can reproduce GPT-2 in about an hour\n",
    "# GPT-2 paper is pretty light on details, so we reference the GPT-3 paper as well (not huge change in architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed473d0",
   "metadata": {},
   "source": [
    "* GPT-2 - [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "* GPT-3 - [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18012535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - original written in tensorflow\n",
    "# - karpathy not a fan\n",
    "# - We use the huggingface transformers code instead of https://github.com/openai/gpt-2.git\n",
    "#\n",
    "# - actual source is here: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ae762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
    "# \n",
    "# note: the \"language modeling\" head is essentially unembedding it\n",
    "#\n",
    "# note: this is the 124M model (small), not the GPT-2 (XL) 1.5B model \n",
    "model_hf: transformers.GPT2LMHeadModel = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6222517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the raw tensors\n",
    "#\n",
    "# TODO(bschoen): Karpathy tends to use jupyter notebook on right of source, probably bigger monitor since GPU\n",
    "#\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "# print(tabulate.tabulate([(k, v.shape) for k, v in sd_hf.items()], headers=['key', 'shape']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225c533",
   "metadata": {},
   "source": [
    "We can look at the raw tensors to interpret the shapes involved:\n",
    "\n",
    "```python\n",
    "# we derived this directly from the shapes\n",
    "MLP_DIMENSIONALITY_FACTOR = 4\n",
    "\n",
    "vocab_size = 50257\n",
    "context_size = 1024\n",
    "d_embed = d_embed\n",
    "\n",
    "d_model_mlp = MLP_DIMENSIONALITY_FACTOR * d_embed\n",
    "\n",
    "assert d_model_mlp == 3072\n",
    "\n",
    "query_size = d_embed\n",
    "key_size   = d_embed\n",
    "value_size = d_embed\n",
    "\n",
    "d_attn = query_size + key_size + value_size\n",
    "\n",
    "assert d_attn == 2304\n",
    "\n",
    "# so we get:\n",
    "\n",
    "# key                                shape\n",
    "# ---------------------------------  ------------------------\n",
    "# token embeddings\n",
    "transformer.wte.weight               torch.Size([vocab_size, d_embed])\n",
    "# positional embeddings\n",
    "transformer.wpe.weight               torch.Size([context_size, d_embed])\n",
    "# add together to size `d_embed`\n",
    "#\n",
    "transformer.h.0.ln_1.weight          torch.Size([d_embed])\n",
    "transformer.h.0.ln_1.bias            torch.Size([d_embed])\n",
    "transformer.h.0.attn.c_attn.weight   torch.Size([d_embed, d_attn])\n",
    "transformer.h.0.attn.c_attn.bias     torch.Size([d_attn])\n",
    "transformer.h.0.attn.c_proj.weight   torch.Size([d_embed, d_embed])\n",
    "transformer.h.0.attn.c_proj.bias     torch.Size([d_embed])\n",
    "transformer.h.0.ln_2.weight          torch.Size([d_embed])\n",
    "transformer.h.0.ln_2.bias            torch.Size([d_embed])\n",
    "transformer.h.0.mlp.c_fc.weight      torch.Size([d_embed, d_model_mlp])\n",
    "transformer.h.0.mlp.c_fc.bias        torch.Size([d_model_mlp])\n",
    "transformer.h.0.mlp.c_proj.weight    torch.Size([d_model_mlp, d_embed])\n",
    "transformer.h.0.mlp.c_proj.bias      torch.Size([d_embed])\n",
    "# note: layer norms at the inputs\n",
    "transformer.h.1.ln_1.weight          torch.Size([d_embed])\n",
    "transformer.h.1.ln_1.bias            torch.Size([d_embed])\n",
    "transformer.h.1.attn.c_attn.weight   torch.Size([d_embed, d_attn])\n",
    "transformer.h.1.attn.c_attn.bias     torch.Size([d_attn])\n",
    "...\n",
    "transformer.h.11.mlp.c_proj.weight   torch.Size([d_model_mlp, d_embed])\n",
    "transformer.h.11.mlp.c_proj.bias     torch.Size([d_embed])\n",
    "# making up for layer norms at the inputs\n",
    "transformer.ln_f.weight              torch.Size([d_embed])\n",
    "transformer.ln_f.bias                torch.Size([d_embed])\n",
    "# map back to vocab\n",
    "lm_head.weight                       torch.Size([vocab_size, d_embed])\n",
    "\n",
    "\n",
    "\n",
    "# key                                shape\n",
    "# ---------------------------------  ------------------------\n",
    "transformer.wte.weight               torch.Size([50257, 768])\n",
    "transformer.wpe.weight               torch.Size([1024, 768])\n",
    "transformer.h.0.ln_1.weight          torch.Size([768])\n",
    "transformer.h.0.ln_1.bias            torch.Size([768])\n",
    "transformer.h.0.attn.c_attn.weight   torch.Size([768, 2304])\n",
    "transformer.h.0.attn.c_attn.bias     torch.Size([2304])\n",
    "transformer.h.0.attn.c_proj.weight   torch.Size([768, 768])\n",
    "transformer.h.0.attn.c_proj.bias     torch.Size([768])\n",
    "transformer.h.0.ln_2.weight          torch.Size([768])\n",
    "transformer.h.0.ln_2.bias            torch.Size([768])\n",
    "transformer.h.0.mlp.c_fc.weight      torch.Size([768, 3072])\n",
    "transformer.h.0.mlp.c_fc.bias        torch.Size([3072])\n",
    "transformer.h.0.mlp.c_proj.weight    torch.Size([3072, 768])\n",
    "transformer.h.0.mlp.c_proj.bias      torch.Size([768])\n",
    "# note: layer norms at the inputs\n",
    "transformer.h.1.ln_1.weight          torch.Size([768])\n",
    "transformer.h.1.ln_1.bias            torch.Size([768])\n",
    "transformer.h.1.attn.c_attn.weight   torch.Size([768, 2304])\n",
    "transformer.h.1.attn.c_attn.bias     torch.Size([2304])\n",
    "...\n",
    "transformer.h.11.ln_1.weight         torch.Size([768])\n",
    "transformer.h.11.ln_1.bias           torch.Size([768])\n",
    "transformer.h.11.attn.c_attn.weight  torch.Size([768, 2304])\n",
    "transformer.h.11.attn.c_attn.bias    torch.Size([2304])\n",
    "transformer.h.11.attn.c_proj.weight  torch.Size([768, 768])\n",
    "transformer.h.11.attn.c_proj.bias    torch.Size([768])\n",
    "transformer.h.11.ln_2.weight         torch.Size([768])\n",
    "transformer.h.11.ln_2.bias           torch.Size([768])\n",
    "transformer.h.11.mlp.c_fc.weight     torch.Size([768, 3072])\n",
    "transformer.h.11.mlp.c_fc.bias       torch.Size([3072])\n",
    "transformer.h.11.mlp.c_proj.weight   torch.Size([3072, 768])\n",
    "transformer.h.11.mlp.c_proj.bias     torch.Size([768])\n",
    "# making up for layer norms at the inputs\n",
    "transformer.ln_f.weight              torch.Size([768])\n",
    "transformer.ln_f.bias                torch.Size([768])\n",
    "# map back to vocab\n",
    "lm_head.weight                       torch.Size([50257, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: positional embeddings in GPT-2 are *learned*, not fixed sinusoidal like they are in the original attention paper\n",
    "\n",
    "# example view of the positional embedding weights\n",
    "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6971c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# note: patterns across dimensions, but unintelligable (especially since also contains relative position)\n",
    "\n",
    "# note: frequently used as a tool to inspect things\n",
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Positional Embedding\")\n",
    "plt.title(\"Positional Embedding Weights\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a few individual column at random\n",
    "#\n",
    "# TODO(bschoen): Channel is a good term for dimension in the embedding space\n",
    "#\n",
    "# Can see some respond more or less depending on the position\n",
    "#\n",
    "# ex: green seems to focus on after 800\n",
    "#\n",
    "# Karpathy: \"why knows\"\n",
    "# Karpathy: Can tell model not fully trained, because would expect this to be smoother\n",
    "# Karpathy: In principle no reason these need to be smooth\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150], label='Position Embedding Dimension 150')\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200], label='Position Embedding Dimension 200')\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250], label='Position Embedding Dimension 250')\n",
    "\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.title('Word Position Embeddings')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e11518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do the same thing with any weights\n",
    "#\n",
    "# you see some structure, but again who knows\n",
    "#\n",
    "# {if you're into mechanistic interpretability}\n",
    "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300,:300], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17859555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: Karpathy took example from https://huggingface.co/openai-community/gpt2#how-to-use\n",
    "\n",
    "# we can also sample from it using the weights\n",
    "#\n",
    "# note: we don't even use our initialized model?\n",
    "#\n",
    "generator = transformers.pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# set seed before generation\n",
    "transformers.set_seed(42)\n",
    "\n",
    "# note: different generations even if fixed seed\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51712fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to write our own GPT-2 class so we can actually understand what's going on there, because it's just too complicated\n",
    "# to use theirs as reference\n",
    "# if want to create one from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we're going to create an instance and:\n",
    "#  - allow loading from pretrained (to basically check against the existing model)\n",
    "#  - figure out the layers we need, and also try to train it ourselves from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we should be able to load the weights\n",
    "from gpt_from_scratch.gpt2_from_scratch.train_gpt2 import (\n",
    "    GPT,\n",
    "    GPTConfig,\n",
    "    get_best_available_torch_device,\n",
    ")\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "model = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: if doing with pipeline get the exact same thing,\n",
    "#       so we're using the model weights correctly, just\n",
    "#       some config in the sampling pipeline is different\n",
    "\n",
    "# now we'll replicate the pipeline thing\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "device = get_best_available_torch_device()\n",
    "\n",
    "# note: dropout is an example of something that's different in eval vs train\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# prefix tokens\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "\n",
    "\n",
    "\n",
    "# 5 samples through\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5, 8)\n",
    "\n",
    "x = tokens.to(device)\n",
    "\n",
    "# generate! right now x is (B, T) where B = 5, T = 8\n",
    "# set the seed to 42\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "\n",
    "        logits = model(x) # (B, T, vocab_size)\n",
    "\n",
    "        # take the logits at the last position\n",
    "        # throw away all the logits from things other than the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        #\n",
    "        # \"anything lower than the 50th, we clamp to 0 and never sample it\"\n",
    "        #\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65732aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f93f36",
   "metadata": {},
   "source": [
    "# Moving To TinyShakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45733ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found in cache: download_cache/4acd659e47adc1daeb7aff503accf0a3\n"
     ]
    }
   ],
   "source": [
    "from gpt_from_scratch import file_utils\n",
    "\n",
    "# load tinyshakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "input_filepath = file_utils.download_file_from_url(url)\n",
    "\n",
    "# Read all text from the input file\n",
    "input_text = input_filepath.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2cff7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40000  202651 1115394 download_cache/4acd659e47adc1daeb7aff503accf0a3\n"
     ]
    }
   ],
   "source": [
    "# lines | words | byte count\n",
    "!wc download_cache/4acd659e47adc1daeb7aff503accf0a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8eccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a705c78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338025 tokens\n",
      "1 epoch = 2640 batches (steps to make one pass through data)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "from gpt_from_scratch.gpt2_from_scratch import data_loader\n",
    "\n",
    "from gpt_from_scratch.gpt2_from_scratch.train_gpt2 import (\n",
    "    GPT,\n",
    "    GPTConfig,\n",
    "    get_best_available_torch_device,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# load text via dataloader\n",
    "#\n",
    "# note: we leave these on CPU, so that the dataloader\n",
    "#       isn't trying to hold the whole set on the GPU\n",
    "#\n",
    "#       so is prefetching moving more data to the GPU?\n",
    "tokens = tokenizer.encode(input_text)\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# basically reshape sequence into 2D (by batches)\n",
    "B, T = 4, 32\n",
    "\n",
    "# create a train loader that will continually give us new batches\n",
    "train_loader = data_loader.DataLoaderLite(B=B, T=T, tokens=tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf1a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 10.97684097290039\n",
      "step 1, loss: 9.678625106811523\n",
      "step 2, loss: 8.977907180786133\n",
      "step 3, loss: 9.124414443969727\n",
      "step 4, loss: 8.801149368286133\n",
      "step 5, loss: 8.393295288085938\n",
      "step 6, loss: 8.975137710571289\n",
      "step 7, loss: 8.78160285949707\n",
      "step 8, loss: 8.152408599853516\n",
      "step 9, loss: 8.008208274841309\n",
      "step 10, loss: 8.296585083007812\n",
      "step 11, loss: 7.606632232666016\n",
      "step 12, loss: 7.849130630493164\n",
      "step 13, loss: 7.489934921264648\n",
      "step 14, loss: 7.557112216949463\n",
      "step 15, loss: 7.378129482269287\n",
      "step 16, loss: 7.419276237487793\n",
      "step 17, loss: 8.321800231933594\n",
      "step 18, loss: 7.283927917480469\n",
      "step 19, loss: 7.783172130584717\n",
      "step 20, loss: 7.545779705047607\n",
      "step 21, loss: 7.821457862854004\n",
      "step 22, loss: 6.483449935913086\n",
      "step 23, loss: 6.845578193664551\n",
      "step 24, loss: 6.892683029174805\n",
      "step 25, loss: 6.64844274520874\n",
      "step 26, loss: 6.843649387359619\n",
      "step 27, loss: 7.587469577789307\n",
      "step 28, loss: 7.22553014755249\n",
      "step 29, loss: 6.929841041564941\n",
      "step 30, loss: 7.04420804977417\n",
      "step 31, loss: 7.322114944458008\n",
      "step 32, loss: 7.16148567199707\n",
      "step 33, loss: 6.965855598449707\n",
      "step 34, loss: 7.899472713470459\n",
      "step 35, loss: 7.78594970703125\n",
      "step 36, loss: 7.6089863777160645\n",
      "step 37, loss: 7.641238212585449\n",
      "step 38, loss: 8.358102798461914\n",
      "step 39, loss: 7.336952209472656\n",
      "step 40, loss: 7.338316917419434\n",
      "step 41, loss: 6.744345664978027\n",
      "step 42, loss: 6.881869316101074\n",
      "step 43, loss: 6.9440813064575195\n",
      "step 44, loss: 7.233992099761963\n",
      "step 45, loss: 6.929455757141113\n",
      "step 46, loss: 6.128724098205566\n",
      "step 47, loss: 6.373309135437012\n",
      "step 48, loss: 6.956882476806641\n",
      "step 49, loss: 6.7791547775268555\n"
     ]
    }
   ],
   "source": [
    "# now we'll try multiple batches\n",
    "device = get_best_available_torch_device()\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "# Karpathy: \"AdamW is basically a bugfix of Adam\"\n",
    "#\n",
    "# note: pretty good default learning rate for early experimentation\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# jesus christ it really does crush the loss:\n",
    "#\n",
    "# step 49, loss: 0.002793300896883011\n",
    "#\n",
    "# ah now that we're loading more of the dataset can see loss around 6.5\n",
    "#\n",
    "# step 49, loss: 6.906085968017578\n",
    "#\n",
    "# we get some big gains right away because there's some tokens that never occur in our set\n",
    "#\n",
    "# after proper initialization\n",
    "#\n",
    "# step 49, loss: 6.7791547775268555\n",
    "#\n",
    "for i in range(50):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # here's where we actually move to GPU\n",
    "    x, y = train_loader.next_batch()\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"step {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15808574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some outputs to get an idea of where we are\n",
    "\n",
    "from gpt_from_scratch import tokenizer_utils\n",
    "\n",
    "def sample_model(\n",
    "    prompt: str,\n",
    "    num_samples: int,\n",
    "    max_tokens: int,\n",
    "    model: nn.Module,\n",
    "    tokenizer: tokenizer_utils.Tokenizer,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (5, 8)\n",
    "\n",
    "    # tokens in this case is just the prompt, and is small enough to fit on GPU\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    while x.size(1) < max_tokens:\n",
    "\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits, loss = model(x) # (B, T, vocab_size)\n",
    "\n",
    "            # take the logits at the last position\n",
    "            # throw away all the logits from things other than the last position\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            #\n",
    "            # \"anything lower than the 50th, we clamp to 0 and never sample it\"\n",
    "            #\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "    # print the generated text\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        tokens = x[i, :max_tokens].tolist()\n",
    "\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        \n",
    "        print(f\"\\n [{i}] >\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d57de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_model(\n",
    "    prompt=\"Romeo\",\n",
    "    num_samples=5,\n",
    "    max_tokens=30,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e982fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3280e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a526be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load tinystories for comparison\n",
    "#\n",
    "# note: `datasets` can list datasets but is deprecated\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caad147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/docs/huggingface_hub/en/guides/download#from-latest-version\n",
    "import dataclasses\n",
    "from typing import Callable\n",
    "import pathlib\n",
    "\n",
    "\n",
    "@dataclasses.dataclass(frozen=True)\n",
    "class TrainAndVal[T]:\n",
    "    \"\"\"Helper for common pattern of transforming both train and val.\"\"\"\n",
    "\n",
    "    train: T\n",
    "    val: T\n",
    "\n",
    "    def apply[R](self, func: Callable[[T], R]) -> 'TrainAndVal[R]':\n",
    "        return dataclasses.replace(self,\n",
    "            train=func(self.train),\n",
    "            val=func(self.val),\n",
    "        )\n",
    "\n",
    "def download_file_from_tinystories(filename: str) -> pathlib.Path:\n",
    "\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    filepath = huggingface_hub.hf_hub_download(\n",
    "        repo_id='roneneldan/TinyStories',\n",
    "        filename=filename,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "\n",
    "    print(f\"Downloaded {filename} to {filepath}\")\n",
    "    return pathlib.Path(filepath)\n",
    "\n",
    "# original in paper\n",
    "# train_filename, val_filename = 'TinyStories-train.txt', 'TinyStories-valid.txt'\n",
    "\n",
    "# GPT-4 only, significantly larger but newer\n",
    "filenames = TrainAndVal('TinyStoriesV2-GPT4-train.txt', 'TinyStoriesV2-GPT4-valid.txt')\n",
    "\n",
    "# download\n",
    "filepaths = filenames.apply(download_file_from_tinystories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122227d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines | words | byte count\n",
    "!echo \"TinyShakespeare\"\n",
    "!wc download_cache/4acd659e47adc1daeb7aff503accf0a3\n",
    "\n",
    "!echo \"TinyStories\"\n",
    "!wc /Users/bronsonschoen/.cache/huggingface/hub/datasets--roneneldan--TinyStories/snapshots/f54c09fd23315a6f9c86f9dc80f725de7d8f9c64/TinyStoriesV2-GPT4-train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass(frozen=True)\n",
    "class WordCount:\n",
    "    lines: int\n",
    "    words: int\n",
    "    bytes: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_tinyshakespeare = WordCount(lines=40000, words=202651, bytes=1115394)\n",
    "wc_tinystories = WordCount(lines=15600056, words=439223236, bytes=2227753162)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9eb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in dataclasses.fields(WordCount):\n",
    "    \n",
    "    field_tinyshakespeare = getattr(wc_tinyshakespeare, field.name)\n",
    "    field_tinystories = getattr(wc_tinystories, field.name)\n",
    "\n",
    "    ratio = float(field_tinystories) / float(field_tinyshakespeare)\n",
    "\n",
    "    print(f' - {field.name}: {round(ratio, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34af86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
