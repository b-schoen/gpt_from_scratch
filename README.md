GPT from scratch via https://github.com/karpathy/nn-zero-to-hero

Includes:
 * jaxtyping annotations for all functions / variables
 * uses `"mps"`

## Lecture 7: Let's build GPT: from scratch, in code, spelled out.

[Colab notebook](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=h5hjCcLDr2WC)

### Code:  `gpt_from_scratch/bigram_with_self_attention.py`

After running locally:

```python
Final loss:
Step 10000, Train Loss: 1.9295, Val Loss: 2.0263
Sampling at 10000:
---
Was caure come jiest
So-Vithith will doth courds, any the ur roant, this acuty-don
As and may no hom shall is not their it in an for hexh the'er what like say'll, the is so-me ored one amans for more 'tales would that thou dewith yeat that of theunk unto eming in my to Haty shall be your
Ans, and thers. Gher,
You her it to have him many you thou and ongs ast.

RANTIO:
Shall! hims.
Which make son bowes wards
as come, so my love for graebherrann tref.

QUEETY DET:
My live,
You that, a plice.
To no
---
```

### Colab: `colab/bigram_with_self_attention.py`

Same as above but run with much larger params:

```python
```
  
## Lecture 8: Let's build the GPT Tokenizer

[Ref Colab Notebook](https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing#scrollTo=pkAPaUCXOhvW)






