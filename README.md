GPT from scratch via https://github.com/karpathy/nn-zero-to-hero

* Lecture 7: Let's build GPT: from scratch, in code, spelled out.
  * Code:  gpt_from_scratch/bigram_with_self_attention.py
  * Colab: colab/bigram_with_self_attention.py
* Lecture 8: Let's build the GPT Tokenizer

Colab notebook: https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=h5hjCcLDr2WC

Includes:
 * jaxtyping annotations for all functions / variables
 * uses `"mps"`


